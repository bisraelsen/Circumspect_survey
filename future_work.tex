% The body of research that is encapsulated by assurances is very large. As such, this document can barely scratch the surface. Having said that the concepts that have been discussed lead to many interesting lines of research that can be pursued in future work. Here we highlight some of those.
%
% \begin{description}
    % \item [Artificial Intelligence/Machine Learning:] There is a need to interpret how and why theoretical AIA models function, in order to know they are being applied correctly and to design new approaches to overcome weaknesses in the existing methods~\cite{Garcia2015-rs,Otte2013-oo}.
    % \item [Interpretable Science:] Scientists need to be able to trust the models created using data analysis, and be able to draw insights from them. Scientific discoveries cannot depend on methods that are not understood~\cite{Faghmous2014-og}.
    % \item [Reliable Machine Learning:] It is critical to have safety guarantees for AIAs that have been deployed in the real world. Failing to do so can result in serious accidents that cause loss of life, or significant damage~\cite{Sugiyama2013-ci,Amodei2016-xi}.
    % \item [Public Policy:] Governments are beginning to enforce regulations on the interpretability of certain algorithms in order to ensure that citizens can understand why AIA driven services make the decisions and predictions that they do. A specific example are the algorithms deployed by credit agencies to approve/reject loans~\cite{Wagner2016-ck}.
    % \item [Medicine:] Medical professionals need to understand why data-driven models give predictions so that they can choose whether or not to follow the recommendations. While AIAs can be a very useful tool, ultimately doctors are liable for the decisions they make and treatments they administer~\cite{Jovanovic2016-gw}.
    % \item [Cognitive Assistance:] Systems are being designed as aids for humans to make complex decisions, e.g. searching and assimilating information from databases of legal proceedings. When an AIA presents perspectives and conclusions as data summaries, it must be able to also present justifying evidence and logic~\cite{Gutfreund2016-xe}.
% \end{description}

\brettcomm{
\begin{itemize}
    \item How to know if assurances are effective. Are explicit assurances being perceived as desired? Are implicit assurances overpowering?
    \item component and composite assurances. What happens when assurances are combined vs. when they exist in isolation? How should assurances be combined?
    \item methods, and modes of expression. What human limitations must be considered
    \item from the perspective of AIAs are there assurances that exist for each of the capabilities that can communicate to the different trust targets?
    \item currently assurances are typically `displayed' as static information to a user, can a user be taught over time by planning assurances?
    \item what cognitive effects inhib assurances
    \item how to select the expression of an assurance? Easy to calculate hard to communicate\ldots
    \item measuring effects of assurances
    \item distrust
    \item two-way trust
\end{itemize}
}
This field is nascent, and there are many opportunities for further research along different lines. This section outlines some possible directions for future work that are promising.

\subsection{Properties of Assurances}
Probably the most straight forward approach could be to draw some guidance from Figure~\ref{fig:assurance_classification}, which shows how to classify an assurance. In this survey we investigate, in some detail, `Level of Integration'. However all of the other grayed-out boxes in that figure have open questions that can still be investigated. 
\input{ass_st.tex}
\input{ass_cc.tex}
\input{ass_ei.tex}
\input{ass_tt.tex}

\subsection{Trust vs. Distrust}
The treatment of assurances in this survey is based, in part, on a model of interpersonal trust. For completeness it will be important to further investigate \textit{distrust}, as reviewed and discussed by \citet{Lewicki1998-ox}, and formalized in \citet{McKnight2001-gz}. Low trust is not the same as distrust, and low distrust is not the same as trust. \citet{McKnight2001-gz} suggest that `the emotional intensity of distrust distinguishes it from trust', and they explain that distrust comes from emotions like: wariness, caution, and fear. Whereas, trust stems from emotions like: hope, safety, and confidence. Trust and distrust are orthogonal elements that define a person's TRB towards a trustee. In this survey, distrust was not considered. However any \emph{complete} treatment of trust relationships, and for our purposes, designed assurances, must consider the dimensions of distrust as well as those of trust. Based on the emotions that drive distrust it will be important to consider in higher-risk human-AIA relationships.

It is not clear to what extent the human-AIA trust model remains effective in the presence of user wariness, caution, or fear. To what extent can behaviors driven by distrust be isolated from those originating from trust? How can those behaviors be detected to begin with? In what circumstances is the extra effort necessary?

\subsection{Human Limitations}
Dealing with human users implies addressing their limitations as well. In \cite{Freedy2007-sg,Riley1996-qm} found evidence of `framing effects' influencing operator behavior. While, this isn't surprising to those familiar with cognitive science, it will likely be surprising to many who are trying to implement assurances. Other cognitive biases and limitations such as `recency effects' (being biased based on recent experience), `focusing effects' (being biased based on a single aspect of an event), or `normalcy biases' (refusal to consider situations which have never occurred before) are also important to consider. 

Besides cognitive biases, humans are also limited in their ability to understand certain kinds of information. Communities that investigate how probabilistic and statistical explanations can be presented to humans will be useful in addressing this question \cite{Rouse1986-dz,Wallace2001-fm,Kuhn1997-qc,Lomas2012-ie,Swartout1983-ko}. It is not immediately clear what methods are most appropriate for application in assurance design, or how they might be applied. Can the AIA detect when cognitive limitations are effecting TRBs? What other limitations of relationships with humans and AIAs need to be characterized? Surely there are equally important limitations identified in psychology, sociology, economics, and others.

\input{perception_mediums.tex}
\input{obs_effects.tex}
