\subsection{Explicit and Implicit Assurances}
As has already been presented, assurances can be either be \emph{explicit} or \emph{implicit}. This is something that \cite{Sheridan1984-kx} briefly alluded to in their discussion of the nature of how humans behave when working with automated systems. They suggested that the operator's perception of the automated system can be effected by `performance' and its `reports on its own performance'. This is an unnamed trend in the existing literature as well, as has been highlighted by the survey sections of this paper. Here the terms are more formally defined as (with some illustrative examples),

\begin{description}
    \item [Explicit:] Assurances that are purposefully given to affect the trust of a user.
    \begin{itemize}
        \item Legible motion \cite{Dragan2013-wd}, which is motion calculated with the intent of being more understandable by a human
        \item Self-confidence \cite{Aitken2016-fb}, a set of designed metrics meant to affect the user's trust
    \end{itemize}
    \item [Implicit:] All other assurances that aren't explicit.
    \begin{itemize}
        \item Reliability in completing a task. Generally, the object of success is not to affect the user's trust (although this is a nice side-effect).
        \item The way an autonomous vehicle appears. For example something that looks neat will have a different effect on trust, than an AIA with wires dragging on the ground. 
    \end{itemize}
\end{description}

It is important for designers of AIAs to be aware of both kinds of assurances. Anything that is consciously designed with the goal of affecting trust is automatically an explicit assurance, from the perspective of the AIA. Another way of stating the idea is that trust relationships between humans and AIAs will form, but all assurances will be implicit if designers do not consciously consider the trust relationship. 

Implicit assurances were the first that were formally researched in literature regarding human-AIA trust relationships. The research suggests that they are effective, and thus should be considered, however the real aim in designing for human-AIA trust relationships is to move toward the design of explicit assurances. Explicit does not mean that the user will always be aware of the communication (like in \cite{Dragan2013-wd}), but that there is a specifically designed method that has been shown to affect trust.
