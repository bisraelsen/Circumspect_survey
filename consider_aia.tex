\subsection{Considering the AIA in Designing Assurances} \label{sec:consider_AIA}
\subsubsection{Calculating Assurances}
    In order to give an assurance an AIA must first be able to create one, or more specifically the AIA must be able to calculate the information that needs to be conveyed, this is a point seen when reading \cite{Kaniarasu2013-ho,Chen2014-dk}. This is currently a limitation of many AIAs that do not possess the ability to calculate assurances for their respective capabilities. AIAs must have the ability to be introspective (perform analysis on \emph{its own} functions, capabilities, and models, to compute assurances), as well as extrospective (perform analysis on the function, capabilities, and  models of \emph{external entities} to compute assurances). An AIA that possesses both together can be referred to as circumspective. Once circumspective analysis has occurred then the AIA must be able to express the assurances. This is a challenge that needs to be addressed directly, and the surveyed work (and other similar work) will help guide the development of those capabilities.

    To date several different promising methods have been used to calculate assurances in different applications. These include modifying the objective to consider a user's trust (i.e. legible motion \cite{Dragan2013-wd}, and safe learning), calculating intended actions, methods operating on POMDPs have also been developed in an attempt to quantify how they perform. Researchers from quadrant III and IV have developed many other promising approaches that involve predicting performance, making models more interpretable by simplifying them and/or adding structure, reducing the dimensionality, explaining the reasoning of decision making AIAs, and checking models, guaranteeing performance through V\&V, learning safely, dealing with non-stationary training/test data, and learning better features. Many of these methods are ready to be used in more formal human-AIA trust studies to verify their utility in this application.

    It is important to recognize the fact that AIAs may possess several different capabilities, and that each may be more or less trustworthy. This suggests that there must be several assurances created in order to assure the user with respect to each of the different capabilities. As discussed by \cite{Chen2014-dk} assurances from different capabilities may be more or less important depending on the situation (i.e. sometimes the user may need to understand why a decision was made, but not at other times).

\subsubsection{Expressing Assurances}
    An AIA must not only be able to calculate an assurance, but express it to the user as well. These expressions must be tailored to human strengths and weaknesses, some of which are mentioned in section \ref{sec:consider_human}. To state the point more bluntly, if an assurance is not expressed, or not perceived by the user, it is useless and has no effect. The expression of assurances to a human user is not a trivial topic and has been investigated mainly by those in quadrants I, and II by basic visualizations and natural language communication (and promising approaches like high dimensional visualization were mentioned in quadrants III and IV). There is, however, a body of research (not surveyed here) that considers how to communicate information to humans (i.e. as probabilities, or fractions, by text, or by plotting, etcetera). It is probable that marketing, and cognitive science have much to contribute to this topic. This research is critical in designing how to efficiently express assurances in a way that they will be perceived correctly by the user with the least possible information loss.

\subsubsection{The Imprecise Nature of Assurances}
    Due to the nature of trust (and humans in general), a single assurance might be targeted at influencing the competence dimension of trust, but it may also have effects on other dimensions. As an example an assurance that targets predictability may also have an affect on the probability of depending.

    Besides being difficult to separate effects on a single user, individual users are different as well. Thus no assurance will have an identical effect when given to two separate users. This makes it difficult to have precise effects on user trust behaviors.

    One might attempt to mitigate this uncertainty by using expressions that are more precise than others, such as displaying a probability distribution rather than on a maximum likelihood. This gets into some considerations about how the presentation of information affects the ability of a human to understand.
