\subsection{Value Alignment} \label{sec:value_alignment}
AIAs operate autonomously in delegated tasks, with the expectation that they behave according to users' intent frames. 
Optimization-based algorithms are arguably among the most common and direct approaches for accomplishing this. 
The general idea is to define a \emph{utility function} that normatively governs the AIA's abilities so that desirable behaviors are elicited through maximization of the utility, i.e. such that the AIA behaves rationally in accordance with the user's intent frame. 
A utility function describes the `long-term desirability' of taking certain actions in certain conditions, i.e. beyond immediate benefits or penalties, and should coherently reflect user preferences about the state of the world and AIA behaviors ~\cite{Russell2010-wv}. 
Such mapping of user intent frames to utility functions has two positive benefits. 
Firstly, it ensures that AIA behaviors can themselves be used as assurances: users will tend to trust AIA's more if they are `well-behaved' and acting in accordance with their desired intent than if they are not. 
Secondly, an AIA can generate assurances via auxiliary behaviors that help ensure its utility function is aligned with the user's intent frame.  Since it is practically quite challenging to encode user preferences and intent frames into utility functions, the process of \emph{value alignment}\footnote{Value alignment is more commonly known as `AI Alignment' in AI research~\cite{Yudkowsky2001-hb,Bensinger2014-ul}.} leads to many different algorithmic strategies for generating assurances. 

Consider a generic decision-making problem where an AIA that must make a choice $a \in {\cal A}$ given some task state $s \in {\cal S}$, with scalar utility function $U_A(a,s)$ and possible sets of actions and states ${\cal A}$ and ${\cal S}$. If a user's true utility is represented by scalar function $U_H(a,s)$, then in the ideal situation the AIA seeks the optimal decision $a^* \in {\cal A}$ such that, for any $s \in {\cal S}$,
\begin{align*}
    a* = \arg\max_{\cal A} U_A(a,s) = \arg \max_{\cal A} U_H(a,s). 
\end{align*}
Hence, value alignment tries to minimize the difference between the utilities of the AIA and the user. When the utility of the robot $U_A(a,s)$ and the human $U_H(a,s)$ are approximately equivalent (within some tolerance) then the values of the AIA are \emph{aligned} with those of the human. An AIA with aligned values will be considered by users to be more predictable (and thus more competent), because the AIA will be more likely to act in desirable ways. 
% 
~\citet{Bostrom2014-fz} provides a well-known example of an AIA whose value is \emph{not} aligned: an autonomous robot is designed, and deployed, with the intent that it make paper clips. To maximize $U_A(s,a)$, the robot then decides to take over the world in order to maximize its resources and ability to make more paper clips. To reasonable human users, this is clearly \enph{not} the intended behavior; the utilities that the robot used for making decisions did not match those that the human must have had. Therefore the robot's resulting behavior is intrinsically an assurance that reduces trust. On the other hand, if the robot were to try to learn from its mistakes and improve (i.e. make $U_A(s,a)$ closer to $U_h(s,a)$) that could be perceived as an assurance that increases trust -- the robot can be `forgiven' for making honest mistakes in trying to optimize an ill-posed/under-specified utility function, as long as it is able to recognize and remedy this. 

\subsubsection{Common Approaches:}
There are two algorithmic strategies for value alignment: (i) indirect: approximate $U_H(a,s)$ explicitly via $U_A(a,s)$, and then use this approximation to find $a^*$; (ii) direct: identify $a^* = \arg \max U_H(a,s)$ directly via the use of optimal state-action value functions $Q^*(s,a)$ (which give the utility to be gained if the AIA were to proceed optimally starting from $s$, regardless of its past states or actions). 
These strategies closely resemble techniques used for reinforcement learning problems and their variants (especially inverse reinforcement learning); not surprisingly, most value alignment techniques are rooted in this domain. 
%%Examples of commonly used utility functions for decision making under uncertainty (e.g. with POMDPs) include expected discounted cumulative rewards, which approximate diminishing marginal returns for sequential decision making problems.
Value alignment research tends to focus on several different issues \cite{Gordon_Worley2018-xy,Amodei2016-xi}; some of the more directly applicable topics and associated methods that point to useful assurance strategies are described below. 
%
%reward hacking (how to prevent the AIA from taking advantage of imperfections in the specification of $U_A(s,a)$?),
%safe exploration (how can $U_h(s,a)$ be safely approximated online when certain combinations of $(s,a)$ may lead to irreversibly bad consequences?); 
%robustness to context shifts (how can the AIA determine when the basis and provenance of its approximation to $U_H(s,a)$ or $Q^*(s,a)$ is no longer valid?); 
%and design/learning/eliciation of appropriate utility functions \cite{Hadfield-Menell2016-ws,Da_Veiga2012-gh,Garcia2015-rs}. 
The solutions to these problems are assurances because they afford opportunities for  users to better understand the actual intentions and goals of the AIA, as well as understand how the AIA actually interprets intent frames. 

%%%\brettcomm{How is user intent addressed by solutions to reward-hacking, safety, and nonstationarity?}

\paragraph{Reward Hacking and Human-Guided Learning}
The reward hacking problem deals with avoiding and removing unintended consequences in AIA behaviors that arise from imperfections in the specification of $U_A(s,a)$ (as in the paper clip-making robot example above). 
The most popular solution strategies use some form of offline supervisory human guidance or training data feedback in the utility function learning process. 
This approach recognizes the intrinsic difficulty of mapping user preferences to a single scalar utility $U_H(s,a)$ for complex tasks, and leverages sophisticated machine learning and reasoning strategies to identify relevant preferences within $U_A(s,a)$ or $Q^*(s,a)$, depending on the kinds of tasks considered. For instance, one-shot/non-sequential decision-making tasks like image recognition or object perception do not necessarily have dynamical state considerations, but may require potential expansion of the action space for sensible labeling of new object categories. %\nisarcomm{to be revised as per comments below}
%\nisarcomm{This doesn't read like a complete thought...not sure what this paragraph is even saying.}Even humans tend to reward hack/game, so the problem is far from solved. However, researchers have made use of practical approaches to attempt to align the values of AIAs with those of human users. One approach is to involve a human in the actual learning process. By doing so the human is able to, in essence, encode a, possibly complex, objective function into the AIA.

In the context of sequential decision making problems, \citet{Hadfield-Menell2017-tl}, \citet{Hadfield-Menell2016-ws}, and \item \citet{Huang2017-lk} all consider variations of the `inverse reward design' problem using inverse reinforcement learning techniques. In these works, discounted cumulative rewards are used to model utility functions $U_A(s,a)$ and $U_H(s,a)$, where the actual reward factors contributing to $U_H(s,a)$ are unknown but can be inferred from user-generated contextual information at design time. Specifically, \cite{Hadfield-Menell2017-tl} notes that reward factors provided by users in limited training contexts serve as `noisy evidence of intent'. Hence, to avoid situations where an AIA trainee demonstrates desirable behaviors in specific training scenarios but later demonstrates undesirable behaviors in novel scenarios, the AIA must be able to reason over the uncertainty in the user's intent in order to fully capture the context in which it was trained. 
In a different task setting, \citet{Freitas2006-qo} compared two approaches to discovering `interesting' knowledge from large data sets, based on the idea that human users require assistance from complex systems in order to find useful patterns and other interesting insights. He mentions `user-driven' methods that involve a user suggesting interesting templates or providing general impressions in the form of IF-THEN rules. A subsequent comparison to different `data-driven' methods suggests that the latter are not very effective in practice. 
Having said that, user-driven approaches may not fare any better when compared over many users, as each user will likely have different preferences. Other scaled up user-driven approaches, e.g. based on crowd-sourcing~\citet{Chang2017-kl}, can also achieve better accuracy for labeling tasks while also exploring new or ambiguous classes that can be ignored with traditional approaches (especially if training data sets are biased or very limited). \citet{Chang2017-kl} also consider a similar, scaled up, `user-driven' approach called `Revolt' that crowd-sources the labeling of images. It is able to attain high accuracy labeling, while also exploring new or ambiguous classes that can be ignored with traditional approaches. 

Some other methods for designing, learning and eliciting appropriate utility functions are also discussed in \cite{Hadfield-Menell2016-ws,Da_Veiga2012-gh,Garcia2015-rs}.
Despite the differences in AIA application contexts, these methods all provide the user with better context for what \emph{should} be known by system, and for how well it can interpolate/extrapolate. 
These processes allow users to refine their own intent in complex settings, e.g. to reveal or resolve subtle low-level inconsistencies in desired task requirements that would otherwise lead a rational AIA to undesirable behaviors. 

\paragraph{Safe learning and correct-by-construction synthesis:}
In many applications, $U_h(s,a)$ must be safely approximated when certain combinations of $(s,a)$ lead to irreversibly bad consequences. 
Hence, as AIAs try to learn what a user's utility is, they must do so in a safe manner. 
For instance, humans do not learn about the dangers of heights from falling off of skyscrapers. 
Instead we have to do so cautiously over time, and extrapolate from much less drastic experience (i.e. tripping on a curb). 
Safe reinforcement learning (safe RL) methods offer formal strategies and assurances for AIAs to learn in similar ways.
Safe RL has been defined as the process of avoiding ``unintended and harmful behavior that [emerges] from machine learning systems''~\cite{Amodei2016-xi}. Two ways to approach safe RL are: (i) modification of the optimality criterion with a safety factor, and (ii) modification of the exploration process through the incorporation of external knowledge~\cite{Garcia2015-rs}. 

For example, \citet{Lipton2016-dq} design an `intrinsic fear' RL approach that uses a deep Q-network and a `supervised danger model'. The danger model stores the likelihood of entering a catastrophe state within a `short number of steps'. This model can be learned by detecting catastrophes through experience and can be improved over time. \citet{Curran2016-ij}, in a more specific application, asks how a robot can learn when a task is too risky, and then avoid those situations, or ask for help. 
Similarly, \citet{Kahn2017-vy} use Bayesian Deep Neural Nets (using bootstrapping and dropout) to learn about the probability (with uncertainty) of an autonomous vehicle colliding in an environment given its current state, observations, and sequence of controls. Using this model they formulate a `velocity-dependent collision cost' that is used for model-based reinforcement learning. With this approach the vehicle naturally proceeds slowly when there is an elevated risk of collision. This `safety-aware' behavior provides an assurance signal to the user. 
%o this, they use a hierarchical POMDP planner that explicitly represents failures as additional state-action pairs.

Aside from purely learning-based approaches, we can also consider Validation and Verification (V\&V) methods. 
Not all practitioners are aware that V\&V techniques can generate soft assurances for users. 
This is because V\&V typically refers to the use of formal methods to guarantee the behavior of a system within some set of specifications, which are handed down by a certification authority as requirements to system designers to generate `hard assurances' (formal proofs of the functionality of the system). 
Although these `hard assurances' are not primarily designed for user consumption, they could in principle be exposed to and interpreted for users in certain contexts. 
A prime example is given by \citet{Raman2013-mz}, who developed a formal way for non-expert users to provide structured natural language task specifications to a robot, such that a `correct-by-construction' controller will be built if the specification is valid. 
Otherwise, the robot will provide an explanation about which specification(s) are unrealizable/inconsistent and will cause failure. 
In the context of a practical self-driving car application, \citet{Ghosh2016-dl} presents a framework called Trusted Machine Learning (TML) for learning models from dynamically generated data that fit pre-determined `trustworthiness' constraints. 
%%Otherwise, the robot will provide an explanation about which specification(s) are unrealizable/inconsistent and will cause failure.  This approach is studied with the goal of implementing the system on a robot assistant in a hospital. Their method involves parsing natural language input (``Go to the kitchen''), and converting that into a linear temporal logic (LTL) task specification. This is then used to synthesize a robot controller if possible; otherwise, the `unrealizable' specifications are communicated back to the user. \nisarcomm{This is far too much detail}
These approaches are promising in that they not only present a way to communicate when and why specified tasks cannot be performed or certain actions cannot be taken, but also provide positive assurances in the form of guaranteed formally verified AIA processes for performing desired tasks (plans, models, etc.). 
While this directly addresses the competence and predictability components of AIA trust, the `raw' expression of these assurances does not formally account for effects on user trust or TRBs in formulating explanations. %%%These assurances are also asymmetrically limited to cases where the robot cannot meet the specifications. \nisarcomm{not true: positive assurance also comes in the form of actual formally verified plan when task achievable}
%
%In the context of a practical self-driving car application, \citet{Ghosh2016-dl} presents a framework called Trusted Machine Learning (TML) for making ML models fit pre-determined `trustworthiness' constraints. 
%%Unlike most other kinds of assurances considered so far, these constraints are handed down by a certification authority as requirements to the ML system designer, and are not meant to be exposed to/determined by users. However, in principle, such hard assurances could be exposed to users in certain contexts. The key idea is to utilize tools from formal methods to provide theoretical proof of the functionality of the system. Note that, in the formal methods literature, such proofs and their supporting evidence are what are referred to as `assurances'; we earlier denoted these as `hard assurances', in contrast to the other kinds of `soft assurances' considered here. \nisarcomm{this and prev paragraph are a bit redundant when it comes to articulating how formal VV can provide soft assurances...tighten up/combine...}

\paragraph{Robustness to context shifts}
How can an AIA determine when the basis and provenance of its approximation to $U_H(s,a)$ or $Q^*(s,a)$ is no longer valid for a particular task? 
%%Autonomous systems and their models are dependent on the data they learned from. %(in defense of AIAs, so are humans). 
This problem has attracted much recent attention in the learning literature under the guise of `nonstationary' learning. 
Nonstationarity refers to the complex challenge of training a model based on data from one distribution $D$ while taking into account that the test distribution $D^\prime$ will likely shift through time~\cite{Quinonero-Candela2009-fj}. 
For instance, in the context of classification problems, \citet{Sugiyama2013-ci} propose using importance sampling Monte Carlo to formally detect events related to `covariate shift' (training and test input data follow different distributions) and `class-balance change' (where the class-prior probabilities are different in training and test phases, but where there is no covariate shift). 
Similarly, \citet{Charikar2017-kr} address learning from `untrusted' data, which could be subject adversarial attack or unknown nonstationarity. 
%A key approach to addressing both problems is to use `importance sampling' which involves weighting the training loss according to the ratio between $p^\prime(x)$ and $p(x)$; this ratio is referred to as the `importance'. \citet{Charikar2017-kr} address the problem of learning from `untrusted' data; this data could be untrusted due to adversarial attack or nonstationarity.

These methods can be more generally adapted and developed beyond learning tasks, in order to evaluate the sensitivity of as-designed AIA's capabilities to possible changes in task context not captured/considered at design time (an example of coping with `unknown unknowns').  %\nisarcomm{somehow need to point out that unlike AIA self-assessment methods, these methods are integral fxns and not supplementary...perhaps mention later?}. 
If the sensitivities imply a significant deviation in $U_A(s,a)$ or $a^*$ from expected values (i.e. from user intent frame as initially understood), or indicate the presence of new $(s,a)$ pairs that are not accounted for by $U(s,a)$ (e.g. test data that is very far from the training set), then the AIA can inform the user accordingly and thus possibly opt out of performing tasks that are now potentially `out of scope'. This provides direct low-level behavioral assurances about changes in predictability, competence, and situation normality, though these may not be immediately understood by non-expert users. %%at a higher-level %%\nisarcomm{...clean up}. 
%As such, these methods are related to introspective AIA self-assessment methods discussed later. 

%%%When an AIA can cope with nonstationary covariates and/or class-balance then it will perform more consistently, and human users will be able to more appropriately trust it.

\subsubsection{Grounding Example:}
In the case of the `VIP Escort' problem (described in Section~\ref{sec:mot_example}), value alignment might be used as an assurance in the following way, starting with the assumptions that:

\begin{itemize}
    \item The UGV has just begun an attempt to escape the road-network
    \item The UGV uses safe RL to learn its escape policy
    \item The operator is able to observe the UGV during its entire escape attempt
\end{itemize}

The operator has used several different UGVs for similar tasks. This newer model uses `safe RL' to learn its policy. When observing the UGV's attempt at escape the operator notices a difference in how the UGV operates. Whereas the older UGV models would sometimes do risky things, this UGV seems to navigate dangerous situations much better. 

\paragraph{\textbf{Discussion of Example:}} In this case, safe RL enabled the UGV to treat situations that an operator might classify as `dangerous' with more care. With this integral capability the UGV assures the operator that it is more competent.
