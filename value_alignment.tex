%%TODO: 
%%1. Rewrite math slightly, not as inline equation
%%2. Regroup exposition of this section: key points to make: 
%%(i) what is the assurance argument? value alignment is integral to behavior of AIA: by definition idea is to choose behaviors that are optimal w.r.t. user intent frame  
%%(ii) what is the mechanism for generating appropriate TRBs? if utility matches, then behavior itself is assurance; otherwise, if AIA tries to visibly align itself with user's intent frame, then efforts to that end can also be leveraged as assurances...
%%%(iii) What are the technical challenges and most common techniques for value alignment? -- Key challenge: mapping user intent frame to utility function is non-trivial.  IRL and imitation learning, reward tuning, ... each of these are typically geared towards different problems, and particular solutions offer some insight as to what aspects of trust associated assurances try to influence: basically can look at different dimensions of/approaches to shaping U_r/its argmax to match U_h/its argmax (other approaches/issues also...)
%%(a)"reward hacking": avoiding unintended consequences from being too open ended or vague about intent scope (paper clip problem) -- there is some uncertainty as to whether U_r actually fully captures U_h? So assurance here is act of obtaining some clarification about user intent scope, or act of updating utility in response to some new information that gives evidence as to intent... designer/user can't necessarily think of everything: key idea is to make sure "bases are covered" as far as being as minimally specific scope as needed, but also to balance with need to avoid overburdening the user with task of providing exact scope stipulations -- gets to delegation aspect of autonomy: ideally AIA should have some reasonable prior on scope, such that user is neither too far removed nor too closely entangled with exactly how AIA does its job... 
%%(b) "safety": similar to, but more specific than, reward hacking: less concerned about restricting AIA's scope than its actions near particular states, which might lead to dangerous or unrecoverable situations for AIA, user, or other entities; like reward hacking, this is challenging from standpoint that it might be very tedious and challenging to enumerate all possible unsafe situations, or may be very challenging to avoid being overly conservative and thus unnecessarily limiting actual usefulness of autonomy; excessive risk aversion can also hinder/slow learning, which in turn may have secondary consequences in terms of affecting trust (AIA can seem incompetent or overly cautious;) assurances stemming from techniques here can take forms of demonstrably or provably safe/conservative behaviors executed by AIA (e.g. that follow close to human tutor trials, and/or feature cautious experimentation with clearly defined `safe fallback' strategies), as well as process of retrieving safe behavior demonstrations or getting required validations from user that system is within acceptable safety margins or getting clearance from user that system can exceed/violate certain margins in certain cases;   
%%%(c) so-called `nonstationary learning', i.e. *task context shifts* (use this phrasing to be more general): concern is about making sure that AIA's assumptions and understanding of problem from previous user intent frame of reference (training data, models, instructions, etc.) correctly map to intent frame of reference for future tasks if these do not perfectly align; posed differently: problem can be stated as trying to estimate adjustments for U_r when there is a possible context shift from U_h to U_h'; this could, for instance, involve change in underlying training data distribution, or change in definitions of states or certain parts of task (gets into transfer learning)...assurances here can take form of ...


\subsection{Value Alignment} \label{sec:value_alignment}
AIAs operate under autonomy in delegated tasks, with expectation that the AIA behave according to the user's intent frame. 
Optimization-based algorithms are arguably among the most common and direct approaches for accomplishing this. 
The general idea is to define a \emph{utility function} that normatively governs the AIA's abilities so that desirable behaviors are elicited through maximization of the utility, i.e. such that the AIA behaves rationally in accordance with the user's intent frame. 
A utility function describes the `long-term desirability' of taking certain actions in certain conditions, i.e. beyond immediate benefits or penalties, and should coherently reflect user preferences about the state of the world and AIA behaviors ~\cite{Russell2010-wv}. 
Such mapping of user intent frames to utility functions has two positive benefits. 
Firstly, it ensures that AIA behaviors can themselves be used as assurances: users will tend to trust AIA's more if they are `well-behaved' and acting in accordance with their desired intent than if they are not. 
Secondly, an AIA can generate assurances via auxiliary behaviors that help ensure its utility function is aligned with the user's intent frame.  Since it is practically quite challenging to encode user preferences and intent frames into utility functions, the process of \emph{value alignment}\footnote{Value alignment is more commonly known as `AI Alignment' in AI research~\cite{Yudkowsky2001-hb,Bensinger2014-ul}.} leads to many different algorithmic strategies for generating assurances. 
%%\brettcomm{value alignment can be a drawback due to the difficulty of implementation, also humans are not always rational}

Consider a generic decision-making problem where an AIA that must make a choice $a \in {\cal A}$ given some task state $s \in {\cal S}$, with utility function $U_A(a,s)$ and possible sets of actions and states ${\cal A}$ and ${\cal S}$. If a user's true utility is represented by $U_H(a,s)$, then in the ideal situation the AIA seeks the optimal decision $a^* \in {\cal A}$ such that, for any $s \in {\cal S}$,
\begin{align*}
    a* = \arg\max_{\cal A} U_A(a,s) = \arg \max_{\cal A} U_H(a,s). 
\end{align*}
Hence, value alignment tries to minimize the difference between the utilities of the AIA and the user. When the utility of the robot $U_A(a,s)$ and the human $U_h(a,s)$ are approximately equivalent (within some tolerance) then the values of the AIA are \emph{aligned} with those of the human. An AIA with aligned values will be considered by users to be more predictable (and thus more competent), because the AIA will be more likely to act in desirable ways. 
% 
~\citet{Bostrom2014-fz} provides a well-known example of an AIA whose value is \emph{not} aligned: an autonomous robot is designed, and deployed, with the intent that it make paper clips. To maximize $U_A(s,a)$, the robot then decides to take over the world in order to maximize its resources and ability to make more paper clips. To reasonable human users, this is clearly \enph{not} the intended behavior; the utilities that the robot used for making decisions did not match those that the human must have had. Therefore the robot's resulting behavior is intrinsically an assurance that reduces trust. On the other hand, if the robot were to try to learn from its mistakes and improve (i.e. make $U_A(s,a)$ closer to $U_h(s,a)$) that could be perceived as an assurance that increases trust -- the robot can be `forgiven' for making honest mistakes in trying to optimize an ill-posed/under-specified utility function, as long as it is able to recognize and remedy this. 

\subsubsection{Common Approaches:}
There are two general algorithmic strategies for value alignment: (i) indirect: approximate $U_H(a,s)$ explicitly via $U_A(a,s)$, and then use this approximation to find $a^*$; (ii) direct: identify $a^* = \arg \max U_H(a,s)$ directly via the use of optimal state-action value functions $Q^*(s,a)$ (which give the utility to be gained if the AIA were to proceed optimally starting from $s$, regardless of its past states or actions). 
These strategies closely resemble techniques used for reinforcement learning problems and their variants (especially inverse reinforcement learning), and thus (not surprisingly) most value alignment techniques are rooted in this domain. 
%%Examples of commonly used utility functions for decision making under uncertainty (e.g. with POMDPs) include expected discounted cumulative rewards, which approximate diminishing marginal returns for sequential decision making problems.
Value alignment research is concerned with several different problems \cite{Gordon_Worley2018-xy,Amodei2016-xi}; among the more directly applicable topics and methods that point to useful assurance strategies are: reward hacking (how to prevent the AIA from taking advantage of imperfections in the specification of $U_A(s,a)$?), safe exploration (how can $U_h(s,a)$ be safely approximated online when certain combinations of $(s,a)$ may lead to irreversibly bad consequences?); robustness to context shifts (how can the AIA determine when the basis and provenance of its approximation to $U_H(s,a)$ or $Q^*(s,a)$ is no longer valid?); and design/learning of appropriate utility functions \cite{Hadfield-Menell2016-ws,Da_Veiga2012-gh,Garcia2015-rs}. The solutions to these problems are assurances because they afford opportunities for  users to better understand the actual intentions and goals of the AIA, as well as understand how the AIA actually interprets their intent frames. 

%%%\brettcomm{How is user intent addressed by solutions to reward-hacking, safety, and nonstationarity?}

\paragraph{Reward Hacking and Human-Guided Learning}
Even humans tend to reward hack/game, so the problem is far from solved. However, researchers have made use of practical approaches to attempt to align the values of AIAs with those of human users. One approach is to involve a human in the actual learning process. By doing so the human is able to, in essence, encode a, possibly complex, objective function into the AIA.

For instance, \citet{Freitas2006-qo} compared two approaches to discovering `interesting' knowledge from large data sets, based on the idea that human users require assistance from complex systems in order to find useful patterns and other interesting insights. He mentions `user-driven' methods that involve a user suggesting interesting templates or providing general impressions in the form of IF-THEN rules. These are then compared to different `data-driven' methods, using other research to suggest that data-driven approaches are not very effective in practice.

Having said that, user-driven approaches may not fare any better when compared over many users, as each user will likely have different preferences. Other scaled up user-driven approaches, e.g. based on crowd-sourcing~\citet{Chang2017-kl}, can also achieve better accuracy for labeling tasks while also exploring new or ambiguous classes that can be ignored with traditional approaches (especially if training data sets are biased or very limited). \citet{Chang2017-kl} also consider a similar, scaled up, `user-driven' approach called `Revolt' that crowd-sources the labeling of images. It is able to attain high accuracy labeling, while also exploring new or ambiguous classes that can be ignored with traditional approaches.

\paragraph{Safety:}
As AIAs try to learn what a user's utility is, they must do so in a safe manner. For instance, humans do not learn about the dangers of heights from falling off of skyscrapers. Instead we have to do so cautiously over time, and extrapolate from much less drastic experience (i.e. tripping on a curb). Safe reinforcement learning (safe RL) directly considers learning in such environments. It has been defined as the process of avoiding ``unintended and harmful behavior that [emerges] from machine learning systems''~\cite{Amodei2016-xi}. Two ways to approach safe RL are: 1) modification of the optimality criterion with a safety factor, and 2) modification of the exploration process through the incorporation of external knowledge~\cite{Garcia2015-rs}.

As one example, \citet{Lipton2016-dq} design an `intrinsic fear' RL approach that uses a deep Q-network (DQN) and a `supervised danger model'. The danger model stores the likelihood of entering a catastrophe state within a `short number of steps'. This model can be learned by detecting catastrophes through experience and can be improved over time. \citet{Curran2016-ij}, in a more specific application, asks how a robot can learn when a task is too risky, and then avoid those situations, or ask for help.

\citet{Kahn2017-vy} use Bayesian DNNs (using bootstrapping and dropout) to learn about the probability (with uncertainty) of an autonomous vehicle colliding in an environment given its current state, observations, and sequence of controls. Using this model they formulate a `velocity-dependent collision cost' that is used for model-based reinforcement learning. With this approach the vehicle naturally proceeds slowly when there is an elevated risk of collision. o this, they use a hierarchical POMDP planner that explicitly represents failures as additional state-action pairs.

Finally, moving on from ML-oriented approaches, one can also consider approaches rooted in Validation and Verification (V\&V). V\&V typically refers to the use of formal methods to guarantee the performance of a system within some set of specifications. However, not all practitioners are aware that V\&V provides ways to assure users. A prime example is given by \citet{Raman2013-mz}, who developed a way by which a user can provide natural language specifications to a robot and a `correct-by-construction' controller will be built if the specification is valid. Otherwise, the robot will provide an explanation about which specification(s) will cause failure. This approach is studied with the goal of implementing the system on a robot assistant in a hospital. Their method involves parsing natural language input (``Go to the kitchen''), and converting that into a linear temporal logic (LTL) task specification. This is then used to synthesize a robot controller if possible; otherwise, the `unrealizable' specifications are communicated back to the user. This approach is promising in that it presents a way to communicate that a specification cannot be met. But, it does not formally account for effects on user trust or TRBs in formulating explanations. The expression of assurances is also asymmetrically limited to cases where the robot cannot meet the specifications. 

In the context of a practical self-driving car application, \citet{Ghosh2016-dl} presents a framework called Trusted Machine Learning (TML) for making ML models fit pre-determined `trustworthiness' constraints. Unlike most other kinds of assurances considered so far, these constraints are handed down by a certification authority as requirements to the ML system designer, and are not meant to be exposed to/determined by users. However, in principle, such hard assurances could be exposed to users in certain contexts. The key idea is to utilize tools from formal methods to provide theoretical proof of the functionality of the system. Note that, in the formal methods literature, such proofs and their supporting evidence are what are referred to as `assurances'; we earlier denoted these as `hard assurances', in contrast to the other kinds of `soft assurances' considered here. 

\paragraph{Learning Under Nonstationarity:}
Autonomous systems and their models are dependent on the data they learned from (in defense of AIAs, so are humans). Nonstationarity refers to the real challenge of training a model based on data from one distribution $D$ while taking into account that the test distribution $D^\prime$ will likely move (i.e. be nonstationary) or `shift' through time. This is a complex problem~\cite{Quinonero-Candela2009-fj}. \citet{Sugiyama2013-ci} suggest approaches for addressing 1) `covariate shift' (training and test input data follow different distributions), and 2) `class-balance change' (where the class-prior probabilities are different in training and test phases, but where there is no covariate shift). A key approach to addressing both problems is to use `importance sampling' which involves weighting the training loss according to the ratio between $p^\prime(x)$ and $p(x)$; this ratio is referred to as the `importance'. \citet{Charikar2017-kr} address the problem of learning from `untrusted' data; this data could be untrusted due to adversarial attack or nonstationarity.

When an AIA can cope with nonstationary covariates and/or class-balance then it will perform more consistently, and human users will be able to more appropriately trust it.

\subsubsection{Grounding Example:}
In the case of the `VIP Escort' problem (described in Section~\ref{sec:mot_example}), value alignment might be used as an assurance in the following way:

We make the following assumptions

\begin{itemize}
    \item The UGV has just begun an attempt to escape the road-network
    \item The UGV uses safe RL to learn its escape policy
    \item The operator is able to observe the UGV during its entire escape attempt
\end{itemize}

The operator has used several different UGVs for similar tasks. This newer model uses `safe RL' to learn its policy. When observing the UGV's attempt at escape the operator notices a difference in how the UGV operates. Whereas the older UGV models would sometimes do risky things, this UGV seems to navigate dangerous situations much better. 

\paragraph{\textbf{Discussion of Example:}} In this case, safe RL enabled the UGV to treat situations that an operator might classify as `dangerous' with more care. With this integral capability the UGV assures the operator that it is more competent.
