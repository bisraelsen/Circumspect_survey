\subsection{Value Alignment} \label{sec:value_alignment}
Recall that AIAs operate under autonomy in tasks that are delegated by users, with expectation that the AIA behave according to the user's intent frame. 
Optimization-based reasoning is perhaps one of the most common approaches to ensuring this. 
The general idea is to define a \emph{value} or \emph{utility function} that governs the AIA's decision making and reasoning process, such that desirable behaviors are elicited through maximization of the utility, i.e. the AIA behaves rationally in accordance with the user's intent frame. 
Formally, a utility function describes the `long-term desirability' of taking certain actions in certain conditions, i.e. beyond immediate benefits or penalties~\cite{Russell2010-wv}, and should ideally be consistent with user preferences pertaining to the state of the world and AIA behaviors. 
In other words, the utility function should ideally encapsulate all aspects of the user's intent frame. 
This has two positive benefits. 
Firstly, this approach ensures that AIA behaviors can themselves be used as assurances: users will tend to trust AIA's more if they are `well-behaved' than if they are not. 
Secondly, an AIA can be assuring by trying to ensure that its utility is aligned with the user's intent frame.
Since it is practically quite challenging to encode user preferences and intent frames into utility functions, the process of \emph{value alignment}\footnote{Value alignment is more commonly known as `AI Alignment' in AI research~\cite{Yudkowsky2001-hb,Bensinger2014-ul}.} leads to many different algorithmic strategies for generating assurances. 
\brettcomm{value alignment can be a drawback due to the difficulty of implementation, also humans are not always rational}

Consider taking an action $a$ from a given state $s$. From this perspective, the goal of value alignment is to minimize the difference between the utilities of a robot $r$ and a human $h$. When the utility of the robot $U_r(a,s)$ and the human $U_h(a,s)$ are approximately equivalent (within some tolerance) then the values of the robot are considered to be \emph{aligned} with those of the human. An AIA with aligned values will be considered by users to be more predictable (and thus more competent), because the AIA will be more likely to act in expected, `natural', ways. This follows from the AIA actually valuing given actions similarly to the human user.
Examples of commonly used utility functions for decision making under uncertainty (e.g. with POMDPs) include expected discounted cumulative rewards, which approximate diminishing marginal returns for sequential decision making problems. 

A roughly summarized example of an AIA whose value is \emph{not} aligned: An autonomous robot is designed, and deployed, with the intent that it make paper clips. This robot then decides to take over the world in order to maximize its resources and ability to make more paper clips~\cite{Bostrom2014-fz}. To reasonable humans, this is clearly \enph{not} the behavior the designer had intended, the utilities that the robot used for making decisions did not match those that the human must have had. Therefore that behavior is instrinsically an assurance that reduces trust. On the other hand, if the robot were to try to learn from its mistakes and improve (i.e. make $U_r$ closer to $U_h$) that could be received as an assurance that increases trust.

\subsubsection{Common Approaches:}
Value alignment research is concerned with several different problems (see \cite{Gordon_Worley2018-xy,Amodei2016-xi}). Among the more directly applicable topics in the scope of this paper are: reward hacking (how can we hinder the AIA from taking advantage of imperfections in specification of rewards), safe exploration (how can an AIA learn in a safe manner), robustness to distributional shift (how can the AIA cope with a difference between training data and test data), and design/learning of appropriate objective functions \cite{Hadfield-Menell2016-ws,Da_Veiga2012-gh,Garcia2015-rs}. The solutions to these problems are assurances because in their absence the user is able to more appropriately understand the capabilities of the AIA without being surprised.

\brettcomm{How is user intent addressed by solutions to reward-hacking, safety, and nonstationarity?}

\paragraph{Human-Guided Learning}
Even humans tend to reward hack/game, so the problem is far from solved. However, researchers have made use of practical approaches to attempt to align the values of AIAs with those of human users. One approach is to involve a human in the actual learning process. By doing so the human is able to, in essence, encode a, possibly complex, objective function into the AIA.

For instance, \citet{Freitas2006-qo} compared two approaches to discovering `interesting' knowledge from large data sets, based on the idea that human users require assistance from complex systems in order to find useful patterns and other interesting insights. He mentions `user-driven' methods that involve a user suggesting interesting templates or providing general impressions in the form of IF-THEN rules. These are then compared to different `data-driven' methods, using other research to suggest that data-driven approaches are not very effective in practice.

Having said that, user-driven approaches may not fare any better when compared over many users, as each user will likely have different preferences. Other scaled up user-driven approaches, e.g. based on crowd-sourcing~\citet{Chang2017-kl}, can also achieve better accuracy for labeling tasks while also exploring new or ambiguous classes that can be ignored with traditional approaches (especially if training data sets are biased or very limited). \citet{Chang2017-kl} also consider a similar, scaled up, `user-driven' approach called `Revolt' that crowd-sources the labeling of images. It is able to attain high accuracy labeling, while also exploring new or ambiguous classes that can be ignored with traditional approaches.

\paragraph{Safety:}
As AIAs try to learn what a user's utility is, they must do so in a safe manner. For instance, humans do not learn about the dangers of heights from falling off of skyscrapers. Instead we have to do so cautiously over time, and extrapolate from much less drastic experience (i.e. tripping on a curb). Safe reinforcement learning (safe RL) directly considers learning in such environments. It has been defined as the process of avoiding ``unintended and harmful behavior that [emerges] from machine learning systems''~\cite{Amodei2016-xi}. Two ways to approach safe RL are: 1) modification of the optimality criterion with a safety factor, and 2) modification of the exploration process through the incorporation of external knowledge~\cite{Garcia2015-rs}.

As one example, \citet{Lipton2016-dq} design an `intrinsic fear' RL approach that uses a deep Q-network (DQN) and a `supervised danger model'. The danger model stores the likelihood of entering a catastrophe state within a `short number of steps'. This model can be learned by detecting catastrophes through experience and can be improved over time. \citet{Curran2016-ij}, in a more specific application, asks how a robot can learn when a task is too risky, and then avoid those situations, or ask for help.

\citet{Kahn2017-vy} use Bayesian DNNs (using bootstrapping and dropout) to learn about the probability (with uncertainty) of an autonomous vehicle colliding in an environment given its current state, observations, and sequence of controls. Using this model they formulate a `velocity-dependent collision cost' that is used for model-based reinforcement learning. With this approach the vehicle naturally proceeds slowly when there is an elevated risk of collision. o this, they use a hierarchical POMDP planner that explicitly represents failures as additional state-action pairs.

Finally, moving on from ML-oriented approaches, one can also consider approaches rooted in Validation and Verification (V\&V). V\&V typically refers to the use of formal methods to guarantee the performance of a system within some set of specifications. However, not all practitioners are aware that V\&V provides ways to assure users. A prime example is given by \citet{Raman2013-mz}, who developed a way by which a user can provide natural language specifications to a robot and a `correct-by-construction' controller will be built if the specification is valid. Otherwise, the robot will provide an explanation about which specification(s) will cause failure. This approach is studied with the goal of implementing the system on a robot assistant in a hospital. Their method involves parsing natural language input (``Go to the kitchen''), and converting that into a linear temporal logic (LTL) task specification. This is then used to synthesize a robot controller if possible; otherwise, the `unrealizable' specifications are communicated back to the user. This approach is promising in that it presents a way to communicate that a specification cannot be met. But, it does not formally account for effects on user trust or TRBs in formulating explanations. The expression of assurances is also asymmetrically limited to cases where the robot cannot meet the specifications. 

In the context of a practical self-driving car application, \citet{Ghosh2016-dl} presents a framework called Trusted Machine Learning (TML) for making ML models fit pre-determined `trustworthiness' constraints. Unlike most other kinds of assurances considered so far, these constraints are handed down by a certification authority as requirements to the ML system designer, and are not meant to be exposed to/determined by users. However, in principle, such hard assurances could be exposed to users in certain contexts. The key idea is to utilize tools from formal methods to provide theoretical proof of the functionality of the system. Note that, in the formal methods literature, such proofs and their supporting evidence are what are referred to as `assurances'; we earlier denoted these as `hard assurances', in contrast to the other kinds of `soft assurances' considered here. 

\paragraph{Learning Under Nonstationarity:}
Autonomous systems and their models are dependent on the data they learned from (in defense of AIAs, so are humans). Nonstationarity refers to the real challenge of training a model based on data from one distribution $D$ while taking into account that the test distribution $D^\prime$ will likely move (i.e. be nonstationary) or `shift' through time. This is a complex problem~\cite{Quinonero-Candela2009-fj}. \citet{Sugiyama2013-ci} suggest approaches for addressing 1) `covariate shift' (training and test input data follow different distributions), and 2) `class-balance change' (where the class-prior probabilities are different in training and test phases, but where there is no covariate shift). A key approach to addressing both problems is to use `importance sampling' which involves weighting the training loss according to the ratio between $p^\prime(x)$ and $p(x)$; this ratio is referred to as the `importance'. \citet{Charikar2017-kr} address the problem of learning from `untrusted' data; this data could be untrusted due to adversarial attack or nonstationarity.

When an AIA can cope with nonstationary covariates and/or class-balance then it will perform more consistently, and human users will be able to more appropriately trust it.

\subsubsection{Grounding Example:}
In the case of the `VIP Escort' problem (described in Section~\ref{sec:mot_example}), value alignment might be used as an assurance in the following way:

We make the following assumptions

\begin{itemize}
    \item The UGV has just begun an attempt to escape the road-network
    \item The UGV uses safe RL to learn its escape policy
    \item The operator is able to observe the UGV during its entire escape attempt
\end{itemize}

The operator has used several different UGVs for similar tasks. This newer model uses `safe RL' to learn its policy. When observing the UGV's attempt at escape the operator notices a difference in how the UGV operates. Whereas the older UGV models would sometimes do risky things, this UGV seems to navigate dangerous situations much better. 

\paragraph{\textbf{Discussion of Example:}} In this case, safe RL enabled the UGV to treat situations that an operator might classify as `dangerous' with more care. With this integral capability the UGV assures the operator that it is more competent.
