%background.tex

%%\nisarcomm{my TODOs: 1. Re-Merge Sec 2 and 3; 2. trim down text/figures to `bare essentials' and definitions required for survey, leave enough material for sensible  transition, refer to deeper dive into definitions/background for later on in discussion/future work; 3. move other material/definitions not needed for survey to new section 4 (currently 5) -- need transition to revisit and expound on these...}
%As per the title, in this paper we present a case for the necessity of understanding assurances, and in doing so also present a definition. Finally we perform a survey of methods by which assurances have been calculated.

%%\subsection{Motivation} \label{sec:motivation}
    %What do people who talk about `comprehensible systems', `interpretable learning', `opaque systems', and `explainable AI' really, fundamentally, care about?
    %Whether formally acknowledged or not, human designers and users want \emph{assurances} to help them appropriately `trust' autonomous and artificially intelligent systems. 
    %%\nisarcomm{for me todo: some of this might be redundant given the intro, basically repeating the same question/points made above?...might be worth trimming down a bit or moving to back for `recap'...include foglights here for this section}
    The need for designed assurances has grown considerably in recent years, as the advanced capabilities of intelligent systems have become more difficult to comprehend and predict \cite{Doshi-Velez2017-xy, Weller2017-zx, Lipton2016-ug, Gunning2017-ih}. 
    When researchers discuss concepts like `comprehensible systems', `interpretable learning', `transparent systems', and `explainable AI', they are looking for deliberately designed mechanisms to help designers and users appropriately `trust' autonomous and artificially intelligent systems as they perform their tasks. 
    For example, many systems are designed to learn from extremely large amounts of data and are expected to regularly perform on never before seen data -- yet, it is rarely obvious if such data conforms to assumptions made at design time. 
    Other systems are designed to perform tasks that are too `dirty, dull, and dangerous' for humans; the separation of users from these tasks often makes it difficult for them to understand whether these systems are performing as desired. 
    The authors, for instance, are interested in the design of unmanned robotic vehicle systems that operate in concert with remote human operators in uncertain dynamic environments. 
    Since operators will generally not be computer scientists or roboticists, it is desirable for such systems to communicate in ways that help operators properly use their abilities in scenarios featuring unexpected or incomplete information, time-critical decisions, and risky outcomes \cite{Hutchins2015-if, Sweet2016-tz}. 
    %%The hope is that, in doing so, the performance of the team can be improved by using the capabilities of both human and unmanned vehicles for intelligent reasoning and decision making \cite{Hutchins2015-if}. 
    This application is explained in more detail later in relation to Figure~\ref{fig:SimpleTrust_one_way}. 
    These issues also have relevance and analogues in other applications of autonomous artificial intelligence, robotics, machine learning and decision making/support systems ~\cite{Garcia2015-rs,Otte2013-oo,Sugiyama2013-ci,Amodei2016-xi}, e.g. for scientific data analysis ~\cite{Faghmous2014-og}, public policy and medicine ~\cite{Wagner2016-ck,Jovanovic2016-gw} and cognitive assistance \cite{Gutfreund2016-xe}.
    
    %To this end, 
    Many studies, models, metrics and methods have been created around these systems to address these issues. 
    Some fields have formally and explicitly considered trust between humans and specific forms of intelligent technology, e.g. e-commerce, automation, and human-robot interaction. However, these research efforts have focused largely on developing formal cognitive and psychological models of trust, rather than system behaviors or algorithms that designers can exploit as assurances. %to calibrate trust.  
    %mainly focused on implicit `uncontrolled' properties of the systems that affect trust. 
    Other fields that have explored assurance design only provide an informal connection to trust and applications to other disciplines, so it is unknown how effective their developed assurances might be in practice, or what principles ought to be considered for other kinds of autonomous and artificially intelligent systems. 
    This paper surveys assurance metrics and methods across relevant application domains, with the goal of %bridging gaps between them by 
    identifying common principles, approaches and questions related to trust-based interaction. %opportunities where each might benefit from another. 
    %Generally, humans want to `trust' 
    %\footnote{Of course, at this point `trust' is quite an imprecise term, and will be defined more formally in Section~\ref{sec:trust}}
    %the tools and systems that they create. 
    To begin with, definitions for the trust cycle in Fig. ~\ref{fig:SimpleTrust_one_way} are given to ground the concept of assurances. A running application example is then provided to compare/contrast technical ideas and implementations of algorithmic assurances throughout the survey in Section \ref{sec:synthesis}. 
     
     
    
    %%Other systems are designed to perform tasks that might take humans entire lifetimes to complete \nisarcomm{...so...and? This is only half an argument as written...also seems orthogonal to what we want to focus on, i.e. robotics and autonomous vehicles} 
    %Below is a sample of some application areas and possible reasons why they have an interest in creating trustworthy systems:

%\nisarcomm{moved to discussion part at end of paper...}
%    \begin{description}
%        \item [Artificial Intelligence/Machine Learning:] There is a need to interpret how and why theoretical AIA models function, in order to know they are being applied correctly and to design new approaches to overcome weaknesses in the existing methods~\cite{Garcia2015-rs,Otte2013-oo}.    
%        \item [Interpretable Science:] Scientists need to be able to trust the models created using data analysis, and be able to draw insights from them. Scientific discoveries cannot depend on methods that are not understood~\cite{Faghmous2014-og}.
%        \item [Reliable Machine Learning:] It is critical to have safety guarantees for AIAs that have been deployed in the real world. Failing to do so can result in serious accidents that cause loss of life, or significant damage~\cite{Sugiyama2013-ci,Amodei2016-xi}.       
%        \item [Public Policy:] Governments are beginning to enforce regulations on the interpretability of certain algorithms in order to ensure that citizens can understand why AIA driven services make the decisions and predictions that they do. A specific example are the algorithms deployed by credit agencies to approve/reject loans~\cite{Wagner2016-ck}.
%        \item [Medicine:] Medical professionals need to understand why data-driven models give predictions so that they can choose whether or not to follow the recommendations. While AIAs can be a very useful tool, ultimately doctors are liable for the decisions they make and treatments they administer~\cite{Jovanovic2016-gw}.
%        \item [Cognitive Assistance:] Systems are being designed as aids for humans to make complex decisions, e.g. searching and assimilating information from databases of legal proceedings. When an AIA presents perspectives and conclusions as data summaries, it must be able to also present justifying evidence and logic~\cite{Gutfreund2016-xe}.
%    \end{description}


%    \brettcomm{Important---Whether formally acknowledged or not, human AIA system designers and users want \emph{assurances} to help them appropriately trust AIAs. There are a few research fields that have formally and explicitly considered trust between humans and technology, e.g. e-commerce, automation, and human-robot interaction. However, these research efforts have mainly focused on implicit properties of the systems that affect trust. Conversely, there are other fields that have informally considered how to affect the trust of designers and users via explicitly designed assurances. However, due to their informal treatment of trust, it is unknown and unclear how effective these designed assurances might be in practice, or what principles ought to be considered when designing assurances for general AIAs. The goal of this paper is to survey these areas and, in doing so, help bridge the gap between them by identifying common goals and approaches, as well as highlighting where the different disciplines might benefit from each other.}

%\subsection{Motivating Application and Basic Definitions} \label{sec:mot_example}

%\nisarcomm{split and move example to after the definitions?? need to shrink, and note that this will be used as a running example throughout the paper to illustrate/ground/relate key ideas...}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Trust Cycle Definitions}
\input{definitions.tex} %%re-merging and heavily editing this subsection

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Recurring Example Application}
    \nisarcomm{for me todo: edit, trim down a bit; what makes this dull, dirty, dangerous task?}
    It is useful to have a concrete grounding example. For instance, consider a toy application motivated by the ``VIP escort'' problem \cite{Humphrey2012-lr}, which also serves as a useful analog for surveillance and reconnaissance operations. 
    A robotic unmanned ground vehicle (UGV) acts as the lead vehicle of a small convoy attempting to navigate its way through a road network that is monitored by accessible unattended ground sensors (UGS). The road network also contains a hostile pursuer that the UGV is trying to evade while exiting the road network. The pursuer's location is unknown but can be estimated using intermittent data from unattended ground sensors (UGS), which only sense portions of the network and can produce false alarms. The UGV's decision space involves selecting a sequence of actions (i.e. go straight, left, right, back, stay in place). The UGS data, the UGV's motion, and the pursuer's behavior are all stochastic, and the problems of decision making and sensing are strongly coupled: some trajectories through the road network allow the UGV to localize the pursuer before heading to the exit (but incur a high time penalty); other trajectories afford rapid exit with high pursuer location uncertainty (increasing the risk of getting caught by the pursuer, which can follow multiple paths). 

    One of the many approaches to modeling and solving this decision making problem for the UGV (the AIA in this example) is to discretize time and vehicle spatial variables, in order to construct a partially observable Markov decision process (POMDP) model of the task. The ideal POMDP solution is an optimal UGV action selection policy that will, \emph{on average}, maximize some utility function whose optimum value coincides with desirable UGV behaviors (i.e. avoiding the pursuer and reaching the exit quickly). Although analytically and computationally intractable to find exactly, POMDP policies can be approximated by any number of sophisticated approaches.

    A human supervisor monitors and interfaces with the UGV during operation. The supervisor does not have a detailed knowledge of how the UGV functions or makes decisions (e.g. according to the POMDP), but can interrogate the system, modify the decision making stance (such as `aggressive' or `conservative'), and provide information and advice to the UGV. In this situation, the supervisor could benefit from the UGVs capability to express confidence in its ability to escape given the current sensor information, and work with the AIA to modify behavior if necessary. 
    
	\begin{figure}[t]%[htbp]
    	\centering
     	\includegraphics[width=0.5\textwidth]{Figures/RoadNet}
    	\caption{Application example of unmanned ground vehicle (UGV) in a road network, trying to evade a pursuer, using information from unmanned ground sensors (UGSs), as well as information and decision making advice from a human operator.} %The operator's actions towards the UGV are trust-based.}
        \label{fig:RoadNet}
    \end{figure}

%%\nisarcomm{for me todo: garnish with some pics of UGV/operator? should UGV being doing something else like gathering intel from UGS, etc.? what makes this task so dull, dirty, dangerous?}

    In this scenario the trust-cycle terms can be defined as follows: \textit{Artificially Intelligent Agent:} the UGV, which must make decisions under uncertainty with little information (the pursuer is only observed sporadically and does not follow a known course); \textit{Trust:} The operator's willingness to rely on the UGV when the outcome of the mission is at stake, such as in a scenario where the UGV is carrying a VIP or some other valuable payload; \textit{Trust-Related Behaviors:} operator's behaviors that indicate trust (or lack thereof) in the UGV, including the information and commands they give to the UGV. This might take the form of approving/rejecting the UGV's decisions, or real-time communication and adjustments of the UGV's information based on what the operator receives from other intelligence sources; \textit{Assurances:} properties and/or behaviors of the UGV that have an effect on the operator's trust. These can include communicating the probability of success for a given policy, or communicating that the mission is not proceeding as expected.

%%\subsection{Related Work}\label{sec:rel_work}
    
%%    \nisarcomm{shrink down...move earlier and merge? too much detail here...}
    
%    The issues of interpretability, explainability, and transparency of AI has garnered considerable recent attention \cite{Doshi-Velez2017-xy, Weller2017-zx, Lipton2016-ug, Gunning2017-ih}. The related body of work has many interesting and important insights regarding the need for transparency, but does not formally acknowledge the role of trust in human decision making, or how interpretability and transparency affect the trust of those who use AIAs. Yet, this work is beneficial because it draws the attention of researchers to this critical area, and it initially formalizes the problem for those actively researching assurance design.

%    \citet{Lillard2016-yg} addressed the role of assurances in the relationship between humans and AIAs, and provides much of the foundation used here for describing the relationships between assurances and trust in human-AIA interaction. Here, the framework for analyzing assurances is presented in a way that is both more general and more detailed, albeit with the same end goal of being applied in a very similar end application. For instance, we consider the full trust model presented by \citet{McKnight2001-fa}, whereas \citeauthor{Lillard2016-yg} only consider a subset of the trust model.

%    Relating to the work of \citet{McKnight2001-fa}, who constructed a typology of interpersonal trust, we adopt the position that (besides being applicable to the e-commerce industry as originally intended) their trust model also applies to relationships between humans and AIAs (as in \cite{Lillard2016-yg}). They refer to something called `vendor interventions' that are related to assurances in this paper. One small, but important distinction from vendor interventions is that assurances cannot directly have an affect on the user trust-related behaviors (TRBs). This is an important point, since we are considering scenarios in which the human and AIA are working together, and not ones where the human is strictly dependent or steered/guided by an AIA. 
    
%    While assurances are defined by \citeauthor{Lillard2016-yg}, and mentioned by \citeauthor{McKnight2001-fa}, and also \citeauthor{Corritore2003-gx}, we investigate in detail how assurances fit within the trust cycle (from Figure~\ref{fig:SimpleTrust_one_way}), survey what methods of assurance have been and are currently being used, then present a refined definition and classification of assurances. In essence, whereas others have noted the \textit{existence} of assurances, we now directly consider the question: What, exactly, \textit{are} assurances, and how can they be \textit{designed}? 
    %To that end we survey literature that formally considers trust between humans and AIAs, as well as literature that informally investigates trust through topics like transparency, explainability, and interpretability, and begin to distill ideas for practically designing assurances in human-AIA trust relationships. \brettcomm{I DON'T THINK THERE'S MUCH DISCUSSION OF THIS ANYWHERE\ldots}
