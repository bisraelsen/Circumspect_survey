\section{Background and Motivation} \label{sec:background}

What do people who talk about `comprehensible systems', `interpretable learning', `opaque systems', and `explainable AI' really care about? This document investigates, in a formal and comprehensive manner, which dimensions of a user's trust should be influenced by the assurances of an AIA, as well as how this might be practically accomplished.

Motivations and a grounding application are presented is sections \ref{sec:motivation} and \ref{sec:mot_example}, along with some application-specific definitions of the trust-cycle terms. A discussion of related work is in section \ref{sec:rel_work}. Assurances are presented and defined in section \ref{sec:assurances}.  Section \ref{sec:aias} defines the term AIA in a general sense, and how they relate to assurances. Section \ref{sec:trust} discusses trust, and provides a model of human-AIA trust that can be used in designing assurances. Finally, section \ref{sec:trbs} discusses trust-related behaviors and how they should be considered when designing assurances.

\subsection{Motivation} \label{sec:motivation}
    Generally, humans have always wanted to trust the tools and systems that they create.  To this end many metrics and methods have been created to help assure the designers and users that the tools and systems are in fact capable of being applied in certain ways, and/or will behave as expected. Of course, at this point in the document `trust' is quite an imprecise term, and needs to be more formally defined; this will be done in section \ref{sec:trust}

    The situation has grown more complicated in recent years because the advanced capabilities of the systems being created can at times be difficult for even those who designed them to comprehend and predict. There are, for instance, systems that have been designed to learn from extremely large amounts of data and expected to regularly perform on never before seen data. In some cases, such systems have been designed to perform tasks that might take humans entire lifetimes to complete. Below is a small sample of some application areas that exist and a possible reason why they -- perhaps unknowingly -- have an interest in creating trustworthy systems.

    \begin{description}
        \item [Interpretable Science:] Scientists need to be able to trust that the models created using data analysis, and be able to draw insights from them. Scientific discoveries cannot depend on methods that are not understood.
        \item [Reliable Machine Learning:] It is critical to have safety guarantees for AIAs that have been deployed in real-world environments. Failing to do so can result in serious accidents that could cause loss of life, or significant damage.
        \item [Artificial Intelligence/Machine Learning:] There is a need to interpret how and why theoretical AIA models function. This is due to the need to know they are being applied correctly, but also to be able to design new methods to overcome weaknesses in the existing methods.
        \item [Government:] Governments are beginning to enforce regulations on the interpretability of certain algorithms in order to ensure that citizens can understand why many services make the decisions and predictions that they do. A specific example are the algorithms deployed by credit agencies to approve/reject loans.
        \item [Medicine:] Medical professionals need to understand why data-driven models give predictions so that they can choose whether or not to follow the recommendations. While AIAs can be a very useful tool, ultimately doctors are liable for the decisions they make and treatments they administer.
        \item [Cognitive Assistance:] Systems are being designed as aids for humans to make complex decisions, such as searching and assimilating information from databases of legal proceedings. When an AIA presents perspectives and conclusions as summaries of this data, it must be able to also present evidence and logic to justify them.
    \end{description}

    The interests of the author lie specifically in the design of unmanned vehicles that operate in concert with human operators in uncertain environments. In this setting, it is desirable for the unmanned vehicle to be able to communicate with a human in some way in order to help them properly use the vehicle. The hope is that, in doing so, the performance of the team can be improved by appropriately utilizing the strengths of both the human and unmanned vehicle. This application is explained in more detail below in relation to Figure \ref{fig:SimpleTrust_one_way}.

    Revisiting a key point from the introduction, humans want to design assurances to help them appropriately trust AIAs. There are a few research fields that have formally and explicitly considered trust between humans and technology. Some examples are: e-commerce, automation, and human-robot interaction. However, due to their main goals they have mainly focused on implicit properties of the systems that affect trust. Conversely, there are other research fields that have informally considered how to affect the trust of designers and users via explicitly designed assurances. However, due to their informal treatment of trust, it is unknown and unclear how effective these designed assurances might be, or what principles ought to be considered when designing assurances for general AIAs. A key objective of this paper is to survey these areas and, in doing so, help bridge the gap between them by identifying common goals and approaches, as well as highlighting where the different groups might benefit from the research of others.

\subsection{Motivating Application and Basic Definitions} \label{sec:mot_example}
    It is useful to have a concrete example on which we can refer for grounding examples. As previously mentioned the specific interests of the author lie in the design of unmanned vehicles that can work in cooperation with human operators.

    Specifically, consider an unmanned ground vehicle (UGV) in a road network with unmanned ground sensors (UGSs). The road network also contains a pursuer that the UGV is trying to evade while exiting the road network. A human operator monitors and interfaces with the UGV during operation. The operator does not have a detailed knowledge of how the UGV functions or makes decisions, but can interrogate the system, modify the decision making stance (such as `aggressive' or `conservative'), and provide information and advice to the UGV. In this situation the operator could benefit from the UGVs ability to express confidence in its ability to escape given the current sensor information, and work with the AIA to modify behavior if necessary. This application serves as a useful analog for many different autonomous system applications, e.g. autonomous VIP escort problems and intelligence, surveillance and reconnaissance (ISR)/counter-ISR operations for defense and security \cite{Kingston2012-va}.
   
	\begin{figure}[htbp]
    	\centering
     	\includegraphics[width=0.4\textwidth]{Figures/RoadNet}
    	\caption{Application example of unmanned ground vehicle (UGV) in a road network, trying to evade a pursuer. The UGV has access to unmanned ground sensors (UGSs) (also an unmanned aerial vehicle (UAV) that can be used as a mobile sensor), as well as information and decision making advice from a non-expert human operator. The operator's actions towards the UGV are trust-based.}
        \label{fig:RoadNet}
    \end{figure}

    In this scenario the trust-cycle terms can be simply defined as follows:

    \begin{description}
        \item [Artificially Intelligent Agent:] The UGV, this problem is difficult because the pursuer is only observed sporadically, and does not follow a known course. The UGV must make decisions under uncertainty, with little information. 
        \item [Trust:] The operator's willingness to rely on the UGV when the outcome of the mission is at stake, such as in a scenario where the UGV is carrying a valuable payload that might be.
        \item [Trust-Related Behaviors:] The operator's behaviors towards the UGV, including the information provided, and the commands given. This might take the form of approving/rejecting the UGV plan upfront, or the possibility of involving real-time communication and adjustments to new information that the operator receives.
        \item [Assurances:] Implicit and explicit communication from the UGV that has an effect on the operator's trust. An assurance might be in the form of communicating the probability of success for a given plan, or communicating that the mission is not proceeding as expected.
    \end{description}

\subsection{Related Work}\label{sec:rel_work}
    \citet{Lillard2016-yg} addressed the role of assurances in the relationship between humans and AIAs, and provides much of the foundation for describing the relationships between assurances and trust in human-AIA interaction. Here, the framework for analyzing assurances is presented in a way that is both more general and more detailed, albeit with the same end goal of being applied in a very similar end application. For instance, instead of assuming that the user-AIA trust loop is very strict and well structured, and that user trust does not take into account the institutional component proposed by \citet{McKnight2001-fa}, we account for the notion of institutional trust in  human-AIA relationships.

    Regarding the relationship with the work of \citet{McKnight2001-fa}, who constructed a typology of interpersonal trust,  we adopt the position that besides being applied to the e-commerce industry their trust model also applies to relationships between humans and AIAs (as in \citet{Lillard2016-yg}). However, here it is argued that assurances cannot directly have an affect on the user trust-related behaviors (TRBs). The premise is that no AIA (or vendor in the words of \citeauthor{McKnight2001-fa}) should be able to control the TRBs of a human. This is an important point because we are considering a scenario (arguably more realistic, and definitely more humane) in which the human and AIA are working together, and not where the human is a puppet controlled by the AIA.

\input{assurances.tex}
\input{aias.tex}
\input{trust.tex}
\input{trbs.tex}
