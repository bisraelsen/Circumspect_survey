\section{Background and Motivation} \label{sec:background}

What do people who talk about `comprehensible systems', `interpretable learning', `opaque systems', and `explainable AI' really care about? This paper provides a formal \edit{comprehensive} look at what components of a user's trust assurances for intelligent systems would want to influence, and how this can be practically accomplished by intelligent systems.

Motivations and a grounding application are presented is sections \ref{sec:motivation} and \ref{sec:mot_example}, along with some application-specific definitions of the trust-cycle terms. A discussion of related work is in section \ref{sec:rel_work}. Assurances are presented and defined in section \ref{sec:assurances}.  Section \ref{sec:aias} defines the term AIA in a general sense, and how they relate to assurances. Section \ref{sec:trust} discusses trust, and provides a model of human-AIA trust that can be used in designing assurances. Finally, section \ref{sec:trbs} discusses trust-related behaviors and how they should be considered when designing assurances.

\subsection{Motivation} \label{sec:motivation}
    Generally, humans have always wanted to trust the tools and systems that they create.  To this end many metrics and methods have been created to help assure the designers and users that the tools and systems are in fact capable of being applied in certain ways, and/or behave as expected. Of course, at this point `trust' is quite an imprecise term, and needs to be more formally defined; this will be done in section \ref{sec:trust}

    The situation has grown more complicated in recent years because the advanced capabilities of the systems being created can at times be difficult for even those who designed them to comprehend and predict. There are, for instance, systems that have been designed to learn from extremely large amounts of data and expected to regularly perform on never before seen data. In some cases, such systems have been designed to perform tasks that might take humans entire lifetimes to complete. Below is a small sample of some application areas that exist and a possible reason why they -- perhaps unknowingly -- have an interest in creating trustworthy systems.
    
	% \begin{sidewaysfigure}[htbp]
        % \includegraphics[width=7.5in]{Figures/WhoCares_cleaned}%
        % \caption{A diagram showing some of the academic disciplines that want to trust AIs more fully}
        % \label{fig:WhoCares}
    % \end{sidewaysfigure}
%
    \begin{description}
        \item [Interpretable Science:] Scientists need to be able to trust that the models created using data analysis, and be able to draw insights from them. Scientific discoveries cannot depend on methods that are not understood.
        \item [Reliable Machine Learning:] It is critical to have safety guarantees for AIAs that have been deployed in real-world environments. Failing to do so can result in serious accidents that could cause loss of life, or significant damage.
        \item [Artificial Intelligence/Machine Learning:] There is a need to interpret how and why theoretical AIA models function. This is due to the need to know they are being applied correctly, but also to be able to design new methods to overcome weaknesses in the existing methods.
        \item [Government:] Governments are beginning to enforce regulations on the interpretability of certain algorithms in order to ensure that citizens can understand why many services make the decisions and predictions that they do. A specific example are the algorithms deployed by credit agencies to approve/reject loans.
        \item [Medicine:] Medical professionals need to understand why data-driven models give predictions so that they can choose to follow the recommendations. While AIAs can be a very useful tool, ultimately doctors are liable for the decisions they make and treatments they administer.
        \item [Cognitive Assistance:] Systems are being designed as aids for humans to make complex decisions, such as searching and assimilating information from databases of legal proceedings. When an AIA presents perspectives and conclusions as summaries of this data, it must be able to also present evidence and logic to justify them.
    \end{description}

    The interests of the author lie specifically in the design of unmanned vehicles that operate in concert with human operators in uncertain environments. In this setting, it is desirable for the unmanned vehicle to be able to communicate with a human in some way in order to help them properly use the vehicle. The hope is that, in doing so, the performance of the team can be improved by appropriately utilizing the strengths of both the human and unmanned vehicle. \edit{This application is explained in more detail below in relation to Figure 1.}

    Revisiting a key point from the introduction, humans want to design assurances to help them appropriately trust AIAs. There are a few research fields that have formally and explicitly considered trust between humans and technology. Some examples are: e-commerce, automation, and human-robot interaction. However, due to their main goals they have mainly focused on implicit properties of the systems that affect trust. Conversely, there are other research fields that have informally considered how to affect the trust of designers and users via explicitly designed assurances. However, due to their informal treatment of trust, it is unknown and unclear how effective these designed assurances might be, or what principles ought to be considered when designing assurances for general AIAs. \edit{A key goal of this paper is to ...survey these areas and, in doing so, help bridge the gap between them by identifying common themes, etc.} \nisarcomm{here is where you can slightly restate and thus reinforce this part of your main argument from the introduction}

\subsection{Motivating Application and Basic Definitions} \label{sec:mot_example}
    It is useful to have a concrete example on which we can refer for grounding examples. As previously mentioned the specific interests of the author lie in the design of unmanned vehicles that can work in cooperation with human operators.

    Specifically, consider an unmanned ground vehicle (UGV) in a road network with unmanned ground sensors (UGSs). The road network also contains a pursuer that the UGV is trying to evade while exiting the road network. A human operator monitors and interfaces with the UGV during operation. The operator does not have a detailed knowledge of how the UGV functions or makes decisions, but can interrogate the system, modify the decision making stance (such as `aggressive' or `conservative'), and provide information and advice to the UGV. In this situation the operator could benefit from the UGVs ability to express confidence in its ability to escape given the current sensor information, and work with the operator to modify behavior if necessary. \edit{This application serves as a useful analog for many different autonomous system applications, e.g. autonomous VIP escort problems and intelligence, surveillance and reconnaissance (ISR)/counter-ISR operations for defense and security [can cite Derek Kingston's paper in the commented text below]. }
    %%@inproceedings{Kingston2012,
% address = {Berlin, Germany},
% author = {Kingston, Derek},
% booktitle = {German Aviation and Aerospace Congress (DLRK 2012)},
% mendeley-groups = {UAVs},
% publisher = {German Society for Aeronautics and Astronautics (DGLR)},
% title = {{Intruder Tracking Using UAV Teams and Ground Sensor Networks}},
% year = {2012}
% }
   
	\begin{figure}[htbp]
    	\centering
     	\includegraphics[width=0.4\textwidth]{Figures/RoadNet}
    	\caption{Application example of unmanned ground vehicle (UGV) in a road network, trying to evade a pursuer. The UGV has access to unmanned ground sensors (UGSs) (also an unmanned aerial vehicle (UAV) that can be used as a mobile sensor), as well as information and decision making advice from a non-expert human operator. The operator's actions towards the UGV are trust-based.}
        \label{fig:RoadNet}
    \end{figure}

    In this scenario the trust-cycle terms can be simply defined as follows:

    \begin{description}
        \item [Artificially Intelligent Agent:] The UGV \nisarcomm{Briefly describe: what makes this problem so hard that AIA is even required in the first place? Can refer to Matt's thesis for full details/spec of problem, but main highlight here is decision making under uncertainty with incomplete information...what decisions does the AIA have to make and what info does it have to make these?} 
        \item [Trust:] The operator's willingness to rely on the UGV when the outcome of the mission is at stake. \nisarcomm{Give an example of what could be at stake, e.g. VIP transport scenario in hostile territory?}
        \item [Trust-Related Behaviors:] The operator's behaviors towards the UGV, including the information provided, and the commands given \edit{for...} \nisarcomm{might be useful to give example of exactly how the user would interact here -- e.g. would it be as simple as a go/no-go command to execute the mission in the first place (e.g. for VIP transport scenario)?}
        \item [Assurances:] Implicit and explicit communication from the UGV that has an effect on the operator's trust. \nisarcomm{What might an example of such an assurance be, e.g. in context of helping operator to decide on a go/no-go for the mission?}
    \end{description}

\subsection{Related Work}\label{sec:rel_work}
    \citet{Lillard2016-yg} addressed the role of assurances in the relationship between humans and AIAs, and provides much of the foundation for describing the relationships between assurances and trust in human-AIA interaction. Here, the framework for analyzing assurances is presented in a way that is both more general and more detailed, albeit with the same end goal of being applied in a very similar end application. For instance, instead of assuming that the user-AIA trust loop is very strict and well structured, and that user trust does not take into account the institutional component proposed by \citet{McKnight2001-fa}, we account for the notion of institutional trust in  human-AIA relationships.

Regarding the relationship with the work of \citet{McKnight2001-fa}, who constructed a typology of interpersonal trust,  we adopt the position that besides being applied to the e-commerce industry their trust model also applies to relationships between humans and AIAs (as in \citet{Lillard2016-yg}). However, here it is argued that assurances cannot directly have an affect on the user trust-related behaviors (TRBs). The premise is that no autonomy (or vendor) should be able to control the TRBs of a human. \nisarcomm{can you say a bit more about why that is, and why the distinction is important? The general reader will probably not immediately appreciate this.}

\input{assurances.tex}
\input{aias.tex}
\input{trust.tex}
\input{trbs.tex}
