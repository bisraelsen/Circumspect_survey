\section{Background and Motivation} \label{sec:background}

What do people who talk about `comprehensible', `interpretable', `opaque', and `explainable' really care about? This paper tries to provide a more formal look at what components of a user's trust these methods would want to influence.

\subsection{Motivation}
    Because everyone wants to trust their AIs, whether that be a single classification or regression algorithm, or a more interactive personal assistant that can understand language and communicate. Everyone wants to know how to trust these systems. \textbf{this needs to be better, discuss how trust really leads to actions (which will be better defined later)}

	\begin{sidewaysfigure}[htbp]
        \includegraphics[width=7.5in]{Figures/WhoCares_cleaned}%
    	\caption{A diagram showing some of the academic disciplines that want to trust AIs more fully}
        \label{fig:WhoCares}
    \end{sidewaysfigure}

    \paragraph{Interpretable Science} use data to find causes and insight
    \paragraph{Reliable Machine Learning} safety guarantees for robots that have been deployed in real-world environments
    \paragraph{Computer Science} Understand how algorithms will function on real data
    \paragraph{AI/ML} interpret how/why theoretical models function
    \paragraph{Government/Law} Regulations on the interpretability of certain algorithms, usage for assistants to lawyers.
    \paragraph{Cognitive Assistance} Ability for user to understand why information was presented
    \paragraph{Medicine} understand why data-driven models give predictions
    \paragraph{HCI} help humans and computers interact in a more natural way (where human-human relationships are typically the definition of normal)
    \paragraph{Robust Learning/Planning}
    
    \textbf{NEED MUCH BETTER BACKGROUND AND MOTIVATION!!!}

\subsection{Related Work}
    \textbf{spend a little time speaking about what Austin actually did, give an intro to his work}This paper is meant to be related to that of \citet{Lillard2016-yg}, but more general. Here the assurance framework is set in a more general light, albeit with the goal of being applied in a very similar application. Instead of treating, or at least inferring, that the user-autonomy trust loop is very strict and well structured, and that user trust does not take into account the institutional component proposed by \citet{McKnight2001-fa}, we re-institute the notion of institutional trust into the human-autonomy relationships.

    Regarding the relationship with the work of \citet{McKnight2001-fa}, we adopt the position that besides being applied to the e-commerce industry their trust model also applies to relationships between humans and autonomy (as in \citet{Lillard2016-yg}). However, here it is argued that assurances cannot directly have an affect on the user TRBs. This premise is that no autonomy (or vendor) should be able to control the TRBs of a human.

    The contributions of this work are to illuminate the connection between general AIA capabilities and general user trust. To classify assurances into several different groups. To suggest that TRBs need to be calibrated \emph{not} trust.



\input{aias.tex}
\input{trust.tex}
\input{trbs.tex}
\input{assurances.tex}
