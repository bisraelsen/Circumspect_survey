The notion of algorithms being used to create assurances for users is \emph{not} new. However, the creation of these assurances (for example in various engineering disciplines, software, science, economics, and others) has historically been done in an ad hoc manner. The need for designed assurances has grown considerably in recent years, as the advanced capabilities of intelligent systems have become more difficult to comprehend and predict \cite{Doshi-Velez2017-xy, Weller2017-zx, Lipton2016-ug, Gunning2017-ih}. Advanced intelligent systems share capabilities with less-advanced counterparts, but generally possess much more delegated responsibility, autonomous functionality, are employed in more uncertain environments, and are operated by a wider demographic of users with different levels of understanding and technical skills. These systems are also going to be more prolific in number, and influence that any other technology known to date (especially among non-technical users). In this atmosphere the practice of designing assurances with very little formal understanding will not be viable very much longer; in short: the old, informal, approach used to work when the stakes weren't quite as high, but that time is about to end.

When researchers discuss concepts like `comprehensible systems', `interpretable learning', `transparent systems', and `explainable AI', they are really interested in making deliberately designed mechanisms to help designers and users appropriately `trust' autonomous and artificially intelligent systems as they perform their tasks. 
For example, many systems are designed to learn from extremely large amounts of data and are expected to regularly perform on never before seen data---yet, it is rarely obvious if such data conforms to assumptions made at design time. 
Other systems are designed to perform tasks that are too `dirty, dull, and dangerous' for humans; the separation of users from these tasks often makes it difficult for them to understand whether these systems are performing as desired. 
The authors, for instance, are interested in the design of unmanned robotic vehicle systems that operate in concert with remote human operators in uncertain dynamic environments. 
Since operators will generally not be computer scientists or roboticists, it is desirable for such systems to communicate in ways that help operators properly use their abilities in scenarios featuring unexpected or incomplete information, time-critical decisions, and risky outcomes~\cite{Hutchins2015-if, Sweet2016-tz}. 
This application is explained in more detail later in relation to Figure~\ref{fig:SimpleTrust_one_way}. 
These issues also have relevance and analogues in other applications of autonomous artificial intelligence, robotics, machine learning and decision making/support systems~\cite{Garcia2015-rs,Otte2013-oo,Sugiyama2013-ci,Amodei2016-xi}, e.g. for scientific data analysis~\cite{Faghmous2014-og}, public policy and medicine~\cite{Wagner2016-ck,Jovanovic2016-gw} and cognitive assistance \cite{Gutfreund2016-xe}.

Many studies, models, metrics and methods have been created around these systems to address these issues.
Some fields have formally and explicitly considered trust between humans and specific forms of intelligent technology, e.g. e-commerce, automation, and human-robot interaction. However, these research efforts have focused largely on developing formal cognitive and psychological models of trust, rather than system behaviors or algorithms that designers can exploit as assurances. 
Other fields that have explored assurance design only provide an informal connection to trust and applications to other disciplines, so it is unknown how effective their developed assurances might be in practice, or what principles ought to be considered for other kinds of autonomous and artificially intelligent systems. 
This paper surveys assurance metrics and methods across relevant application domains, with the goal of
identifying common principles, approaches and questions related to trust-based interaction.
To begin with, definitions for the trust cycle in Fig.~\ref{fig:SimpleTrust_one_way} are given to formally ground the concept of assurances. A running application example is then provided to compare/contrast technical ideas and implementations of algorithmic assurances throughout the survey in Section~\ref{sec:synthesis}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Trust Cycle Definitions} \label{sec:trust_definitions}
\input{definitions.tex} %%re-merging and heavily editing this subsection

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Recurring Example Application} \label{sec:mot_example}
    %\nisarcomm{for me todo: edit, trim down a bit; what makes this dull, dirty, dangerous task?}
    %It is useful to have a concrete grounding example. 
    To illustrate the assurances surveyed in the next section, a running example application based on the ``VIP escort'' problem~\cite{Humphrey2012-lr} is provided, motivated by the authors' work in unmanned systems. 
    %Consider the ``VIP escort'' problem \cite{Humphrey2012-lr}, which serves as a useful analog for surveillance and reconnaissance operations. 
    An unmanned ground vehicle (UGV) leads a small convoy through a road network monitored by friendly unattended ground sensors (UGS). The road network also contains a hostile pursuer that the UGV is trying to evade while exiting the network as quickly as possible. 
    The pursuer's location is unknown but can be estimated using intermittent data from the UGS, which only sense portions of the network and can produce false alarms. The UGV's decision space involves selecting a sequence of actions (i.e. go straight, turn left, turn right, go back, stay in place). The UGS data, UGV motion, and pursuer behavior are all stochastic, and the problems of decision making and sensing are strongly coupled: some trajectories through the network allow the UGV to localize the pursuer before heading to the exit (but incur a high time penalty); other trajectories afford rapid exit with high pursuer location uncertainty (increasing the risk of getting caught by the pursuer, which can take multiple paths). 
    A human supervisor monitors the UGV during operation. 
    The supervisor does not have detailed knowledge of the UGV -- but can interrogate its actions, modify its decision making stance (`aggressive' vs. `conservative'), and provide extra information about the pursuer (which is sporadically observed and follows an unknown course). %%locations and actions. 
    %%The supervisor could benefit from the UGV's capability to express confidence in its ability to escape given the current sensor information, and work with the AIA to modify behavior or information if necessary. 
    
	\begin{figure}[t]%[htbp]
    	\centering
     	\includegraphics[width=0.5\textwidth]{Figures/RoadNet}
    	\caption{Application example of unmanned ground vehicle (UGV) in a road network, trying to evade a pursuer, using information from unmanned ground sensors (UGSs), as well as information from a human supervisor.} %The operator's actions towards the UGV are trust-based.}
        \label{fig:RoadNet}
    \end{figure}

%%\nisarcomm{for me todo: garnish with some pics of UGV/operator? should UGV being doing something else like gathering intel from UGS, etc.? what makes this task so dull, dirty, dangerous?}

    One way to construct an autonomous UGV path planner is to discretize time and spatial variables to build a partially observable Markov decision process (POMDP) model \cite{Kochenderfer2015-uu} of the navigation task. 
    The ideal POMDP solution is an optimal UGV action selection policy that will, \emph{on average}, maximize some utility function whose optimum value coincides with desirable UGV behaviors (i.e. avoiding the pursuer and exiting quickly). 
    POMDP policies can be found by any number of sophisticated approximations that operate on probability distributions for the unknown pursuer state, which in turn can be found via Bayesian sensor fusion \cite{Ahmed2017-ph}. 
    This defines at least two AIA capabilities per Fig. \ref{fig:AIcapabilities}: knowledge representation and planning \footnote{consideration of lower-level UGV state estimation and control also leads to perception and motor control/execution.}. 
    The trust-cycle terms here can then be defined as follows relative to the supervisor (user): \textit{AIA:} the combined POMDP planning and data fusion agent, which must make decisions under uncertainty; %(the pursuer is only observed sporadically and follows an unknown course); 
    \textit{Trust:} the supervisor's willingness to rely on the UGV's planning and data fusion algorithms when the safety of the VIP being escorted is at stake;  
    \textit{TRBs:} supervisor's behaviors that indicate trust (or lack thereof) in the UGV's planner; these include approving/rejecting the planner's actions, or real-time adjustments of the data fusion output based on what the supervisor receives from other intelligence sources; \textit{Assurances:} properties and behaviors of the planning agent that effect the supervisor's trust, e.g. communication of the escape success probability, reports that unexpected UGS data have been registered, or explanations of actions taken. 
