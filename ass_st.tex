\emph{Source-Target Classification:}
It is convenient to refer to assurances by way of their source and target. More specifically their source AIA capability (see Figure \ref{fig:AIcapabilities}) and their target user trust dimension (see figure \ref{fig:Assurance_classes}). For example human-like gestures could be considered a `motion-predictability' assurance. Even though, in practice, it is not easy to clearly separate AIA capabilities or trust dimensions due to the inherent cross-over in many cases. In retrospect, the reader may be able to identify work from \cite{Dragan2013-wd} as a `motion-predictability' assurance. \citet{Wang2016-id} considered `perception-competence', and `planning-predictability' assurances (among others). Meanwhile, \citet{Aitken2016-fb} considered a large set of assurances that span several source capabilities, and target trust dimensions. 

This classification is useful because different classes of algorithms will likely present themselves as useful for assurances whose target is `predictability' for example, than those whose target is `situational normality'. It is not immediately clear what the landscape of assurances from that perspective. Future research might begin by looking for holes in a certain source-target pair. For example, are there satisfactory assurances for the `learning-situational-normality' source-target pair? Or can assurance $x$ for `perception-competence' also be applied to `learning-competence'? Finally, are there certain classes of algorithms that are suited for communicating to the `predictability' dimension of trust, and can they be adapted from one AIA capability to another?
