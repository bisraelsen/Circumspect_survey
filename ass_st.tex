\emph{Source-Target Classification:}
It would be especially convenient for designers of AIAs to be able to refer to assurances by way of their source AIA capability (see Figure \ref{fig:AIcapabilities}) and their target user trust dimension (see Figure \ref{fig:Assurance_classes}). 
For example, human-like gestures could be considered a `motion-predictability' assurance. For instance, the reader may be able, in retrospect, to identify work from \cite{Dragan2013-wd} as a `motion-predictability' assurance, while the work by \citet{Wang2016-id} could be considered to describe `perception-competence' and `planning-predictability' assurances (among others). Meanwhile, \citet{Aitken2016-fb} considered a large set of assurances that span several source capabilities, and target trust dimensions. %In practice, 
However, it is not always easy to clearly separate AIA capabilities or trust dimensions due to the inherent cross-over.  %in many cases. 

Still, such classifications are useful because different classes of algorithms will likely present themselves as useful in applications for which assurances must target `predictability' dimensions of trust, for example, as opposed to `situational normality'. 
Given the inherent difficulty of precisely modeling and measuring trust, it is not immediately clear how such mappings can be precisely delineated.  
Future research might begin by looking for missing correspondences or correlations in the literature for notional capability source-trust target pairs. For example, have satisfactory assurances been developed for the `learning-situational-normality' source-target pair? Or, to what extent (if any) can assurance $x$ for `perception-competence' also be applied to `learning-competence'? Finally, are there certain classes of algorithms that are suited for communicating to the `predictability' dimension of trust, and can they be adapted from one AIA capability to another?
