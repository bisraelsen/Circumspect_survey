\subsubsection{Source-Target Classification}
It is convenient to refer to assurances by way of their source and target. More specifically their source AIA behavior (see Figure \ref{fig:AIcapabilities}) and their user trust target component (see figure \ref{fig:Assurance_classes}). Intuitively, there may be a set of different algorithms that are useful for making assurances that convey information about planning to the competence dimension of the user's trust. It is easier to refer to these assurances in terms of their source and target. So, for this example that class of algorithms would be the `planning-competence' class.

Not only is the source-target notation useful shorthand for communicating about the purpose of the assurance, but it is useful in classifying the range of assurance algorithms that exist. There may also be a class of algorithms that span multiple source-target capabilities. For example there may be a kind of algorithm that can give a `learning-competence' assurance, as well as a `planning-competence' assurance.

In retrospect the reader may be able to identify work like \citet{Dragan2013-wd} as a `motion-predictability' assurance, or \cite{Wang2016-id} considered `perception-competence', and `planning-predictability' assurances (among others). \citet{Aitken2016-fb} considered a large set of assurances that span several source capabilities, and target trust dimensions.

Future research might begin by looking for holes in a certain source-target pair. For example, are there satisfactory assurances for the learning-situational normality source-target? Or can assurance $x$ for perception-competence also be applied to learning-competence?
