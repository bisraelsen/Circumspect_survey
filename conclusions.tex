The issues of user trust in AIAs and appropriate deployment/use of AIAs have become very prominent.  Assurances are the method by which AIAs can influence humans to trust and (more importantly) \emph{use} them appropriately. We have presented here a definition, case for, and survey of algorithmic assurances in the context of human-AIA trust relationships. A formal treatment is necessary because the ecosystem of AIAs evolving more rapidly than ever before; consequently, previous informal approaches to designing algorithmic assurances are insufficient. 

This survey was performed, to some extent, from a standpoint of designing intelligent unmanned vehicle systems that must work in concert with a human supervisor. However, the theoretical framework and categorization of assurances is meant to be generally applicable to a broad range of AIAs. A major motivation for this survey was the observation that there are many researchers in different but related domains such as human factors, robotics, machine learning, artificial intelligence, and others who are (unknowingly) working along different parts of the same human-AIA assurance spectrum. It is important for members of each community to recognize this, so that research efforts can be  methodically organized to answer related open questions in this important area. Assurances have historically been ignored from a practical standpoint, and are the least understood component of human-AIA trust relationships. There have been many researchers who have recognized the concepts behind assurances, but no detailed definitions have been given until now.

There are three main contributions from this work: 1) we have drawn from multiple bodies of research in order to fill in the missing details for the human-AIA trust cycle (Fig.~\ref{fig:SimpleTrust_one_way}) and to formally define assurances within this cycle; 2) we present a classification of assurances in Sec.~\ref{sec:assurances}; 3) we identify an `assurance integration continuum' shown in Fig.~\ref{fig:assurance_continuum}. On that continuum seven different classes of algorithms were identified. Practitioners can use these classes to select and design assurances for AIAs. Given the material provided herein, those who design assurances should have the tools required to approach design and future research from a solid theoretical foundation.

A final important and sobering takeaway is that there is not a single `silver bullet' algorithmic assurance that will perform the best in all situations. 
Given enough time, it is quite possible that highly specialized assurances could be designed for many situations. Even so, we warn that, for the research and design of assurances to be sustainable in the current environment of fast-paced development of new technology, it is important to consider approaches that are as principally grounded as possible, in order to be more easily used with yet-to-be-invented methods for implementing various AIA capabilities. We have identified many future opportunities for research on AIA assurance design and their influence on human trust, and hope researchers will begin looking outside of their own disciplines to discover, design and formally test new tools and ideas for assurance design and implementation. The framework presented here should unify research efforts by providing a common taxonomy in relation to human-AIA trust relationships. We believe it will help researchers see the field from a larger perspective, classify the type of research they are performing, and consider the greater implications of their work. The field of algorithmic assurances has an abundance of avenues for new and challenging research, and we encourage researchers to pursue them.
