\section{Conclusions}\label{sec:conclusions}
    Now, more than ever, there is a great need for humans to be able to trust the AIAs that we are creating. Assurances are the method by which AIAs can encourage humans to trust them appropriately, and to then use them appropriately. We have presented here a definition, case for, and survey of assurances in the context of human-AIA trust relationships. These assurances have been classified according to different properties.
    
    The survey was performed, to some extent, from a standpoint of designing an unmanned ground vehicle that is working in concert with a human supervisor. However, the theoretical framework, and classification of assurances is meant to be general in order to apply to a broad range of AIAs.

    While the basic definition of assurances (i.e. feedback to user trust, in the human-AIA trust cycle) is simple from a theoretical standpoint, the exercise of gathering related literature helped to illuminate some important considerations and details regarding the design of assurances. These include the observations that assurances can be implicit or explicit, component or composite, tutoring or telling, and that they can be classified by their source AIA capability and target user trust dimension. We also see that it is critical to take human perception and cognition into account when designing assurances.

    From the survey we learned that much of the formal research has focused on implicit properties of AIAs, which were not designed to affect trust, but that are a property of the AIA itself. We learn that trust can be measured by user's actions, which we call TRBs, or by self-reported measures in the form of questionnaires. Due to the inherent differences between people, TRBs are the more quantitative way to measure the effects of assurances, in some sense an AIA should care more about appropriate TRBs than appropriate trust as measuring trust is quite subjective.

    In view of this we propose that, when possible, TRBs should be calibrated instead of trust. This is due to the nature of subjective surveys and human psychology, which is that a human might rate their trust higher, but act no differently (as discussed in \cite{Dzindolet2003-ts}).

    There is sometimes an misconception that AIAs should be more `human-like' to be more trustworthy, but we have seen evidence that suggests that this is dependent on the situation. It is common, in order to have more interpretable models, to use a simple global model, and a more complex local model when accuracy is needed. There are many different situations in which assurances must be used (i.e. make models, evidence, or reasoning more predictable, or to show competence, or to display or communicate assurances in certain ways), and so there will never be a single perfect assurance that performs the best in all situations.

    \input{distrust.tex}

    There is a fairly large body of research that is focused, in some way, on influencing trust in human-AIA relationships. However, there is probably a larger portion of research that deals with techniques that would be useful in designing assurances, but that has not directly or consciously considered trust as a formal design goal. Research from these areas (such as V\&V, active learning, and safety) should provide a rich collection of methods to be studied and formally applied to human-AIA trust relationships.

\newpage

