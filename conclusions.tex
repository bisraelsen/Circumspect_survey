\section{Discussion}\label{sec:discussion} 
    After having investigated some of the literature there are several thoughts about assurances that can be made more concrete. Below we revisit the classification of assurances.
    \input{ass_conclusions.tex}

\section{Conclusions}\label{sec:conclusions}
    Now, more than ever, there is a great need for humans to be able to trust the AIAs that we are creating. Assurances are the method by which AIAs can encourage humans to trust them appropriately, and to then use them appropriately. We have presented here a definition, case for, and survey of assurances in the context of human-AIA trust relationships. These assurances have been classified according to different properties.
    
    The survey was performed from a standpoint of designing an unmanned ground vehicle that is working in concert with a human supervisor. However, the theoretical framework, and classification of assurances is meant to be general in order to apply to a broad range of AIAs.

    We propose that, when possible, TRBs should be calibrated instead of trust. This is due to the nature of subjective surveys and human psychology, which is that a human might rate their trust higher, but act no differently (as discussed in \cite{Dzindolet2003-ts}). 

    Generally opportunities for future research include performing experiments to test methods from \hyperref{Quadrant III.}{sec:q3} and identifying their effects. Furthermore, the fields of Validation and Verification, Representation Learning, Active Learning, Safety, Empirical Performance Prediction, and Model checking seem to be ripe for use in explicit assurances.

    \input{distrust.tex}

\newpage

