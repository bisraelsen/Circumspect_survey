\section{Conclusions}\label{sec:conclusions}
    Now, more than ever, there is a great need for humans to be able to trust the AIAs that we are creating. Assurances are the method by which AIAs can encourage humans to trust them appropriately, and to then use them appropriately. We have presented here a definition, case for, and survey of assurances in the context of human-AIA trust relationships.
    
    This survey was performed, to some extent, from a standpoint of designing an unmanned ground vehicle that is working in concert with a human supervisor. However, the theoretical framework, and classification of assurances is meant to be general in order to apply to a broad range of AIAs. One of the main motivations of this survey was the insight that there is an extremely large community of researchers working on human-AIA assurances (perhaps unknowingly). It is important to recognize this so that we can start to organize our efforts and begin methodically answering the open questions of this important field. Arguably, the ultimate goal is to develop a sufficient set of assurances that can be located in Quadrant III (i.e. those that have been designed and experimentally verified), of course this requires cooperation among the community.

    The most surprising insight from compiling this survey was the absence of a detailed definition and classification of assurances. Assurances have been, by far, the most ignored component of human-AIA trust relationships. There have been many researchers who have recognized the existence of assurances (usually by other names), but there has been no detailed definition until now. We have drawn from multiple bodies of research in order to `fill in' the details of the human-AIA trust cycle (Figure~\ref{fig:SimpleTrust_one_way}) (a novel contribution in itself). This led to our main contribution: to formally define and classify assurances within the human-AIA trust cycle. We have introduced the idea that assurances must stem from different AIA capabilities, that they can be implicit or explicit, that the way in which they are expressed to a user can be just as important as how they are calculated/designed. We have also highlighted that methods for measuring the effects of assurances must also exist. And, we have shown how each of these fits into the larger trust cycle.
    
    There is a fairly large body of research that is focused, in some way, on influencing trust in human-AIA relationships. However, there is a larger portion of research that deals with techniques that would be useful in designing assurances but that, to date, has not directly or knowingly considered affecting human-AIA trust through assurances as a formal design goal. Research from these areas (such as V\&V, active learning, and safety) should provide a rich collection of methods to be studied and formally applied to human-AIA trust relationships.

    While the basic definition of assurances (i.e. feedback to user trust, in the human-AIA trust cycle) is simple from a theoretical standpoint, the exercise of gathering related literature helped to illuminate some important considerations and details regarding the design of assurances. In Section~\ref{sec:synthesis} we present \ref{fig:refined_assurances} which is an original synthesis of the surveyed literature. We show that designers must be able to design ways for an AIA to calculate, design, and plan explicit assurances. Designers must also account for the expression and perception of assurances, this involves considering how effective a given method/medium might be in conveying an assurance to a user. Finally, the whole purpose of assurances is lost if there is no way to measure/quantify the effects of the assurances; effort must be spent in creating appropriate metrics and experiments by which to do this.

    A sobering reminder is that there is not a single assurance that will perform the best in all situations. It is almost certain that given time highly specialized assurances can be designed for many situations. Even so we warn that, for the research and design of assurances to be sustainable in the current environment of fast-paced development of new technology, it is important to consider approaches that are as general as possible in order to be more easily used with newly developed methods for implementing AIA capabilities.

    The treatment of assurances in this survey are based, in part, on a model of trust. For completeness it is important to mention distrust. As reviewed and discussed by \citet{Lewicki1998-ox}, and formalized in \cite{McKnight2001-hm,McKnight2001-gz}. Low trust is not the same as distrust, neither is low distrust the same as trust. \citet{McKnight2001-gz} suggest that ``the emotional intensity of distrust distinguishes it from trust'', and they explain that distrust comes from emotions like: wariness, caution, and fear. Whereas, trust stems from emotions like: hope, safety, and confidence. Trust and distrust are orthogonal elements that define a person's TRB towards a trustee. In this survey distrust was not considered, however it must be made clear that any \emph{complete} treatment of trust relationships, and for our purposes, designed assurances, must consider the dimensions of distrust as well as those of trust. For now, this investigation is left as an avenue for future research.

    We hope that researchers can begin to reach across perceived lines in order to find more tools to appropriately design and test assurances. We hope that the material is Section~\ref{sec:background} will provide a common foundation on which researchers from all quadrants can build on in order to unify research efforts. More specifically, we hope that those in the fields of CS, ML, and AI can begin to use the principles outlined in this survey to help guide their search for more `interpretable', `explainable', and `comprehensible' systems. It is important for them to understand the existence of a trust model, and the human-AIA trust cycle. If they consider these points, they will be able to identify better methods, and design more effective assurances.
    
    Likewise, those who formally consider trust (i.e. researchers in HRI, e-commerce, UI) should now be able to identify more methods and approaches for designing assurances, so that they can perform experiments to validate them. They will also have a better idea of what kinds of experiments have been performed, and possible new areas to investigate.

    We have identified many opportunities for further research in how AIAs can influence human trust through assurances. The framework found herein will help other researchers to see the field from a larger perspective, to classify the type of research they are performing, and help them to consider the greater implications of their work.
