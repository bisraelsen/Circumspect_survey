\section{Conclusions}\label{sec:conclusions}
%%    \edit{discuss licensing ``muse of the possibility of licensing intelligent autonomous agents via `soft' assurances, in the same way we license human drivers and pilots''.}
    %Now, more than ever, there is a great need for humans to be able to trust the AIAs that we are creating. 
    The issues of human user trust in AIAs and appropriate deployment/use of AIAs have become very prominent. 
    Assurances are the method by which AIAs can encourage humans to trust and (more importantly) use them appropriately. 
    We have presented here a definition, case for, and survey of assurances in the context of human-AIA trust relationships. 
    
This survey was performed, to some extent, from a standpoint of unmanned vehicle systems that must work in concert with a human supervisor. 
However, the theoretical framework and classification of assurances is meant to be general, in order to apply to a broad range of AIAs. 
One of the main motivations of this survey was the insight that there is an extremely large community of researchers working on human-AIA assurances (perhaps unknowingly). 
It is important to recognize this, so that researchers can organize their efforts and begin to methodically answer related open questions in this important area. 
Arguably, a key goal is to develop a sufficient set of assurances that can be located in Quadrant III (i.e. those that have been designed and experimentally verified); this requires cooperation and cross-pollination of ideas among the different research communities mentioned here.

The most surprising insight from this survey was the absence of a detailed definition and classification of assurances. 
Assurances have been, by far, the most ignored component of human-AIA trust relationships. 
There have been many researchers who have recognized the existence of assurances (usually by other names, e.g. in the e-commerce literature), but no detailed definition has been given until now. 
As our main novel contribution, we have drawn from multiple bodies of research in order to fill in the missing details for the human-AIA trust cycle (Fig.~\ref{fig:SimpleTrust_one_way}) and to formally define and classify assurances within this cycle. 
We have also formally introduced and discussed several other key aspects of assurances, namely: the idea that assurances must stem from different AIA capabilities (Figure \ref{fig:AIcapabilities}); that assurances can be implicit or explicit; and that the way in which assurances are expressed to a user can be just as important as how they are calculated/designed. 
We have also highlighted that methods for measuring the effects of assurances must also exist. 
Finally, we have discussed how each of these aspects of assurances fits into the larger trust cycle. %% (Fig. \ref{fig:refined_assurances}).
    
    Another key takeaway from this work is that there is a fairly large body of research that is focused, in some way, on influencing trust in human-AIA relationships. However, there is a larger portion of research that deals with techniques that would be useful in designing assurances, but that (to date) has not directly/knowingly considered human-AIA trust effects as a formal design goal. 
    Research from these areas (such as V\&V, active learning, safe learning, etc.) should provide a rich collection of methods to be studied and formally applied to human-AIA trust relationships.
Furthermore, while the basic definition of assurances (i.e. feedback to user trust, in the human-AIA trust cycle) is simple from a theoretical standpoint, the exercise of gathering related literature helped to illuminate some important considerations and details regarding the design of assurances. In Section~\ref{sec:synthesis} we present Figure~\ref{fig:refined_assurances}, which is an original synthesis of the surveyed literature. We show that designers must be able to design ways for an AIA to calculate, design, and plan explicit assurances. Designers must also account for the expression and perception of assurances, which involves considering how effective a given method/medium might be in conveying an assurance to a user. 
Finally, the whole purpose of assurances is lost if there is no way to measure/quantify the effects of the assurances; effort must be spent in creating appropriate metrics and experiments to do this. A sobering reminder is that there is not a single assurance that will perform the best in all situations. It is almost certain that (given enough time) highly specialized assurances can be designed for many situations. Even so, we warn that, for the research and design of assurances to be sustainable in the current environment of fast-paced development of new technology, it is important to consider approaches that are as general as possible, in order to be more easily used with newly developed methods for implementing AIA capabilities.

    The treatment of assurances in this survey is based, in part, on a model of interpersonal trust. For completeness it is important to mention the concept of \textit{distrust}, as reviewed and discussed by \citet{Lewicki1998-ox}, and formalized in \citet{McKnight2001-hm,McKnight2001-gz}. Low trust is not the same as distrust, and low distrust is not the same as trust. \citet{McKnight2001-gz} suggest that ``the emotional intensity of distrust distinguishes it from trust'', and they explain that distrust comes from emotions like: wariness, caution, and fear. Whereas, trust stems from emotions like: hope, safety, and confidence. Trust and distrust are orthogonal elements that define a person's TRB towards a trustee. In this survey, distrust was not considered. However it must be made clear that any \emph{complete} treatment of trust relationships, and for our purposes, designed assurances, must consider the dimensions of distrust as well as those of trust. For now, this is left as an avenue for future research (one yet to be picked up by mainstream AIA researchers).

We have identified many future opportunities for research on AIA assurance design and their influence on human trust, and hope researchers will being reaching outside their own disciplines to discover, design and formally test new tools and ideas for assurance design and implementation.  
We hope that the material in Section~\ref{sec:background} will provide a common foundation on which researchers from all quadrants can build on, in order to unify research efforts. 
More specifically, we hope that those in the relevant technical fields of machine learning, AI, robotics, and autonomous/unmanned systems can begin to use the principles outlined in this survey to help guide their search for more trustworthy systems that are `interpretable', `explainable', and `comprehensible'. 
It is important for researchers in these diverse technical fields to understand the existence of formal user trust models and the human-AIA trust cycle, in order to identify better methods and design more effective assurances. 
Likewise, those who formally consider human trust (i.e. researchers in HRI, e-commerce, UI, ergonomics, human factors) should now be able to identify more formal techniques for designing and expressing assurances in AIAs, so that they can perform experiments to validate their effects on trust, and explore possible new research questions. 
The framework herein will help researchers see the field from a larger perspective, classify the type of research they are performing, and consider the greater implications of their work.
