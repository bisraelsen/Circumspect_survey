\subsection{AIA Self-Assessment} \label{sec:aia_self_assessment}

\subsubsection{Common Approaches:}
The literature in this category can be split into two high-level categories. The first approach is for the AIA to quantify the uncertainty it is dealing with (i.e. uncertainty in its models of its world, sensors, and capabilities). The second is to attempt to reduce its complex, `uninterpretable', capabilities into more interpretable ones.

\paragraph{Quantify Uncertainty} \label{sec:QU}
talk about these \citet{Wu2012-qi, Chen2018-xq, Choi2017-th, Kahn2017-vy, Peterson2017-dd, Kendall2017-ry, Zhang2014-he, Churchill2015-ei, Paul2011-vr, Grimmett2013-gj, Triebel2013-ku, Triebel2013-ow, Triebel2016-kj, Berczi2015-rd, Grimmett2016-yc, Dequaire2016-kh, Gurau2016-hs, Kuter2015-qh, Aitken2016-cv, MacKay1992-sp, Zagorecki2015-qy, Hutchins2015-if, Laskey1991-mf, Kaipa2015-hy, Habbema1976-xd}

Although active learning does not explicitly consider safety, the underlying approaches can be useful because active learners need to be able to search the problem space to reduce uncertainty; this requires an internal representation of uncertainty. The applications surveyed here are all mainly related to image classification and robotics. In the context of image classification, \citet{Paul2011-vr} introduced `perplexity' as a metric that represents uncertainty in predicting a single class and is used to select the `most perplexing' images for further learning. There have also been several attempts to use Gaussian processes (GPs) to actively learn and assign probabilistic classifications \cite{MacKay1992-sp,Triebel2016-kj,Triebel2013-ow,Triebel2013-ku,Grimmett2013-gj,Grimmett2016-yc,Berczi2015-rd,Dequaire2016-kh}. As with perplexity-based classifiers, the key insight is that if a classifier possesses a measure of uncertainty, then that uncertainty can be used for efficient instance searching, comparison, and learning, as well as reporting a measure of confidence to users. The key property of GPs to this end is their ability to produce output confidence/uncertainty estimates that grow more uncertain away from the training data. This information can be readily assessed and conveyed to users, even in high-dimensional problems. This property has also found much use in other AIA active learning problems, e.g. Bayesian optimization \cite{Snoek2012-tt, Brochu2010-tj,Israelsen2017-zb}. 

An AIA that can predict its performance on different tasks can provide assurances about competence, predictability, and the situational normality of a given task. Several authors have worked to improve this ability in visual classification \cite{Zhang2014-he,Gurau2016-hs,Churchill2015-ei,Kaipa2015-hy}. 
For example, to ensure that visual classifiers don't fail silently in novel scenarios, 
\citet{Zhang2014-he} learned models of errors on training images to predict errors on test images. 
\citet{Kaipa2015-hy} consider 3D visual classification of assembly line parts for robotic pick and place tasks, and develop statistical goodness-of-fit tests to estimate the likelihood that robots can use their sensors to find parts matching desired ones. %To accomplish this they apply the `Iterative Closest Point' (ICP) method, to match a point cloud measurement of the part with a ground-truth 3D model of the part. 
These approaches allow the AIA to assess capability and present appropriate assurances to users, though without any formal notions of trust. 

Models and logic are not trustworthy by themselves; they may be flawed to begin with, or become invalid when certain assumptions or specifications are violated. Thus, there is great interest in providing assurances that the models and assumptions underlying different AIA processes are in fact sound. \citet{Laskey1991-mf} -- with the intention of communicating model validity to users of `probability-based decision aids' -- notes that it is infeasible to perform a decision-theoretic calculation to determine if model revision is necessary. 
She presents a class of theoretically justified model revision indicators which are based on the idea of constructing a computationally simple alternate model and then initiating model revision if the likelihood ratio of alternate model becomes too large (see also \citet{Zagorecki2015-qy,Habbema1976-xd} --these ideas also provide a potential basis for the `model validity' machine self-confidence factors from Quadrant II).
\citet{Ghosh2016-dl}  present `model repair' and `data repair' strategies that can be used when the current model doesn't match the observed data, at which point the model and data can be repaired, and control actions can be replanned in order to conform with the formal method specifications. One challenge is how the `trustable' constraints should be identified, as this places a strong burden on the certifying authorities and system designer to foresee all possible failures.

\paragraph{Reduce Complexity} \label{sec:reduce_complexity}
talk about these: \citet{Liu2017-xw, Strumbelj2018-ou, Pynadath2018-ck, Abdollahi2018-uw, Robnik-Sikonja2018-jz, Browne2018-me, Huang2017-lk, Wang2018-br, Hayes2017-nt, Huang2017-zt, Olah2018-rp, Kuhn1997-qc, Rouse1986-dz, Swartout1983-ko, Wang2016-id, Kaniarasu2013-ho, Wallace2001-fm, Aitken2016-cv, Lacave2002-cu, Ribeiro2016-uc}

Other assurances for machine learning algorithms are inspired by the well-known trade-off between accuracy and interpretability, whereby improving the accuracy of a learner generally reduces its interpretability. \citet{Ruping2006-xj} asks how classification results and the accuracy-interpretability trade-off can be made more transparent to those who design and use classifiers, by combining simpler global models with more complex local models that are built around learning results (\citet{Otte2013-oo} and \citet{Ribeiro2016-uc} implement similar ideas as well). 

\subsubsection{Grounding Example:}
In the case of the `VIP Escort' problem (described in Section~\ref{sec:mot_example}), value alignment might be used as an assurance in the following way:

We make the following assumptions

\begin{itemize}
    \item The UGV has just begun an attempt to escape the road-network
    \item 
    \item 
\end{itemize}

\paragraph{\textbf{Discussion of Example:}} 
