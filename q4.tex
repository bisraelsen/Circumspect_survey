\subsection{Quadrant IV. (Implicit Assurances, Informal Trust Treatment)}\label{sec:q4}
    The work in this section is easily distinguished from that in Quadrant I because it does not discuss trust in any way. However, it is only subtly different from that in Quadrant III, which explicitly focused on things like interpretability and explanation, according to the authors of those works. Conversely, the research found in this quadrant is only related to trust and assurances by those who are familiar with the underlying concepts in this paper. This research is created with the intent of making the AIAs inherently more safe, better aware of reasoning processes, and better at performing/understanding tasks, the world, and/or self representations in some way, \textit{without the express intention of communicate these improvements or properties to users}. 
%This research group thus features design ideas and concepts that can be exploited deliberately to create trustworthy AIAs. 
Here are found the researchers who created AIAs with properties, like reliability, that can then be investigated by those who formally acknowledge human-AIA trust in Quadrants I and II. 
In other words, these are the methods can be turned into explicit assurances by designers who intentionally do so. 

One very promising area is research regarding safety and learning under nonstationarity. While a fairly high-level treatment, \citet{Amodei2016-xi} are concerned with `AI safety', which is in essence: how to make sure that there is no ``unintended and harmful behavior that [emerges] from machine learning systems''. Given this definition, much of the paper discusses concepts that are critical to AIA assurances. Among the more directly applicable topics in the scope of this paper are: safe exploration (how to learn in a safe manner), robustness to distributional shift (a difference between training data and test data) and design/learning of appropriate objective functions \cite{Sugiyama2013-ci,Quinonero-Candela2009-fj,Hadfield-Menell2016-ws,Da_Veiga2012-gh,Garcia2015-rs}. %To restate more concretely, there is a need to design objective functions that more accurately reflect the true objective function. 
A popular example (roughly summarized here) from \citet{Bostrom2014-fz} is a robot that has an objective of making paper clips, it then decides to take over the world in order to maximize its resources and ability to make more paper clips; this highlights the point that sometimes `simple' objective functions can result in unintended/unsafe behaviors. 

Another promising direction is safe reinforcement learning (safe RL), which considers reinforcement learning is environments where failure is extremely costly, such as when using an expensive aerospace vehicle. 
Safe RL is a particularly important area that requires assurances, as the systems are designed specifically to evolve without supervision. \citet{Garcia2015-rs} perform a survey about safe RL highlight two main methods: 1) modification of the optimality criterion with a safety factor, and 2) modification of the exploration process through the incorporation of external knowledge. They present a useful hierarchy of approaches and implementations. 
As one example, \citet{Lipton2016-dq} design an `intrinsic fear' RL approach that uses a deep Q-network (DQN) and a `supervised danger model'. The danger model stores the likelihood of entering a catastrophe state within a `short number of steps'. This model can be learned by detecting catastrophes through experience and can be improved over time. \citet{Curran2016-ij}, in a more specific application, asks how a robot can learn when a task is too risky, and then avoid those situations, or ask for help. To do this, they use a hierarchical POMDP planner that explicitly represents failures as additional state-action pairs. %%In this way, the resulting policy can be averse to risk.

    Although active learning does not intrinsically consider safety, the underlying approaches can be useful because active learners need to be able to search the problem space to reduce uncertainty; this requires an internal representation of uncertainty. The applications surveyed here are all mainly related to image classification and robotics. In the context of image classification, \citet{Paul2011-vr} introduced `perplexity' as a metric that represents uncertainty in predicting a single class and is used to select the `most perplexing' images for further learning. 
There have also been several attempts to use Gaussian processes (GPs) to actively learn and assign probabilistic classifications  \cite{MacKay1992-sp,Triebel2016-kj,Triebel2013-ow,Triebel2013-ku,Grimmett2013-gj,Grimmett2016-yc,Berczi2015-rd,Dequaire2016-kh}. 
 As with perplexity-based classifiers, the key insight is that if a classifier possesses a measure of uncertainty, then that uncertainty can be used for efficient instance searching, comparison, and learning, as well as reporting a measure of confidence to users. The key property of GPs to this end is their ability to produce output confidence/uncertainty estimates that grow more uncertain away from the training data. %%%, so that the AIA `knows what it doesn't know'. -- i'm starting to think that this is not quite an accurate way to phrase this...a little too colloquial for a technical discussion...%%%
This information can be readily assessed and conveyed to users, even in high-dimensional problems. This property has also found much use in other AIA active learning problems, e.g. Bayesian optimization \cite{Snoek2012-tt, Brochu2010-tj,Israelsen2017-zb}. 

Learning of human-understandable representations for data and feature selection also provides another avenue for developing assurances  \cite{Bengio2013-uv, Guyon2003-fj}. From some of the discussion of interpretable models in section \ref{sec:q3}, we find that representation is important for developing interpretable models. 
For instance, \citet{Mikolov2013-lt} studied how to represent words and phrases in a vector space for natural language text learning; this enables simple vector operations for understanding word sense similarity and relative relationships learned from text corpora. For example the vector addition operation $airlines+German$ yields similar entries that include $Lufthansa$. Such representations encodes knowledge that can be easily checked and understood by humans, and thus implicitly facilitate interaction and calibration of trust (see \cite{Haury2011-zi} for another example). 
The problem of discovering human understandable features and representations in more general settings still remains an open question. Currently, the main question for representation learning is how to find the `best representations' for a particular application -- not necessarily the representations and features that are `most humanly understandable'. This is not surprising, since human-understandable representations and features are not necessarily optimal for the criteria that AIAs are typically designed against. 
%%to be optimal using other objective functions (which are arguably more appropriate, if humans don't need to understand what is going on). 

% \subsubsection{Summary}
% %%nra: This section is short enough that the bullet list is not really needed, especially in light of the fact that this is NOT an exhaustive set of topics to consider...so being brief here is fine...
% %     Several promising fields of research focus on implicit assurances and informal treatments of trust, which we believe will contribute to the development of explicit assurances from AIAs to humans:
% %     \begin{itemize}
% %         \item Safety, learning under non-stationarity, and risk averse learning -- the methods applied to address these challenging problems can be used for creating assurances.
% %         \item Active learning -- AIAs that are able to actively learn are able to assess holes in their capabilities. The ability to assess these limitations can be used to create better assurances.
% %         \item Representation learning and feature selection -- learning representations and selecting features that are inherently meaningful to humans will help improve the ability for AIAs to convey assurances to human users.
% %     \end{itemize}
% %%
% Several promising fields of research focus on implicit assurances and informal treatments of trust, which we believe will contribute to the development of explicit assurances from AIAs to humans. The literature surveyed in this section is \emph{not} exhaustive, nor could it reasonably ever claim to be. One key takeaway point is that, in every application where designers want to ensure reliable and correct application of an AIA (an informal acknowledgment of the importance of user trust), implicit assurances at a minimum are available which can be intentionally converted into explicit assurances by the designer. 
% The works selected here are a subset aligned with the author's interests in unmanned vehicles. There are doubtless many other topic areas that can be considered to enable the design of more advanced and capable explicit assurances. 