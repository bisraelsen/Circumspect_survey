\subsection{Quadrant IV. (Implicit Assurances, Informal Trust Treatment)}\label{sec:q4}
    The work in this section is easily distinguished from that in Quadrant I because it does not discuss trust in any way. However, it is only subtly different from that in Quadrant III. The research in Quadrant III is explicitly focused on things like interpretability and explanation, according to the authors of those works. Conversely, the research found in this quadrant is only related to trust and assurances by those who are familiar with the underlying concepts in this paper. This research is created with the intent of making the AIAs: inherently more safe; aware of reasoning processes; and possess better task, world, and/or self representations in some way, without intending to communicate these properties to users. 
%This research group thus features design ideas and concepts that can be exploited deliberately to create trustworthy AIAs. 
Here are found the researchers who created AIAs with properties, like reliability, that can then be investigated by those who formally acknowledge human-AIA trust in Quadrants I and II. In other words, these are the methods can be turned into explicit assurances by designers who intentionally do so. 

One very promising area is research regarding safety and learning under nonstationarity. While a fairly high-level treatment, \citet{Amodei2016-xi} are concerned with `AI safety', which is in essence: how to make sure that there is no ``unintended and harmful behavior that [emerges] from machine learning systems''. Given this definition, much of the paper discusses concepts that are critical to AIA assurances. Among the more directly applicable topics in the scope of this paper are: safe exploration (how to learn in a safe manner), and robustness to distributional shift (a difference between training data and test data). They also discuss designing objective functions that are more appropriate. %To restate more concretely, there is a need to design objective functions that more accurately reflect the true objective function. 
A popular example (roughly summarized here) from \citet{Bostrom2014-fz} is a robot that has an objective of making paper clips, it then decides to take over the world in order to maximize its resources and ability to make more paper clips. This highlights the point that sometimes over-simplistic objective functions can result in unintended and unsafe behaviors. There are several different researchers who have investigated these ideas (for example see \cite{Sugiyama2013-ci,Quinonero-Candela2009-fj,Hadfield-Menell2016-ws,Da_Veiga2012-gh,Garcia2015-rs}.

Another promising direction is safe reinforcement learning (safe RL), which considers reinforcement learning is environments where failure is extremely costly, such as when using an expensive aerospace vehicle. 
Safe RL is a particularly important area that requires assurances, as the systems are designed specifically to evolve without supervision. \citet{Garcia2015-rs} perform a survey about safe reinforcement learning (RL). They state that there are two main methods: 1) modifying the optimality criterion with a safety factor, and 2) modification of the exploration process through the incorporation of external knowledge. They present a useful hierarchy of approaches and implementations that currently exist. 
As one example, \citet{Lipton2016-dq} design a reinforcement learner that uses a deep Q-network (DQN) and a `supervised danger model'. They call their method `intrinsic fear'. Basically, the danger model stores the likelihood of entering a catastrophe state within a `short number of steps'. This model can be learned by detecting catastrophes through experience and can be improved over time. \citet{Curran2016-ij}, in a more specific application, asks how a robot can learn when a task is too risky, and then avoid those situations, or ask for help. To do this, they use a hierarchical POMDP planner that explicitly represents failures as additional state-action pairs. In this way, the resulting policy can be averse to risk.

    Even though active learning does not intrinsically consider safety, the underlying approaches can be useful because active learners need to be able to search environment in order to reduce uncertainty. This means that they have an internal representation of uncertainty. \citet{Paul2011-vr} introduced `perplexity' as it applied to image classification. In this setting, perplexity is a metric that represents uncertainty in predicting a single class. This measure is used to select the most perplexing images for further learning. 
Recently, there have been several papers that attempt to use Gaussian processes (GPs) as a method to actively learn and assign probabilistic classifications (see \citet{MacKay1992-sp,Triebel2016-kj,Triebel2013-ow,Triebel2013-ku,Grimmett2013-gj,Grimmett2016-yc,Berczi2015-rd,Dequaire2016-kh}). The applications surveyed here are all mainly related to image classification and robotics. As with perplexity-based classifiers, the key insight is that if a classifier possesses a measure of uncertainty, then that uncertainty can be used for efficient instance searching, comparison, and learning, as well as reporting a measure of confidence to users. The key property of GPs that makes them attractive for this purpose is their ability to produce confidence/uncertainty estimates that grow more uncertain away from the training data. That is, GPs have the inherent ability to `know what they don't know', and this information can be readily assessed and conveyed to users, even in high-dimensional reasoning problems. This property of GPs has also found great use in other active learning applications for AIAs, such as Bayesian optimization (see \citet{Williams1998-kr}, \citet{Snoek2012-tt}, \citet{Brochu2010-tj}, and \citet{Israelsen2017-zb}).

Another promising field of research is related to learning representations of data and selecting data features. These two topics are surveyed by \citet{Bengio2013-uv} and \citet{Guyon2003-fj} respectively. From some of the discussion of interpretable models in section \ref{sec:q3}, we find that representation is important for developing interpretable models. 
Having appropriate representations (i.e. like the ones humans use and understand) is a large step forward in developing assurances for users. As another example, \citet{Mikolov2013-lt} studied how to represent words and phrases in a vector format for natural language text learning. Using this representation, they are able to perform simple vector operations to understand similar words, and the relative relationships learned. For example the operation $airlines+German$ yields similar entries that include $Lufthansa$. This type of representation encodes information that can be easily checked and understood by humans, and thus implicitly facilitates interaction and calibration of trust (see \cite{Haury2011-zi} for another example). 
The problem of discovering human understandable features and representations in more general settings still remains an open question. Currently, the main question in the representation learning field is how to find the `best representations' for a particular application, not necessarily the representations and features that are `most humanly understandable'. This is not surprising, since human representations and features are not necessarily optimal, and AIAs are being designed to be optimal using other objective functions (which are arguably more appropriate, if humans don't need to understand what is going on). 

\subsubsection{Summary}
    Here we have presented several promising fields of research that focus on implicit assurances and informal treatments of trust, which we believe will contribute to the development of explicit assurances from AIAs to humans.
    \begin{itemize}
        \item Safety, learning under non-stationarity, and risk averse learning -- the methods applied to address these challenging problems can be used for creating assurances.
        \item Active learning -- AIAs that are able to actively learn are able to assess holes in their capabilities. The ability to assess these limitations can be used to create better assurances.
        \item Representation learning and feature selection -- learning representations and selecting features that are inherently meaningful to humans will help improve the ability for AIAs to convey assurances to human users.
    \end{itemize}

The literature surveyed in this section is \emph{not} exhaustive, nor could it reasonably ever claim to be. One key takeaway point is that, in every application where designers want to ensure reliable and correct application of an AIA (an informal acknowledgment of the importance of user trust), implicit assurances at a minimum are available which can be intentionally converted into explicit assurances by the designer. 
The disciplines selected in this paper are a subset that are aligned with the author's interests in unmanned vehicles. There are doubtless many other applicable areas of research, and to this end we hope to have whetted readers' minds in order to promote the design of more advanced and capable explicit assurances.
