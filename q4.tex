\subsection{Quadrant IV. (Implicit Assurances, Informal Trust Treatment)}\label{sec:q4}
This section provides our perspective on some of the research disciplines that are working on techniques that will be critical to designing effective assurances.

\subsubsection{Validation and Verification}
    Validation and Verification (V\&V) typically refers to using formal methods to guarantee the performance of a system within in some set of specifications. Not all practitioners are aware that V\&V provides ways to assure users, arguably they probably have this idea in their mind, but have not considered how to communicate the existence and implications of V\&V to users.

    In relation to cooperative multi-agent systems, \citet{Da_Silva2016-qb} ask how their performance can be verifiable. They suggest a `top-down' and `bottom-up' approach, in which high-level planning happens from the top, and then verifiable motion planning happens from the bottom. In this way, agents move in a verified way, while being able to satisfy a higher level plan. This is similar to work by \citet{Conner2007-uw}, except they directly consider multi-agent systems instead of a single vehicle. This approach is more feasible in application of AIAs in real situations where specifications can break down.

    \citet{Kress-Gazit2009-wf} also consider the top-down and bottom up approach in constructing verified controllers for robots. They uses linear-temporal logic (LTL) to express the task specifications, and then create hybrid controllers (involving high-level discrete robot actions and low-level control inputs of motion controllers) that are `correct-by-construction'. They point out that the computability  is polynomial in state space, but exponential in the input/output space. Regardless, these methods seem to be well suited \emph{when} the environment specification holds (i.e. it conforms to the assumptions made bout it).

    V\&V can be especially challenging in real environments where specifications are frequently violated. \citet{Weng_Wong2014-tj} address this challenge by using `environment characterization' as a runtime verification to ensure there are not violations of the environment. When a violation is detected, they use recovery that can give correct actions, in order to ensure safety and continued progress to the goal if possible. \citet{Nishi2016-zq} proposes something similar in being able to quantify the degree of confidence in the estimated world state, and then calculating confidence limits on planning.


\subsubsection{Safety and Learning under nonstationarity and risk aversion}
    While a fairly high-level treatment, \citet{Amodei2016-xi} are concerned with `AI safety', which is in essence how to make sure that there is no ``unintended and harmful behavior that [emerges] from machine learning systems''. Given this definition, much of the paper discusses concepts that are critical to AIA assurances. Among the more directly applicable topics in the scope of this paper are: safe exploration (how to learn in a safe manner), and robustness to distributional shift (a difference between training data and test data). They also discuss designing objective functions that are more appropriate, in other words there is a need to design objective functions that more accurately reflect the true objective function. A popular example (roughly summarized here) from \citet{Bostrom2014-fz} is a robot that has an objective of making paper clips, it then decides to take over the world in order to maximize its resources and ability to make more paper clips. This highlights the point that sometimes over-simplistic objective functions can result in unintended and unsafe behaviors.

    Generally, stationary data (data whose distribution does not change after training) is assumed in supervised ML. \citet{Sugiyama2013-ci} considers what to do when there is `covariate shift' (or when both training and test data change distribution, but their relation does not change between training and test phases), and `class-balance change' (where the class-prior probabilities are different in training and test phases, but the input distribution of each class does not change). They design and present tools that help diagnose and treat those conditions (this is follow on work for some of what is presented in \citet{Quinonero-Candela2009-fj} where they consider dataset shift). A key approach is to use importance sampling, which involves weighting the training loss by the ratio between the probability of the test data and that of the training data.

    \citet{Hadfield-Menell2016-ws}, in considering an AI safety problem, address the problem of verifying that a robot can be turned off, even when it might have an objective function that indicates that disabling the `off-switch' will reduce the chance of failure. This kind of scenario, or something similar, can easily occur with a sufficiently sophisticated and capable AIA, and a complex enough set of objectives that might result in unintended consequences. They propose that one objective would be for the robot to maximize value for the human, and to not assume that it knows how to perfectly measure that value. 

    \citet{Da_Veiga2012-gh} discuss a form of safety where they are concerned with nonparametric classification and regression with constraints. More specifically, they are concerned about learning Gaussian process (GP) models with inequality constraints, and present a method to do this by using conditional expectations of the truncated multivariate normal distribution (or Tallis formulas). This is not the only work that references learning with constrained GPs. It is also not the only work that considers constrained modeling, but it would take too long to review all of those papers. The main claim here is that constrained models are a way to guarantee the properties of a model within some specifications.

    \citet{Garcia2015-rs} perform a survey about safe reinforcement learning (RL). Safety in RL can be critical based on the application, such as an aerospace vehicle that can cost several thousands of dollars. They state that there are two main methods: 1) modifying the optimality criterion with a safety factor, and 2) modification of the exploration process through the incorporation of external knowledge. They also present a hierarchy of approaches and implementations. Some approaches used when modifying the optimality criterion are worst-case, risk-sensitive, and constrained criterion. Whereas, modifying the exploration process is done through using external knowledge, and as well as using risk-directed exploration. Safe RL is a particularly important area that requires assurances, as the systems are designed specifically to evolve without supervision.

    As one example, \citet{Lipton2016-dq} design a reinforcement learner that uses a deep Q-network (DQN) and a `supervised danger model'. Basically, the danger model stores the likelihood of entering a catastrophe state within a `short number of steps', this model can be learned by detecting catastrophes through experience and can be improved over time.  In this way they show that their method, they call `intrinsic fear', is able to overcome the sensitive nature of DQNs. There is a clear limitation of the danger model, in that it does not contain useful information until a catastrophe is detected. 

    \citet{Curran2016-ij}, in a more specific application, asks how a robot can learn when a task is too risky, and then avoid those situations, or ask for help. To do this, they use a hierarchical POMDP planner that explicitly represents failures as additional state-action pairs. In this way the resulting policy can be averse to risk. They say that this method can be especially useful when optimal actions are not straight-forward, and they state that it can use any reward function. It seems that this method might suffer from some of the typical problems of POMDPs, which are computational complexity in high-dimensional state spaces.

\subsubsection{Active Learning}
    \citet{Paul2011-vr} is concerned with whether a robot can improve its own performance over time, with the goal of `life-long learning'. They use `perplexity' which is a method first introduced in language models and adapted to work with images. Perplexity, in their application, is a measure that indicates the uncertainty in predicting a single class. Over time, the most perplexing images are stored and used in expanding the sample set. This work is interesting for application in assurances because the ability to quantify something that is perplexing is a predecessor to being able to communicate that to a human user.

    Recently, there have been several papers that attempt to use Gaussian processes (GPs) as a method to actively learn and assign probabilistic classifications (see \citet{MacKay1992-sp,Triebel2016-kj,Triebel2013-ow,Triebel2013-ku,Grimmett2013-gj,Grimmett2016-yc,Berczi2015-rd,Dequaire2016-kh}). The applications surveyed here are all mainly related to image classification and robotics. As with perplexity-based classifiers, the key insight is that if a classifier possesses a measure of uncertainty, then that uncertainty can be used for efficient instance searching, comparison, and learning, as well as reporting a measure of confidence to users. The key property of GPs that makes them an attractive for this purpose is their ability to produce confidence/uncertainty estimates that grow more uncertain away from the training data. That is, GPs have the inherent ability to `know what they don't know', and this information can be readily assessed and conveyed to users, even in high-dimensional reasoning problems. This property of GPs has also found great use in other active learning applications, such as  Bayesian optimization (see \citet{Williams1998-kr}, \citet{Snoek2012-tt}, \citet{Brochu2010-tj}, and \citet{Israelsen2017-zb}).

\subsubsection{Representation learning and Feature Selection}
    Another promising field of research is related to learning representations of data and selecting data features. These two topics are surveyed by \citet{Bengio2013-uv} and \citet{Guyon2003-fj} respectively. From some of the discussion of interpretable models in section \ref{sec:model_interp} we find that representation is important for making interpretable models. Having appropriate representations (i.e. like the ones humans use and understand) is a large step forward in making assurances for humans.

    For instance, in their work related to interpreting molecular signatures \citet{Haury2011-zi} investigate the influence of different feature selection methods on the accuracy, stability and interpretability of molecular signatures. They compared different feature selection methods such as: filter methods, wrapper methods, and ensemble feature selection. They found that the effects of feature learning greatly influenced the results. 

    As another example, \citet{Mikolov2013-lt} studied how to represent words and phrases in a vector format. Using this representation, they are able to perform simple vector operations to understand similar words, and the relative relationships learned. For example the operation $airlines+German$ yields similar entries that include $Lufthansa$. This type of representation encodes information that can be checked and understood by humans. 
    
    How can human understandable features and representations be discovered? This is still an open question. The main question in the representation learning world is how to find the best representations, not necessarily the representation and features that are most human. This is not surprising because human representations and features are not necessarily optimal, and AIAs are being designed to be optimal using other objective functions (arguably more appropriate functions, if humans don't need to understand what is going on).

\subsubsection{Empirical Performance Prediction}
    \citet{Leyton-Brown2009-yr} investigate how to empirically predict the performance of algorithms in real applications, using empirical hardness models (EHMs). This is a topic of interest to them because they investigate algorithms that are theoretically NP-Complete. However, in practice many problems can be solved more quickly than the non-polynomial time theoretical guarantee. They created features of the inputs to the algorithm and were able to make a regression to predict the runtime of an algorithm based on those features. This method introduced EHMs, and used fairly simple feature constructions. It was limited somewhat in being able to quantify the uncertainty in the runtime prediction.

    To this end, \citet{Hutter2006-ak} builds on \cite{Leyton-Brown2009-yr} by using GPs to predict the performance of algorithms. In this way the runtime can be predicted, but with a measure of uncertainty.  This kind of approach could help AIAs to predict their performance on problems they haven't encountered before. Their approach is especially useful because they also consider the ability to tune the parameters of algorithms.
    
    One of the main limitations is that these are supervised learning methods that, according to \cite{Leyton-Brown2009-yr} require a lot of data to train properly, training data is not always easily available to some AIAs. Although, using GPs like \cite{Hutter2006-ak} should allow active (i.e. data efficient) learning of the EHMs.

\subsubsection{Summary}
    The literature surveyed in this section is \emph{not} exhaustive, nor could it reasonably ever claim to be. One thing that should be painfully clear is that in every field where designers want to ensure reliable and correct application of an AIA, there will be assurances that are designed. The disciplines selected in this paper are a subset that are aligned with the author's interests in unmanned vehicles.
    
    Specifically we see that V\&V offers great promise for assurances in human-AIA trust relationships. Having an AIA that is validated (and somehow communicating this to users) should affect their competence trust dimension, although this would need to be verified. Of course, as specifications break down the AIA would need to be able to give appropriate assurances to the human in that situation as well. Similar reasoning leads to the benefits about the literature that considers safety and learning under uncertainty, AIAs designed with these methods will be more predictable and competent in a wider variety of situations.

    Active learning involves the AIA being able to quantify uncertainty in its own model, and then selecting new experiences to reduce that uncertainty. These types of algorithms are precursors for giving assurances related to certain AIA capabilities. Making better representations and features can in turn make models more readily interpretable by humans. Although there can be a trade-off between accuracy and interpretability several studies have shown that learning with expert added structure both improves interpretability, while only losing a little interpretability. However, that comes at the cost of needing to design said features and representations.

    Finally, making models like EHMs that can predict empirical performance can be useful in situations where AIAs encounter unknown environments. For example, given an environment with these features the AIA should be able to perform at this level. 
