\subsection{Quadrant IV. (Implicit Assurances, Informal Trust Treatment)}\label{sec:q4}
This section provides our perspective on some of the research disciplines that are working on techniques that will be critical to designing effective assurances.

\subsubsection{Validation and Verification}
\nisarcomm{It seems you forgot to mention Hadas Kress-Gazit's work from Cornell here: she does a lot of work in robotics where formal V\&V methods (specifically linear temporal logic (LTL) tools) are used to describe to users whether or not robots can/cannot complete certain specified tasks, using `natural language' (i.e. canned semantic English, ala Cops and Robots) translations of complex LTL predicate formulas...}

Validation and Verification (V\&V) typically refers to using formal methods to guarantee the performance of a system within in some set of specifications. Not all practitioners are aware that V\&V provides ways to assure users, arguably they probably have this idea in their mind, but have not considered how to communicate the existence and implications of V\&V to users.

    While not a typical approach, we claim that \citet{Da_Veiga2012-gh} discuss a form of V\&V. They are concerned with nonparametric classification and regression with constraints. More specifically, they are concerned about learning Gaussian process (GP) models with inequality constraints, and present a method to do this by using conditional expectations of the truncated multivariate normal distribution (or Tallis formulas). This is not the only work that references learning with constrained GPs. It is also not the only work that considers constrained modeling, but it would take too long to review all of those papers. The main claim here is that constrained models are a verified way to guarantee the properties of a model within some specifications. \nisarcomm{a little unclear what the practical significance here is in relation to V\&V?? How would constrained GPs be used to this end? Also, what are caveats/limitations? }

    In relation to cooperative multi-agent systems, \citet{Da_Silva2016-qb} ask how their performance can be verifiable. They suggest a `top-down' and `bottom-up' approach, in which high-level planning happens from the top, and then verifiable motion planning happens from the bottom. In this way, agents move in a verified way, while being able to satisfy a higher level plan. This is similar to work by \citet{Conner2007-uw}, except they directly consider multi-agent systems instead of a single vehicle. \nisarcomm{what is the interesting/novel/different angle that's important to consider here in relation to trust/assurances, i.e. as opposed to other typical V\&V approaches?}

    V\&V is especially challenging in real environments where specifications are commonly \nisarcomm{`frequently'??} violated. \citet{Weng_Wong2014-tj} address this challenge by using `environment characterization' as a runtime verification to not violations of the environment \nisarcomm{sentence doesn't make sense???}. When a violation is detected, they use recovery that can give correct actions, in order to ensure safety and continued progress to the goal if possible. \citet{Nishi2016-zq} proposes something similar in being able to quantify the degree of confidence in the estimated world state, and then calculating confidence limits on planning.

    \citet{Hadfield-Menell2016-ws}, in considering an AI safety problem, address the problem of verifying that a robot can be turned off, even when it might have an objective function that indicates that disabling the `off-switch' will reduce the chance of failure. This kind of scenario, or something similar, can easily occur with a sufficiently sophisticated and capable AIA, and a complex enough set of objectives that might result in unintended consequences. They propose that one objective would be for the robot to maximize value for the human, and to not assume that it know how to perfectly measure that value. \nisarcomm{note that some of Anca Dragan's recent work with Pieter Abeel and Stuart Russell also looks at this from the inverse reinforcement learning viewpoint...but they do not consider formal V\&V}

\subsubsection{Safety and Learning under nonstationarity and risk aversion}
    While a fairly high-level treatment, \citet{Amodei2016-xi} are concerned with `AI safety', which is in essence how to make sure that there is no ``unintended and harmful behavior that [emerges] from machine learning systems''. Given this definition, much of the paper discusses concepts that are critical to AIA assurances. Among the more directly applicable topics in the scope of this paper are: safe exploration (how to learn in a safe manner), and robustness to distributional shift (a difference between training data and test data). They also discuss designing objective functions that are more appropriate \nisarcomm{clarify, e.g. more appropriate *than what*?}.

    Generally, stationary data is assumed in supervised ML \nisarcomm{what does `stationary data' mean?}. \citet{Sugiyama2013-ci} considers what to do when there is `covariate shift' (or when both training and test data change distribution, but their relation does not change between training and test phases), and `class-balance change' (where the class-prior probabilities are different in training and test phases, but the input distribution of each class does not change). They design and present tools that help diagnose and treat those conditions (this is follow on work for some of what is presented in \citet{Quinonero-Candela2009-fj} where they consider dataset shift). \nisarcomm{what are some of those tools? give some highlights of technical points (recall: part of the goal of your survey is to point out specific techniques that are interesting...but you should say something about what is actually involved with those techniques!)}

    \citet{Garcia2015-rs} perform a survey about safe reinforcement learning (RL). \nisarcomm{briefly: why is this an important problem? e.g. consider for aerospace vehicles, e.g. airplanes and quadrotors -- can they afford to fall down and go boom very often?} They state that there are two main methods: 1) modifying the optimality criterion with a safety factor, and 2) modification of the exploration process through the incorporation of external knowledge. They also present a hierarchy of approaches and implementations \nisarcomm{briefly summarize these??}. 
Safe RL is a particularly important area that requires assurances, as the systems are designed specifically to evolve without supervision.

    As one example, \citet{Lipton2016-dq} design a reinforcement learner that uses a deep Q-network (DQN) and a `supervised danger model'. Basically, the danger model stores the likelihood of entering a catastrophe state within a `short number of steps', this model can be learned by detecting catastrophes through experience and can be improved over time.  In this way they show that their method, they call `intrinsic fear', is able to overcome the sensitive nature of DQNs. There is a clear limitation of the danger model, in that it does not contain useful information until a catastrophe is detected. \nisarcomm{this is definitely interesting! would make for a fun reading group paper one day...} 
\citet{Curran2016-ij}, in a more specific application, asks how a robot can learn when a task is too risky, and then avoid those situations, or ask for help. To do this, they use a hierarchical POMDP planner that explicitly represents failures as additional state-action pairs. In this way the resulting policy can be averse to risk. \nisarcomm{limiting assumptions/caveats?}
    

\subsubsection{Active Learning}
    \citet{Paul2011-vr} is concerned with whether a robot can improve its own performance over time, with the goal of `life-long learning'. They use `perplexity' which is a method first introduced in language models and adapted to work with images. Perplexity is a measure that indicates the uncertainty in predicting \edit{the ...class??} of a single \edit{a single what??}. Over time, the most perplexing images are stored and used in expanding the sample set. The ability to quantify something that is perplexing is a predecessor to being able to communicate that information to a human user. \nisarcomm{what information, specifically? also, is this a claim made by the authors?}

    Recently, there have been several papers that attempt to use Gaussian processes (GPs) as a method to actively learn and assign probabilistic classifications (see \citet{MacKay1992-sp,Triebel2016-kj,Triebel2013-ow,Triebel2013-ku,Grimmett2013-gj,Grimmett2016-yc,Berczi2015-rd,Dequaire2016-kh}). The applications surveyed here are all mainly related to image classification and robotics. As with perplexity-based classifiers, the key insight is that if a classifier possesses a measure of uncertainty, then that uncertainty can be used for efficient instance searching, comparison, and learning, as well as reporting a measure of confidence to users. \edit{The key property of GPs that makes them an attractive for this purpose is their ability to produce confidence/uncertainty estimates that grow more uncertain away from the training data. That is, GPs have the inherent ability to `know what they don't know', and this information can be readily assessed and conveyed to users, even in high-dimensional reasoning problems.}
This property of GPs has also found great use in other active learning applications, such as  Bayesian optimization \nisarcomm{cite some papers}. 

\subsubsection{Representation learning and Feature Selection}
    Another promising field of research is related to learning representations of data and selecting data features. These two topics are surveyed by \citet{Bengio2013-uv} and \citet{Guyon2003-fj} respectively. \edit{...finish the transition:...} \nisarcomm{specifically,explain the connection to AIAs: why is it important to consider techniques for representation learning and feature selection? The basic idea that you seem to be getting at is that it helps if the representations/models themselves and the data used by AIAs are already interpretable to begin with, i.e. by construction?? Or is there more to what you're trying to say here}

    For instance, in their work related to interpreting molecular signatures \citet{Haury2011-zi} investigate the influence of different feature selection methods on the accuracy, stability and interpretability of molecular signatures. They compared different feature selection methods such as: filter methods, wrapper methods, and ensemble feature selection. They found that the effects of feature learning greatly influenced the results. 

    As another example, \citet{Mikolov2013-lt} studied how to represent words and phrases in a vector format. Using this representation, they are able to perform simple vector operations to understand similar words, and the relative relationships learned. For example the operation $airlines+German$ yields similar entries that include $Lufthansa$. This type of representation encodes information that can be checked and understood by humans. 
    
    \nisarcomm{open issues/caveats with this body of work as far as trust/assurances goes?}

\subsubsection{Empirical Performance Prediction}
    \citet{Leyton-Brown2009-yr} investigate how to empirically predict the performance of algorithms in real applications, \edit{using empirical hardness models (EHMs)}. This is a topic of interest to them because they investigate algorithms that are theoretically NP-Complete. However, in practice many problems can be solved more quickly \nisarcomm{be more precise -- more quickly than what? you seem to have skipped over a few steps in your explanation...}. They created features of the problems and were able to make a regression to predict the runtime of an algorithm based on the problem features. \nisarcomm{you need to spell out this last part a bit more...won't make sense to lay reader...and again: caveats/etc.}

\citet{Hutter2006-ak} builds on this work by using GPs to predict the performance of algorithms. In this way the runtime can be predicted, but with a measure of uncertainty.  This kind of approach could help AIAs to predict their performance on problems they haven't encountered before. \nisarcomm{caveats/limitations??}

\subsubsection{Model checking} 
\nisarcomm{is this section kind not similar to the other model checking section from earlier?? Also feels a bit empty -- listing of papers, but no insights/connecting ideas?}
Many AIAs are dependent on models. To that end checking the model, and having some metric of uncertainty in its validity, is critical in being able to assure a human user of the competence and predictability of the system.
    
    Research for goodness-of-fit metrics for Bayesian models has been fairly popular as found in \citet{Dannemann2008-ch,Johnson2004-mv,Yuan2012-tb} and \citet{Spiegelhalter2002-ia}.

    \cite{MacKay_Altman2004-fl} also present methods for assessing the goodness of fit of Hidden-Markov models. Later, \citet{Titman2008-ct,Titman2012-zw,Titman2010-qx} continued that investigated more methods for Markov, Hidden-Markov, and multi-state models, and try to make more principled formal approaches.

    
    \citet{Laskey1995-jp} investigated how to analyze the sensitivity of Bayesian networks, and \citet{Sinharay2006-yc} further investigated diagnostics for Bayesian networks.

\subsubsection{Summary}
    The literature surveyed in this section is \emph{not} exhaustive, nor could it reasonably ever claim to be. One thing that should be painfully clear is that every field where designers want to ensure reliable and correct application of an AIA, there will be assurances that are designed. The disciplines selected in this paper are a subset that are aligned with the author's interests in unmanned vehicles. \nisarcomm{revise in light of all above edits...}

% \subsubsection{Representation of Uncertainty}
%
    % \citet{Costa2012-fa} and \citet{Laskey2015-gz} address how to represent uncertainty in modeling so that it isn't lost in operations like information fusion. The proprose a framework called the Uncertainty Representation and Reasoning Evaluation Framework (URREF), and suggest that it provides guidance in defining

% \subsubsection{Not sure} , \citet{Gutfreund2016-xe} (constructs automatic arguments, perhaps similar to counter-planning?), \citet{Charif2013-vo} (agents cooperate based on abilities to accomplish goal)
