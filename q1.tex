\subsection{Quadrant I.}
\citet{Sheridan1984-kx} was perhaps the first to attempt a formal approach to understanding the relationship between an operator/designer and a supervisory control system. They considered psychological models of the humans and asked questions about how the human could understand how such systems worked. To this end they suggest that aspects of the control system need to be made transparent to the user so that the user has an accurate model of the system, but they don't offer concrete ways in which this can be accomplished. They mention that control displays should be designed according to the \emph{designer's} level of trust in the individual elements, but they seem to overlook the critical nature of what explicit assurances should be displayed to the \emph{operator} of the system, \edit{as opposed to} what information might allow the \emph{designer} to have appropriate trust in the control system.

\citet{Muir1996-gt} performed an experiment where participants were trained to operate a simulated pasteurizer plant. During operation they were able to intervene in the fully-autonomous system if they felt is was necessary to obtain better performance. Trust was quantified by self-reported questionnaire responses, as well as by the level of reliance on the automation during the simulation. She noted that operators could learn to trust an unreliable system if it was consistent. The only indications that the participants received was the observed reliability of the pump. 

In their experiment involving thirty students and thirty-four professional pilots \citet{Riley1996-qm} investigated how reliability, and workload effected the participant's likelihood of trusting in automation. Two simulated environments were created to this end. First was to use/not use an automated aid (with variable reliability) to classify characters while also performing a distraction task. Interestingly they found that pilots (those with extensive experience working with automated systems) had a bias to use more automation, but reacted similarly to students in the face of dynamic reliability changes. Findings also showed that the use of automation is highly based on individual traits.

Also considering the performance of pilots \citet{Wickens1999-la} investigated the effect of semi-reliable data while piloting a plane. They also investigated semi-reliable performance of a system that highlighted important data for the pilot to see. The pilots were aware that the measurements/highlighting system might be innacurate before the experiment. The reliability of the systems did have an effect on the outcome of the experiment, but interestingly did not make a measurable effect on the pilot's self-reported trust (another reason to worry about TRBs instead of trust).

McKnight and collaborators have spent significant time investigating trust between humans and technology. His initial research was focused on e-commerce settings but later moved to trust between humans and technology. In \cite{Mcknight2011-gv} they gather self-reported trust through a questionairre. Their experiment was interested in identifying the dimensions of trust effected by learning to use Excel for use in a business class. The results were based solely on the intrinsic properties of excel and how each individual perceived them.

In \cite{Lankton2008-ct} and later in \cite{Tripp2011-cq} they investigate the difference in trust between humans and trust between a human and technology. They found that as the technology becomes more `human-like' the self-reported trust has more similarities to trust between humans. This study was performed using Microsoft Access, a recommendation assistant, and Facebook. Respondents were asked to rate how each software `kept its commitments' and how `human-like' it was. Again, these impressions were based solely on the intrinsic properties of each of the three AIAs used in the experiment.

\citet{Freedy2007-sg} studied how `mixed-initiative teams' (MITs, their term for human-robot teams) might have their performance measured. The premise of the work is that MITs can only be successful if ``humans know how to appropriately trust and hence appropriately rely on the automation''. They explore this idea by making using a tactical game where human participants supervised a unmanned ground vehicle (UGV) as part of a reconnaissance platoon. This UGV had autonomous targeting and firing capability which the operator needed to monitor in case the UGV could not perform as desired.

Operators were trained to recognize signs of failure, and to only intervene if they thought the mission completion time would suffer.Trust was formally acknowledged in this survey and was quantified by using Relative Expected Loss (REL), which is basically the mean expected loss of robot control over $n$ trial runs. Similar to \cite{Muir1996-gt} the operators learned to trust a UGV with low competence as long as it behaved consistently.

In a similar vein \citet{Desai2012-rc} investigated the effects of robot reliability on the trust of human operators. In this case a human participant needed to work with an autonomous robot to search for victims in a building, while avoiding obstacles. The operator had the ability to switch the robot from manual (teleoperated) mode, to semi-autonomous, or autonomous mode depending on how they thought they could trust the system to perform.

During this experiment the reliability of the robot was changed in order to observe the effects on the operator's reliance to the robot. They measured trust by the amount of time the robot spent in different levels of autonomy (i.e. manual vs. autonomous), and found that trust changed based on the levels of reliability of the robot.

\citet{Salem2015-md} investigated the effects of error, task type, and personality on cooperation and trust between a human and robot. In this case the robot was a domestic robot that performed tasks around the house. A human guest was welcomed to the home and observed the robot operating on different tasks. After this observation (in which the robot implicitly showed competence by its mannerisms and success/failure) the human participant was asked to cooperate with the robot on certain tasks.

Interestingly they found that self-reported trust was affected by faulty operation of the robot, but that it didn't seem to have an effect on whether the human would cooperate on other tasks. This seems to suggest that the effect of institutional trust (i.e. this robot may not be competent, but whoever designed it must have known what they were doing) allow user's to continue to cooperate with a faulty system even if they have low trust of it.

\citet{Bainbridge2011-pl} investigated the difference in trust between a human and a robot,  in cases where the robot was physically present and where the robot was only displayed on a screen (i.e. not physically present). In this experiment the only method of communication from the robot was gestures. They measured trust by the willingness of the human participants to cooperate with the robot. Among other interesting findings regarding how the participants interacted with the robot, they found that they were more likely to cooperate with the robot when it was physically present. \citeauthor{Bainbridge2011-pl} suggest that this is due to increased trust.

With the aim of understanding how individuals currently trust autonomous vehicles \citet{Munjal_Desai2009-en} perform experiment with around 175 participants. The participants were asked to rate their level of comfort with six different situations. These situations ranged from parking your own car, having an autonomous vehicle with manual override park the car, and having a fully autonomous vehicle that could not be overridden park the car. There were also questions that asked the opposite, like how comfortable users would be with having autonomous vehicles park near their car. The survey found that, based supposedly on the current levels of institutional trust, the participants were most comfortable with parking their own car, and least comfortable with having a fully autonomous vehicle (with no manual override) park their car.

\nisarcomm{here is where you should insert an assessment/takeaways for Quadrant I: how would you summarize the key ideas and state of the art in this particular quadrant? what are the open questions or opportunities that become apparent, or what important lessons and limitations have been learned from this subset of the literature?}