\subsection{Quadrant I. (Implicit Assurances, Formal Trust Treatment)}\label{sec:q1}

Several experiments have formally shown the effect of implicit assurances (AIA features that were not purposefully designed to affect trust) on a user's trust towards an AIA. Generally, implicit assurances can affect any of the three trust dimensions highlighted in Fig. ~\ref{fig:Assurance_classes}. \citet{Muir1996-gt}, for example, showed that trained participants could learn to trust a fully autonomous simulated pasteurizer that operated with a consistently unreliable pump system. The participants could intervene as operators if they felt is was necessary to obtain better performance. The only explicit assurances afforded here to participants were visual performance indicators of the unreliable pump's flow rate and resulting tank liquid level over time, from which the participants implicitly created mental models of the system's reliability. 
Trust was quantified by self-reported questionnaire responses, based on the level of reliance on the automation during the simulation. 
This study attempted to identify the effects of reliability on the user's perception of the AIAs competence and predictability, but the results are difficult to generalize since only six participants were used for this particular system. 

Implicit assurances tied to perceptions of reliability built over contiguous observations of performance have also been experimentally investigated in other contexts. For instance, \citet{Wickens1999-la} investigated how the presentation of semi-reliable information by an automated airplane flight software system affects human pilot performance. Interestingly, while system reliability did have an effect on pilot performance, it did not make a measurable effect on the pilots' self-reported trust levels. In experiments comparing thirty university students and thirty-four professional pilots, \citet{Riley1996-qm} studied how reliability and workload affected the participant's likelihood of trusting in automation aids (with varying reliability levels) in tasks where participants had to classify characters and also perform a distraction task. It was found that the professional pilots (who had extensive experience working with automated systems) had a bias to use more automation, but reacted similarly to students in the face of dynamic reliability changes. Specifically both pilots and students, on average, responded quite quickly when the automation became unreliable, by turning it off, but a larger portion of pilots continued using the automation during the low-reliability portions of the experiment. However, in investigating human operator trust for an autonomous search and rescue task, \citet{Desai2012-rc} found that trust (as measured by the amount of time the human kept the robot engaged in fully manual teleoperation, semi-autonomous, or fully autonomous operating modes) was highly correlated to experimentally controlled robot reliability levels. 

The effects of perceived AIA `humanness' on trust have also been studied. 
Refs. \cite{Lankton2008-ct} and \cite{Tripp2011-rx} compared human trust in other humans against human trust in intelligent interactive technology, which in this case was represented by Microsoft Access, an intelligent recommendation assistant, and Facebook. 
They found that, as the technology becomes more `human-like', self-reported levels of trust in technology become more similar to levels of trust in other humans. 
\citet{Salem2015-md} investigated the effects of autonomous task errors, task types, and `system personality' on cooperation and trust for humans who observed a domestic robot performing house tasks, such that the robot implicitly showed competence by its mannerisms and successes/failures during tasks. When participants were asked to cooperate with the robot on certain other tasks, the faulty operation of the robot was found to affect self-reported trust levels (implicit reliability again), but did not have an effect on whether the human would cooperate on other tasks. %\hlr{WAS THIS BECAUSE HUMANS PERCEIVED THE ROBOT AS KNOWING ITS OWN LIMITATIONS, i.e. PERSONALITY/MANNERISMS?? i.e. more trustworthy because it admits it can't do something? therefore it must implicitly have some sort of `benign awareness' of its surroundings and circumstances, and therefore can be trusted?}  
%This seems to suggest that the effect of institutional trust (i.e. this robot may not be competent, but whoever designed it must have known what they were doing) allowed users to continue to cooperate with a faulty system, even if they have low levels of trust in it. \hlr{WAS THIS ACTUALLY A FACTOR??} 
In a less cooperative/more adversarial context, \citet{Wu2016-ei} investigated how a person's decisions in a coin entrustment game are affected by their belief in whether they are against an AI agent or another human player (which, unbeknownst to participants, was in fact an AI with some programmed human-like idiosyncrasies, e.g. variable wait times between turns). 
Trust in this context was measured directly by the number of coins a participant was willing to lose by putting them at risk to the other player. The experiment found that the participants trusted the AI opponent more than they trusted the `human' opponent; the authors suggest that this may be due to the perception that the AI opponent did not have feelings and operated in a more predictable and consistent `machine-like' way. 
%Given that the `human' was an AI as well, this experiment shows that `machine-like' behavioral consistency can lead to implicit positive effects the trust of the participant in certain contexts. 
%On the surface, their work is meant to be a study of the implicit differences in trustworthiness between humans and robots. 
%However in their experiment the `human' was actually an AI with some programmed idiosyncrasies to lead the human player to believe the AI was a human. 

Finally, several studies have examined how user's observations of AIA performance and the physical presence/embodiment implicitly influences trust. %%Note that `performance' is distinct from `reliability' as discussed above, although both notions are closely related. Performance pertains to implicit trust cues based on the actual outcomes of an AIA's execution of certain tasks and/or achievement of objectives (i.e. features that suggest that an AIA is `getting the job done'); reliability describes implicit trust cues based on the AIA's ability to continue functioning robustly and performing properly (i.e. features that suggest that an AIA will not `fall apart at the seams'). 
\citet{Freedy2007-sg} studied how mixed-initiative human-AIA teams might have their performance measured, and examined the extent to which such teams can only be successful if ``humans know how to appropriately trust and hence appropriately rely on the automation''. 
They explore this idea by using a tactical reconnaissance scenario where human participants supervised an unmanned ground vehicle (UGV)  platoon with three levels of autonomous targeting/firing capability (low, medium, high); these levels were dependent on the experimental conditions. The operator needed to monitor the UGV in case it couldn't perform as desired.
Operators were trained to recognize signs of task failure, and to only intervene if they thought the mission completion time would suffer. %Trust was formally acknowledged in this survey and was quantified by using the Relative Expected Loss (REL) measure, which is the mean number of trials to expected loss of robot control over $n$ experimental trial runs. 
Operators were more likely to trust a `medium' ability UGV if they had first encountered a `high' ability UGV, as opposed to encountering a `low' ability UGV first, which is another manifestation of framing effects like \cite{Riley1996-qm}. %\hlr{sounds like operators were given a choice of which ability level to use???} 
As in \cite{Muir1996-gt}, operators learned to trust a UGV with low competence as long as it behaved consistently. 
Related to physical presence, \citet{Bainbridge2011-pl} studied physical gesturing as a form of cooperative human-robot team communication.  It was found that trust (as measured by willingness of human to cooperate with the robot) was greater in cases where robots are physically present with human users, versus cases where robots were only displayed on screens. 
%%Trust was measured by the willingness of human participants to cooperate with the robot \hlr{what does `cooperate with the robot' mean -- cooperate in what sense?}. Among other interesting results, participants were more likely to cooperate with the robot when it was physically present. \citeauthor{Bainbridge2011-pl} suggest that this is due to increased trust. %, in this case cooperation is a TRB.
\subsubsection{Summary}
Implicit assurances have been formally considered vis-a-vis trust along the following lines:
\textit{perception of an AIA's reliability} (implicit cues on ability to function properly) -- even if an AIA is not reliable, it may behave consistently enough that a human user can still learn to trust it appropriately; 
\textit{perception of an AIA's capabilities} (implicit cues scope of tasks it can carry out and problems it can solve) -- this aspect has  been formally examined for relatively simple systems and tasking contexts, but deserves additional attention to better understand the dangers of potentially underselling/overselling the capabilities of AIAs via implicit associations, e.g. based on names/labels (`autopilots' for self-driving cars), appearance (humanoid robots that have trouble climbing stairs), etc.; 
\textit{perception of AIA's humanness and personality} (cues on ability to demonstrate human-like understanding and communication of tasks, context, environment, and itself) -- this aspect goes beyond the task space in that human qualities that normally affect human-human trust (intentionality, desire, feelings, fear, etc.) can be projected onto AIAs through (mis)interpreted implicit assurances; 
\textit{perception of AIA's performance and physical presence} (implicit cues on actual ability of AIA to execute tasks, communicate effectively with team mates for cooperation, and achieve goals, regardless of how reliably it functions) -- unsurprisingly, both performance and presence of an AIA are significant drivers of trust, but are not the only important drivers. 
% \begin{itemize}
% 	\item perception of an AIA's reliability (i.e. implicit cues on ability to function properly) -- even if an AIA is not reliable, it may behave consistently enough that a human user can still learn to trust it appropriately. 
% 	\item perception of an AIA's capabilities (i.e. implicit cues scope of tasks it can carry out and problems it can solve) -- this aspect has  been formally examined for relatively simple systems and tasking contexts, but deserves additional attention to better understand the dangers of potentially underselling/overselling the capabilities of AIAs via implicit associations, e.g. based on names/labels (`autopilots' for self-driving cars), appearance (humanoid robots that have trouble climbing stairs), etc. 
% 	\item perception of AIA's humanness and personality (i.e. cues on ability to demonstrate human-like understanding and communication of tasks, context, environment, and itself) -- this aspect goes beyond the task space in that human qualities that normally affect human-human trust (intentionality, desire, feelings, fear, etc.) can be projected onto AIAs through (mis)interpreted implicit assurances.
% 	\item perception of AIA's performance and physical presence (i.e. implicit cues on actual ability of AIA to execute tasks, communicate effectively with team mates for cooperation, and achieve goals, regardless of how reliably it functions) -- unsurprisingly, both performance and presence of an AIA are significant drivers of trust, but are not the only important drivers.  
% \end{itemize}

The range of different trust measures used by the works in this quadrant underscores the point that trust-related behaviors (TRBs) ought to be the focus of assurances as opposed to trust itself, since trust is a subjective measure that may (if defined properly) or may not (if not properly defined) predict whether a person uses an AIA appropriately. 
This not only depends heavily on how one `measures trust', but also depends on individual user traits. 
Throughout this paper, we will see evidence for the idea that a user will gather and respond to assurances, whether implicit or explicit, in order to execute TRBs. 
A key takeaway from papers in Quadrant I is that, in the absence of explicitly provided assurances (i.e. if only implicit assurances are available), a user will use other perceived properties, cues and behaviors to gather assurances and inform their TRBs. 
Most of the implicit assurances considered above largely come in the form of visual cues, though other cues like waiting times or spoken language features (e.g. tone, volume, pace, etc.) can also play roles. 
It is thus also important to consider the role of human cognitive tendencies and limitations when assessing what implicit assurances are present in an AIA. %%\hlr{NRA note: recall your definition of implicit assurance: it is an assurance that is NOT something designed into the system on purpose, but it still affects trust outcomes...}. 
This was directly observed in \cite{Freedy2007-sg,Riley1996-qm}, where framing effects biased operator's behaviors towards the AIAs. 
Other cognitive biases such as `recency effects' (being biased based on recent experience), `focusing effects' (being biased based on a single aspect of an event), or `normalcy biases' (refusal to consider situations which have never occurred before) are also important to consider. 
%
%%Finally, humans naturally attempt to construct statistical models (albeit not especially accurate ones) of the world around them in order to predict and operate within it. %%In light of this AIA designers must also consider the limitation (time and otherwise) for human users to build statistical models, e.g. for reliability, when only instance by instance data can be gathered by users.
%%Even if an AIA has the ability to calculate an assurance, it must still have a way by which to express that assurance to a human user. The human user then perceives the assurance, perhaps through interaction with the system, or only through more passive observation of the system. These kinds of perceptions can be based on displayed information, or on how the AIA `behaves' (as in \cite{Salem2015-md}). Once the user perceives some kind of assurances (perhaps not purposefully communicated), those assurances are integrated into the trust of the user towards the AIA. In most cases this group of research focuses on assurances given through visual cues.

%We also see evidence that human cognitive limitations need to be taken into account when designing assurances. This was directly observed in \cite{Freedy2007-sg,Riley1996-qm} where framing effects biased operator's behaviors towards the AIAs. This also suggests that other cognitive biases such as `recency effects' (being biased based on recent experience), and others will also apply as well. A couple of examples include well known cognitive biases such as `focusing effects' (being biased based on a single aspect of an event), or the `normalcy bias' (refusal to consider a situation which has never occurred before). Finally, humans naturally attempt to construct statistical models (albeit not especially accurate ones) of the world around them in order to predict and operate within it. In light of this designers must also consider the limitation (time and otherwise) for humans to build statistical models, such as reliability, when only instance by instance data is available. %These ideas still remain to be verified by further, more focused, experimentation.


%%%OLD
%\citet{Muir1996-gt} performed an experiment where participants were trained to operate a simulated pasteurizer plant. During operation they were able to intervene in the fully-autonomous system if they felt is was necessary to obtain better performance. Trust was quantified by self-reported questionnaire responses, as well as by the level of reliance on the automation during the simulation. She noted that operators could learn to trust an unreliable system if it was consistent. The participants were only able to observe the reliability of the pump (i.e. the performance of the pumps over time, from which the user created a mental model of reliability).

%In experiments involving thirty students and thirty-four professional pilots, \citet{Riley1996-qm} investigated how reliability and workload affected the participant's likelihood of trusting in automation. Two simulated environments were created to this end. First was for participants to use/not use an automated aid (with variable reliability) to classify characters while also performing a distraction task. Interestingly, they found that pilots (those with extensive experience working with automated systems) had a bias to use more automation, but reacted similarly to students in the face of dynamic reliability changes. In this setting, the bias to use more automation would be known as `framing effects' (where a human's trust is biased by the trust they have in previously encountered systems) in cognitive science. Findings also showed that the use of automation is highly based on individual traits.

%Also considering the performance of pilots, \citet{Wickens1999-la} investigated the effect of semi-reliable data while piloting a plane. They also investigated semi-reliable performance of a system that highlighted important data for the pilot to see. The pilots were aware that the measurements/highlighting system might be inaccurate before the experiment. The reliability of the systems did have an effect on the outcome of the experiment, but interestingly did not make a measurable effect on the pilot's self-reported trust. This underscores the point that TRBs ought to be the focus of assurances as opposed to trust itself, since trust is a subjective measure that may or may not actually change a person's TRBs.

%%hlr{Cutting out this reference, since McKnight is not talking about AIAs -- Excel doesn't count!!}
%McKnight and collaborators have spent significant time investigating trust between humans and technology. His initial research was focused on e-commerce settings but later moved to trust between humans and technology. In \cite{Mcknight2011-gv} they gather self-reported trust through a questionnaire. Their experiment was interested in identifying the dimensions of trust effected by learning to use Excel for use in a business class. The results were based solely on the intrinsic properties of Excel and how each individual perceived them.


%\citet{Freedy2007-sg} studied how `mixed-initiative teams' (MITs, their term for human-robot teams) might have their performance measured. The premise of the work is that MITs can only be successful if ``humans know how to appropriately trust and hence appropriately rely on the automation''. They explore this idea by using a tactical game where human participants supervised an unmanned ground vehicle (UGV) as part of a reconnaissance platoon. UGVs had three levels of capability (low, medium, high), and had autonomous targeting and firing capability which the operator needed to monitor in case the UGV could not perform as desired. Operators were trained to recognize signs of failure, and to only intervene if they thought the mission completion time would suffer. Trust was formally acknowledged in this survey and was quantified by using Relative Expected Loss (REL), which is the mean expected loss of robot control over $n$ trial runs. Operators were found to be more likely to use a `medium' ability UGV if they had first encountered a `high' ability UGV, as opposed to encountering a `low' ability UGV first, which is another manifestation of framing effects like \cite{Riley1996-qm}. Similar to \cite{Muir1996-gt} the operators learned to trust a UGV with low competence as long as it behaved consistently.

%In a similar vein \citet{Desai2012-rc} investigated the effects of robot reliability on the trust of human operators. In this case a human participant needed to work with an autonomous robot to search for victims in a building, while avoiding obstacles. The operator had the ability to switch the robot from manual (teleoperated) mode, to semi-autonomous, or autonomous mode depending on how they thought they could trust the system to perform. During this experiment the reliability of the robot was changed in order to observe the effects on the operator's reliance to the robot. Trust was measured by the amount of time the robot spent in different levels of autonomy (i.e. manual vs. autonomous), and it was found that trust changed based on the levels of reliability of the robot.

%\citet{Salem2015-md} investigated the effects of error, task type, and personality on cooperation and trust between a human and robot. In this case the AIA was a domestic robot that performed tasks around the house. A human guest was welcomed to the home and observed the robot operating on different tasks. After this observation (in which the robot implicitly showed competence by its mannerisms and successes/failures) the human participant was asked to cooperate with the robot on certain tasks. Interestingly, it was found that self-reported trust was affected by faulty operation of the robot, but that it didn't seem to have an effect on whether the human would cooperate on other tasks. This seems to suggest that the effect of institutional trust (i.e. this robot may not be competent, but whoever designed it must have known what they were doing) allowed users to continue to cooperate with a faulty system, even if they have low levels of trust in it.

%\citet{Wu2016-ei} use game theory to investigate whether a person's decisions are affected by whether they believe they are playing a game against a human or an AI. This idea was studied in the context of a coin entrustment game, in which trust is measured by the number of coins a participant is willing to lose by putting them at risk of the other player. On the surface, their work is meant to be a study of the implicit differences in trustworthiness between humans and robots; however in their experiment the `human' was actually an AI with some programmed idiosyncrasies to lead the human player to believe the AI was a human. This was done by adding a random wait time, as opposed to an instantaneous move that the AI would make. There were also prompts at the beginning of the `human' version of the experiment that suggested that the participant was waiting for another human player to join the game. The experiment found that humans trust an AI more than they trust a `human'. The authors suggest that this may be due to the perception that an AI does not have feelings and is operating in a more predictable way. Given that the `human' was an algorithm as well, this experiment shows that consistency (i.e. no variable wait times) was a factor that affected the trust of the participant.

%\citet{Bainbridge2011-pl} investigated the difference in trust between a human and a robot, in cases where the robot was physically present and where the robot was only displayed on a screen (i.e. not physically present). In this experiment, the only method of communication from the robot was through physical gestures. Trust was measured by the willingness of the human participants to cooperate with the robot. Among other interesting findings regarding how the participants interacted with the robot, it was found that participants were more likely to cooperate with the robot when it was physically present. \citeauthor{Bainbridge2011-pl} suggest that this is due to increased trust, in this case cooperation is a TRB.

%%\hlr{taking this one out also: not really looking at implicit assurances at all here -- simply a survey that asks whether or not people trust the car -- would be different if users also given pictures or some other description/'feel' for an actual car...}
%With the aim of understanding how individuals currently trust autonomous vehicles, \citet{Munjal_Desai2009-en} performed a survey of around 175 participants. The participants were asked to rate their level of comfort with six different situations. These situations ranged from parking your own car, having an autonomous vehicle with manual override park the car, and having a fully autonomous vehicle that could not be overridden park the car. There were also questions related to user comfort with autonomy in situations where they still retained control, like how comfortable users would be with having autonomous vehicles park near their car. The survey found that the participants were most comfortable with parking their own car, and least comfortable with having a fully autonomous vehicle (with no manual override) park their car. These findings are supposedly related to institutional trust, as those surveyed did not necessarily have any experience with autonomous vehicles.

%\subsubsection{Summary}
%We see that there have been several experiments that have formally shown the effect of implicit assurances (assurances that were not purposefully designed to affect trust) on a user's trust towards an AIA. Generally, implicit assurances can affect any of the three trust dimensions highlighted in Figure~\ref{fig:Assurance_classes}. To use a practical example recall that reliability (or rather the perception of reliability built over contiguous observations of performance) was frequently investigated as an assurance in this section. A reliable AIA can seem more competent, and predictable to a human user. Currently, it isn't very clear how different assurances affect the trust dimensions. \Citet{Muir1996-gt} attempted to identify the effects of reliability on the user's perception of the AIAs competence and predictability, but only six participants were used, so the results are questionable. Quantifying the effects of assurances on different dimensions of a user's trust is still an open research question.

%Throughout this work we see evidence for the idea that a user will gather assurances, whether these are implicit or explicit, in order to execute TRBs. To restate the point, in the absence of explicitly provided assurances (such as investigated by every research paper in this quadrant) a user will still use other perceived properties and behaviors and gather assurances in order to inform their trust-related behaviors.
%
%Even if an AIA has the ability to calculate an assurance, it must still have a way by which to express that assurance to a human user. The human user then perceives the assurance, perhaps through interaction with the system, or only through more passive observation of the system. These kinds of perceptions can be based on displayed information, or on how the AIA `behaves' (as in \cite{Salem2015-md}). Once the user perceives some kind of assurances (perhaps not purposefully communicated), those assurances are integrated into the trust of the user towards the AIA. In most cases this group of research focuses on assurances given through visual cues.
%
%We also see evidence that human cognitive limitations need to be taken into account when designing assurances. This was directly observed in \cite{Freedy2007-sg,Riley1996-qm} where framing effects biased operator's behaviors towards the AIAs. This also suggests that other cognitive biases such as `recency effects' (being biased based on recent experience), and others will also apply as well. A couple of examples include well known cognitive biases such as `focusing effects' (being biased based on a single aspect of an event), or the `normalcy bias' (refusal to consider a situation which has never occurred before). Finally, humans naturally attempt to construct statistical models (albeit not especially accurate ones) of the world around them in order to predict and operate within it. In light of this designers must also consider the limitation (time and otherwise) for humans to build statistical models, such as reliability, when only instance by instance data is available. These ideas still remain to be verified by further, more focused, experimentation.
%
% Finally, we are able to begin to develop ideas regarding what indicates changes in trust, and how those changes should be measured. Specifically: through measuring TRBs, and through self-reported changes in perceived trustworthiness. Implicit assurances are not targeted at specific trust dimensions, this is by definition, because they weren't designed to affect trust at all. It seems, that except in a very controlled environment, that they originate from several different sources of AIA capabilities at once, as they are only based on a user's perception and would be unconsciously combined into an overall `sense' of trustworthiness.

