\subsection{Quadrant I. (Implicit Assurances, Formal Trust Treatment)}\label{sec:q1}
\citet{Muir1996-gt} performed an experiment where participants were trained to operate a simulated pasteurizer plant. During operation they were able to intervene in the fully-autonomous system if they felt is was necessary to obtain better performance. Trust was quantified by self-reported questionnaire responses, as well as by the level of reliance on the automation during the simulation. She noted that operators could learn to trust an unreliable system if it was consistent. The participants were only able to observe the reliability of the pump (i.e. the performance of the pumps over time, from which the user created a mental model of reliability).

In experiments involving thirty students and thirty-four professional pilots, \citet{Riley1996-qm} investigated how reliability and workload affected the participant's likelihood of trusting in automation. Two simulated environments were created to this end. First was for participants to use/not use an automated aid (with variable reliability) to classify characters while also performing a distraction task. Interestingly, they found that pilots (those with extensive experience working with automated systems) had a bias to use more automation, but reacted similarly to students in the face of dynamic reliability changes. In this setting, the bias to use more automation would be known as `framing effects' (where a human's trust is biased by the trust they have in previously encountered systems) in cognitive science. Findings also showed that the use of automation is highly based on individual traits.

Also considering the performance of pilots, \citet{Wickens1999-la} investigated the effect of semi-reliable data while piloting a plane. They also investigated semi-reliable performance of a system that highlighted important data for the pilot to see. The pilots were aware that the measurements/highlighting system might be inaccurate before the experiment. The reliability of the systems did have an effect on the outcome of the experiment, but interestingly did not make a measurable effect on the pilot's self-reported trust. This underscores the point that TRBs ought to be the focus of assurances as opposed to trust itself, since trust is a subjective measure that may or may not actually change a person's TRBs.

McKnight and collaborators have spent significant time investigating trust between humans and technology. His initial research was focused on e-commerce settings but later moved to trust between humans and technology. In \cite{Mcknight2011-gv} they gather self-reported trust through a questionnaire. Their experiment was interested in identifying the dimensions of trust effected by learning to use Excel for use in a business class. The results were based solely on the intrinsic properties of excel and how each individual perceived them.

In \cite{Lankton2008-ct} and later in \cite{Tripp2011-rx} they investigate the difference in trust between humans and trust between a human and technology. They found that as the technology becomes more `human-like' the self-reported trust has more similarities to trust between humans. This study was performed using Microsoft Access, a recommendation assistant, and Facebook. Respondents were asked to rate how each software `kept its commitments' and how `human-like' it was. Again, these impressions were based solely on the intrinsic properties of each of the three AIAs used in the experiment.

\citet{Freedy2007-sg} studied how `mixed-initiative teams' (MITs, their term for human-robot teams) might have their performance measured. The premise of the work is that MITs can only be successful if ``humans know how to appropriately trust and hence appropriately rely on the automation''. They explore this idea by using a tactical game where human participants supervised an unmanned ground vehicle (UGV) as part of a reconnaissance platoon. UGVs had three levels of capability (low, medium, high), and had autonomous targeting and firing capability which the operator needed to monitor in case the UGV could not perform as desired. Operators were trained to recognize signs of failure, and to only intervene if they thought the mission completion time would suffer. Trust was formally acknowledged in this survey and was quantified by using Relative Expected Loss (REL), which is the mean expected loss of robot control over $n$ trial runs. Operators were found to be more likely to use a `medium' ability UGV if they had first encountered a `high' ability UGV, as opposed to encountering a `low' ability UGV first, which is another manifestation of framing effects like \cite{Riley1996-qm}. Similar to \cite{Muir1996-gt} the operators learned to trust a UGV with low competence as long as it behaved consistently. \edit{make sure REL is explained well}

In a similar vein \citet{Desai2012-rc} investigated the effects of robot reliability on the trust of human operators. In this case a human participant needed to work with an autonomous robot to search for victims in a building, while avoiding obstacles. The operator had the ability to switch the robot from manual (teleoperated) mode, to semi-autonomous, or autonomous mode depending on how they thought they could trust the system to perform. During this experiment the reliability of the robot was changed in order to observe the effects on the operator's reliance to the robot. Trust was measured by the amount of time the robot spent in different levels of autonomy (i.e. manual vs. autonomous), and it was found that trust changed based on the levels of reliability of the robot.

\citet{Salem2015-md} investigated the effects of error, task type, and personality on cooperation and trust between a human and robot. In this case the robot was a domestic robot that performed tasks around the house. A human guest was welcomed to the home and observed the robot operating on different tasks. After this observation (in which the robot implicitly showed competence by its mannerisms and successes/failures) the human participant was asked to cooperate with the robot on certain tasks. Interestingly, it was found that self-reported trust was affected by faulty operation of the robot, but that it didn't seem to have an effect on whether the human would cooperate on other tasks. This seems to suggest that the effect of institutional trust (i.e. this robot may not be competent, but whoever designed it must have known what they were doing) allowed users to continue to cooperate with a faulty system, even if they have low levels of trust in it.

\citet{Wu2016-ei} use game theory to investigate whether a person's decisions are affected by whether they believe they are playing a game against a human or an AI. This idea was studied in the context of a coin entrustment game, in which trust is measured by the number of coins a participant is willing to lose by putting them at risk of the other player. On the surface, their work is meant to be a study of the implicit differences in trustworthiness between humans and robots; however in their experiment the `human' was actually an AI with some programmed idiosyncrasies to lead the human player to believe the AI was a human. This was done by adding a random wait time, as opposed to an instantaneous move that the AI would make. There were also prompts at the beginning of the `human' version of the experiment that suggested that the participant was waiting for another human player to join the game. The experiment found that humans trust an AI more than they trust a `human'. The authors suggest that this may be due to the perception that an AI does not have feelings and is operating in a more predictable way. Given that the `human' was an algorithm as well, this experiment shows that consistency (i.e. no variable wait times) was a factor that affected the trust of the participant.

\citet{Bainbridge2011-pl} investigated the difference in trust between a human and a robot, in cases where the robot was physically present and where the robot was only displayed on a screen (i.e. not physically present). In this experiment, the only method of communication from the robot was through physical gestures. Trust was measured by the willingness of the human participants to cooperate with the robot. Among other interesting findings regarding how the participants interacted with the robot, it was found that participants were more likely to cooperate with the robot when it was physically present. \citeauthor{Bainbridge2011-pl} suggest that this is due to increased trust, in this case cooperation is a TRB.

With the aim of understanding how individuals currently trust autonomous vehicles, \citet{Munjal_Desai2009-en} performed a survey of around 175 participants. The participants were asked to rate their level of comfort with six different situations. These situations ranged from parking your own car, having an autonomous vehicle with manual override park the car, and having a fully autonomous vehicle that could not be overridden park the car. There were also questions related to user comfort with autonomy in situations where they still retained control, like how comfortable users would be with having autonomous vehicles park near their car. The survey found that the participants were most comfortable with parking their own car, and least comfortable with having a fully autonomous vehicle (with no manual override) park their car. These findings are supposedly related to institutional trust, as those surveyed did not necessarily have any experience with autonomous vehicles.

\subsubsection{Summary}
There have been several experiments that have formally shown that implicit assurances have an effect on a user's trust towards an AIA. Generally these findings have been accompanied by the advice that designers should consider that trust is an important element and can affect user's behavior towards an AIA. 

Generally intrinsic assurances can affect any of the three key trust dimensions (from Figure~\ref{fig:Assurance_classes}). To speak more specifically, reliability (or rather the perception of reliability built over contiguous observations) was frequently used in this section. A reliable AIA seems more competent, and predictable to a human user. This is to say that there are not inherent limitations on intrinsic assurances that limit the dimensions of trust that they can affect.

Generally this body of work investigates what kinds of properties of AIAs affect the trust of human users. We see some evidence for the idea that a user will gather assurances, whether these are implicit or explicit, in order to execute TRBs. That is to say that in the absence of explicitly provided assurances a user will still use other perceived properties and behaviors and gather assurances in order to inform their trust-related behaviors.

Even if an AIA has the capability to calculate an assurance, it must still have a way by which to express that assurance to a human user. The human user then perceives the assurance, perhaps through interaction with the system, or only from observation of the system. These kinds of perceptions can be based on displayed information, or on how the AIA `behaves' (as in \cite{Salem2015-md}). Once the user perceives some kind of assurances (perhaps not purposefully communicated), those assurances are integrated into the trust of the user towards the AIA. In most cases the surveyed research focuses on assurances given through sight, but sound and touch cannot be totally discounted because they may likely have played a part in some of the interfaces between the humans and AIAs. 

We also see evidence that human cognitive limitations need to be taken into account when designing assurances. This was directly observed in \cite{Freedy2007-sg,Riley1996-qm} where framing effects biased operators decisions. This also suggests that other cognitive limitations such as `recency effects' (biased based on recent experience), and others likely apply. Another limitation is the time required for humans to build statistical models, such as reliability, when only instance by instance data is available.

From this research we are able to get a feeling for what indicates changes in trust, and how it should be measured. Specifically: through measuring different actions, called TRBs in this document, and through self-reported changes in perceived trustworthiness. Implicit assurances are not targeted at specific trust dimensions. It seems, that except in a very controlled environment, that they originate from several different sources of AIA capabilities at once, as they are only based on a user's perception and would be unconsciously combined into an overall `sense' of trustworthiness.
