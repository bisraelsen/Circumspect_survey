\subsection{Quadrant II.}\label{sec:q2}
Muir investigated explicit assurances that automation could give to human operators in order to affect trust. She began by investigating formal models of trust between humans and then extending those concepts to trust between humans and automation. In \cite{Muir1987-mk} and \cite{Muir1994-ow} she investigated how decision-aids could be designed in order to affect trust. Within the framework of a trust model she suggested that a user must be able to ``perceive a decision aid's trustworthiness'', which could be accomplished by providing data regarding the system's competence on a certain task. She also suggests that summary information regarding the system's reliability over time would help. Finally she suggests improving the ``observability'' of the machine behaviors so that the user can understand it more fully.

She goes on to suggest that the user must have an accurate criteria for evaluating the system's trustworthiness. This would involve understanding the domain of competence, the history of competence, and criteria for acceptable performance of the system. She also suggests that a user must be able to allocate tasks to the system in order to feel equal in the trust relationship. The idea is that a relationship in which only one entity makes decisions is not amenable to calibrated trust. Finally she suggests that it is important to identify and `calibrate' areas where a user may trust the system inappropraitely. One key shortcoming of her work is that she suggests types of explicit assurances, but does not suggest concrete approaches to implement them, or test them by experimentation.

\citet{Dragan2013-wd} investigated how a robot could move in a `legible' way, or in other words, make movements that in themselves convey the intended destination. They investigate this within the context of how quickly a human participant is able to predict the goal of a moving point, before the point actually reaches that goal. They found that legible motion does in fact improve the user's ability to understand and predict where the robot is trying to move. It is difficult to classify this work because it does not directly address or consider human trust, but it is clearly an explicit motion-predictability assurance. Furthermore, they run human experiments in order to validate that the calculated motions are in fact more interpretable to users. \nisarcomm{How generalizable or extensible are their ideas to other domains of AIA capabilities, i.e. perception, reasoning, etc. -- are they only limited to talking about physical motion? Is this a limitation of their work?}

\citet{Kaniarasu2013-ho} examine whether or not misuse and disuse (over and under trust, in their terminology) can be detected and the calibrated (aligned) to be within its competence. They use data from an experiment of a robot with a confidence indicator in a user interface. A user was asked to provide trust feedback every twenty seconds while operating a robot along a specified path without hitting obstacles and responding to secondary tasks. The user was able to switch between partially autonomous and fully autonomous modes. The experiment indicated that indicators of confidence did in fact reduce misuse and disuse. \nisarcomm{HOW was the confidence computed? Is this a generalizable method? }

Although they don't perform an experiment \citet{Chen2014-dk} lay out a framework that is based in formal trust models. The aim is to make the situation and agent more transparent to a collaborative human user. They propose explicit feedback that can support the three levels of an operator's situational awareness (SA). They call their model the SA-based Agent Transparency (SAT) model. The first level -- Basic Information -- includes understanding the purpose, process, and performance of the system. The second level -- Rationale -- includes being able to understand the reasoning process of the system. The third level -- Outcomes -- involves being aware of future projections and potential limitations of the system. They suggest that two main components are display of information, and the display of uncertainty, but note that there are many considerations to take into account when trying to communicate information to human users. For example, numerical probabilities can be confusing and may need to be replaced by confidence limits or ranges. \nisarcomm{again, talk about impact and limitations/open questions}

\citet{Chadalavada2015-wx} investigated way to make interaction between humans and robots more natural in a setting where they may occupy the same work space. Their approach was to have the robot project its intended movements onto the ground in order to indicate where it would be moving. They performed experiments in which participants were asked to answer questionairres regarding how predictable, reliable, and transparent the robot was when it was projecting its intentions, and when it wasn't. There was a significant improvement in all measures when the robot was projecting its intended movements. \nisarcomm{again, talk about impact and limitations/open questions}

\citet{Wang2016-id} investigate how explanations of robot reasoning can effect human-robot trust as measured by subjective questionnaires. In their experiment a human and robot need to investigate a town; the robot has sensors that detect danger, and will recommend that the human wear protective gear if it senses danger. However, the human may choose to not accept the recommendation based on their trust in the robot, and the need to avoid the delay associated with using the protective gear. The robot is able to pose natural-language explanations for its recommendations, as well as report its ability. They used explanations generated by components of the robot's planning model (the robot uses a partially observable Markov decision process (POMDP) planner). This allowed it to make statements like: ``I think the doctor's office is safe'', or ``My sensors have detected traces of dangerous chemicals'', or ``My image processing will fail to detect armed gunmen 30\% of the time''. They found that when the robot's ability was low, the explanations helped build trust. Whereas, when the capability was high, the explanations didn't have a significant effect. \nisarcomm{again, talk about impact and limitations/open questions of this approach}


Also regarding a robot with POMDP planning and actions, \citet{Aitken2016-fb} and \citet{Aitken2016-cv} consider a formal model of trust, and propose a metric called `self-confidence' that is a composite assurance for a robot that uses a POMDP for action. There are five component assurances that make up the composite: 1) Model Validity, 2) Expected Outcome Assessment, 3) Solver Quality, 4) Interpretation of User Commands, and 5) Past Performance. 

Self-confidence is reported as a single value between $-1$ (complete lack of confidence), and $1$ (complete confidence). A self-confidence value of $0$ reflects total uncertainty. Each of the component assurances would be useful on its own, but the composite is meant to distill the information from the five different areas so that a (possibly novice) user can quickly, and easily evaluate the confidence of the robot to perform in a given situation. Currently only one of the five algorithms (Expected Outcome Assessment) has been developed, but there is continuing work on the other metrics. As of the writing of this document no human experiments have been performed to validate the effect of the self-confidence metric.

Finally, \citet{Wu2016-ei} use game theory to investigate whether a person's decisions are affected by whether they believe they are playing a game against a human or an AI. This idea was studied in the context of a coin entrustment game, they measure trust as the number of coins a participant is willing to put at risk. On the surface the paper is meant to be a study of the implicit differences between humans and robots, however in their experiment the `human' was actually an AI with some programmed idiosyncrasies to lead the human player to believe the AI was a human. This was done by adding a random wait time, as opposed to an instantaneous move that the AI would make. There were also prompts at the beginning of the `human' version of the experiment that suggested that the participant was waiting for another human player to join the game. The experiment found that humans trust an AI more than they trust a `human'. The authors suggest that this may be due to the perception that an AI does not have feelings and is operating in a more predictable way. Given that the `human' was an algorithm as well, this experiment shows that consistency (i.e. no variable wait times) was a factor that affected the trust of the participant. Since this behavior was explicitly added to the logic it is a planning-predictability assurance. \nisarcomm{again, talk about impact and limitations/open questions of this approach -- e.g. could/have such approaches be extended to things like designing perception or learning assurances, or is this an open question? }
