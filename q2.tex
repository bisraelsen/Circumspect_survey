\subsection{Quadrant II. (Explicit Assurances, Formal Trust Treatment)}\label{sec:q2}
Muir investigated explicit assurances that automation could give to human operators in order to affect trust. She began by investigating formal models of trust between humans and then extending those concepts to trust between humans and automation. In \cite{Muir1987-mk} and \cite{Muir1994-ow} she investigated how decision-aids could be designed in order to affect trust. Within the framework of a trust model she suggested that a user must be able to ``perceive a decision aide's trustworthiness'', which could be accomplished by providing data regarding the system's competence on a certain task. She also suggests that summary information regarding the system's reliability over time would help. Finally she suggests improving the ``observability'' of the machine behaviors so that the user can understand it more fully.

She goes on to suggest that the user must have an accurate criteria for evaluating the system's trustworthiness. This would involve understanding the domain of competence, the history of competence, and criteria for acceptable performance of the system. She also suggests that a user must be able to allocate tasks to the system in order to feel equal in the trust relationship. The idea is that a relationship in which only one entity makes decisions is not amenable to calibrated trust. Finally she suggests that it is important to identify and `calibrate' areas where a user may trust the system inappropriately. One key shortcoming of her work is that she suggests types of explicit assurances, but does not suggest concrete approaches to implement them, or test them by experimentation.

\citet{Kaniarasu2013-ho} examine whether or not misuse and disuse (over and under trust, in their terminology) can be detected and the calibrated (aligned) to be within its competence. They use data from an experiment of a robot with a confidence indicator in a user interface (an indication based on the robot's knowledge of its own reliability). A user was asked to provide trust feedback every twenty seconds while operating a robot along a specified path without hitting obstacles and responding to secondary tasks. The user was able to switch between partially autonomous and fully autonomous modes. The experiment indicated that the indicators of confidence did in fact reduce misuse and disuse. However at this point of time most robots do not `know' how reliable they are. This highlights a common shortcoming in the design of current robots: that they cannot quantify their own reliability.

Although they don't perform any formal experiments, \citet{Chen2014-dk} lay out a framework for agent transparency. This is in the setting of situational awareness (SA, \cite{Endsley1995-ie}) of an autonomous squad member in a military operation. The framework is based in formal trust models. The aim is to make the mission SA and agent more transparent, so that the user will be able to trust the agent and use its assistance. They propose explicit feedback that can support the three levels of an operator's SA. They call their model the SA-based Agent Transparency (SAT) model. The first level -- Basic Information -- includes understanding the purpose, process, and performance of the system. The second level -- Rationale -- includes being able to understand the reasoning process of the system. The third level -- Outcomes -- involves being aware of future projections and potential limitations of the system. They suggest that two main components are display of information, and the display of uncertainty, but note that there are many considerations to take into account when trying to communicate information to human users. For example, numerical probabilities can be confusing and may need to be replaced by confidence limits or ranges. This work indicates the importance of different levels of assurances; in some situations only a certain subset of information may be needed. A key limitation is that, generally, not all of the elements of the SAT framework are available to AIAs at this time, this can be due to design limitations as well as theoretical limitations (i.e. no method might exist to quantify the future performance of a model in an environment in which it has never been deploye).

\citet{Dragan2013-wd} investigated how a robot could move in a `legible' way, or in other words, make movements that in themselves convey the intended destination. This kind of problem is important for situations in which a robot and person are collaboratively working in close proximity to each other. They investigate this within the context of how quickly a human participant is able to predict the goal of a moving point, before the point actually reaches that goal. They found that legible motion does in fact improve the user's ability to understand and predict where the robot is trying to move. It is difficult to classify this work because it does not directly address or consider human trust, but it is clearly an explicit motion-predictability assurance. Furthermore, they run human experiments in order to validate that the calculated motions are in fact more interpretable to users. Their work focuses mainly on the premise that some deviation from the most cost optimal trajectory makes motion more legible, this idea can be extended in certain situations where humans rely on redundant or non-optimal behavior to predict outcomes, although it is contrast to \citet{Wu2016-ei}.

\citet{Wu2016-ei} use game theory to investigate whether a person's decisions are affected by whether they believe they are playing a game against a human or an AI. This idea was studied in the context of a coin entrustment game, in which trust is measured by the number of coins a participant is willing to lose by putting them at risk of the other player. On the surface, their work is meant to be a study of the implicit differences in trustworthiness between humans and robots; however in their experiment the `human' was actually an AI with some programmed idiosyncrasies to lead the human player to believe the AI was a human. This was done by adding a random wait time, as opposed to an instantaneous move that the AI would make. There were also prompts at the beginning of the `human' version of the experiment that suggested that the participant was waiting for another human player to join the game. The experiment found that humans trust an AI more than they trust a `human'. The authors suggest that this may be due to the perception that an AI does not have feelings and is operating in a more predictable way. Given that the `human' was an algorithm as well, this experiment shows that consistency (i.e. no variable wait times) was a factor that affected the trust of the participant. Since this behavior was explicitly added to the logic, it could be considered an explicit type of assurance. This is a nuance that, while forced by the author in this situation, is important to understand. If an assurance is purposefully designed to affect the trust of a user, then it is explicit (i.e. implicit assurances are non-purposeful). This kind of assurance is similar to that of \citeauthor{Dragan2013-wd}, except not backed with theory.

In a similar vein, \citet{Chadalavada2015-wx} investigated ways to make interaction between humans and robots more natural, in settings where they may occupy the same work space. Their approach was to have the robot project its intended movements onto the ground in order to indicate where it would be moving. They performed experiments in which participants were asked to answer questionnaires regarding how predictable, reliable, and transparent the robot was when it was projecting its intentions, and when it wasn't. There was a significant improvement in all measures when the robot was projecting its intended movements. This is strong evidence for an assurance aimed at predictability.

Turning to the question of how explanations of robot reasoning can effect human-robot trust,  \citet{Wang2016-id} performed an experiment in which a human and robot need to investigate a town; the robot has sensors that detect danger, and will recommend that the human wear protective gear if it senses danger. However, the human may choose to not accept the recommendation based on their trust in the robot, and the need to avoid the delay associated with using the protective gear. The robot is able to pose natural-language explanations for its recommendations, as well as report its ability. They used explanations generated by components of the robot's planning model (the robot uses a partially observable Markov decision process (POMDP) planner). This allowed it to make statements like: ``I think the doctor's office is safe'', or ``My sensors have detected traces of dangerous chemicals'', or ``My image processing will fail to detect armed gunmen 30\% of the time''. They found that when the robot's ability was low, the explanations helped build trust. Whereas, when the capability was high, the explanations didn't have a significant effect. This suggests that in some cases some kinds of assurances may be overridden by other `stronger' ones, specifically the explanations were rendered useless by the high reliability. 

Also examining POMDP-based robot planning and actions, \citet{Aitken2016-fb} and \citet{Aitken2016-cv} consider a formal model of trust, and propose a metric called `self-confidence' that is an assurance for UGV that is with a  POMDP planner (in this case, for he road network application described earlier in Section \ref{sec:mot_example}. It is made of a combination of five component assurances: 1) Model Validity, 2) Expected Outcome Assessment, 3) Solver Quality, 4) Interpretation of User Commands, and 5) Past Performance. The components considered are fairly general, and applicable to most planners, but would require new algorithms to be designed for those methods. Model validity attempts to quantify the validity of a model within the current situation. The expected outcome assessment uses the distribution over rewards to indicate how beneficial or detrimental the outcome is likely to be. Solver quality seeks to quantify how well a specific POMDP solver is likely to perform in the given problem setting (i.e. how precise the solution can be given a POMDP description). The interpretation of commands component is meant to quantify how well the objective has been interpreted (i.e. how sure am I that I was commanded to move forward). Finally past performance, is meant to add in empirical experience from past missions, in order to make up for theoretical oversights.

Self-confidence is reported as a single value between $-1$ (complete lack of confidence), and $1$ (complete confidence). A self-confidence value of $0$ reflects total uncertainty. Each of the component assurances would be useful on its own, but the composite `sum' of each factor is meant to distill the information from the five different areas, so that a (possibly novice) user can quickly and easily evaluate the ability of the robot to perform in a given situation. Currently, only one of the five algorithms (Expected Outcome Assessment) has been developed, but there is continuing work on the other metrics. As of the writing of this document, no human experiments have been performed to validate the usefulness of the self-confidence metric. 

\subsubsection{Summary}
There is an interesting comparison between \cite{Wu2016-ei} and \cite{Dragan2013-wd}: whereas \cite{Wu2016-ei} was able to demonstrate increased trust by removing the idea of human variability and greed, \cite{Dragan2013-wd} exploited the imperfection of human motion in order to help humans trust AIAs more. This indicates that the effectiveness of assurances change with the context of the interaction and task being accomplished. There are also times when certain assurances (performance) will override other assurances (display of explanations). This is not surprising because of our experience in interpersonal relationships, e.g. where `actions speak louder than words'. 

\citeauthor{Wang2016-id} and \citeauthor{Aitken2016-fb} consider giving calculated assurances to a user in the form of natural language feedback and simplified analysis of the task. Both (and this could be said of the other papers in this section as well) calculate assurances that are the same in given scenarios (i.e. they won't change with of different sill level). \citet{Wang2016-id} is focused on feedback \emph{during} the task, while \citet{Aitken2016-fb} is focused on analysis before the task, although it seems like both could be extended in theory.
