\subsection{Quadrant II.}
Muir investigated explicit assurances that automation could give to human operators in order to affect trust. She began by investigating formal models of trust between humans and then extending those concepts to trust between humans and automation. In \cite{Muir1987-mk} and \cite{Muir1994-ow} she investigated how decision-aids could be designed in order to affect trust. Within the framework of a trust model she suggested that a user must be able to ``perceive a decision aid's trustworthiness'', which could be accomplished by providing data regarding the system's competence on a certain task. She also suggests that summary information regarding the system's reliability over time would help. Finally she suggests improving the ``observability'' of the machine behaviors so that the user can understand it more fully.

She goes on to suggest that the user must have an accurate criteria for evaluating the system's trustworthiness. This would involve understanding the domain of competence, the history of competence, and criteria for acceptable performance of the system. She also suggests that a user must be able to allocate tasks to the system in order to feel equal in the trust relationship. The idea is that a relationship in which only one entity makes decisions is not amenable to calibrated trust. Finally she suggests that it is important to identify and `calibrate' areas where a user may trust the system inappropraitely. One key shortcoming of her work is that she suggests types of explicit assurances, but does not suggest concrete approaches to implement them, or test them by experimentation.

\citet{Dragan2013-wd} investigated how a robot could move in a `legible' way, or in other words, make movements that in themselves convey the intended destination. They investigate this within the context of how quickly a human participant is able to predict the goal of a moving point, before the point actually reaches that goal. They found that legible motion does in fact improve the user's ability to understand and predict where the robot is trying to move. It is difficult to classify this work because it does not directly address or consider human trust, but it is clearly an explicit motion-predictability assurance. Furthermore, they run human experiments in order to validate that the calculated motions are in fact more interpretable to users.

\citet{Kaniarasu2013-ho} whether or not misuse and disuse (over and under trust, in their terminology) can be detected and the calibrated (aligned) to be within its competence. They use data from an experiment of a robot with a confidence indicator in a user interface. A user was asked to provide trust feedback every twenty seconds while operating a robot along a specified path without hitting obstacles and responding to secondary tasks. The user was able to switch between partially autonomous and fully autonomous modes. The experiment indicated that indicators of confidence did in fact reduce misuse and disuse.

Although they don't perform an experiment \citet{Chen2014-dk} lay out a framework that is based in formal trust models. The aim is to make the situation and agent more transparent to a collaborative human user. They propose explicit feedback that can support the three levels of an operator's situational awareness (SA). They call their model the SA-based Agent Transparency (SAT) model. The first level -- Basic Information -- includes understanding the purpose, process, and performance of the system. The second level -- Rationale -- includes being able to understand the reasoning process of the system. The third level -- Outcomes -- involves being aware of future projections and potential limitations of the system. 

They suggest that two main components are display of information, and the display of uncertainty, but note that there are many considerations to take into account when trying to communicate information to human users. For example, numerical probabilities can be confusing and may need to be replaced by confidence limits or ranges.

\citet{Wang2016-id} investigate how explanations of robot reasoning can effect human-robot trust as measured by subjective questionairres. In their experiment a human and robot need to investigate a town; the robot has sensors that detect danger, and will recommend that the human wear protective gear if it senses danger. However, the human may choose to not accept the recommendation based on their trust in the robot, and the need to avoid the delay associated with using the protective gear.

The robot is able to pose natural-language explanations for its recommendations, as well as report its ability. They found that using explanations that are generated based on the components of the robot's planning model (the robot uses a partially observable Markov decision process (POMDP) planner). And can make statements like: ``I think the doctor's office is safe'', or ``My sensors have detected traces of dangerous chemicals'', or ``My image processing will fail to detect armed gunmen 30\% of the time''. They found that when the robot's ability was low, the explanations helped build trust. Whereas, when the capability was high, the explanations didn't have a significant effect.

Also regarding a robot with POMDP planning and actions, \citet{Aitken2016-fb} and \citet{Aitken2016-cv} consider a formal model of trust, and propose a metric called `self-confidence' that is a composite assurance for a robot that uses a POMDP for action. There are five component assurances that make up the composite: 1) Model Validity, 2) Expected Outcome Assessment, 3) Solver Quality, 4) Interpretation of User Commands, and 5) Past Performance. 

Self-confidence is reported as a single value between $-1$ (complete lack of confidence), and $1$ (complete confidence). A self-confidence value of $0$ reflects total uncertainty. Each of the component assurances would be useful on its own, but the composite is meant to distill the information from the five different areas so that a (possibly novice) user can quickly, and easily evaluate the confidence of the robot to perform in a given situation. Currently only one of the five algorithms (Expected Outcome Assessment) has been developed, but there is continuing work on the other metrics. As of the writing of this document no human experiments have been performed to validate the effect of the self-confidence metric.
