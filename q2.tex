\subsection{Quadrant II. (Explicit Assurances, Formal Trust Treatment)}\label{sec:q2}
One of the first considerations in the study of human-automation trust relationships was the role of human perception. 
The question highlighted by this line of research is: how can a user trust an AIA that they cannot understand in some way? Furthermore, how can a user's trust be affected unless they can perceive the assurances being produced by the AIA?
\citet{Sheridan1984-kx} were interested in how users might understand a supervisory control system in a petroleum refinery, and suggested that information be made more `transparent' to the system designers. \citet{Muir1987-mk,Muir1994-ow} considered similar questions and concluded that making systems more `observable' is one way to improve the trust between a human operator and an autonomous control system. 

To improve the user's ability to understand how an AIA functions and correctly use it within its designed autonomous operating envelope, \citet{Aitken2016-fb} and \citet{Aitken2016-cv} propose a metric called `machine self-confidence' for providing users with better insight into autonomous decision making under uncertainty (in this case, for the road network UGV navigation application described earlier in Section \ref{sec:mot_example}). 
Self-confidence is defined as the machine's own perception of its ability to carry out tasks in light of uncertainties in its knowledge of the world, its own/self states, and its reasoning process and execution abilities. 
In this sense, self-confidence is an AIA's metacognitive assessment of its own behavior and `competency boundaries'. 
A computational measure for POMDP-based autonomous planning is defined from five component assurances (which are fairly general and applicable to most other kinds of planners, but would likely require new algorithms to be designed for those methods): 1) Model Validity, 2) Expected Outcome Assessment, 3) Solver Quality, 4) Interpretation of User Commands, and 5) Past Performance. 

The key idea behind this set of measures is to assess where and when approximations required for planning under uncertainty are expected to break down. Model validity attempts to quantify the validity of a model within the current situation. The expected outcome assessment uses the distribution over rewards to indicate how beneficial or detrimental the outcome is likely to be. Solver quality quantifies how a specific POMDP solver is likely to perform in the given problem setting (i.e. how close to optimal the solution policy an approximate solution policy can get). 
The interpretation of commands component is meant to quantify how well the objective has been interpreted (i.e. how sure is the AIA that it correctly interpreted mission specifications into relevant tasks and suitable goals). 
Finally, past performance is meant to add in empirical experience from past missions, in order to make up for theoretical oversights and account for learning-based processes. 
Self-confidence is reported as a single value between $-1$ (complete lack of confidence in achieving mission objectives) and $1$ (complete confidence in achieving mission objectives); a self-confidence value of $0$ reflects total uncertainty. 
Each of the component assurances could be useful on its own, but the composite `sum' of the factors is meant to distill the information from the five different areas, so that a (possibly novice) user can quickly and easily evaluate the ability of the AIA to perform in a given situation. 
Currently, only one of the five metrics (Expected Outcome Assessment) has been developed quantitatively, but there is continuing work on the other metrics and perform human experiments to validate the usefulness of the self-confidence metrics for AIAs. Other approaches for computing and communicating AIA self-confidence have also been proposed for more specific applications \cite{Hutchins2015-if, Kaipa2015-hy, Zagorecki2015-qy, Kuter2015-qh}. 

Related to the idea of machine introspection and self-confidence, another important line of work is the detection of and correction of appropriate (and inappropriate) uses of AIAs \cite{Muir1994-ow,Kaniarasu2013-ho}. 
\citet{Kaniarasu2013-ho} examined whether or not misuse and disuse (user over-/under-trust, in their terminology) can be detected and then calibrated to be within the AIA's competency boundaries. 
They use data from an experiment with a semi-autonomous robot that had a `confidence indicator' displayed through a user interface; the confidence level was heuristically pre-determined to indicate the robot's ability to navigate an obstacle-filled environment on its own. 
A user was asked to provide trust feedback every twenty seconds while operating a robot along a specified path without hitting obstacles and responding to secondary tasks. 
User trust was quantified by measuring how frequently they switched between partially autonomous and fully autonomous modes. The experiment found that the indicators of confidence did in fact reduce misuse and disuse. 
However, we note that the robot in this experiment (like other AIAs that exist today) are not equipped to `know' how reliable they are. This highlights a common shortcoming in the design of current robots: that they cannot quantify their own reliability. This motivates research into approaches like machine self-confidence. 

Several researchers have also examined the effects of `natural communication' between humans and AIAs. %This is done is several different ways by the literature surveyed here.
For example, \citet{Dragan2013-wd} found that `legible motion planning', i.e. planned robotic physical movements and gestures that, by themselves, convey intended actions and goals, could improve a user's ability to understand and predict where the robot is trying to move. 
Legible motion is used by humans, and is important for situations in which a robot and person are collaboratively working in close proximity to each other. %They investigate this within the context of how quickly a human participant is able to predict the goal of a moving point, before the point actually reaches that goal. 
This work focuses mainly on the premise that some deviation from the most cost optimal trajectory makes motion more legible, although the idea can be extended in certain situations where humans rely on redundant or non-optimal behavior to predict outcomes (contrast this with \citet{Wu2016-ei} from Section~\ref{sec:q1}). 
Along similar lines, \citet{Chadalavada2015-wx} experimentally showed the effectiveness a robot projecting its intended movements onto the ground to indicate where it would be moving.  %In their experiments, there was a significant improvement in all measures when the robot was projecting its intended movements. 
\citet{Szafir2014-ok,Szafir2015-iy} also experimentally showed the effectiveness of using a quad-copter's motion patterns and `turn signal' lighting to help users more easily interpret the intended movements and actions. 
These works provide strong support for `natural communication' assurances aimed at predictability. 

It is also natural for humans to explicitly explain their reasoning, decisions, and actions when interacting with other humans. 
\citet{Wang2016-id} explore this idea in an experiment where a human and robot investigate an urban environment, where the robot could detect potentially dangerous phenomena and recommend that the human wear protective gear if it sensed danger. 
However, the human could choose to not accept the recommendation based on their trust in the robot and their desire to avoid the delays associated with donning the protective gear. 
The robot provided explanations for its recommendations, and reported its ability to take certain actions, by translating the robot's POMDP planning model into human-understandable statements, e.g.: ``I think the doctor's office is safe'', ``My sensors have detected traces of dangerous chemicals'', or ``My image processing will fail to detect armed gunmen 30\% of the time''. 
The experiment used two levels of robot capability: `high', and `low'; and three levels of explanations: `confidence level', `observation', and `none'. 
It was found that explanations helped build trust when the robot's ability was low. 
Generally, confidence level and observation explanations had similar effects on trust, which showed improvements over no explanations. 
However, the explanations did not have a significant effect when the robot's capability was high. 
This suggests that, in some cases, some kinds of assurances are overridden by other `stronger' ones, i.e.  explanations (explicitly designed for affecting trust) can be rendered useless by high reliability (an implicit assurance). 
However, this work does not assess how much changes are trust are due to explanation content that helps the user actually understand the robot's decision making vs. the fact that the robot is attempting to provide some kind of explanation. 

\citet{Chen2014-dk} develop a formal trust-based framework for making autonomous agent decision-making processes and situational awareness (SA) \cite{Endsley1995-ie} more transparent for military operations. Their SA-based Agent Transparency (SAT) model uses explicit feedback from the agent to operator to support the three levels of an operator's SA. 
The first level -- Basic Information -- includes understanding the purpose, process, and performance of the system. 
The second level -- Rationale -- includes being able to understand the reasoning process of the system. 
The third level -- Outcomes -- involves being aware of future projections and potential limitations of the system. 
The authors propose that two major components of communication are display of information and the display of uncertainty. 
However, the authors note that there are many considerations to take into account when trying to communicate information to human users (for example, numerical probabilities can be confusing and may need to be replaced by confidence limits or ranges). 
This work emphasizes the importance of different levels of assurances: in some situations, only a certain subset of information will be needed. 
A key limitation is that this approach is largely notional; not all of the reporting capabilities required by the SAT framework are available to AIAs at this time. 
This is due to design limitations as well as theoretical limitations (i.e. no methods exist to quantify the future performance of a model in a completely novel environment). 

\subsubsection{Summary}
Explicit assurances have been formally considered vis-a-vis formal trust models as follows:
\begin{itemize}
    \item Making internal functions of an AIA transparent to users (i.e. explicit cues helping users understand different AIA capabilities) -- this includes consideration of how explanations can be generated for users, as well as how they should be presented. This can also include consideration of meta-level reasoning, i.e. explaining \textit{how} an AIA makes decisions, perceives, learns, etc., and not just \textit{what} it specifically decided, perceived, learned, etc. 
		%%Assurances can only be effective if a user can perceive them.
    \item Quantifying changes in trust and detecting appropriate/inappropriate trust levels (i.e. being able to measure the effects of different assurances) -- not only does this aid in verifying whether certain assurances function correctly, but also helps to identify certain behaviors/characteristics of users.
    \item Making interaction with an AIA `more natural' (i.e. explicit cues that compliment a human's existing capability for interacting in trust relationships) -- in essence, designing AIAs to utilize the existing interaction capabilities of humans, instead of relying on human users to develop new capabilities specialized for interactions with AIAs. This also considers \emph{how} an assurance can be calculated, and not only \emph{what} should be communicated. 
    \item Considering different levels of trust (and by extension, different levels of assurances) -- sometimes more basic information such as understanding the AIAs purpose is adequate for the level of trust necessary. At other times information about the AIAs rationale, or the outcomes is necessary for properly affecting trust.
\end{itemize}

Explicit assurances attempt to directly influence the dimensions of trust from Figure~\ref{fig:Assurance_classes}. 
It is therefore important to understand how assurances affect the user's ideas about the `competence', `predictability', and `situational normality' of an AIA in different situations and contexts. 
Here, researchers used metrics for tracking changes in the TRBs of a human user (e.g. by measuring the frequency of a user switching control from autonomous to semi-autonomous) in lieu of attempting to understand the self-reported user trust levels. 
This underscores the point made earlier for Quadrant I that -- unless the goal of the AIA is to affect the user's self-reported trust -- TRBs are arguably the most appropriate metric that can be used to gauge the effectiveness of assurances. %This is an idea that began in Quadrant I, but that became more clear in this quadrant.
