\subsection{Quadrant II. (Explicit Assurances, Formal Trust Treatment)}\label{sec:q2}
One of the first considerations in the study of human-automation trust relationships was the role of human perception. \citet{Sheridan1984-kx} was interested in how human users might understand how users might understand a supervisory control system in a petroleum refinery, and suggested that information be made more `transparent' to the system designers. \citet{Muir1987-mk,Muir1994-ow} considered similar questions and concluded that making systems more `observable' is one way to improve the trust between a human operator and an autonomous control system. The critical question highlighted by this research is: how can a user trust an AIA that they cannot understand in some way? Furthermore, how can a user's trust be affected unless they can perceive the assurances being produced by the AIA?

Also with the aim of improving the user's ability to understand how the AIA functions, \citet{Aitken2016-fb} and \citet{Aitken2016-cv} propose a metric called `self-confidence' that is an assurance for a UGV that is using a POMDP planner (in this case, for the road network application described earlier in Section \ref{sec:mot_example}). This metric is made of a combination of five component assurances: 1) Model Validity, 2) Expected Outcome Assessment, 3) Solver Quality, 4) Interpretation of User Commands, and 5) Past Performance. The components considered are fairly general, and applicable to most planners, but would require new algorithms to be designed for those methods. Model validity attempts to quantify the validity of a model within the current situation. The expected outcome assessment uses the distribution over rewards to indicate how beneficial or detrimental the outcome is likely to be. Solver quality seeks to quantify how well a specific POMDP solver is likely to perform in the given problem setting (i.e. how precise the solution can be given a POMDP description). The interpretation of commands component is meant to quantify how well the objective has been interpreted (i.e. how sure am I that I was commanded to move forward). Finally past performance, is meant to add in empirical experience from past missions, in order to make up for theoretical oversights.

Self-confidence is reported as a single value between $-1$ (complete lack of confidence), and $1$ (complete confidence). A self-confidence value of $0$ reflects total uncertainty. Each of the component assurances would be useful on its own, but the composite `sum' of each factor is meant to distill the information from the five different areas, so that a (possibly novice) user can quickly and easily evaluate the ability of the robot to perform in a given situation. Currently, only one of the five algorithms (Expected Outcome Assessment) has been developed, but there is continuing work on the other metrics. As of the writing of this document, no human experiments have been performed to validate the usefulness of the self-confidence metric. 

Another important consideration is detecting appropriate (and inappropriate) use of the AIA, and then calibrating it. This is something mentioned by \cite{Muir1994-ow,Kaniarasu2013-ho}. Specifically, \citet{Kaniarasu2013-ho} examined whether or not misuse and disuse (over and under trust, in their terminology) can be detected and then calibrated to be within the AIA's competence. They use data from an experiment with a robot that had a confidence indicator displayed through a user interface. A user was asked to provide trust feedback every twenty seconds while operating a robot along a specified path without hitting obstacles and responding to secondary tasks. The user trust was quantified by measuring how frequently they switched between partially autonomous and fully autonomous modes. The experiment found that the indicators of confidence did in fact reduce misuse and disuse. However, we note that at this point in time most robots are not equipped to `know' how reliable they are. This highlights a common shortcoming in the design of current robots: that they cannot quantify their own reliability.

Several researchers have attempted to make interaction between humans and AIAs more `natural'. This is done is several different ways by the literature surveyed here. For example, \citet{Dragan2013-wd} investigated how a robot could move in a `legible' way, or in other words, make movements that in themselves convey the intended destination. This kind of motion is used by humans, and is important for situations in which a robot and person are collaboratively working in close proximity to each other. They investigate this within the context of how quickly a human participant is able to predict the goal of a moving point, before the point actually reaches that goal. They found that legible motion does in fact improve the user's ability to understand and predict where the robot is trying to move. This work focuses mainly on the premise that some deviation from the most cost optimal trajectory makes motion more legible; this idea can be extended in certain situations where humans rely on redundant or non-optimal behavior to predict outcomes (contrast this with \citet{Wu2016-ei} from Section~\ref{sec:q1}).

Another example is the work of \citet{Chadalavada2015-wx} who investigated ways to make interaction between humans and robots more natural (i.e. being more predictable and using common human methods for communicating especially non-verbal communication) in settings where they occupy the same work space. Their approach was to have the robot project its intended movements onto the ground in order to indicate where it would be moving. In their experiments there was a significant improvement in all measures when the robot was projecting its intended movements. This is strong evidence for an assurance aimed at predictability. Similarly, in \citet{Szafir2014-ok,Szafir2015-iy} they investigated using a quad-copter's motion patterns, as well as signaling with light, to help users more easily interpret the intended movements and actions.

It is natural for humans to explain reasoning, and actions, when interacting with other humans. This idea is used by \citet{Wang2016-id} in their experiment in which a human and robot investigate a town. The robot in their experiment has sensors that detect danger, and will recommend that the human wear protective gear if it senses danger. However, the human can choose to not accept the recommendation based on their trust in the robot, and the need to avoid the delay associated with using the protective gear. The robot is able to pose natural-language explanations for its recommendations, as well as report its ability. This was done by creating explanations generated by translating the components of the robot's planning model (the robot uses a partially observable Markov decision process (POMDP) planner) to natural language. Examples of explanations include: ``I think the doctor's office is safe'', or ``My sensors have detected traces of dangerous chemicals'', or ``My image processing will fail to detect armed gunmen 30\% of the time''. During the experiment they used two levels of robot capability: `high', and `low'; and three levels of explanations: `confidence level', `observation', and `none'. They found that when the robot's ability was low, the explanations helped build trust. Generally, confidence level, and observation explanations had a similar effect on trust and both were an improvement over no explanations. Whereas, when the capability was high, the explanations didn't have a significant effect. This suggests that in some cases some kinds of assurances are overridden by other `stronger' ones, specifically the explanations (explicitly designed for affecting trust) were rendered useless by the high reliability (which can be thought of as an implicit assurance in this case) of the AIA.

\citet{Chen2014-dk} lay out a framework for agent transparency based on formal trust models. This work is in the setting of situational awareness (SA, \cite{Endsley1995-ie}) of an autonomous squad member in a military operation. The aim is to make the mission SA and agent more transparent, so that the user will be able to trust the agent and use its assistance. They propose explicit feedback that can support the three levels of an operator's SA. They call their model the SA-based Agent Transparency (SAT) model. The first level -- Basic Information -- includes understanding the purpose, process, and performance of the system. The second level -- Rationale -- includes being able to understand the reasoning process of the system. The third level -- Outcomes -- involves being aware of future projections and potential limitations of the system. They suggest that two main components are display of information, and the display of uncertainty, but note that there are many considerations to take into account when trying to communicate information to human users (for example, numerical probabilities can be confusing and may need to be replaced by confidence limits or ranges). This work indicates the importance of different levels of assurances; in some situations only a certain subset of information will be needed. A key limitation is that, generally, not all of the elements of the SAT framework are available to AIAs at this time, this can be due to design limitations as well as theoretical limitations (i.e. no method might exist to quantify the future performance of a model in an environment in which it has never been deployed).

\subsubsection{Summary}
Explicit assurances have been formally considered vis-a-vis trust in the following ways:

\begin{itemize}
    \item Making internal functions of an AIA observable to a human user (i.e. explicit cues helping users understand different AIA capabilities) -- this includes calculating appropriate values to help a human understand, as well as how to present them to the user. Assurances can only be effective if a user can perceive them.
    \item Quantifying trust (i.e. being able to measure the effects of different assurances) -- in order to ensure that trust is being affected one must be able to measure it. Not only does this aid in verifying whether certain assurances function correctly, but also identifying certain behaviors/characteristics of specific users.
    \item Making interaction with an AIA more natural (i.e. explicit cues that compliment a human's existing capability for interacting in trust relationships) -- in essence, designing AIAs to utilize the existing interaction capabilities of humans, instead of relying on human users to develop new capabilities specialized for interactions with AIAs. This also includes concerns regarding \emph{how} an assurance can be calculated, and not only \emph{what} should be communicated.
    \item Considering different levels of trust -- there are different levels of trust, and by extension assurances. Sometimes more basic information such as understanding the AIAs purpose is adequate for the level of trust necessary. At other times information about the AIAs rationale, or the outcomes is necessary for properly affecting trust.
\end{itemize}

Using explicit assurances is, unknowingly to some, an attempt to directly influence the dimensions of trust from Figure~\ref{fig:Assurance_classes}. Understanding how assurances affect the user's ideas about the `competence', `predictability', and `situational normality' of an AIA in a certain situation is an important consideration, but perhaps only in certain situations. Here researchers used metrics for tracking changes in the TRBs of a human user in lieu of attempting to understand the self-reported level of trust of the user, for example by measuring the frequency of a user switching control from autonomous to semi-autonomous. Further consideration highlights the point that -- unless the goal of the AIA is to affect the user's self-reported trust -- TRBs are really the most appropriate metric that can be used. This is an idea that began in Quadrant I, but that became more clear in this quadrant.
