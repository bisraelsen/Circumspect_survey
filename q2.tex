\subsection{Quadrant II. (Explicit Assurances, Formal Trust Treatment)}\label{sec:q2}
\citet{Sheridan1984-kx} were perhaps the first to attempt a formal approach to understanding the relationship between an operator/designer and a supervisory control system, such as a control system for a petroleum refinery. They considered psychological models of the human user and asked questions about how the user could understand how such systems worked. To this end they suggest that aspects of the control system need to be made transparent to the user so that the user has an accurate model of the system, but they don't offer concrete ways in which this can be accomplished. They mention that control displays should be designed according to the \emph{designer's} level of trust in the individual elements, but they seem to overlook the critical nature of what explicit assurances should be displayed to the \emph{operator} of the system, as opposed to what information might allow the \emph{designer} to have appropriate trust in the control system.

Muir investigated explicit assurances that automation could give to human operators in order to affect trust. She began by investigating formal models of trust between humans and then extending those concepts to trust between humans and automation. In \cite{Muir1987-mk} and \cite{Muir1994-ow} she investigated how decision-aids could be designed in order to affect trust. Within the framework of a trust model she suggested that a user must be able to ``perceive a decision aide's trustworthiness'', which could be accomplished by providing data regarding the system's competence on a certain task. She also suggests that summary information regarding the system's reliability over time would help. Finally she suggests improving the ``observability'' of the machine behaviors so that the user can understand it more fully.

She goes on to suggest that the user must have an accurate criteria for evaluating the system's trustworthiness. This would involve understanding the domain of competence, the history of competence, and criteria for acceptable performance of the system. She also suggests that a user must be able to allocate tasks to the system in order to feel equal in the trust relationship. The idea is that a relationship in which only one entity makes decisions is not amenable to calibrated trust. Finally she suggests that it is important to identify and `calibrate' areas where a user may trust the system inappropriately. One key shortcoming of her work is that she suggests types of explicit assurances, but does not suggest concrete approaches to implement them, or test them by experimentation.

\citet{Kaniarasu2013-ho} examine whether or not misuse and disuse (over and under trust, in their terminology) can be detected and then calibrated (aligned) to be within the AIA's competence. They use data from an experiment of a robot with a confidence indicator in a user interface (an indication based on the robot's knowledge of its own reliability). \nisarcomm{WHAT WAS THIS INDICATOR EXACTLY?? WAS IT A DISPLAY OF PROBABILITIES OF COLLISION? A COLOR CODED ALARM? SOMETHING ELSE? HOW WAS THE DATA DRIVING THE INDICATOR COMPUTED?? OR WAS THE INDICATOR DATA JUST RANDOMLY GENERATED??} A user was asked to provide trust feedback every twenty seconds while operating a robot along a specified path without hitting obstacles and responding to secondary tasks. The user was able to switch between partially autonomous and fully autonomous modes. The experiment found that the indicators of confidence did in fact reduce misuse and disuse. However, we note that at this point in time most robots are not equipped to `know' how reliable they are. This highlights a common shortcoming in the design of current robots: that they cannot quantify their own reliability.

Although they don't perform any formal experiments, \citet{Chen2014-dk} lay out a framework for agent transparency based on formal trust models. This is in the setting of situational awareness (SA, \cite{Endsley1995-ie}) of an autonomous squad member in a military operation. %The framework is based in formal trust models. 
The aim is to make the mission SA and agent more transparent, so that the user will be able to trust the agent and use its assistance. They propose explicit feedback that can support the three levels of an operator's SA. They call their model the SA-based Agent Transparency (SAT) model. The first level -- Basic Information -- includes understanding the purpose, process, and performance of the system. The second level -- Rationale -- includes being able to understand the reasoning process of the system. The third level -- Outcomes -- involves being aware of future projections and potential limitations of the system. They suggest that two main components are display of information, and the display of uncertainty, but note that there are many considerations to take into account when trying to communicate information to human users. For example, numerical probabilities can be confusing and may need to be replaced by confidence limits or ranges. This work indicates the importance of different levels of assurances; in some situations only a certain subset of information may be needed. A key limitation is that, generally, not all of the elements of the SAT framework are available to AIAs at this time, this can be due to design limitations as well as theoretical limitations (i.e. no method might exist to quantify the future performance of a model in an environment in which it has never been deployed).

\citet{Dragan2013-wd} investigated how a robot could move in a `legible' way, or in other words, make movements that in themselves convey the intended destination. This kind of problem is important for situations in which a robot and person are collaboratively working in close proximity to each other. They investigate this within the context of how quickly a human participant is able to predict the goal of a moving point, before the point actually reaches that goal. They found that legible motion does in fact improve the user's ability to understand and predict where the robot is trying to move. It is difficult to classify this work because it does not directly address or consider human trust, but it is clearly an explicit assurance making the AIA motion capability more predictable. Furthermore, they run human experiments in order to validate that the calculated motions are in fact more interpretable to users. Their work focuses mainly on the premise that some deviation from the most cost optimal trajectory makes motion more legible, this idea can be extended in certain situations where humans rely on redundant or non-optimal behavior to predict outcomes, although it is contrast to \citet{Wu2016-ei}.

In a similar vein, \citet{Chadalavada2015-wx} investigated ways to make interaction between humans and robots more natural, in settings where they may occupy the same work space. Their approach was to have the robot project its intended movements onto the ground in order to indicate where it would be moving. They performed experiments in which participants were asked to answer questionnaires regarding how predictable, reliable, and transparent the robot was when it was projecting its intentions, and when it wasn't. There was a significant improvement in all measures when the robot was projecting its intended movements. This is strong evidence for an assurance aimed at predictability.

Turning to the question of how explanations of robot reasoning can effect human-robot trust, \citet{Wang2016-id} performed an experiment in which a human and robot investigate a town; the robot has sensors that detect danger, and will recommend that the human wear protective gear if it senses danger. However, the human may choose to not accept the recommendation based on their trust in the robot, and the need to avoid the delay associated with using the protective gear. The robot is able to pose natural-language explanations for its recommendations, as well as report its ability. They used explanations generated by components of the robot's planning model (the robot uses a partially observable Markov decision process (POMDP) planner). This allowed it to make statements like: ``I think the doctor's office is safe'', or ``My sensors have detected traces of dangerous chemicals'', or ``My image processing will fail to detect armed gunmen 30\% of the time''. 
\nisarcomm{HOW DID IT ALLOW THE SYSTEM TO MAKE THESE STATEMENTS? WAS THIS A TRANSLATION OF THE POLICY, THE UNDERLYING STATE MODEL, SOMETHING ELSE -- THE POINT HERE IS NOT TO RECITE WHAT THE PAPER WAS ABOUT, BUT TO DISCUSS THE TECHNICALLY RELEVANT ASPECTS AND THE CONTRIBUTIONS/IMPACT ON WHAT YOU ARE TRYING TO STUDY, i.e. *HOW* TO COME UP ASSURANCES THAT AFFECT TRUST, AND ASESSING WHEN/WHERE TO USE SUCH ASSURANCES...}
They found that when the robot's ability was low, the explanations helped build trust. Whereas, when the capability was high, the explanations didn't have a significant effect. This suggests that in some cases some kinds of assurances may be overridden by other `stronger' ones, specifically the explanations (explicitly designed for affecting trust) were rendered useless by the high reliability (which can be thought of as an implicit assurance in this case) of the AIA.

Also examining POMDP-based robot planning and actions, \citet{Aitken2016-fb} and \citet{Aitken2016-cv} consider a formal model of trust, and propose a metric called `self-confidence' that is an assurance for UGV that is using a  POMDP planner (in this case, for the road network application described earlier in Section \ref{sec:mot_example}). It is made of a combination of five component assurances: 1) Model Validity, 2) Expected Outcome Assessment, 3) Solver Quality, 4) Interpretation of User Commands, and 5) Past Performance. The components considered are fairly general, and applicable to most planners, but would require new algorithms to be designed for those methods. Model validity attempts to quantify the validity of a model within the current situation. The expected outcome assessment uses the distribution over rewards to indicate how beneficial or detrimental the outcome is likely to be. Solver quality seeks to quantify how well a specific POMDP solver is likely to perform in the given problem setting (i.e. how precise the solution can be given a POMDP description). The interpretation of commands component is meant to quantify how well the objective has been interpreted (i.e. how sure am I that I was commanded to move forward). Finally past performance, is meant to add in empirical experience from past missions, in order to make up for theoretical oversights.

Self-confidence is reported as a single value between $-1$ (complete lack of confidence), and $1$ (complete confidence). A self-confidence value of $0$ reflects total uncertainty. Each of the component assurances would be useful on its own, but the composite `sum' of each factor is meant to distill the information from the five different areas, so that a (possibly novice) user can quickly and easily evaluate the ability of the robot to perform in a given situation. Currently, only one of the five algorithms (Expected Outcome Assessment) has been developed, but there is continuing work on the other metrics. As of the writing of this document, no human experiments have been performed to validate the usefulness of the self-confidence metric. 

\subsubsection{Summary}
The research in this section seems to focus on \emph{what} needs to be expressed as an assurance as well as \emph{how} to express it. One obvious take-away is that no AIA can express an explicit assurance unless it is able to first calculate it. We see that not all instances require the same kind of information, as discussed by \cite{Chen2014-dk}, depending on the task at hand different assurances will be more appropriate.

Some of the main approaches for assurances were: display of uncertainty \cite{Muir1996-gt,Aitken2016-fb}, natural language \cite{Wang2016-id}, legible motion \cite{Dragan2013-wd}, and display of intention \cite{Chadalavada2015-wx}. Each can affect the different dimensions of trust. For example displaying uncertainty can indicate how competent the AIA is, and  \edit{HELP -- you probably think this is funny, but I'm not really sure what to write here. I can argue that all assurances can be used for all dimensions of trust. but I don't think that is very useful.}

As in the first quadrant these researchers are formally considering the human users, but they are interested in different aspects of the human-AIA relationship. Both groups measure the TRBs and self-reported trust of human users, but the main differences are that this group is concerned with how to calculate assurances \emph{and} how to express those assurances.

We begin to see the critical role of human perception in assurances. Basically, an assurance can only be effective if a user can perceive it. To this end the researchers investigate ideas like visually displaying `summary data' \cite{Muir1996-gt}, displaying intended actions \cite{Chadalavada2015-wx}, making movements legible \cite{Dragan2013-wd}, and using natural language in order to aide the users in perceiving the assurance \cite{Wang2016-id}.

\citeauthor{Wang2016-id} and \citeauthor{Aitken2016-fb} consider giving calculated assurances to a user in the form of natural language feedback and simplified analysis of the task. Both (and this could be said of the other papers in this section as well) calculate assurances that are the same in given scenarios (i.e. they won't change with of different skill level, or a different user). \citet{Wang2016-id} is focused on feedback \emph{during} the task, while \citet{Aitken2016-fb} is focused on analysis before the task, although it seems like both could be extended in theory.
