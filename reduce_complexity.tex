\paragraph{Reduce Complexity} \label{sec:reduce_complexity}
Representations within an AIA are often complex. Sometimes using inherently less complex, `interpretable models' (as discussed in \ref{sec:interp_models}), is the most straight forward way to address this challenge. However, In some cases it is desirable to maintain complex, less interpretable representations and then reduce the inherent complexities (possibly post-hoc) to aid human users.

How should explanations be provided? There are also considerations regarding whether explanations should occur by two-way interaction between system and user, by natural language interaction, or by probabilities. This begins to cross into the realm of cognitive science. Some have used natural language (see \cite{Hayes2017-nt}) and other visualizations. Specifically, \citet{Olah2018-rp} investigate how predictions of NNs can be explained through visualizing how different parts of the network respond to certain images. They propose combining several different approaches to get a wholistic view of the NN behavior. Specifically, they use feature visualization (what a neuron is looking for), and attribution (how it affects the output).

There are several classes of explanations. \citet{Abdollahi2018-uw} propose three in the context of collaborative filtering: `neighbor style' (explanation based on examples from similar situations), `influence style' (present the most influential items that led to a certain model output), and `keyword style' (identify common features between user keywords, and content). \citet{Otte2013-oo} and \citet{Ribeiro2016-uc} implement analagous ideas in the realm of safe ML and interpreting classifiers respectively.

\citet{Huang2017-lk} use `algorithmic teaching'~\cite{Balbach2009-jw} as inspiration for helping human users learn a robot's true objectives. Algorithmic teaching involves having a model of a students learning algorithm, and then presenting training examples to allow the student to learn a target model. In this case the `student' is the human user, and the `teacher' is the robot that is trying to teach the human its own objective function by presenting a set of (optimal) training examples. Here we would consider the robots training examples as assurances.

Another consideration is whether an explanation is meant to be descriptive or aimed at ensuring comprehension, as well as whether explanations need to be on a macro or micro scale relative for parts of the Bayesian network (similar to globally/locally interpretable learned models \cite{Ruping2006-xj}).

\citet{Lacave2002-cu} address the AIA reduction of complexity from the perspective of explaining probabilistic inference in Bayesian networks---specifically, \emph{how} and \emph{why} a Bayesian network reaches a conclusion given some imputed evidence. 
They present three properties of explanation: 1) content (what to explain), 2) communication (how to explain), and 3) adaptation (how to adapt based on who the user is). Several key points for designing assurances arise from considering the differences between explaining evidence (i.e. data), the model (i.e. the Bayesian network itself), or the reasoning (i.e. the inference process).

\citet{Aitken2016-cv} propose a metric called `machine self-confidence' for providing users with better insight into autonomous decision making under uncertainty. This insight comes from breaking down the complex influences of uncertainty on the decision making process and presenting them to the user in a simple way. Self-confidence is defined as the machine's own perception of its ability to carry out tasks in light of uncertainties in its knowledge of the world, its own/self states, and its reasoning process and execution abilities. In this sense, self-confidence is an AIA's metacognitive assessment of its own behavior and `competency boundaries'. A computational measure for POMDP-based autonomous planning is defined from five component assurances (which are fairly general and applicable to most other kinds of planners): 1) Model Validity, 2) Expected Outcome Assessment, 3) Solver Quality, 4) Interpretation of User Commands, and 5) Past Performance. 

The key idea behind this set of measures is to assess where and when approximations required for planning under uncertainty are expected to break down. Model validity attempts to quantify the validity of a model within the current situation. The expected outcome assessment uses the distribution over rewards to indicate how beneficial or detrimental the outcome is likely to be. Solver quality quantifies how a specific POMDP solver is likely to perform in the given problem setting (i.e. how close to optimal the solution policy an approximate solution policy can get). The interpretation of commands component is meant to quantify how well the objective has been interpreted (i.e. how sure is the AIA that it correctly interpreted mission specifications into relevant tasks and suitable goals). Finally, past performance is meant to add in empirical experience from past missions, in order to make up for theoretical oversights and account for learning-based processes.

Self-confidence is reported as a single value between $-1$ (complete lack of confidence in achieving mission objectives) and $1$ (complete confidence in achieving mission objectives); a self-confidence value of $0$ reflects total uncertainty. Each of the component assurances could be useful on its own, but the composite `sum' of the factors is meant to distill the information from the five different areas, so that a (possibly novice) user can quickly and easily evaluate the ability of the AIA to perform in a given situation. Currently, only one of the five metrics (Expected Outcome Assessment) has been developed quantitatively, but there is continuing work on the other metrics and perform human experiments to validate the usefulness of the self-confidence metrics for AIAs. Other approaches for computing and communicating AIA self-confidence have also been proposed for more specific applications \cite{Hutchins2015-if, Kaipa2015-hy, Zagorecki2015-qy, Kuter2015-qh}. 
