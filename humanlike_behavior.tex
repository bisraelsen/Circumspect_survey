%%(i) assurance argument: what specifically is the assurance signal/rationale?  -- both value alignment and interpretable model approaches have the implicit aim of grounding user trust in an understanding of rational decision making processes; however, cognitive scientists and many AIA experts will point out, humans are not entirely rational actors. As such, what accounts for human-human trust?  Like interpretability, a precise definition is hard to pin down -- but perhaps essence can be best summed up as encoding AIA behaviors that avoid `inhuman-like' characteristics. This can be viewed as the algorithmic equivalent of avoiding the so-called `uncanny valley' in humanoid and social robotics and VR/AR domains. Indeed, the mere association of `human-like' behavior to an AIA can be enough to endow levels of trust in AIAs that are comparable to human-human relationships [Tripp, et al]. Basic idea then is to endow AIA with capabilities that can be interpreted by user as though they could have come from another human. 
%One example is provided by Wink Bennett's work for LVC-based simulation training of Air Force fighter pilots -- in the NotSoGrandChallenge, basic idea was to develop realistic enemy fighter plane AI that would keep trainee pilots honest, i.e. from finding loop holes in computer logic that would allow them to easily game the simulation and win predictably -- this requires not only intelligent mechanisms for adapting to different human trainees and exploring/exploiting their weaknesses through simulated combat interactions, but also requires AI to maintain some level of human credibility --- since trainees will eventually be up against real human fighter opponents, this application requires realistic human behavior for both trainers and trainees to trust the AIA is providing useful sim experience...
%%(ii) what is mechanism for generating appropriate TRBs? 
%Key idea here is to interpret/acknowledge the user's intent frame by producing suitable set of `expected' human behaviors. Assurances generated along these line tend towards improving an understanding of an AIA's predictability and situation normality, most often via verbal or non-verbal human interaction cues to express intent or uncertainty. This can be especially useful in complex problem settings where a utility-based or interpretable model-based understanding of the problem is insufficient for completely quantifying user preferences that can be used to inform `trustworthy' AIA responses (e.g. dog-fighting, or even highway driving [slowing down and coming to complete stop to yield right of way for pedestrians cross, using blinkers and horns to signal other drivers, etc.]...). This is also especially useful for interacting with users who are not privy to access `innards' of AIA, i.e. who cannot potentially act as all-seeing/all-knowing supervisors or operators, per se, but as in situ participants in task/environment that AIA must operate in... Caveat here is that users tend to fill in a lot of blanks on their own, so anthropomorphizing AIA can lead to other unintended consequences that are not present in value alignment or interpretable modeling -- namely, users might extrapolate other assurances or capabilities from AIA that it does not actually mean to signal or possess (e.g. eyes on quadcopter for signaling can't actually see anything...self-driving car cannot hear sounds or horns, even though it looks like a human-driven car and behaves as if it is being driven by a human...)
%%(iii) how can designers build/exploit for AIA assurances, i.e. what techniques available for human-like behavior?: wide variety of heuristics, but two salient categories pertaining to trust and assurances are 
%%(a) nonverbal communication of intent (e.g. physical gesturing, legible planning, turn signaling, etc.) 
%%(b) mannerisms: establishing implicit cues for interaction

\subsection{Human-Like Behavior} \label{sec:human_behavior}
Since humans are used to forming and evaluating trusting relationships with each other, imitation of human-human communication and interactive behaviors provides yet another avenue for developing AIA assurance strategies. 
%Researchers have pursued this idea by examining the communication humans use during interaction that helps them to understand and trust each other? 
Support for this idea is given by \citet{Tripp2011-rx}, who compared human trust in other humans against human trust in intelligent interactive technology. %, which in this case was represented by Microsoft Access, an intelligent recommendation assistant, and Facebook. 
They found that, as the technology becomes more `human-like', self-reported levels of trust in technology become more similar to levels of trust in other humans.
%%\brettcomm{picks up with (possibly) irrational behavior, not included in `value alignment'}
%
\citet{De_Visser2018-kd} also specifically discusses different methods by which AIAs can be more human-like in order to `repair trust' with users (here, trust repair is roughly analogous to assurances but focuses on re-building trust after it is lost). Among several other possibilities, they suggest that an AIA might repair trust by anthropomorphizing (responding using a human communication channel), or by explaining its actions in the same way a person would. %%\brettcomm{Perhaps discuss other methods?}
Such human-like behavior opens the door for AIAs to exhibit `non-rationally motivated' behaviors (i.e. suboptimal, as opposed to irrational actions), if these conform to social norms or other psychological cues that provide useful assurances about predictability (e.g. a robot arm that executes legible motions), competency (e.g. a robot which slowly backs away from unfamiliar or potentially dangerous objects), or situation normality (e.g. a robot car that apparently rubbernecks near an unfamiliar scene on the road). 

\subsubsection{Common Approaches}
Generally, we do not have algorithms that describe how humans interact with each other, and must settle for heuristics or best attempts to create human-like behavior via algorithms. From a high level, researchers have addressed these: nonverbal communication, and mannerisms.

\paragraph{Nonverbal Communication:} 
Nonverbal communication can take many different forms. One popular approach is to use motion or gestures.\citet{Szafir2014-ok} investigated how to enable `Assisted Free Flyer' robots (quad-copters that are made to interact with humans in close spaces) to communicate by using gestures. In doing so they use `motion primitives' (a basic vocabulary of movements) that were inspired by basic `character animation' principles \cite{Van_Breemen2004-rz}. 
In their evaluations of these primitives with human participants in the presence of free flyers, they found that human users significantly found the free flyers to be more natural, and felt safer around them. Later \citet{Szafir2015-iy} also experimentally showed the effectiveness of using illuminated `turn signals' and pairs of human-like `eyes' that shifted with free flyer heading (much as human eyes do when people walk in a crowd) to help users more easily interpret the vehicle's intended movements and actions. These works provide strong support for `commonsense communication' assurances aimed at predictability in physical user-AIA interactions (even if indicators like `moving eyes' do not actually see anything). %%note: FF and Baxter have non-seeing eyes which fulfil the same role...
%
Likewise, \citet{Dragan2013-wd} investigate `legible motion planning', i.e. planned robotic physical movements and gestures that, by themselves, convey intended actions and goals. For example, a table-setting robot may grasp a plate on both sides from the top using two end effectors if it intends to shift the position of the plate along the table surface, whereas it may grasp the same plate with only one hand from the side if it intends to pull away and remove the plate from the table. %These are meant to improve a user's ability to understand and predict where the robot is trying to move. 
Legible motion is used by humans working in close proximity, and so can also be useful and important for situations in which a physically embodied AIA and person are collaboratively working in close proximity to each other. Similarly, in more recent work \citet{Kwon2018-xt} investigates calculating trajectories that convey `incapability', which is \emph{what} the AIA is trying to do, and \emph{why} it is unable to do so. 
See also \cite{Admoni2016-db} for related work. %\cite{Dragan2013-wd} calculated sub-optimal paths in order to communicate \emph{intent} of a gesture, by gathering data from human participants they were able to find the parameters for their cost function that best matched the expectations. Likewise \cite{Kwon2018-xt} tested several different objectives using user studies to guide the selection. See also \cite{Admoni2016-db} for related work.

\paragraph{Mannerisms:}
Humans are naturally inclined to leverage social interaction cues and adherence to/violation of social norms as evidence for assessing the trustworthiness of other humans in everyday interactions. 
AIAs can leverage these inclinations to provide simultaneous assurances of their competence, predictability, and situation normality. 
Consider, for instance, a recent `mini-Turing Test' example from the popular media: at Google/IO 2018, Google Duplex \cite{Google2018-eb} was introduced through a demo where it placed a phone call to make a reservation. 
An oft-remarked feature of this demo is the great difficultly (if not near impossibility) of detecting whether or not the Duplex voice is human -- down to the words spoken, tone of voice, and speech mannerisms (which included `um\ldots', pauses, and shortened sentences). 
The human on the other end of the call was none the wiser, and trusted that they were in fact speaking to a regular human customer -- when in fact they were speaking in a completely natural manner to the product of a recurrent neural network (RNN) trained on anonymized phone conversation data. 

More formally, \citet{Salem2015-md} investigated the effects of autonomous task errors, task types, and `system personality' on cooperation and trust for humans who observed a domestic robot performing house tasks, such that the robot implicitly showed competence by its mannerisms and successes/failures during tasks. In this case, the mannerisms and competency of the robot were completely under control and hard-coded into the system. Regardless, when participants were asked to cooperate with the robot on certain other tasks, the faulty operation of the robot was found to influence the self-reported trust levels of the participants.

\citet{Wu2016-ei} investigated how a person's decisions in a coin entrustment game are affected by their belief in whether they are competing against an AIA or another human player (which, unbeknownst to participants, was in fact an AI with some programmed human-like idiosyncrasies, e.g. variable wait times between turns). Trust in this context was measured directly by the number of coins a participant was willing to lose by putting them at risk to the other player. The experiment found that the participants trusted the AI opponent more than they trusted the `human' opponent; the authors suggest that this may be due to the perception that the AI opponent did not have feelings and operated in a more predictable and consistent `machine-like' way. Given that the `human' was an AI as well, this experiment illustrates that `machine-like' behavioral consistency can lead to implicit positive effects the trust of the participant in certain contexts.

%
%To achieve this impressive result, Duplex was trained using a recurrent neural network (RNN) on anonymized phone conversation data. 
%Returning to the case of Google Duplex, another striking characteristic was that Duplex used mannerisims of speech such as saying `um\ldots', including pauses, and sometimes using shortened sentences, which made it even more difficult to distinguish between it and a real human. And during the demo calls, the human on the other end of the line was none the wiser, and trusted that they were in fact speaking to a human.
%%\brettcomm{Add something about etiquette?} \nisarcomm{this belongs in mannerisms}

\subsubsection{Grounding Example:}
In the case of the `VIP Escort' problem (described in Section~\ref{sec:mot_example}), human-like behavior might be used as an assurance in the following way:

We make the following assumptions

\begin{itemize}
    \item The UGV is about to begin an attempt at escaping the road-network
    \item The operator can observe all the actions of the UGV via video feeds at intersections
    \item The UGV has been designed with the ability to use gestures in order to indicate its `incapability' as in \cite{Kwon2018-xt}
\end{itemize}

As the UGV begins the escort problem, the human supervisor is monitoring progress. As the UGV reaches a certain intersection of the road network the supervisor expects the UGV to take a path $A$, but it does not. However, before choosing to take path $B$, the UGV made a movement that, to the operator, indicated that it considered attempting to traverse $A$. Due to the attempt the supervisor was able to surmise that the UGV wanted to take that path but couldn't due to some limitation.

\paragraph{\textbf{Discussion of Example:}} In this case the UGV is able to maintain appropriate trust of the supervisor because the supervisor was able to interpret the `gesture' that UGV was using. This highlights the assuring effects that human-like communication/behaviors can have on users.
