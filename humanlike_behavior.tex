%%(i) assurance argument: what specifically is the assurance signal/rationale?  -- both value alignment and interpretable model approaches have the implicit aim of grounding user trust in an understanding of rational decision making processes; however, cognitive scientists and many AIA experts will point out, humans are not entirely rational actors. As such, what accounts for human-human trust?  Like interpretability, a precise definition is hard to pin down -- but perhaps essence can be best summed up as encoding AIA behaviors that avoid `inhuman-like' characteristics. This can be viewed as the algorithmic equivalent of avoiding the so-called `uncanny valley' in humanoid and social robotics and VR/AR domains. Indeed, the mere association of `human-like' behavior to an AIA can be enough to endow levels of trust in AIAs that are comparable to human-human relationships [Tripp, et al]. Basic idea then is to endow AIA with capabilities that can be interpreted by user as though they could have come from another human. 
%One example is provided by Wink Bennett's work for LVC-based simulation training of Air Force fighter pilots -- in the NotSoGrandChallenge, basic idea was to develop realistic enemy fighter plane AI that would keep trainee pilots honest, i.e. from finding loop holes in computer logic that would allow them to easily game the simulation and win predictably -- this requires not only intelligent mechanisms for adapting to different human trainees and exploring/exploiting their weaknesses through simulated combat interactions, but also requires AI to maintain some level of human credibility --- since trainees will eventually be up against real human fighter opponents, this application requires realistic human behavior for both trainers and trainees to trust the AIA is providing useful sim experience...
%%(ii) what is mechanism for generating appropriate TRBs? 
%Key idea here is to interpret/acknowledge the user's intent frame by producing suitable set of `expected' human behaviors. Assurances generated along these line tend towards improving an understanding of an AIA's predictability and situation normality, most often via verbal or non-verbal human interaction cues to express intent or uncertainty. This can be especially useful in complex problem settings where a utility-based or interpretable model-based understanding of the problem is insufficient for completely quantifying user preferences that can be used to inform `trustworthy' AIA responses (e.g. dog-fighting, or even highway driving [slowing down and coming to complete stop to yield right of way for pedestrians cross, using blinkers and horns to signal other drivers, etc.]...). This is also especially useful for interacting with users who are not privy to access `innards' of AIA, i.e. who cannot potentially act as all-seeing/all-knowing supervisors or operators, per se, but as in situ participants in task/environment that AIA must operate in... Caveat here is that users tend to fill in a lot of blanks on their own, so anthropomorphizing AIA can lead to other unintended consequences that are not present in value alignment or interpretable modeling -- namely, users might extrapolate other assurances or capabilities from AIA that it does not actually mean to signal or possess (e.g. eyes on quadcopter for signaling can't actually see anything...self-driving car cannot hear sounds or horns, even though it looks like a human-driven car and behaves as if it is being driven by a human...)
%%(iii) how can designers build/exploit for AIA assurances, i.e. what techniques available for human-like behavior?: wide variety of heuristics, but two salient categories pertaining to trust and assurances are 
%%(a) nonverbal communication of intent (e.g. physical gesturing, legible planning, turn signaling, etc.) 
%%(b) mannerisms: establishing implicit cues for interaction

\subsection{Human-Like Behavior} \label{sec:human_behavior}
In trying to enable AIAs to better participate in trust-relationships with human users, many have been inspired by the natural analog of human-human. Humans are used to forming trusting relationships with other humans. In essence they ask: what kinds of communication do humans use during interaction that helps them to understand and trust each other? This idea was investigated by \citet{Tripp2011-rx} who compared human trust in other humans against human trust in intelligent interactive technology, which in this case was represented by Microsoft Access, an intelligent recommendation assistant, and Facebook. They found that, as the technology becomes more `human-like', self-reported levels of trust in technology become more similar to levels of trust in other humans.
\brettcomm{picks up with (possibly) irrational behavior, not included in `value alignment'}

\citet{De_Visser2018-kd} specifically discusses different methods by which AIAs can be more human-like in order to `repair trust' with humans (here trust repair is roughly analogous to assurances, but focusing on re-building trust after it is lost). Among several other possibilities, they suggest that an AIA might repair trust by anthropomorphizing (responding using a human communication channel), or by explaining their actions. \brettcomm{Perhaps discuss other methods?}

\subsubsection{Common Approaches}
Generally, we do not have algorithms that describe how humans interact, and must settle for heuristics, or best attempts to create human-like behavior via algorithms. From a high level, there are two main ways that researchers have been addressing this challenge: Direct communication, and mannerisims.

\paragraph{Direct Communication:} \nisarcomm{this is a misnomer -- `nonverbal' communication is what you mean to say??}
In designing assurances that use direct communication, the communication can take many different forms. One popular approach is to use motion or gestures.\citet{Szafir2014-ok} investigated how to enable `Assisted Free Flyer' robots (quad-copters that are made to interact with humans in close spaces) to communicate by using gestures. In doing so they use `motion primitives' (a basic vocabulary of movements) that were inspired by animation \cite{Van_Breemen2004-rz}. In their evaluations of these primitives with human participants, they found that human users significantly found the AFFs to be more natural, and felt safer around them. Later \citet{Szafir2015-iy} also experimentally showed the effectiveness of using a quad-copter's `turn signal' lighting to help users more easily interpret the intended movements and actions. These works provide strong support for `natural communication' assurances aimed at predictability.

\citet{Dragan2013-wd} investigate `legible motion planning' (LMP). LMP is, in essence, planned robotic physical movements and gestures that, by themselves, convey intended actions and goals. They are meant to improve a user's ability to understand and predict where the robot is trying to move. Their insight is that legible motion is used by humans, and is important for situations in which a robot and person are collaboratively working in close proximity to each other. Similarly, in more recent work \citet{Kwon2018-xt} investigates calculating trajectories that convey `incapability', which is \emph{what} the AIA is trying to do, and \emph{why} it is unable to do so. \cite{Dragan2013-wd} calculated sub-optimal paths in order to communicate \emph{intent} of a gesture, by gathering data from human participants they were able to find the parameters for their cost function that best matched the expectations. Likewise \cite{Kwon2018-xt} tested several different objectives using user studies to guide the selection. See also \cite{Admoni2016-db} for related work.

As good example in the domain of natural language communication, at Google/IO 2018, `Google Duplex' (\cite{Google2018-eb}) was introduced through a demo where it called a business establishment to make a reservation. One of the striking characteristics of the system is that it was very difficult (if not impossible) to detect whether the Duplex voice was a human or not. This applied both for the words that is spoke and the accent of the voice. In order to achieve this impressive result, they trained a recurrent neural network (RNN) on anonymized phone conversation data.



\paragraph{Mannerisms:}
\citet{Salem2015-md} investigated the effects of autonomous task errors, task types, and `system personality' on cooperation and trust for humans who observed a domestic robot performing house tasks, such that the robot implicitly showed competence by its mannerisms and successes/failures during tasks. In this case the mannerisms and competency of the robot were completely under control and hard-coded into the system. Regardless, when participants were asked to cooperate with the robot on certain other tasks, the faulty operation of the robot was found to affect the self-reported trust levels of the participants.

\citet{Wu2016-ei} investigated how a person's decisions in a coin entrustment game are affected by their belief in whether they are competing against an AIA or another human player (which, unbeknownst to participants, was in fact an AI with some programmed human-like idiosyncrasies, e.g. variable wait times between turns). Trust in this context was measured directly by the number of coins a participant was willing to lose by putting them at risk to the other player. The experiment found that the participants trusted the AI opponent more than they trusted the `human' opponent; the authors suggest that this may be due to the perception that the AI opponent did not have feelings and operated in a more predictable and consistent `machine-like' way. Given that the `human' was an AI as well, this experiment illustrates that `machine-like' behavioral consistency can lead to implicit positive effects the trust of the participant in certain contexts.

Returning to the case of Google Duplex, another striking characteristic was that Duplex used mannerisims of speech such as saying `um\ldots', including pauses, and sometimes using shortened sentences, which made it even more difficult to distinguish between it and a real human. And during the demo calls, the human on the other end of the line was none the wiser, and trusted that they were in fact speaking to a human.
\brettcomm{Add something about etiquette?} \nisarcomm{this belongs in mannerisms}

\subsubsection{Grounding Example:}
In the case of the `VIP Escort' problem (described in Section~\ref{sec:mot_example}), human-like behavior might be used as an assurance in the following way:

We make the following assumptions

\begin{itemize}
    \item The UGV is about to begin an attempt at escaping the road-network
    \item The operator can observe all the actions of the UGV via video feeds at intersections
    \item The UGV has been designed with the ability to use gestures in order to indicate its `incapability' as in \cite{Kwon2018-xt}
\end{itemize}

As the UGV begins the escort problem, the human supervisor is monitoring progress. As the UGV reaches a certain intersection of the road network the supervisor expects the UGV to take a path $A$, but it does not. However, before choosing to take path $B$, the UGV made a movement that, to the operator, indicated that it considered attempting to traverse $A$. Due to the attempt the supervisor was able to surmise that the UGV wanted to take that path but couldn't due to some limitation.

\paragraph{\textbf{Discussion of Example:}} In this case the UGV is able to maintain appropriate trust of the supervisor because the supervisor was able to interpret the `gesture' that UGV was using. This highlights the assuring effects that human-like communication/behaviors can have on users.
