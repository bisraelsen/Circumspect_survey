\subsection{Human-Like Behavior} \label{sec:human_behavior}
Since humans are accustomed to forming and evaluating trusting relationships with each other, imitation of human-human communication and interactive behaviors provides yet another avenue for developing AIA assurance strategies. 
Support for this idea is given by \citet{Tripp2011-rx}, who compared human trust in other humans against human trust in intelligent interactive technology. 
They found that, as the technology becomes more `human-like', self-reported levels of trust in technology become more similar to levels of trust in other humans.

\citet{De_Visser2018-kd} also specifically discusses different methods by which AIAs can be more human-like in order to `repair trust' with users (here, trust repair is roughly analogous to assurances, but focuses on re-building trust after it is lost). Among several other possibilities, they suggest that an AIA might repair trust by anthropomorphizing (responding using a human communication channel), or by explaining its actions in the same way a person would. 
Such human-like behavior opens the door for AIAs to exhibit `non-rationally motivated' behaviors (i.e. suboptimal, as opposed to irrational actions), if these conform to social norms or other psychological cues that provide useful assurances about predictability (e.g. a robot arm that executes legible motions), competency (e.g. a robot which slowly backs away from unfamiliar or potentially dangerous objects), or situation normality (e.g. a robot car that apparently rubbernecks near an unfamiliar scene on the road). 

\subsubsection{Common Approaches}
Generally, we do not have algorithms that describe how humans interact with each other (yet), and must settle for heuristics or best attempts to create human-like behavior via algorithms. From a high level, researchers have addressed these: nonverbal communication, and mannerisms.

\paragraph{Nonverbal Communication:} 
Nonverbal communication can take many different forms. One popular approach is to use motion or gestures. \citet{Szafir2014-ok} investigated how to enable `Assisted Free Flyer' robots (quad-copters that are made to interact with humans in close spaces) to communicate by using gestures. In doing so they use `motion primitives' (a basic vocabulary of movements) that were inspired by basic `character animation' principles \cite{Van_Breemen2004-rz}. 
In their evaluations of these primitives with human participants in the presence of free flyers, they found that human users significantly found the free flyers to be more natural, and felt safer around them. Later \citet{Szafir2015-iy} also experimentally showed the effectiveness of using illuminated `turn signals' and pairs of human-like `eyes' that shifted with free flyer heading (much as human eyes do when people walk in a crowd) to help users more easily interpret the vehicle's intended movements and actions. These works provide strong support for `commonsense communication' assurances aimed at predictability in physical user-AIA interactions (even if indicators like `moving eyes' do not actually see anything). 
Likewise, \citet{Dragan2013-wd} investigate `legible motion planning', i.e. planned robotic physical movements and gestures that, by themselves, convey intended actions and goals. For example, a table-setting robot may grasp a plate on both sides from the top using two end effectors if it intends to shift the position of the plate along the table surface, whereas it may grasp the same plate with only one hand from the side if it intends to pull away and remove the plate from the table. 
Legible motion is used by humans working in close proximity, and so can also be useful and important for situations in which a physically embodied AIA and person are collaboratively working in close proximity to each other. Similarly, in more recent work \citet{Kwon2018-xt} investigates calculating trajectories that convey `incapability', which is \emph{what} the AIA is trying to do, and \emph{why} it is unable to do so. 
See also \cite{Admoni2016-db} for related work. 

\paragraph{Mannerisms:}
Humans are naturally inclined to leverage social interaction cues and adherence to/violation of social norms as evidence for assessing the trustworthiness of other humans in everyday interactions. 
AIAs can leverage these inclinations to provide simultaneous assurances of their competence, predictability, and situation normality. 
Consider, for instance, a recent `mini-Turing Test' example from the popular media: at Google/IO 2018, Google Duplex \cite{Google2018-eb} was introduced through a demo where it placed a phone call to make a reservation. 
An oft-remarked feature of this demo is the great difficultly (if not near impossibility) of detecting whether or not the Duplex voice is human -- down to the words spoken, tone of voice, and speech mannerisms (which included `um\ldots', pauses, and shortened sentences). 
The human on the other end of the call was none the wiser, and trusted that they were in fact speaking to a regular human customer -- when in fact they were speaking in a completely natural manner to the product of a recurrent neural network (RNN) trained on anonymized phone conversation data. 

More formally, \citet{Salem2015-md} investigated the effects of autonomous task errors, task types, and `system personality' on cooperation and trust for humans who observed a domestic robot performing house tasks, such that the robot implicitly showed competence by its mannerisms and successes/failures during tasks. In this case, the mannerisms and competency of the robot were completely under control and hard-coded into the system. Regardless, when participants were asked to cooperate with the robot on certain other tasks, the strange/unexpected operation of the robot was found to influence the self-reported trust levels of the participants.

\citet{Wu2016-ei} investigated how a person's decisions in a coin entrustment game are affected by their belief in whether they are competing against an AIA or another human player (which, unbeknownst to participants, was in fact an AI with some programmed human-like idiosyncrasies, e.g. variable wait times between turns). Trust in this context was measured directly by the number of coins a participant was willing to lose by putting them at risk to the other player. The experiment found that the participants trusted the AI opponent more than they trusted the `human' opponent; the authors suggest that this may be due to the perception that the AI opponent did not have feelings and operated in a more predictable and consistent `machine-like' way. Given that the `human' was an AI as well, this experiment illustrates that `machine-like' behavioral consistency can lead to implicit positive effects the trust of the participant in certain contexts.

\subsubsection{Grounding Example:}
In the case of the `VIP Escort' problem (described in Section~\ref{sec:mot_example}), human-like behavior might be used as an assurance in the following way, starting with the assumptions that:

\begin{itemize}
    \item The UGV is about to begin an attempt at escaping the road-network
    \item The operator can observe all the actions of the UGV via video feeds at intersections
    \item The UGV has been designed with the ability to use gestures in order to indicate its `incapability' as in \cite{Kwon2018-xt}
\end{itemize}

As the UGV begins the escort problem, the human supervisor is monitoring progress. When the UGV reaches a certain intersection of the road network the supervisor expects the UGV to take a path $A$, but it does not. However, before choosing to take path $B$, the UGV made a movement that, to the operator, indicated that it considered attempting to traverse $A$. Due to the attempt the supervisor was able to surmise that the UGV wanted to take that path but couldn't due to some limitation.

\paragraph{\textbf{Discussion of Example:}} In this case the UGV is able to maintain appropriate trust of the supervisor because the supervisor was able to interpret the `gesture' that UGV was using. This highlights the assuring effects that human-like communication/behaviors can have on users.
