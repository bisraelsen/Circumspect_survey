\subsection{Observing Effects of Assurances} \label{sec:measuring_effects}
    Since assurances are meant to influence TRBs, it is important to quantify those effects so that:  1) the AIA system designer can understand how effective the assurances are; and 2) the AIA can observe and respond to/adjust the efficacy of its assurances. To our knowledge, there has not been any work that enables an AIA to observe user responses to assurances and then adapt behaviors appropriately (at least not in the trust cycle setting). 
    Yet, this capability is crucial for enabling AIAs to meet different user's needs. 
Theoretically, any method that is made for the designer to measure the effects of assurances could also be deployed by the AIA itself to assess the effects of assurances on user TRBs. 
The surveyed literature gives some insights into how that has been done to date; namely, there are two main approaches: (1)  Gather self-reported changes in trust from human users; and (2) Measure changes in user's TRBs. 
    
\subsubsection{Self-Reported Changes in Trust} Assessing self-reported changes in trust involves asking users to answer questions, such as `how trustworthy do you feel the system is?'; or `to what extent do you find the system to be interpretable?', either while using a system or afterwards \cite{Mcknight2011-gv,Muir1996-gt,Wickens1999-la,Salem2015-md,Kaniarasu2013-ho}. These kinds of questions are useful in verifying whether the assurances are having the expected effects. It is not unreasonable to imagine that an AIA might be equipped to ask users questions about their trust, process those responses, and modify assurances appropriately.

Self-reports are the most useful when trying to understand the true effects of an assurance. Does a certain assurance, assumed to affect `situational normality', actually do that? 
Does displaying a specific plot actually convey information about `predictability'? 
There is much room for research in this area, which can be used to inform the selection of the methods of assurance. 
However, changes in self-reported trust do not always result in changes in TRBs \cite{Dzindolet2003-ts}. From the AIAs perspective this means that --- unless the object of the assurances is to make the person's level of self-reported trust change --- the assurances may not be providing any tangible benefit. 
As previously discussed, a more concrete objective for designing assurances in human-AIA interaction is to elicit appropriate TRBs from the human user. 
From this perspective, measuring changes in TRBs is the more direct and objective approach to assessing effectiveness.% of assurances.

\subsubsection{Measuring Changes in TRBs} Researchers often measure how long AIAs are able to run under full autonomy, before the autonomy is turned off by users \cite{Freedy2007-sg,Desai2012-rc}. 
Other researchers assess user's willingness to cooperate with AIAs \cite{Salem2015-md,Wu2016-ei,Bainbridge2011-pl}. 
A more ideal metric is the likelihood that users will use certain AIA capabilities `appropriately'. 
However, this is more difficult to formally define/calculate in different situations. 
As a concrete example for the UGV road network problem, %%%there is not an option to `turn off' the UGV's autonomy --but the user could switch off the planning feature...
the remote supervisor can make decisions such as accepting a plan or policy formulated by the UGV, or switching off the autonomous planner to provide their own plan to be implemented by the UGV. 
In this situation, the effect of assurances might be measured by how likely the operator is to accept a generated plan, instead of overriding it (recall that the goal may not be to have the generated plan accepted 100\% of the time, but rather that it be accepted with respect to how appropriate it is in a given context).

In practical application, assurances designed to lead the user to believe that the AIA is more competent, predictable or reliable than the user initially believed do not achieve their objectives if the user doesn't treat the AIA any differently than before/without the assurances. 
This assumes that it is possible for appropriate TRBs to be defined and observable in the first place. 
If, for example, an appropriate TRB hypothetically involves user verification of a sensor reading, can the AIA perceive whether or not such behavior takes place? 
%\edit{...good following: need to revise/polish...}
If the user is queried about this, can the user always be trusted to provide an honest/correct response or behave appropriately? 
%Is there a way to verify the user behavior is actually appropriate? 
This issue has gained notoriety with the current generation of autonomous cars, where users still need to attentively sit in the driver's seat in case the vehicle cannot perform correctly. This underscores the importance of designing methods for perceiving (in)appropriate TRBs. 
%
% \subsection{The Imprecise Nature of Assurances} \label{sec:imprecise_nature}
    % Due to the nature of trust (and humans in general), a single assurance might be targeted at influencing the competence dimension of trust, but it may also have effects on other dimensions. As an example an assurance that targets predictability may also have an affect on the probability of depending.
%
    % Besides being difficult to separate effects on a single user, individual users are different as well. Thus no assurance will have an identical effect when given to two separate users. This makes it difficult to have precise effects on user trust behaviors.
%
    % One might attempt to mitigate this uncertainty by using expressions that are more precise than others, such as displaying a probability distribution rather than on a maximum likelihood. This gets into some considerations about how the presentation of information affects the ability of a human to understand.
