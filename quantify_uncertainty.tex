\paragraph{Accounting for Uncertainty:} \label{sec:acct_uncertainty}
An AIA that can predict its performance on different tasks can provide assurances about competence, predictability, and the situational normality of a given task. Several researchers have worked to improve this ability in visual classification \cite{Zhang2014-he,Gurau2016-hs,Churchill2015-ei,Kaipa2015-hy}. 

For example, to ensure that visual classifiers don't fail silently in novel scenarios, \citet{Zhang2014-he} learned models of errors on training images to predict errors on test images. \citet{Kaipa2015-hy} consider 3D visual classification of assembly line parts for robotic pick and place tasks, and develop statistical goodness-of-fit tests to estimate the likelihood that robots can use their sensors to find parts matching desired ones. These approaches allow the AIA to assess capability and present appropriate assurances to users, though without any formal notions of trust. 

In the context of image classification, \citet{Paul2011-vr} introduced `perplexity' as a metric that represents uncertainty in predicting a single class and is used to select the `most perplexing' images for further learning. There have also been several attempts to use Gaussian processes (GPs) to actively learn and assign probabilistic classifications \cite{MacKay1992-sp,Triebel2016-kj,Triebel2013-ow,Triebel2013-ku,Grimmett2013-gj,Grimmett2016-yc,Berczi2015-rd,Dequaire2016-kh}. As with perplexity-based classifiers, the key insight is that if a classifier possesses a measure of uncertainty, then that uncertainty can be used for efficient instance searching, comparison, and learning, as well as reporting a measure of confidence to users. The key property of GPs to this end is their ability to produce output confidence/uncertainty estimates that grow more uncertain away from the training data. This information can be readily assessed and conveyed to users, even in high-dimensional problems. This property has also found much use in other AIA active learning problems, e.g. Bayesian optimization \cite{Snoek2012-tt, Brochu2010-tj,Israelsen2017-zb}.

Neural network models are commonly considered black-box models, and methods to represent uncertainty have not historically be available. However, there have been several recent advances to make this possible to some extent~\cite{Gal2016-om,Gal2016-eq}. Bayesian neural networks (BNNs) are a method by which we can draw insight about the uncertainty of a neural network's predictions; this is possible by placing prior distributions over the weights in a NN. \citet{Kendall2017-ry}, in the context of computer vision, also use deep BNNs to help visualize epistemic (input) and aleatoric (model) uncertainty for each pixel of an image.

Similarly, \citet{Kahn2017-vy} use deep BNNs to learn about the probability (with uncertainty) of an autonomous vehicle colliding in an environment given its current state, observations, and sequence of controls. Using this model they formulate a `velocity-dependent collision cost' that is used for model-based reinforcement learning.

In order to help predict uncertainty in real-time robotic applications that learn from demonstrations \citet{Choi2017-th} use mixture density networks (MDNs)---neural networks that learn parameters of a Gaussian mixture distributions---to model complex distributions from human demonstrations.

Models and logic are not trustworthy by themselves; they may be flawed to begin with, or become invalid when certain assumptions or specifications are violated. Thus, there is great interest in providing assurances that the models and assumptions underlying different AIA processes are in fact sound. \citet{Laskey1991-mf}---with the intention of communicating model validity to users of `probability-based decision aids'---notes that it is infeasible to perform a decision-theoretic calculation to determine if model revision is necessary. She presents a class of theoretically justified model revision indicators which are based on the idea of constructing a computationally simple alternate model and then initiating model revision if the likelihood ratio of alternate model becomes too large (see also \citet{Zagorecki2015-qy,Habbema1976-xd} --these ideas also provide a potential basis for the `model validity' machine self-confidence factors from Quadrant II). \citet{Ghosh2016-dl}  present `model repair' and `data repair' strategies that can be used when the current model doesn't match the observed data, at which point the model and data can be repaired, and control actions can be replanned in order to conform with the formal method specifications. One challenge is how the `trustable' constraints should be identified, as this places a strong burden on the certifying authorities and system designer to foresee all possible failures.
