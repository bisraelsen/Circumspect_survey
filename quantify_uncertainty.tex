\paragraph{Quantify Uncertainty} \label{sec:QU}

Although active learning does not explicitly consider safety, the underlying approaches can be useful because active learners need to be able to search the problem space to reduce uncertainty; this requires an internal representation of uncertainty. The applications surveyed here are all mainly related to image classification and robotics. In the context of image classification, \citet{Paul2011-vr} introduced `perplexity' as a metric that represents uncertainty in predicting a single class and is used to select the `most perplexing' images for further learning. There have also been several attempts to use Gaussian processes (GPs) to actively learn and assign probabilistic classifications \cite{MacKay1992-sp,Triebel2016-kj,Triebel2013-ow,Triebel2013-ku,Grimmett2013-gj,Grimmett2016-yc,Berczi2015-rd,Dequaire2016-kh}. As with perplexity-based classifiers, the key insight is that if a classifier possesses a measure of uncertainty, then that uncertainty can be used for efficient instance searching, comparison, and learning, as well as reporting a measure of confidence to users. The key property of GPs to this end is their ability to produce output confidence/uncertainty estimates that grow more uncertain away from the training data. This information can be readily assessed and conveyed to users, even in high-dimensional problems. This property has also found much use in other AIA active learning problems, e.g. Bayesian optimization \cite{Snoek2012-tt, Brochu2010-tj,Israelsen2017-zb}. 

\citet{Choi2017-th} investigates how mixture density networks (MDNs)---neural networks that learn parameters of a Gaussian mixture distributions---can be used to help a controller switch modes based on the MDN's prediction of 

Bayesian neural networks (BNNs) are a method by which we can have insight into the uncertainty of a neural network model. Using BNNs \citet{Kendall2017-ry}, in the context of computer vision, also use deep BNNs to help visualize epistemic (input) and aleatoric (model) uncertainty for each pixel of an image. 

Similarly \citet{Kahn2017-vy} use deep BNNs to learn about the probability (with uncertainty) of an autonomous vehicle colliding in an environment given its current state, observations, and sequence of controls. Using this model they formulate a `velocity-dependent collision cost' that is used for model-based reinforcement learning. With this approach the vehicle naturally proceeds slowly when there is an elevated risk of collision. \brettcom{not sure if this goes here, or in the `value alignment' section\ldots it goes here if i downplay the built-in nature of the behavior, and instead focus on the ability to quantify uncertainty}

An AIA that can predict its performance on different tasks can provide assurances about competence, predictability, and the situational normality of a given task. Several authors have worked to improve this ability in visual classification \cite{Zhang2014-he,Gurau2016-hs,Churchill2015-ei,Kaipa2015-hy}. 
For example, to ensure that visual classifiers don't fail silently in novel scenarios, 
\citet{Zhang2014-he} learned models of errors on training images to predict errors on test images. 
\citet{Kaipa2015-hy} consider 3D visual classification of assembly line parts for robotic pick and place tasks, and develop statistical goodness-of-fit tests to estimate the likelihood that robots can use their sensors to find parts matching desired ones. %To accomplish this they apply the `Iterative Closest Point' (ICP) method, to match a point cloud measurement of the part with a ground-truth 3D model of the part. 
These approaches allow the AIA to assess capability and present appropriate assurances to users, though without any formal notions of trust. 

Models and logic are not trustworthy by themselves; they may be flawed to begin with, or become invalid when certain assumptions or specifications are violated. Thus, there is great interest in providing assurances that the models and assumptions underlying different AIA processes are in fact sound. \citet{Laskey1991-mf} -- with the intention of communicating model validity to users of `probability-based decision aids' -- notes that it is infeasible to perform a decision-theoretic calculation to determine if model revision is necessary. 
She presents a class of theoretically justified model revision indicators which are based on the idea of constructing a computationally simple alternate model and then initiating model revision if the likelihood ratio of alternate model becomes too large (see also \citet{Zagorecki2015-qy,Habbema1976-xd} --these ideas also provide a potential basis for the `model validity' machine self-confidence factors from Quadrant II).
\citet{Ghosh2016-dl}  present `model repair' and `data repair' strategies that can be used when the current model doesn't match the observed data, at which point the model and data can be repaired, and control actions can be replanned in order to conform with the formal method specifications. One challenge is how the `trustable' constraints should be identified, as this places a strong burden on the certifying authorities and system designer to foresee all possible failures.

