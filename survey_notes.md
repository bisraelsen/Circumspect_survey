# q1 take-aways

* assurances can come from observed behavior -- i.e. cumulative performance form a pump, also reliability in the student/pilot study, again Desai looked at perceived reliability, 
* Here we get hints of the difference between measuring self-reported trust and TRBs
* Limitation of humans being able to observe trustworthiness (i.e. capability, predictability) -- perhaps AIAs need to be very explicit in order to avoid communication breakdown
* humans can learn to trust an AIA even if it doesn't have high capability or predictability (Muir, Freedy)
* PERCEPTION, how humans perceive certain attributes. Some attributes are easier to perceive than others.
* The main investigations have been with respect to "reliability". What other factors are there that a human might perceive?
    * humans perceive through senses: sight, touch, smell, sound, tasting, experience (i.e. each of the senses carried through time)
    * Certain "cosmetic" perceptions (i.e. perceptions that don't actually matter in the long run), might only be important in trust formation, these might be the easiest or most obvious things to perceive.
    * Other more fundamental perceptions (i.e. the actual ability to complete a task) may not play as large of a part in the initial stages.
* reliability has basically been investigated as it relates to sight, and experience. Bainbridge investigated whether physical presence had an effect. Little investigation has taken place with respect to the effects of smell, sound, taste, etc... it seems clear that there will be an affect on trust, but that effect remains un-quantified.

# q2 Take-aways

* Muir suggests that "summary data" to improve "observability" (I say perception, like from section I).
* Muir -- user needs to feel equal in the relationshis
* Muir -- mentions calibrating TRBs
* Kaniarasu -- discuss a confidence indicator from a robot, this is different from "perceive" competence
* Kaniarasu, Chen -- robots need to be equipped to quantify reliability
* Chen -- hit on the idea of different kinds of information that a user needs to be aware of.
* Chen -- user needs to be able to understand different things (basically different AIA capabilities), and must trust each of them to some degree
* Dragan/Wu -- optimality/sub-optimality can have different effects it is based on the kind of interaction between human and robot. Probably biased by the slower-time-scale trust dimensions -- there are probably those who would never trust an AI more than a human.

* Big picture observations:
    * assurances must be perceived by human users, investigate different ways of communicating: projection, movement, natural language, other display
    * There is a wide-spread shortcoming of robots to be able to compute information necessary in order to compute assurances
    * must consider the human element, i.e. "feeling equavalent" or part of the relationship, sometimes being human is good, others bad.


# q3 take-aways

* Performance prediction, Interpretable models, visualization and dimensionality reduction, explanation, model checking, human-involved learning
* seems like there has been some un-guided attempts to begin addressing the AIA-source trust-target assurances.
    * one problem is that there are **so many** different methods and approaches. There really aren't "simple" solutions to this problem. 
* adopting the formal assurance model outlined in this paper, research can be guided in a more organized way.
* No-free-lunch applies to assurances too
* Assurances need to be considered from the initial design of an AIA
* statistics help to address competence, and structure helps to address predictability

# q4 take-aways

* approaches that contribute to better systems, act as assurances, or at least provide the framework for assurances to be created.
* add stuff

# Overall

I have re-convinced myself that I can use the 4 quadrants, as presently constituted. I don't think that it is too much of a stretch to fit q4 in. In my mind q3 is about the same amount of stretch. 

# Meeting with NIsar

assurances from user side, and designer side. did the designer purposfully design the assurance? 

vertical axis -- studied about trust and assumed about trust
horizontal axis -- what affects a persons trust, what do designers think affects trust


#### another meeting with Nisar
take a high-level look and decide a few key points that I want to make in the paper, and then go back through the paper and check to see if there is anything distracting from those points.

argue about why human-like properties are good/bad, and what considerations need to take place. argue from different angles, why aren't all robots human-like? autonomous vehicles shouldn't be human-like because that could hinder their functionality.

fill out more questions, and reasoning about the points in the synthesis

these are recommendations for design choices, then we need to have recommendations that come out from the discussion.

need to tie back to situational normality, predictability, and competence. Use road network example to help dive deeper. Maybe at the top of section 5, list the key question, and the say I'm going to evaluate them according to the road-net example.

5.1.2 - self-reports are good for evaluating efffected dimension of trust. are there classes of TRBs that come out of the survey.. can they be cataloged? 

use #2 from Mike's comments as an overlay on figure 9, to help guide the synthesis.
