\subsection{Expression and Perception of Assurances} \label{sec:express_assurances}
Expression and perception of assurances have been combined in this section because they share several critical aspects. The key points to be considered here in design of assurances are:
(1) Mediums; (2) Methods; and (3) Efficacy.     
%     \begin{itemize}
%         \item Mediums
%         \item Methods
%         \item Efficacy
%     \end{itemize}
    
For explicit assurance design, the medium and method of expression must account for the AIA's limitations. 
Here medium denotes the means by which an assurances is expressed; this could be through any of the senses by which humans perceive, such as sight, sound, touch, smell, and taste. The method of assurance is the way by which the assurance is expressed. For example: a time series plot may be conveyed visually in the typical way, or via a spoken or textual description; in this case the plot or text description is the method, and sight or sound are the different mediums through which it can be communicated. An AIA might be limited in methods of expression, e.g. because it does not have a display or a speaker. %%In such cases, how is the user supposed to receive assurances?

A designer must also consider whether a human can perceive the assurances being given. If so, to what extent is the information from the assurance transfered, or how efficacious is the assurance? A few examples include: an AIA giving an auditory assurance in a noisy room and the user not hearing it (such as an alert bell in a factory where the workers use ear-plugs), or an AIA attempting to display an assurance to a user that has obstructed vision. 
An AIA may also have the ability to store data about its performance, and compute a statistic regarding its reliability -- but if it cannot successfully express (or communicate) that information in some way, the information is useless. 
If an assurance is not expressed, or not perceived by the user, it is useless and has no effect. 

%%...need to connect this paragraph to the previous one, otherwise it's a bit of a non-sequitur
It is also important to bear in mind that users will always produce some kind of TRB when interacting with an AIA (even if this only means choosing to ignore the AIA and not use it), and in the absence of explicit assurances users will instead use implicit assurances to inform their TRBs. 
However, users generally will not know which assurances are implicit or explicit -- e.g. humans participating in research from Quadrants I and II were generally not aware which aspects of the AIAs they interacted with were/were not deliberately designed by the researchers as assurances. 
Hence, there is always a danger that users can latch onto the `wrong' assurances, i.e. AIA features that are not meant to be interepreted as assurances but are nevertheless easily perceived (possibly moreso than intended explicit assurances). There is also the danger of overwhelming the user with too many easily perceivable explicit assurances, e.g. sounding and displaying several alarm indicators at once in an aircraft cockpit. 
%%Recall from Section~\ref{sec:assurances} that, to a user, all assurances are the same, i.e. any property or behavior of an AIA that affects trust is an assurance to a user, and it doesn't matter whether the assurance was designed for that purpose (explicit) or not (implicit). 

\subsubsection{Mediums}
In general, assurances are most often expressed visually. For example an AIA might give visual performance feedback to display different performance characteristics \cite{Chadalavada2015-wx,Muir1996-gt}. Written or spoken natural language can also be used \cite{Wang2016-id} -- given the impressive strides made by NLP researchers and practitioners lately, it is nowadays a simple matter to convert between written natural language and spoken natural language. 
These can be used to augment or replace more conventional audio-visual assurance indicators traditionally used and studied for human-machine/human-automation interaction, e.g. blinking lights, colored boxes in graphical displays, ringing bells/buzzers, recorded voice alerts, etc.
    
Other senses (touch, smell, and taste) are not well explored in literature related to human-AIA trust. Generally, any human sense could be used as a medium. Besides sight and sound, tactile feedback has been used extensively in robotics for `haptic feedback' (where the user receives mechanical feedback through the robot controls). This medium is used to create a more immersive user interface in robotics, to help users feel more connected to the robot (especially important for telerobotics applications). 
While one can imagine smell and taste having obvious applications in designing assurances for a cooking robot, other applications very likely exist and are open to further research.

\subsubsection{Methods}
Assurances associated with displaying AIA performance variables sound banal (e.g. flow rate for an automated pump \cite{Muir1996-gt}), but actually involves a nuanced point: the displayed performance value actually serves to inform the user's own mental model of the trustworthiness of an AIA capability. That is, the user's trust in the AIA's capability does not change only in response to the instantaneous `goodness/badness' of the AIA's performance, but accounts for the past history of the AIA's performance as well as any observed discrepancies between the AIA's expected behavior and its actual behavior.  
The user's trust dimensions (`competence', 'predictability', etc) are then affected by their perception of trustworthiness according to the combined model and data delivered by the display. 
This approach (also noted and discussed by \cite{Wickens1999-la,Sheridan1984-kx,Hutchins2015-if}) is effective, but relies heavily on the implicit assumption that the user will create a `good enough statistical model' of the AIA's behavior from data presented by the AIA. With this in mind, one might train a user to recognize signs of failure/success in different interactions with an AIA as assurances \cite{Freedy2007-sg,Desai2012-rc,Salem2015-md}. 
The main drawback of this idea is that it still relies on users' ability to construct `good enough' mental models of AIA behavior and characteristics from noisy observations to avoid misinterpreting AIA behaviors. 
However, this training can require intensive and costly special effort for non-expert or non-specialist users. 
A more ideal approach in such cases would be to design explicit assurances that help users construct correct/consistent mental process models of AIA behavior and thus reduce the risk of misinterpretation.

More direct methods of expressing assurances include displaying the intended actions, e.g. to indicate movements via visual projection of a planned mobile robot path \cite{Chadalavada2015-wx}. This is subtly but significantly different from making the user infer the intended action. Analogously, natural language expressions (written or otherwise) attempt a more active method of assurance expression. One might also display plans and logic in different formats, e.g. tables, trees, radar charts  \cite{Van_Belle2013-ph, Huysmans2011-th, Hutchins2015-if}, to remove some uncertainty regarding the user's ability to create an adequate mental model. %%%As humans are fond of saying ``You can't assume that I can read your mind!'', in essence more passive expressions from AIAs are relying on humans to read AIA's `minds' (we can't even do that with other humans).

It is often assumed that making an AIA more `human-like' will make it more trustworthy. 
An algorithm may be human-like when it represents knowledge in a human-understandable way, or executes logic in a way that a human can follow. 
A robot that is humanoid becomes more human-like in appearance \cite{Bainbridge2011-pl}, and thus implicitly projects that it has certain physical (and possibly mental) capabilities in common with humans as well. 
A system that uses natural language becomes more human-like in communication \cite{Lacave2002-cu}, and again projects that it has certain capabilities to understand or possibly hold a conversation at some level with a human user. 
The human-AIA trust relationship depends on assurances that, in essence, are conversions from AIA capabilities to human-perceptible/human-understandable behaviors and properties.  
Since assurances are the means of communication by which humans develop trust in AIAs, it is expected that all assurances have to at least be made human-understandable in some way (otherwise assurances will be totally ineffective). Therefore, it can be argued that assurances must make AIAs `human-like' in some regard.  

Interestingly, however, the converse is not true: making an AIA more `human-like' in any arbitrary way does not imply that it automatically provides assurances that make it more trustworthy. 
In \cite{Dragan2013-wd} the AIA is made more trustworthy by making the robot motions more human-like, whereas in \cite{Wu2016-ei} making the AIA more human-like resulted in a decrease of trustworthiness. In this case the difference came from the type of task: in the first case, the AIA (a robot) was physically working in proximity to a human, while in the other case the user was playing a competitive game against the AIA (a computer program). 
It has been observed that humans trust more `human-like' AIAs in more human-like ways \citet{Tripp2011-rx}. 
It is thus plausible to suppose that the term `human-like' can be more formally defined in terms of the difficulty that a typical user would have in relating to the AIA. 
Following on this idea, the benefits/drawbacks of human-like characteristics would be influenced by a user's general impressions and feelings of how trustworthy humans are in similar situations. 
This would also involve aspects of psychology and sociology, and would be very difficult to control and account for. 
Nevertheless, the problem of coping with such factors is an open and important research question that will impact the design of assurances for AIAs. 

It is also worth considering, in more detail, what implications the existence of implicit and explicit assurances means practically for AIA system designers when it comes to considering and implementing assurances. 
Since it is unrealistic for designers to take all possible kinds assurances into account, they will need to focus their efforts on how to identify and focus on only the most important ones. 
The foremost consideration is that an analysis of the interaction between the human and user needs to be made in order to identify the critical assurances for a given scenario. 
For example, in the road network problem, an analysis might find that the most critical assurances are about the competence of the UGV's planner. 
In this case the designer must take time to design an explicit assurance that is directed at the user's perception of the AIA's competence -- let's call this a `planning-competence' assurance. 
One difficulty arises from this approach is that there doesn't seem to be a way to determine what other implicit assurances might drown out explicit assurances. 
Continuing the example, the system designer may come up with a well thought out planning-competence assurance, but failed to consider the effect of how the UGV appears -- it may be old, have loose panels, and rust holes. Generally, designers overlook implicit assurances (i.e. do not consider them explicitly in design) because they assume that they will have no effect (i.e. why does it matter if there are rust-holes if the UGV works?). This can stem from ignorance of human-AIA trust dynamics, or failure to identify which assurances are most important to users.
%
% \edit{...move to end...trim also -- not sure it's saying much...and feels out of place given next paragraphs}
% Any of these methods can be more or less effective based on the task and context in which they are used. 
% How should uncertainly be displayed (i.e. as a distribution, summary statistics, fractions or decimals)?  Unsurprisingly we find that the answer is `it depends' \cite{Chen2014-dk,Wallace2001-fm,Kuhn1997-qc,Lacave2002-cu}. 

    While it might be desirable, it is generally unreasonable and practically inefficient to attempt a study of \emph{every possible} assurance from an AIA to a user and then select the most important. Perhaps one way a designer might try to identify which assurances are important is to perform user studies, to obtain feedback about which characteristics of the AIA most affected user trust. An approach like this would help determine if explicit assurances are being picked up, and if there are implicit assurances that are overly influential or that overwhelm explicitly designed assurances. With such feedback, designers would have a realistic idea about whether their explicitly designed assurances are having the desired effect on user TRBs. We use the UGV road-network problem to illustrate: after designing an explicit assurance, the supervisor-UGV team could work together in a training mission. Afterwards, the supervisor could rank the different behaviors/properties of the AIA affected their trust in it. In this way, the critical implicit and explicit assurances will be identified. If the explicit assurance is near the top of the list of influencing assurances, then it is working; if not a re-design may need to occur. 
Of course, even this approach has its own caveats, as factors such as the experience of the user, or the nature of the information being displayed, must be taken into account, as these will affect the user's ability to interpret explicit assurances or extract implicit assurances on their own. Absent data for analyzing such considerations, the best that can be done is to select explicit assurances that will work for the largest group of typical users of the AIA. A sufficiently advanced AIA might also learn how best to communicate to individual users. %%, although such adaptability can also make it difficult to formally establish the efficacy of assurances via user studies. 

One final point is that several potential sources for explicit assurances lack well-established human-understandable expressions, and thus are not yet widely utilized as effective assurances. For example, it is unclear how an AIA can best express that it has been formally validated and verified for similar operational settings. 
Similarly, it is unclear how information related to random variables can be best communicated to users besides showing them histogram or probability distribution plots (only useful for 1 or 2 dimensional distributions), or displaying statistics such as means and variances. 
Investigating and understanding how such useful, but otherwise difficult to understand, types of information can be expressed as explicit assurances will be critical to enhancing human-AIA trust relationships. 

\subsubsection{Efficacy}
Some kinds of expression are very `one-dimensional' in that they only rely on one medium or method. This, again, has been seen in practice by the use of plotting a certain AIA performance variable value over time. Because of this, much of the research to date involves assurances that are not robust to loss in transfer, i.e. the approaches rely heavily on a specific medium and method to work, otherwise the whole assurance is rendered useless. 
Hence, the problem of robustly communicating assurances remains an open research question. 
An analogy can be made here to a person speaking to another person with their voice, while also making facial expressions and gestures with their hands, thus simultaneously utilizing several mediums/methods helps to ensure the effectiveness of an assurance. 
If the person instead tries to simply repeat the same message over and over many times to the other person in a monotone voice without changing facial expressions or making any other gestures, then this would be considered inefficient, especially if the message is not received or considered to be effective after the first attempt. %This raises the idea of efficiency in expression. 
    
    %...again this paragraph feels out of place and disjointed...going to try to rearrange the ideas here, since they don't make sense in current form, not clear what the point is...
    %%
%    Perhaps less obvious is a situation in which the user has to supplement an incomplete assurance. A user can create a mental model of the trustworthiness of an AIA capability based on repeated observations over time. Creating this mental model takes time/effort, and the model is prone to cognitive biases. 
 %   In this case the assurance is communicated slowly and indirectly. Generally, a highly effective assurance would have precise information communicated in a way that is easy for the user to perceive, with little loss. Whereas, an inefficient and ineffective assurance may be more vague and wasteful (i.e. repeating the same thing many differet times), and susceptible to loss in communication. The solutions to efficacy lay in selecting appropriate methods, and mediums for expression of the assurance, and by designing for appropriate levels of redundancy to ensure that the assurance is received.
We might also consider situations where a user has to supplement an incomplete assurance from an AIA on their own. 
For instance, in the UGV road network scenario, suppose the hypothetical planning-competence assurance discussed earlier is implemented in such a way that it is communicated slowly and indirectly back to the supervisor. 
The supervisor can create and use a mental model of the AIA's capabilities (based on repeated observations over time or previous interactions) to `fill in' what an AIA might mean when it tries to express this assurance (e.g. to anticipate what it might say or display next).  
Creating and leveraging this mental model takes time/effort on the part of the user, and the process of interpreting the assurance by guessing at its content or meaning opens becomes prone to cognitive biases. %(e.g. if the user becomes impatient and starts to incorrectly categorize/infer the meaning of assurances printed on a display, and then acting on the incorrect interpretations before they have even finished printing). 
Ideally, a highly effective assurance communicate in a precise and direct way that is easy for the user to recognize and understand, with little loss in content or ability to act correctly on the assurance in a given situation. 
The keys to efficacy lie in the selection of appropriate methods and mediums for assurance expression, and in the design of appropriate levels of redundancy to ensure that assurances are correctly received and interpreted in a timely manner.
