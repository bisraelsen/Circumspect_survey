%%% ====================================================================
%%%  BibTeX-file{
%%%     author          = "Gerry Murray",
%%%     version         = "1.2",
%%%     date            = "2 April 2012",
%%%     filename        = "acmsmall-sample-bibfile.bib",
%%%     address         = "ACM, NY",
%%%     email           = "murray at hq.acm.org",
%%%     codetable       = "ISO/ASCII",
%%%     keywords        = "ACM Reference Format, bibliography, citation, references",
%%%     supported       = "yes",
%%%     docstring       = "This BibTeX database file contains 'bibdata' entries
%%%                        that 'match' the examples provided in the Specifications Document
%%%                        AND, also, 'legacy'-type bibs. It should assist authors in 
%%%                        choosing the 'correct' at-bibtype and necessary bib-fields
%%%                        so as to obtain the appropriate ACM Reference Format output. 
%%%			   It also contains many 'Standard Abbreviations'. "
%%%  }
%%% ====================================================================

% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@BOOK{Back1996-jp,
  title     = "Evolutionary Algorithms in Theory and Practice: Evolution
               Strategies, Evolutionary Programming, Genetic Algorithms",
  author    = "Back, Thomas",
  abstract  = "This book presents a unified view of evolutionary algorithms:
               the exciting new probabilistic search tools inspired by
               biological models that have immense potential as practical
               problem-solvers in a wide variety of settings, academic,
               commercial, and industrial. In this work, the author compares
               the three most prominent representatives of evolutionary
               algorithms: genetic algorithms, evolution strategies, and
               evolutionary programming. The algorithms are presented within a
               unified framework, thereby clarifying the similarities and
               differences of these methods. The author also presents new
               results regarding the role of mutation and selection in genetic
               algorithms, showing how mutation seems to be much more important
               for the performance of genetic algorithms than usually assumed.
               The interaction of selection and mutation, and the impact of the
               binary code are further topics of interest. Some of the
               theoretical results are also confirmed by performing an
               experiment in meta-evolution on a parallel computer. The
               meta-algorithm used in this experiment combines components from
               evolution strategies and genetic algorithms to yield a hybrid
               capable of handling mixed integer optimization problems. As a
               detailed description of the algorithms, with practical
               guidelines for usage and implementation, this work will interest
               a wide range of researchers in computer science and engineering
               disciplines, as well as graduate students in these fields.",
  publisher = "Oxford University Press",
  month     =  "11~" # jan,
  year      =  1996,
  language  = "en"
}

@article{Ernest2016,
    abstract = {Volume 6 • Issue 1 • 1000144 J Def Manag ISSN: 2167-0374 JDFM, an open access journal While this automates the process, the computational cost of the genetic algorithm for searching over a practically infinite solution space causes the traditional genetic fuzzy system to be infeasible in more complex problems. Methodologies have been developed to help mitigate this, but one method in particular is capable of applying fuzzy control to problems of this scope. The Genetic Fuzzy Tree (GFT) has shown an incredible ability to obtain unparalleled levels of performance in very large and complex problems that contain all of the difficulties that alternative intelligent systems have issues coping with. This new subtype of genetic fuzzy system was recently developed during Dr. Ernest's graduate studies, under the guidance of Dr.'s Cohen and Schumacher and supported by the Dayton Area Graduate Studies Institute. The aim of this initial work was to control a flight of ground strike UCAVs in a low-fidelity simulation environment [5,6]. The success of this study led to Psibernetix Inc. partnering with the US air force research laboratory (AFRL) to apply the GFT methodology to a much more complex problem. Just as UAVs represented a revolutionary capability for the USAF in the mid-1990s, Manned-Unmanned Autonomous Teaming in an air combat environment will certainly represent a revolutionary leap in capability of airpower in the near future. Air combat, as it is performed by human pilots today is a highly dynamic application of aerospace physics, skill, art, and intuition to maneuver a fighter aircraft and missile against an adversary moving at high speeds in three dimensions. Today's fighters close on each other at speeds in excess of 1,500 MPH while flying at altitudes above 40,000 feet. The selection and application of air-to-air tactics requires assessing a tactical advantage or disadvantage and reacting appropriately in microseconds. The cost of mistakes is high.},
    author = {Ernest, N and Carroll, D and Schumacher, C and Clark, M and Cohen, K},
    doi = {10.4172/2167-0374.1000144},
    file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ernest et al. - 2016 - Genetic Fuzzy based Artificial Intelligence for Unmanned Combat Aerial Vehicle Control in Simulated Air Combat Mi.pdf:pdf},
    journal = {J Def Manag},
    title = {{Genetic Fuzzy based Artificial Intelligence for Unmanned Combat Aerial 

             Vehicle Control in Simulated Air Combat Missions}},
    volume = {6},
    year = {2016}
}
@book{pyzdek2003quality,
    title={Quality engineering handbook},
    author={Pyzdek, Thomas and Keller, Paul A},
    year={2003},
    publisher={CRC Press}
}
@article{croarkin2006nist,
    author = {Croarkin, Carroll and Tobian, Paul},
    journal = {NIST/SEMATECH, Available online: http://www. itl. nist. gov/div898/handbook},
    mendeley-groups = {BayesOpt/Acquisition/InfillFxns},
    title = {{e-Handbook of Statistical Methods}},
    url = {http://www.itl.nist.gov/div898/handbook/},
    year = {2016}
}
@inproceedings{eggensperger2013,
abstract = {Progress in practical Bayesian optimization is hampered by the fact that the only available standard benchmarks are artificial test functions that are not representative of practical applications. To alleviate this problem, we introduce a library of benchmarks from the prominent application of hyperparameter optimization and use it to compare Spearmint, TPE, and SMAC, three recent Bayesian optimization methods for hyperparameter optimization.},
author = {Eggensperger, Katharina and Feurer, Matthias and Hutter, Frank and Bergstra, James and Snoek, Jasper and Hoos, Holger H and Leyton-Brown, Kevin},
booktitle = {NIPS workshop on Bayesian Optimization in Theory and Practice},
file = {:home/brett/GoogleDrive/Education/Graduate/Mendeley/Eggensperger et al/Eggensperger et al. - Unknown - Towards an Empirical Foundation for Assessing Bayesian Optimization of Hyperparameters.pdf:pdf},
mendeley-groups = {BayesOpt},
pages = {5},
title = {{Towards an Empirical Foundation for Assessing Bayesian Optimization of Hyperparameters}},
year = {2013}
}
@article{Thompson1933,
author = {Thompson, William R.},
doi = {10.2307/2332286},
file = {:home/brett/GoogleDrive/Education/Graduate/Mendeley/Thompson/2332286.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
month = {Dec},
number = {3/4},
pages = {285},
title = {{On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples}},
url = {http://www.jstor.org/stable/2332286?origin=crossref},
volume = {25},
year = {1933}
}
@article{Wang2016,
abstract = {We consider parallel global optimization of derivative-free expensive-to-evaluate functions, and propose an efficient method based on stochastic approximation for implementing a conceptual Bayesian optimization algorithm proposed by Ginsbourger et al. (2007). To accomplish this, we use infinitessimal perturbation analysis (IPA) to construct a stochastic gradient estimator and show that this estimator is unbiased. We also show that the stochastic gradient ascent algorithm using the constructed gradient estimator converges to a stationary point of the q-EI surface, and therefore, as the number of multiple starts of the gradient ascent algorithm and the number of steps for each start grow large, the one-step Bayes optimal set of points is recovered. We show in numerical experiments that our method for maximizing the q-EI is faster than methods based on closed-form evaluation using high-dimensional integration, when considering many parallel function evaluations, and is comparable in speed when considering few. We also show that the resulting one-step Bayes optimal algorithm for parallel global optimization finds high quality solutions with fewer evaluations that a heuristic based on approximately maximizing the q-EI. A high quality open source implementation of this algorithm is available in the open source Metrics Optimization Engine (MOE).},
archivePrefix = {arXiv},
arxivId = {1602.05149},
author = {Wang, Jialei and Clark, Scott C. and Liu, Eric and Frazier, Peter I.},
eprint = {1602.05149},
file = {:home/brett/GoogleDrive/Education/Graduate/Mendeley/Wang et al/Wang et al. - 2016 - Parallel Bayesian Global Optimization of Expensive Functions.pdf:pdf},
keywords = {Acquisition/InfillFxns,BayesOpt,TALAF,batch selection,qEI},
mendeley-groups = {BayesOpt/Acquisition/InfillFxns},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,TALAF,batch selection,qEI},
month = {Feb},
title = {{Parallel Bayesian Global Optimization of Expensive Functions}},
url = {http://arxiv.org/abs/1602.05149},
year = {2016}
}
@article{Johnson,
author = {Johnson, Steven G.},
mendeley-groups = {AFRL{\_}STTR},
title = {{The NLopt nonlinear-optimization package}},
url = { http://ab-initio.mit.edu/nlopt},
month = {May},
year = {2014}
}
@phdthesis{Schonlau1997,
author = {Schonlau, M},
file = {:home/brett/GoogleDrive/Education/Graduate/Mendeley/Schonlau/Schonlau - 1998 - Computer experiments and global optimization.pdf:pdf},
mendeley-groups = {BayesOpt,BayesOpt/Acquisition/InfillFxns},
pages = {143},
school = {University of Waterloo},
title = {{Computer experiments and global optimization}},
year = {1997}
}
@article{Srinivas2010,
abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
archivePrefix = {arXiv},
arxivId = {0912.3995},
author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias},
doi = {10.1109/TIT.2011.2182033},
eprint = {0912.3995},
file = {:home/brett/GoogleDrive/Education/Graduate/Mendeley/Srinivas et al/Srinivas et al. - 2010 - Gaussian Process Optimization in the Bandit Setting No Regret and Experimental Design.3995:3995},
mendeley-groups = {BayesOpt,BayesOpt/Acquisition/InfillFxns},
month = {Jun},
title = {{Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design}},
year = {2010}
}
@book{forrester2008engineering,
  title={Engineering design via surrogate modelling: a practical guide},
  author={Forrester, Alexander and Sobester, Andras and Keane, Andy},
  year={2008},
  publisher={John Wiley \& Sons}
}
@inproceedings{Doyle2014,
abstract = {For many years it has been recognized that the design, development, and execution of adaptive threat generation systems and the use of rapid modeling techniques within applied research and training environments poses many methodological and integration challenges. With support from the Air Force Research Laboratory, 711th Human Performance Wing (711/HPW) Warfighter Readiness Research Division (WRRD) at Wright-Patterson Air Force Base, Ohio, through collaboration with Tier1, Aptima, Charles River Analytics, CHI Systems, SoarTech, Alion, and Stottler-Henke, and AFRL's Performance and Learning Models (PALM) branch, Phase I of the " Not-So-Grand Challenge " (NSGC) was launched to assess the critical issues facing current and future threat generation systems, the models used in them, and the extent to which current behavior and systems modeling architectures could provide military training with flexible, adaptable accurate/credible models of human behavior and realistic threats.},
author = {Doyle, Margery J and Portrey, Antoinette M},
booktitle = {Proceedings of the 23rd Conference on Behavior Representation in Modeling and Simulation (BRIMS)},
file = {:home/brett/GoogleDrive/Education/Graduate/Mendeley/Doyle, Portrey/Doyle, Portrey - 214 - Rapid Adaptive Realistic Behavior Modeling is Viable for Use in Training.pdf:pdf},
title = {{Rapid Adaptive Realistic Behavior Modeling is Viable for Use in Training}},
year = {2014}
}
@phdthesis{Gelbart2015,
abstract = {Bayesian optimization is an approach for globally optimizing black-box functions that are expensive to evaluate, non-convex, and possibly noisy. Recently, Bayesian optimization has been used with great effectiveness for applications like tuning the hyperparameters of machine learning algorithms and automatic A/B testing for websites. This thesis considers Bayesian optimization in the presence of black-box constraints. Prior work on constrained Bayesian optimization consists of a variety of methods that can be used with some efficacy in specific contexts. Here, by forming a connection with multi-task Bayesian optimization, we formulate a more general class of constrained Bayesian optimization problems that we call Bayesian optimization with decoupled constraints. In this general framework, the objective and constraint functions are divided into tasks that can be evaluated independently of each other, and resources with which these tasks can be performed. We then present two methods for solving problems in this general class. The first method, an extension to a constrained variant of expected improvement, is fast and straightforward to implement but performs poorly in some circumstances and is not sufficiently flexible to address all varieties of decoupled problems. The second method, Predictive Entropy Search with Constraints (PESC), is highly effective and sufficiently flexible to address all problems in the general class of decoupled problems without any ad hoc modifications. The two weaknesses of PESC are its implementation difficulty and slow execution time. We address these issues by, respectively, providing a publicly available implementation within the popular Bayesian optimization software Spearmint, and developing an extension to PESC that achieves greater speed without significant performance losses. We demonstrate the effectiveness of these methods on real-world machine learning meta-optimization problems.},
author = {Gelbart, Michael Adam},
file = {:home/brett/GoogleDrive/Education/Graduate/Mendeley/Gelbart/Gelbart - 2015 - Constrained Bayesian Optimization and Applications.pdf:pdf;:home/brett/GoogleDrive/Education/Graduate/Mendeley/Gelbart/Gelbart - 2015 - Constrained Bayesian Optimization and Applications.html:html},
keywords = {Acquisition/InfillFxns,BayesOpt,TALAF},
language = {en},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,TALAF},
month = {may},
title = {{Constrained Bayesian Optimization and Applications}},
year = {2015}
}
@inproceedings{Snoek2015,
abstract = {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.},
archivePrefix = {arXiv},
arxivId = {1502.05700},
author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Md. Mostofa Ali and Prabhat and Adams, Ryan P.},
booktitle = {Proceedings of The 32nd International Conference on Machine Learning},
eprint = {1502.05700},
file = {:home/brett/GoogleDrive/Education/Graduate/Mendeley/Snoek et al/Snoek et al. - 2015 - Scalable Bayesian Optimization Using Deep Neural Networks.pdf:pdf},
month = {Feb},
pages = {2171--2180},
title = {{Scalable Bayesian Optimization Using Deep Neural Networks}},
year = {2015}
}
@inproceedings{Yannakakis2012,
address = {New York, New York, USA},
author = {Yannakakis, Geogios N.},
booktitle = {Proceedings of the 9th conference on Computing Frontiers - CF '12},
doi = {10.1145/2212908.2212954},
file = {:home/brett/Downloads/gameAI.pdf:pdf},
isbn = {9781450312158},
keywords = {GameAI,TALAF},
mendeley-groups = {AFRL{\_}STTR},
mendeley-tags = {GameAI,TALAF},
pages = {285},
publisher = {ACM Press},
title = {{Game AI revisited}},
year = {2012}
}
@book{Olver2010,
author = {Olver, F.W.J.(Editor)},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Olver - 2010 - NIST Handbook of Mathematical Functions.pdf:pdf},
keywords = {Functions,TALAF},
mendeley-groups = {TextBooks},
mendeley-tags = {Functions,TALAF},
publisher = {Cambridge University Press, Cambridge, London and New York},
title = {{NIST Handbook of Mathematical Functions}},
year = {2010}
}
@techreport{AFRLc,
author = {AFRL},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/AFRL - Unknown - Group definitions and behaviors.pdf:pdf},
keywords = {AFRL,TALAF},
mendeley-tags = {AFRL,TALAF},
title = {{Group definitions and behaviors}}
}
@techreport{AFRL,
author = {AFRL},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/AFRL - Unknown - PETS interface design document.pdf:pdf},
keywords = {AFRL,TALAF},
mendeley-tags = {AFRL,TALAF},
title = {{PETS interface design document}}
}
@techreport{AFRLa,
author = {AFRL},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/AFRL - Unknown - Constructive Entity Behavior v2.pdf:pdf},
keywords = {AFRL,TALAF},
mendeley-tags = {AFRL,TALAF},
title = {{Constructive Entity Behavior v2}}
}
@techreport{AFRLb,
author = {AFRL},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/AFRL - Unknown - Interface Design Document for the Not So Grand Challenge ( NSGC ) Project.pdf:pdf},
keywords = {AFRL,TALAF},
mendeley-tags = {AFRL,TALAF},
title = {{Interface Design Document for the Not So Grand Challenge ( NSGC ) Project}}
}
@article{Aikins1983,
abstract = {Knowledge of situations typically encountered in performing a task is an important and useful source of information for solving that task. This paper presents a system that uses a representation of prototypical knowledge to guide computer consultations, and to focus the application of production rules used to represent inferential knowledge in the domain. The explicit representation of control knowledge for each prototypical situation is also emphasized. ?? 1983.},
author = {Aikins, Janice S.},
doi = {10.1016/0004-3702(83)90017-6},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aikins - 1983 - Prototypical knowledge for expert systems.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aikins - 1983 - Prototypical knowledge for expert systems.html:html},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {ConsultationSystems,TALAF},
mendeley-tags = {ConsultationSystems,TALAF},
month = {Feb},
number = {2},
pages = {163--210},
title = {{Prototypical knowledge for expert systems}},
volume = {20},
year = {1983}
}
@article{Alvarez2011,
abstract = {Recently there has been an increasing interest in regression methods that deal with multiple outputs. This has been motivated partly by frameworks like multitask learning, multisensor networks or structured output data. From a Gaussian processes perspective, the problem reduces to specifying an appropriate covariance function that, whilst being positive semi-definite, captures the dependencies between all the data points and across all the outputs. One approach to account for non-trivial correlations between outputs employs convolution processes. Under a latent function interpretation of the convolution transform we establish dependencies between output variables. The main drawbacks of this approach are the associated computational and storage demands. In this paper we address these issues. We present different efficient approximations for dependent output Gaussian processes constructed through the convolution formalism. We exploit the conditional independencies present naturally in the model. This leads to a form of the covariance similar in spirit to the so called PITC and FITC approximations for a single output. We show experimental results with synthetic and real data, in particular, we show results in school exams score prediction, pollution prediction and gene expression data. © 2011 Mauricio A. {\'{A}}lvarez and Neil D. Lawrence.},
author = {Alvarez, Mauricio A. and Lawrence, Neil D Nd},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alvarez, Lawrence - 2011 - Computationally efficient convolved multiple output gaussian processes.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {BayesOpt,TALAF,convolution processes,efficient approximations,gaussian processes,ing,multitask learn-,multivariate processes,structured outputs},
mendeley-tags = {BayesOpt,TALAF},
pages = {1459--1500},
title = {{Computationally efficient convolved multiple output gaussian processes}},
volume = {12},
year = {2011}
}
@article{Andedward1981,
author = {Andedward, Teach},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andedward - 1981 - An Analysis of Physician Attitudes Regarding Clinical Consultation Systems ”.pdf:pdf},
keywords = {ConsultationSystems,TALAF},
mendeley-tags = {ConsultationSystems,TALAF},
pages = {542--558},
publisher = {Springer},
title = {{An Analysis of Physician Attitudes Regarding Clinical Consultation Systems ”}},
volume = {558},
year = {1981}
}
@article{Bakkes2004,
abstract = {Many commercial computer games allow a team of players to match their skills against another team, controlled by humans or by the computer. Most players prefer human opponents, since the artificial intelligence of a computer-controlled team is in general inferior. An adaptive mechanism for team-oriented artificial intelligence would allow computer-controlled opponents to adapt to human player behaviour, thereby providing a means of dealing with weaknesses in the game AI. Current commercial computer games lack challenging adaptive mechanisms. This paper proposes ” TEAM”, a novel team-oriented adaptive mechanism which is inspired by evolutionary algorithms. The performance of TEAM is evaluated in an experiment involving an actual commercial computer game (the Capture The Flag team-based game mode of the popular commercial computer game Quake III). The experimental results indicate that TEAM succeeds in endowing computer-controlled opponents with successful adaptive performance. We therefore conclude that TEAM can be successfully applied to generate challenging adaptive opponent behaviour in team-oriented commercial computer games.},
annote = {Bakkes et al. - 2004 - TEAM The Team-Oriented Evolutionary Adaptability .pdfUse GA with fitness function based on long term team results to tune 4 AI's on a team.},
author = {Bakkes, Sander and Spronck, Pieter and Postma, Eric},
editor = {Rauterberg, Matthias},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakkes, Spronck, Postma - 2004 - Team The team-oriented evolutionary adaptability mechanism.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakkes, Spronck, Postma - 2004 - Team The team-oriented evolutionary adaptability mechanism.html:html},
isbn = {0302-9743},
issn = {03029743},
journal = {Entertainment Computing–ICEC 2004},
keywords = {Artificial Intelligence (incl. Robotics),Computer Appl. in Arts and Humanities,Computer Applications,GameAI,Information Systems Applications (incl. Internet),Multimedia Information Systems,TALAF,User Interfaces and Human Computer Interaction},
language = {en},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Computer Appl. in Arts and Humanities,Computer Applications,GameAI,Information Systems Applications (incl. Internet),Multimedia Information Systems,TALAF,User Interfaces and Human Computer Interaction},
month = {Sep},
pages = {273--282},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
shorttitle = {TEAM},
title = {{Team: The team-oriented evolutionary adaptability mechanism}},
year = {2004}
}
@article{Bergstra2011,
abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel ap- proaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it pos- sible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neu- ral networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the ex- pected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreli- able for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Bergstra, James and Bardenet, Remi and Bengio, Yoshua and Kegl, Balazs},
eprint = {1206.2944},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra et al. - 2011 - Algorithms for Hyper-Parameter Optimization.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems},
keywords = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF},
pages = {2546--2554},
title = {{Algorithms for Hyper-Parameter Optimization}},
year = {2011}
}
@article{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza- tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar- ison with a large previous study that used grid search and manual search to configure neural net- works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con- figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent “High Throughput”methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that randomsearch is a natural base- line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
author = {Bergstra, James and Bengio, Yoshua},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra, Bengio - 2012 - Random Search for Hyper-Parameter Optimization.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal ofMachine Learning Research},
keywords = {BayesOpt,GPs,Important,OptimizingHyperparameters,TALAF,deep learning,global optimization,model selection,neural networks,response surface},
mendeley-tags = {BayesOpt,GPs,Important,OptimizingHyperparameters,TALAF},
number = {1},
pages = {281--305},
title = {{Random Search for Hyper-Parameter Optimization}},
volume = {13},
year = {2012}
}
@article{Bergstra2013,
abstract = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method's full potential. Compounding matters, these parameters of-ten must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on per-sonal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the al-gorithm and the details of its tuning, it is sometimes di to know whether a given technique is genuinely better, or simply bet-ter tuned. In this work, we propose a meta-modeling ap-proach to support automated hyperparam-eter optimization, with the goal of provid-ing practical tools that replace hand-tuning with a reproducible and unbiased optimiza-Proceedings of the 30 th International Conference on Ma-chine Learning, Atlanta, Georgia, USA, 2013. JMLR: W{\&}CP volume 28. Copyright 2013 by the author(s). tion process. Our approach is to expose the underlying expression graph of how a perfor-mance metric (e.g. classification accuracy on validation examples) is computed from hy-perparameters that govern not only how indi-vidual processing steps are applied, but even which processing steps are included. A hy-perparameter optimization algorithm trans-forms this graph into a program for opti-mizing that performance metric. Our ap-proach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architec-tures.},
author = {Bergstra, James and Yamins, Daniel and Cox, D D},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra, Yamins, Cox - 2013 - Making a Science of Model Search Hyperparameter Optimization in Hundreds of Dimensions for Vision Archite.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
keywords = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
pages = {115--123},
shorttitle = {Making a science of model search},
title = {{Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures}},
year = {2013}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
address = {New York},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, CM Christopher M.},
booktitle = {Pattern Recognition},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bishop - 2006 - Pattern recognition and machine learning.pdf:pdf},
isbn = {9780387310732},
issn = {10179909},
keywords = {BayesOpt,GPs,Important,Machine learning,Pattern perception,TALAF},
mendeley-tags = {BayesOpt,GPs,Important,Machine learning,Pattern perception,TALAF},
number = {4},
pages = {738},
pmid = {8943268},
publisher = {Springer},
series = {Information science and statistics},
title = {{Pattern recognition and machine learning}},
volume = {4},
year = {2006}
}
@article{Bonilla2008,
abstract = {In this paper we investigate multi-task learning in the context of Gaussian Processes (GP). We propose a model that learns a shared covariance function on input-dependent features and a ``free-form'' covariance matrix over tasks. This allows for good flexibility when modelling inter-task dependencies while avoiding the need for large amounts of data for training. We show that under the assumption of noise-free observations and a block design, predictions for a given task only depend on its target values and therefore a cancellation of inter-task transfer occurs. We evaluate the benefits of our model on two practical applications: a compiler performance prediction problem and an exam score prediction task. Additionally, we make use of GP approximations and properties of our model in order to provide scalability to large data sets.},
author = {Bonilla, Edwin and Chai, Kian Ming and Williams, Christopher},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonilla, Chai, Williams - 2008 - Multi-task Gaussian Process Prediction.pdf:pdf},
isbn = {160560352X},
journal = {Advances in Neural Information Processing Systems},
keywords = {BayesOpt,NotRead,TALAF,learning,statistics {\&} optimisation},
mendeley-tags = {BayesOpt,NotRead,TALAF},
number = {October},
pages = {153--160},
title = {{Multi-task Gaussian Process Prediction}},
volume = {20},
year = {2008}
}
@article{Boyle2007,
abstract = {Gaussian processes have proved to be useful and powerful constructs for the purposes of regression. The classical method proceeds by parameterising a covariance function, and then infers the parameters given the training data. In this thesis, the classical approach is augmented by interpreting Gaussian processes as the outputs of linear filters excited by white noise. This enables a straightforward definition of dependent Gaussian processes as the outputs of a multiple output linear filter excited by multiple noise sources. We show how dependent Gaussian processes defined in this way can also be used for the purposes of system identification. Onewell known problemwithGaussian process regression is that the compu- tational complexity scales poorlywith the amount of training data.We review one approximate solution that alleviates this problem, namely reduced rank Gaussian processes. We then show how the reduced rank approximation can be applied to allow for the efficient computation of dependent Gaussian pro- cesses. We then examine the application ofGaussian processes to the solution of other machine learning problems. To do so, we review methods for the parameter- isation of full covariance matrices. Furthermore, we discuss how improve- ments can be made by marginalising over alternative models, and introduce methods to perform these computations efficiently. In particular, we intro- duce sequential annealed importance sampling as a method for calculating model evidence in an on-line fashion as newdata arrives. Gaussian process regression can also be applied to optimisation. An algo- rithm is described that uses model comparison between multiple models to find the optimumof a function while taking as fewsamples as possible. This algorithm shows impressive performance on the standard control problem of double pole balancing. Finally, we describe how Gaussian processes can be used to efficiently estimate gradients of noisy functions, and numerically estimate integrals.},
author = {Boyle, Phillip},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boyle - 2007 - Gaussian processes for regression and optimisation.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boyle - 2007 - Gaussian processes for regression and optimisation.html:html},
keywords = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF},
language = {en{\_}NZ},
mendeley-tags = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF},
pages = {190},
title = {{Gaussian processes for regression and optimisation}},
year = {2007}
}
@article{Brochu2010,
annote = {Brochu et al{\_}2010{\_}A tutorial on Bayesian optimization of expensive cost functions, with.pdf
Contents
* 1 Introduction
1.1 An Introduction to Bayesian Optimization
1.2 Overview
2 The Bayesian Optimization Approach
2.1 Priors over functions
2.2 Choice of covariance functions
2.3 Acquisition Functions for Bayesian Optimization
2.3.1 Improvement-based acquisition functions
2.3.2 Exploration-exploitation trade-off
2.3.3 Confidence bound criteria
2.3.4 Maximizing the acquisition function
2.4 Noise
2.5 A brief history of Bayesian optimization
2.6 Kriging
2.7 Experimental design
2.8 Active learning
2.9 Applications
3 Bayesian Optimization for Preference Galleries
3.1 Probit model for binary observations
3.2 Application: Interactive Bayesian optimization for material design
3.2.1 User Study
4 Bayesian Optimization for Hierarchical Control
4.1 Hierarchical Reinforcement Learning
4.1.1 Semi-MDPs
4.1.2 Hierarchical Value Function Decomposition
4.2 Application: The Vancouver Taxi Domain
4.2.1 State Abstraction, Termination and Rewards
4.3 Bayesian Optimization for Hierarchical Policies
4.3.1 Active Policy Optimization
4.3.2 Active Value Function Learning
4.4 Simulations
5 Discussion and advice to practitioners},
author = {Brochu, Eric and Cora, Vlad M and {De Freitas}, Nando L B - BOTutorial},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brochu, Cora, De Freitas - 2010 - A tutorial on Bayesian optimization of expensive cost functions, with application to active user model.pdf:pdf},
journal = {arXiv preprint arXiv:1012.2599},
keywords = {BayesOpt,Important,TALAF},
mendeley-tags = {BayesOpt,Important,TALAF},
title = {{A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning}},
year = {2010}
}
@article{Browne2012,
abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of co...},
author = {Browne, C B and Powley, E. and Whitehouse, D. and Lucas, S M and Cowling, P I and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
doi = {10.1109/TCIAIG.2012.2186810},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Browne et al. - 2012 - A Survey of Monte Carlo Tree Search Methods.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Browne et al. - 2012 - A Survey of Monte Carlo Tree Search Methods.html:html},
isbn = {1943-068X VO - 4},
issn = {1943-068X},
journal = {Computational Intelligence and AI in Games, IEEE Transactions on},
keywords = {Artificial intelligence (AI),Computers,Decision theory,Game theory,Games,MCTS research,Markov processes,Monte Carlo methods,Monte Carlo tree search (MCTS),Monte carlo tree search methods,NotRead,TALAF,artificial intelligence,bandit-based methods,computer Go,game search,key game,nongame domains,random sampling generality,tree searching,upper confidence bounds (UCB),upper confidence bounds for trees (UCT)},
mendeley-tags = {Artificial intelligence (AI),Computers,Decision theory,Game theory,Games,MCTS research,Markov processes,Monte Carlo methods,Monte Carlo tree search (MCTS),Monte carlo tree search methods,NotRead,TALAF,artificial intelligence,bandit-based methods,computer Go,game search,key game,nongame domains,random sampling generality,tree searching,upper confidence bounds (UCB),upper confidence bounds for trees (UCT)},
month = {mar},
number = {1},
pages = {1--43},
title = {{A Survey of Monte Carlo Tree Search Methods}},
volume = {4},
year = {2012}
}
@article{Caserta2010a,
abstract = {We present a math-heuristic algorithm for the lot sizing problem with carryover. The proposed algorithm uses mathematical$\backslash$n programming techniques in a metaheuristic fashion to iteratively solve smaller portions of the original problem. More specifically,$\backslash$n we draw ideas from the corridor method to design and impose exogenous constraints on the original problem and, subsequently,$\backslash$n we solve to optimality the constrained problem using a MIP solver. The algorithm iteratively builds new corridors around the$\backslash$n best solution found within each corridor and, therefore, explores adjacent portions of the search space. In the attempt of$\backslash$n fostering diversification while exploring the original search space, we generate a pool of incumbent solutions for the corridor$\backslash$n method and, therefore, we reapply the corridor method using different starting points. The algorithm has been tested on instances$\backslash$n of a standard benchmark library and the reported results show the robustness and effectiveness of the proposed scheme.},
address = {Cham},
annote = {From Duplicate 1 (Applications of Evolutionary Computation - Caserta, Marco; Ramirez, Adriana; Vo{\ss}, Stefan)

Mora and Squillero - 2015 - Applications of Evolutionary Computation.pdf
Contents
* 1 Introduction
2 Previous Work
2.1 MiniDungeons Game
2.2 Procedural Personas
2.3 Constrained Optimization of Game Levels
3 Methodology
3.1 Evolving Levels for MiniDungeons
3.2 Assessing Playability with Personas
3.3 Assessing Level Quality with Personas
4 Experiments
4.1 Discovery of Feasible Content
4.2 Quality of Feasible Content
5 Discussion
6 Conclusion
References

From Duplicate 2 (Applications of Evolutionary Computation - Caserta, Marco; Ramirez, Adriana; Vo{\ss}, Stefan)

Mora and Squillero - 2015 - Applications of Evolutionary Computation.pdf
Contents
* 1 Introduction
2 Previous Work
2.1 MiniDungeons Game
2.2 Procedural Personas
2.3 Constrained Optimization of Game Levels
3 Methodology
3.1 Evolving Levels for MiniDungeons
3.2 Assessing Playability with Personas
3.3 Assessing Level Quality with Personas
4 Experiments
4.1 Discovery of Feasible Content
4.2 Quality of Feasible Content
5 Discussion
6 Conclusion
References},
author = {Caserta, Marco and Ramirez, Adriana and Vo{\ss}, Stefan},
doi = {10.1007/978-3-642-12242-2},
editor = {Mora, Antonio M and Squillero, Giovanni},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Caserta, Ramirez, Vo{\ss} - 2010 - Applications of Evolutionary Computation.pdf:pdf},
isbn = {978-3-642-12241-5},
issn = {03029743},
journal = {Applications of Evolutionary Computation},
keywords = {GameAI,TALAF,agent-based model,conformity,emergent,game theory},
mendeley-tags = {GameAI,TALAF},
pages = {462--471},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Applications of Evolutionary Computation}},
volume = {6025},
year = {2010}
}
@article{Cawley2007,
abstract = {While the model parameters of a kernel machine are typically given by the solution of a convex optimisation problem, with a single global optimum, the selection of good values for the regularisation and kernel parameters is much less straightforward. Fortunately the leave-one-out cross-validation procedure can be performed or a least approximated very efficiently in closed form for a wide variety of kernel learning methods, providing a convenient means for model selection. Leave-one-out cross-validation based estimates of performance, however, generally exhibit a relatively high variance and are therefore prone to over-fitting. In this paper, we investigate the novel use of Bayesian regularisation at the second level of inference, adding a regularisation term to the model selection criterion corresponding to a prior over the hyper-parameter values, where the additional regularisation parameters are integrated out analytically. Results obtained on a suite of thirteen real-world and synthetic benchmark data sets clearly demonstrate the benefit of this approach.},
author = {Cawley, Gavin C. and Talbot, Nicola L C},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cawley, Talbot - 2007 - Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {BayesOpt,GPs,OptimizingHyperparameters,TALAF,bayesian regularisation,kernel methods,model selection},
mendeley-tags = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
pages = {841--861},
title = {{Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters}},
volume = {8},
year = {2007}
}
@article{Cawley2010,
abstract = {Model selection strategies for machine learning algorithms typically involve$\backslash$nthe numerical optimisation of an appropriate model selection criterion, often$\backslash$nbased on an estimator of generalisation performance, such as k-fold $\backslash$ncross-validation. The error of such an estimator can be broken down into bias $\backslash$nand variance components. While unbiasedness is often cited as a beneficial $\backslash$nquality of a model selection criterion, we demonstrate that a low variance is $\backslash$nat least as important, as a non-negligible variance introduces the potential $\backslash$nfor over-fitting in model selection as well as in training the model. While $\backslash$nthis observation is in hindsight perhaps rather obvious, the degradation in $\backslash$nperformance due to over-fitting the model selection criterion can be $\backslash$nsurprisingly large, an observation that appears to have received little $\backslash$nattention in the machine learning literature to date. In this paper, we show $\backslash$nthat the effects of this form of over-fitting are often of comparable $\backslash$nmagnitude to differences in performance between learning algorithms, and thus $\backslash$ncannot be ignored in empirical evaluation. Furthermore, we show that some $\backslash$ncommon performance evaluation practices are susceptible to a form of selection $\backslash$nbias as a result of this form of over-fitting and hence are unreliable. We $\backslash$ndiscuss methods to avoid over-fitting in model selection and subsequent $\backslash$nselection bias in performance evaluation, which we hope will be incorporated $\backslash$ninto best practice. While this study concentrates on cross-validation based $\backslash$nmodel selection, the findings are quite general and apply to any model $\backslash$nselection practice involving the optimisation of a model selection criterion $\backslash$nevaluated over a finite sample of data, including maximisation of the Bayesian $\backslash$nevidence and optimisation of performance bounds.},
annote = {Shows that MLE on hyperparameters will overfit.
 
LOO x-validation has nearly zero bias, but high variance.

* One way to address the problem is to have more data (not likely going to happen in our case)
* for small datasets monte-carlo integration should be a realistic possibility},
author = {Cawley, Gavin C. and Talbot, Nicola L. C.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cawley, Talbot - 2010 - On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {BayesOpt,GPs,Important,OptimizingHyperparameters,TALAF,bias-variance trade-off,model selection,over-,performance evaluation,selection bias},
mendeley-tags = {BayesOpt,GPs,Important,OptimizingHyperparameters,TALAF},
month = {Aug},
pages = {2079−2107},
title = {{On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation}},
volume = {11},
year = {2010}
}
@article{Chaslot2008,
abstract = {Classic approaches to game AI require either a high quality of domain knowledge, or a long time to generate effective AI behaviour. These two characteristics hamper the goal of establishing challenging game AI. In this paper, we put forward Monte-Carlo Tree Search as a novel, uniﬁed framework to game AI. In the framework, randomized explorations of the search space are used to predict the most promising game actions. We will demonstrate that Monte-Carlo Tree Search can be applied effectively to (1) classic board-games, (2) modern board-games, and (3) video games.},
author = {Chaslot, Guillaume and Bakkes, Sander and Szita, Istvan and Spronck, Pieter},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaslot et al. - 2008 - Monte-Carlo Tree Search A New Framework for Game AI.pdf:pdf},
isbn = {9781577353911},
journal = {Aiide},
keywords = {NotRead,TALAF,demonstration papers},
mendeley-tags = {NotRead,TALAF},
pages = {216--217},
shorttitle = {Monte-Carlo Tree Search},
title = {{Monte-Carlo Tree Search: A New Framework for Game AI.}},
year = {2008}
}
@inproceedings{Chevalier2013,
abstract = {The Multi-points Expected Improvement criterion (or q-EI) has recently been studied in batch-sequential Bayesian Optimization. This paper deals with a new way of computing q-EI, without using Monte-Carlo simulations, through a closed-form formula. The latter al- lows a very fast computation of q-EI for reasonably low values of q (typ- ically, less than 10). New parallel kriging-based optimization strategies, tested on different toy examples, show promising results.},
annote = {Chevalier and Ginsbourger - 2013 - Fast computation of the multi-points expected impr.pdfmulti-point EI},
author = {Chevalier, Cl{\'{e}}ment and Ginsbourger, David},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-44973-4{\_}7},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chevalier, Ginsbourger - 2013 - Fast computation of the multi-points expected improvement with applications in batch selection.pdf:pdf},
isbn = {9783642449727},
issn = {03029743},
keywords = {Acquisition/InfillFxns,BayesOpt,Computer experiments,Expected improvement,Kriging,Parallel optimization,TALAF,qEI},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,TALAF,qEI},
pages = {59--69},
publisher = {Springer},
title = {{Fast computation of the multi-points expected improvement with applications in batch selection}},
volume = {7997 LNCS},
year = {2013}
}
@article{Ciavarelli1980,
abstract = {The content of this paper summarizes four years of research designed to develop valid and reliable performance criteria for the Navy's Tactical Aircrew Combat Training System (TACTS). Performance measurement methods for assessing missile envelope recognition and air combat engagements have been developed and applied in an operational setting. TACTS measures used in performance assessment were selected on the basis of their operational importance and their demonstrated statistical relationship to successful completion of such air combat tasks as missile launch success and engagement outcomes. A measurement framework has evolved and may be appropriately applied to estimate overall air combat effectiveness, and to provide diagnostic performance analysis of critical air combat tasks. The resulting measurement framework has been applied operationally to evaluate U.S. Navy competitive air combat exercises and to provide diagnostic performance feedback to aircrews undergoing TACTS training. More recently, these measures and assessment methods have been incorporated in a computer-based TACTS debrief system called the Performance Assessment and Appraisal System (PAAS). The PAAS is representative of an emerging technology which uses automated performance measurement methods for enhancing the training process.},
annote = {Metrics for rating air crew.},
author = {Ciavarelli, A. P. and Williams, A. M. and Krasovec, F.},
doi = {10.1177/1071181380024001144},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ciavarelli, Williams, Krasovec - 1980 - Operational Performance Measures for Air Combat Development and Application.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ciavarelli, Williams, Krasovec - 1980 - Operational Performance Measures for Air Combat Development and Application.html:html},
issn = {1541-9312,},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
keywords = {NotRead,TALAF},
language = {en},
mendeley-tags = {NotRead,TALAF},
month = {Oct},
number = {1},
pages = {560--564},
shorttitle = {Operational Performance Measures for Air Combat},
title = {{Operational Performance Measures for Air Combat: Development and Application}},
volume = {24},
year = {1980}
}
@article{Clough2002,
abstract = {The recently released DoD Unmanned Aerial Vehicles Roadmap discusses advancements in UAV autonomy in terms of autonomous control levels (ACL). The ACL concept was pioneered by researchers in the Air Force Research Laboratory's Air Vehicles Directorate who are charged with developing autonomous air vehicles. In the process of developing intelligent autonomous agents for UAV control systems we were constantly challenged to "tell us how autonomous a UAV is, and how do you think it can be measured?" Usually we hand-waved away the argument and hoped the questioner will go away since this is a very subjective, and complicated, subject, but within the last year we've been directed to develop national intelligent autonomous UAV control metrics - an IQ test for the flyborgs, if you will. The ACL chart is the result. We've done this via intense discussions with other government labs and industry, and this paper covers the agreed metrics (an extension of the OODA - observe, orient, decide, and act - loop) as well as the precursors, "dead-ends", and out-and-out flops investigated to get there.},
annote = {From Duplicate 1 (Metrics, schmetrics! How the heck do you determine a UAV's autonomy anyway? - Clough, Bt)

http://www.dtic.mil/cgi-bin/GetTRDoc?Location=U2{\&}amp;doc=GetTRDoc.pdf{\&}amp;AD=ADA515926
Contents
* Background
Quick Difference Between Autonomous and Automatic (our definition)
We Need To Measure Autonomy, Not Intelligence
Well, It Should Be Easy To Find Metrics, One Has The Web And Other Info Sources, Right?
Los Alamos National Lab: Mobility, Acquisition, and Protection (MAP)
Draper 3D Intelligence Space
Initial Autonomous Control Level (ACL) Chart
If You’re Replacing A Human, Why Not Measure Like
Summary
References
Notes----------Clough{\_}2002{\_}Metrics, Schmetrics.pdf
Contents
* Background
Quick Difference Between Autonomous and Automatic (our definition)
We Need To Measure Autonomy, Not Intelligence
Well, It Should Be Easy To Find Metrics, One Has The Web And Other Info Sources, Right?
Los Alamos National Lab: Mobility, Acquisition, and Protection (MAP)
Draper 3D Intelligence Space
Initial Autonomous Control Level (ACL) Chart
If You’re Replacing A Human, Why Not Measure Like
Summary
References
Notes

From Duplicate 2 (Metrics, schmetrics! How the heck do you determine a UAV's autonomy anyway? - Clough, Bt)

http://www.dtic.mil/cgi-bin/GetTRDoc?Location=U2{\{}{\&}amp;{\}}amp;doc=GetTRDoc.pdf{\{}{\&}amp;{\}}amp;AD=ADA515926
Contents
* Background
Quick Difference Between Autonomous and Automatic (our definition)
We Need To Measure Autonomy, Not Intelligence
Well, It Should Be Easy To Find Metrics, One Has The Web And Other Info Sources, Right?
Los Alamos National Lab: Mobility, Acquisition, and Protection (MAP)
Draper 3D Intelligence Space
Initial Autonomous Control Level (ACL) Chart
If You’re Replacing A Human, Why Not Measure Like
Summary
References
Notes----------Clough{\{}{\_}{\}}2002{\{}{\_}{\}}Metrics, Schmetrics.pdf
Contents
* Background
Quick Difference Between Autonomous and Automatic (our definition)
We Need To Measure Autonomy, Not Intelligence
Well, It Should Be Easy To Find Metrics, One Has The Web And Other Info Sources, Right?
Los Alamos National Lab: Mobility, Acquisition, and Protection (MAP)
Draper 3D Intelligence Space
Initial Autonomous Control Level (ACL) Chart
If You’re Replacing A Human, Why Not Measure Like
Summary
References
Notes},
author = {Clough, Bt},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Clough - 2002 - Metrics, schmetrics! How the heck do you determine a UAV's autonomy anyway.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Clough - 2002 - Metrics, schmetrics! How the heck do you determine a UAV's autonomy anyway.html:html},
issn = {1048-776X},
journal = {Security},
keywords = {*ACL(AUTONOMOUS CONTROL LEVELS),*AUTONOMY METRICS,*DETECTORS,*DRONES,*MACHINE INTELLIGENCE METRICS,*MEASUREMENT,*PLANNING PROGRAMMING BUDGETING,*WORKSHOPS,AERONAUTICAL LABORATORIES,AFRL,AIR FORCE RESEARCH,Adjustable autonomy,CONTROL SYSTEMS,METRICS,Pilotless Aircraft,SELF OPERATION,SITUATIONAL AWARENESS,SURVIVAL SPACE,TALAF},
language = {en},
mendeley-tags = {*ACL(AUTONOMOUS CONTROL LEVELS),*AUTONOMY METRICS,*DETECTORS,*DRONES,*MACHINE INTELLIGENCE METRICS,*MEASUREMENT,*PLANNING PROGRAMMING BUDGETING,*WORKSHOPS,AERONAUTICAL LABORATORIES,AFRL,AIR FORCE RESEARCH,CONTROL SYSTEMS,METRICS,Pilotless Aircraft,SELF OPERATION,SITUATIONAL AWARENESS,SURVIVAL SPACE,TALAF},
month = {Aug},
number = {990},
pages = {313--319},
title = {{Metrics, schmetrics! How the heck do you determine a UAV's autonomy anyway?}},
year = {2002}
}
@article{Cole2004a,
abstract = {First-person shooter robot controllers (bots) are generally rule-based expert systems written in C/C++. As such, many of the rules are parameterized with values, which are set by the software designer and finalized at compile time. The effectiveness of parameter values is dependent on the knowledge the programmer has about the game. Furthermore, parameters are non-linearly dependent on each other. This paper presents an efficient method for using a genetic algorithm to evolve sets of parameters for bots which lead to their playing as well as bots whose parameters have been tuned by a human with expert knowledge about the game's strategy. This indicates genetic algorithms as being a potentially useful method for tuning bots.},
author = {Cole, Nicholas and Louis, Sushil J. and Miles, CHris},
doi = {10.1109/CEC.2004.1330849},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cole, Louis, Miles - 2004 - Using a genetic algorithm to tune first-person shooter bots.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cole, Louis, Miles - 2004 - Using a genetic algorithm to tune first-person shooter bots.html:html},
isbn = {0-7803-8515-2},
journal = {Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)},
keywords = {CONTROL SYSTEMS,Counter Strike game,Counting circuits,Expert systems,Game theory,GameAI,Logic,NotRead,Programming profession,Robot control,TALAF,Weapons,artificial intelligence,bots tuning,computer games,first-person shooter robot controllers,game artificial intelligence,genetic algorithm,genetic algorithms,software agents,software design},
mendeley-tags = {CONTROL SYSTEMS,Counter Strike game,Counting circuits,Expert systems,Game theory,GameAI,Logic,NotRead,Programming profession,Robot control,TALAF,Weapons,artificial intelligence,bots tuning,computer games,first-person shooter robot controllers,game artificial intelligence,genetic algorithm,genetic algorithms,software agents,software design},
month = {Jun},
pages = {139--145},
title = {{Using a genetic algorithm to tune first-person shooter bots}},
volume = {1},
year = {2004}
}
@inproceedings{Contal2013,
abstract = {In this paper, we consider the challenge of maximizing an unknown function f for which evaluations are noisy and are acquired with high cost. An iterative procedure uses the previous measures to actively select the next estimation of f which is predicted to be the most useful. We focus on the case where the function can be evaluated in parallel with batches of fixed size and analyze the benefit compared to the purely sequential procedure in terms of cumulative regret. We introduce the Gaussian Process Upper Confidence Bound and Pure Exploration algorithm (GP-UCB-PE) which combines the UCB strategy and Pure Exploration in the same batch of evaluations along the parallel iterations. We prove theoretical upper bounds on the regret with batches of size K for this procedure which show the improvement of the order of sqrt{\{}K{\}} for fixed iteration cost over purely sequential versions. Moreover, the multiplicative constants involved have the property of being dimension-free. We also confirm empirically the efficiency of GP-UCB-PE on real and synthetic problems compared to state-of-the-art competitors.},
annote = {Contal et al{\_}2013{\_}Parallel Gaussian process optimization with upper confidence bound and pure.pdf
Contents
* Introduction
Problem Statement and Background
Sequential Batch Optimization
Objective
Gaussian Processes
Parallel Optimization Procedure
Confidence Region
Relevant Region
GP-UCB-PE
Numerical Complexity
Regret Bounds
Main Result
Discussion
Proofs of the Main Result
Experiments
Protocol
Description of Data Sets
Comparison of Algorithms
Conclusion},
archivePrefix = {arXiv},
arxivId = {arXiv:1304.5350v3},
author = {Contal, Emile and Buffoni, David and Robicquet, Alexandre and Vayatis, Nicolas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-40988-2{\_}15},
eprint = {arXiv:1304.5350v3},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Contal et al. - 2013 - Parallel Gaussian process optimization with upper confidence bound and pure exploration.pdf:pdf},
isbn = {9783642409875},
issn = {03029743},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
number = {PART 1},
pages = {225--240},
publisher = {Springer},
title = {{Parallel Gaussian process optimization with upper confidence bound and pure exploration}},
volume = {8188 LNAI},
year = {2013}
}
@misc{Cook2010,
abstract = {There is hardly ever a good reason to invert a matrix. What do you do if you need to solve Ax = b where A is an n x n matrix? Isn't the solution A-1 b? Yes,},
author = {Cook, John C},
booktitle = {Johndcook.com},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cook - 2010 - Don't invert that matrix.html:html},
keywords = {Acquisition/InfillFxns,BayesOpt,TALAF},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,TALAF},
title = {{Don't invert that matrix}},
year = {2010}
}
@misc{DBSCAN2016,
abstract = {Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, J{\"{o}}rg Sander and Xiaowei Xu in 1996. It is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature. In 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, KDD.},
author = {DBSCAN{\_}wiki},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2016 - DBSCAN.html:html},
keywords = {Acquisition/InfillFxns,BayesOpt,TALAF},
language = {en},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,TALAF},
month = {Jan},
title = {{DBSCAN}},
year = {2016}
}
@article{Dick2013,
abstract = {This paper is a contemporary review of QMC (‘quasi-Monte Carlo’) methods, that is, equal-weight rules for the approximate evaluation of high-dimensional integrals over the unit cube [0, 1]s,where s may be large, or even infinite. Af- ter a general introduction, the paper surveys recent developments in lattice methods, digital nets, and related themes. Among those recent developments are methods of construction of both lattices and digital nets, to yield QMC rules that have a prescribed rate of convergence for sufficiently smooth func- tions, and ideally also guaranteed slow growth (or no growth) of the worst-case error as s increases. A crucial role is played by parameters called ‘weights’, since a careful use of the weight parameters is needed to ensure that the worst-case errors in an appropriately weighted function space are bounded, or grow only slowly, as the dimension s increases. Important tools for the analysis are weighted function spaces, reproducing kernel Hilbert spaces, and discrepancy, all of which are discussed with an appropriate level of detail.},
author = {Dick, Josef and Kuo, Frances Y. and Sloan, Ian H.},
doi = {10.1017/S0962492913000044},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dick, Kuo, Sloan - 2013 - High-dimensional integration The quasi-Monte Carlo way.pdf:pdf},
issn = {0962-4929},
journal = {Acta Numerica},
keywords = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
language = {en},
mendeley-tags = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
month = {may},
number = {April 2013},
pages = {133--288},
shorttitle = {High-dimensional integration},
title = {{High-dimensional integration: The quasi-Monte Carlo way}},
volume = {22},
year = {2013}
}
@article{Dietterich2000,
abstract = {This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {cs/9905014},
author = {Dietterich, Thomas G.},
doi = {10.1613/jair.639},
eprint = {9905014},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dietterich - 2000 - Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition.pdf:pdf},
isbn = {978-3-540-67839-7},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
keywords = {TALAF},
mendeley-tags = {TALAF},
pages = {227--303},
primaryClass = {cs},
title = {{Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition}},
volume = {13},
year = {2000}
}
@inproceedings{Domingos1998,
abstract = {Current methods to avoid overfitting are either data-oriented (using separate data for validation) or representation-oriented (penalizing complexity in the model). This paper proposes process-oriented evaluation, where a model's expected generalization error is computed as a function of the search process that led to it. The paper develops the necessary theoretical framework, and applies it to one type of learning: rule induction. A process-oriented version of the CN2 rule learner is empirically compared with the default CN2. The process-oriented version is more accurate in a large majority of the datasets, with high significance, and also produces simpler models. Experiments in artificial domains suggest that processoriented evaluation is particularly useful in high-dimensional domains. 1 INTRODUCTION Overfitting avoidance is often considered the central problem of machine learning (e.g., (Cheeseman {\&} Oldford, 1994)). If a learner is sufficiently powerful, it must guard against selec...},
author = {Domingos, Pedro},
booktitle = {Proceedings of the Fifteenth International Conference on Machine learning},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Domingos - 1998 - A process-oriented heuristic for model selection.html:html;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Domingos - 1998 - A process-oriented heuristic for model selection.pdf:pdf},
keywords = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF,imported},
mendeley-tags = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF},
pages = {127--135},
title = {{A process-oriented heuristic for model selection}},
year = {1998}
}
@inproceedings{Drachen2009a,
abstract = {We present a study focused on constructing models of players for the major commercial title Tomb Raider: Underworld (TRU). Emergent self-organizing maps are trained on high-level playing behavior data obtained from 1365 players that completed the TRU game. The unsupervised learning approach utilized reveals four types of players which are analyzed within the context of the game. The proposed approach automates, in part, the traditional user and play testing procedures followed in the game industry since it can inform game developers, in detail, if the players play the game as intended by the game design. Subsequently, player models can assist the tailoring of game mechanics in real-time for the needs of the player type identified.},
annote = {From Duplicate 1 (Player modeling using self-organization in Tomb Raider: Underworld - Drachen, Anders; Canossa, Alessandro; Yannakakis, Georgios N.)

http://ieeexplore.ieee.org/ielx5/5278415/5286449/05286500.pdf?tp={\&}amp;arnumber=5286500{\&}amp;isnumber=5286449get emergent behavior from 1300+ human players.----------Drachen et al{\_}2009{\_}Player modeling using self-organization in Tomb Raider.pdfget emergent behavior from 1300+ human players.

From Duplicate 2 (Player modeling using self-organization in Tomb Raider: Underworld - Drachen, Anders; Canossa, Alessandro; Yannakakis, Georgios N)

http://ieeexplore.ieee.org/ielx5/5278415/5286449/05286500.pdf?tp={\{}{\&}amp;{\}}amp;arnumber=5286500{\{}{\&}amp;{\}}amp;isnumber=5286449get emergent behavior from 1300+ human players.----------Drachen et al{\{}{\_}{\}}2009{\{}{\_}{\}}Player modeling using self-organization in Tomb Raider.pdfget emergent behavior from 1300+ human players.},
author = {Drachen, Anders and Canossa, Alessandro and Yannakakis, Georgios N.},
booktitle = {CIG2009 - 2009 IEEE Symposium on Computational Intelligence and Games},
doi = {10.1109/CIG.2009.5286500},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Drachen, Canossa, Yannakakis - 2009 - Player modeling using self-organization in Tomb Raider Underworld.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Drachen, Canossa, Yannakakis - 2009 - Player modeling using self-organization in Tomb Raider Underworld.html:html},
isbn = {9781424448159},
keywords = {Automatic testing,Computer industry,Computerized monitoring,Data mining,Emergent self-organizing maps,GameAI,Gold,Instruments,Player modeling,Production,Self organizing feature maps,TALAF,Tomb Raider Underworld,Tomb Raider: Underworld,Toy industry,Unsupervised learning,computer games,emergent self-organizing maps,game design,game industry,high-level playing behavior data obtained,learning (artificial intelligence),player modeling,self-organising feature maps,unsupervised learning,user modelling},
mendeley-tags = {Automatic testing,Computer industry,Computerized monitoring,Data mining,GameAI,Gold,Instruments,Production,Self organizing feature maps,TALAF,Tomb Raider Underworld,Tomb Raider: Underworld,Toy industry,computer games,emergent self-organizing maps,game design,game industry,high-level playing behavior data obtained,learning (artificial intelligence),player modeling,self-organising feature maps,unsupervised learning,user modelling},
month = {Sep},
pages = {1--8},
shorttitle = {Player modeling using self-organization in Tomb Ra},
title = {{Player modeling using self-organization in Tomb Raider: Underworld}},
year = {2009}
}
@article{Efron1997,
abstract = {A study investigates the error rate of a rule for predicting future responses constructed from a training set of data. Results are nonparametric and apply to any possible prediction rule.},
author = {Efron, Bradley and Tibshirani, Robert},
doi = {10.1080/01621459.1997.10474007},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Efron, Tibshirani - 1997 - Improvements on cross-validation The .632 plus bootstrap method.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF,classification,cross-validation bootstrap,prediction rule},
language = {en},
mendeley-tags = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF},
month = {Jun},
number = {438},
pages = {548},
pmid = {370},
shorttitle = {Improvements on Cross-Validation},
title = {{Improvements on cross-validation: The .632 plus bootstrap method}},
volume = {92},
year = {1997}
}
@inproceedings{Eggensperger2013,
annote = {Eggensperger et al{\_}2013{\_}Towards an empirical foundation for assessing bayesian optimization of.pdf
Contents
* Introduction
Bayesian Optimization Methods for Hyperparameter Optimization
Hyperparameter Optimization Benchmarks
Experiments
Conclusion and Future Work},
author = {Eggensperger, Katharina and Feurer, Matthias and Hutter, Frank and Bergstra, James and Snoek, Jasper and Hoos, Holger and Leyton-Brown, Kevin},
booktitle = {BayesOpt workshop (NIPS)},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eggensperger et al. - 2013 - Towards an Empirical Foundation for Assessing Bayesian Optimization of Hyperparameters.pdf:pdf},
keywords = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
title = {{Towards an Empirical Foundation for Assessing Bayesian Optimization of Hyperparameters}},
year = {2013}
}
@book{Ericsson2009,
author = {Ericsson, KA A},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ericsson - 2009 - Development of professional expertise Toward measurement of expert performance and design of optimal learning environm.pdf:pdf},
keywords = {AFRL,TALAF},
mendeley-tags = {AFRL,TALAF},
title = {{Development of professional expertise: Toward measurement of expert performance and design of optimal learning environments}},
year = {2009}
}
@inproceedings{Ester1996,
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"{o}}rg and Xu, Xiaowei},
booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), Portland, Oregon, USA},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ester et al. - 1996 - A density-based algorithm for discovering clusters in large spatial databases with noise.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ester et al. - 1996 - A density-based algorithm for discovering clusters in large spatial databases with noise.html:html},
isbn = {1-57735-004-9},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {226--231},
publisher = {AAAI Press},
title = {{A density-based algorithm for discovering clusters in large spatial databases with noise}},
year = {1996}
}
@inproceedings{Feurer2014,
abstract = {Model selection and hyperparameter optimization is cru-cial in applying machine learning to a novel dataset. Recently, a sub-community of machine learning has focused on solving this prob-lem with Sequential Model-based Bayesian Optimization (SMBO), demonstrating substantial successes in many applications. However, for expensive algorithms the computational overhead of hyperpa-rameter optimization can still be prohibitive. In this paper we ex-plore the possibility of speeding up SMBO by transferring knowl-edge from previous optimization runs on similar datasets; specifi-cally, we propose to initialize SMBO with a small number of config-urations suggested by a metalearning procedure. The resulting simple MI-SMBO technique can be trivially applied to any SMBO method, allowing us to perform experiments on two quite different SMBO methods with complementary strengths applied to optimize two ma-chine learning frameworks on 57 classification datasets. We find that our initialization procedure mildly improves the state of the art in low-dimensional hyperparameter optimization and substantially im-proves the state of the art in the more complex problem of combined model selection and hyperparameter optimization.},
author = {Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank},
booktitle = {CEUR Workshop Proceedings},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feurer, Springenberg, Hutter - 2014 - Using meta-learning to initialize Bayesian optimization of hyperparameters.pdf:pdf},
issn = {16130073},
keywords = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
pages = {3--10},
title = {{Using meta-learning to initialize Bayesian optimization of hyperparameters}},
volume = {1201},
year = {2014}
}
@article{Finkel2003,
abstract = {O objetivo deste guia {\'{e}} breve para introduzir o leitor para o algoritmo de otimiza{\c{c}}{\~{a}}o DIRECT, descrever o tipo de problemas que resolve, como usar o programa que acompanha MATLAB direct.m, e fornecer uma sinopse de como ele procura o m{\'{\i}}nio global . Um exemplo de DIRECT sendo usado em um problema de teste {\'{e}} fornecida, e os motiviation para o algoritmo {\'{e}} tamb{\'{e}}m discutida. O ap{\^{e}}ndice fornece f{\'{o}}rmulas e dados para o 7 fun{\c{c}}{\~{o}}es de teste que foram utilizadas em [3].},
author = {Finkel, Daniel E.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Finkel - 2003 - DIRECT optimization algorithm user guide.pdf:pdf},
journal = {Center for Research in Scientific Computation, North Carolina State University},
keywords = {BayesOpt,DIRECT,TALAF},
mendeley-tags = {BayesOpt,DIRECT,TALAF},
pages = {1--14},
title = {{DIRECT optimization algorithm user guide}},
volume = {2},
year = {2003}
}
@article{Gardner2014,
abstract = {Bayesian optimization is a powerful frame- work for minimizing expensive objective functions while using very few function eval- uations. It has been successfully applied to a variety of problems, including hyperparam- eter tuning and experimental design. How- ever, this framework has not been extended to the inequality-constrained optimization setting, particularly the setting in which eval- uating feasibility is just as expensive as eval- uating the objective. Here we present con- strained Bayesian optimization, which places a prior distribution on both the objective and the constraint functions. We evaluate our method on simulated and real data, demon- strating that constrained Bayesian optimiza- tion can quickly find optimal and feasible points, even when small feasible regions cause standard methods to fail.},
author = {Gardner, Jacob R. and Kusner, Matt J. and Xu, Zhixiang Eddie and Weinberger, Kilian Q. and Cunningham, John P.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gardner et al. - 2014 - Bayesian Optimization with Inequality Constraints(2).pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of the 31st International Conference on Machine Learning},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {937--945},
title = {{Bayesian Optimization with Inequality Constraints}},
volume = {32},
year = {2014}
}
@article{Garg2010,
annote = {{\#} Shows that Occams razor does hold in Bayesian learning, but that it isn't directly associated with the number of parameters, rather the complexity of the model.----------Rasmussen{\_}Ghahramani{\_}2001{\_}Occam's razor.pdf
Contents
* Introduction
View 1: Model size selection
View 2: Large models
Linear in the parameters models -- Example: the Fourier model
Inference in the Fourier model
Example
Discussion
Conclusion},
author = {Garg, A.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garg - 2010 - Occam's razor.pdf:pdf},
journal = {A.Word.A.Day},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {294--300},
title = {{Occam's razor}},
year = {2010}
}
@phdthesis{Gelbart2015,
abstract = {Bayesian optimization is an approach for globally optimizing black-box functions that are expensive to evaluate, non-convex, and possibly noisy. Recently, Bayesian optimization has been used with great effectiveness for applications like tuning the hyperparameters of machine learning algorithms and automatic A/B testing for websites. This thesis considers Bayesian optimization in the presence of black-box constraints. Prior work on constrained Bayesian optimization consists of a variety of methods that can be used with some efficacy in specific contexts. Here, by forming a connection with multi-task Bayesian optimization, we formulate a more general class of constrained Bayesian optimization problems that we call Bayesian optimization with decoupled constraints. In this general framework, the objective and constraint functions are divided into tasks that can be evaluated independently of each other, and resources with which these tasks can be performed. We then present two methods for solving problems in this general class. The first method, an extension to a constrained variant of expected improvement, is fast and straightforward to implement but performs poorly in some circumstances and is not sufficiently flexible to address all varieties of decoupled problems. The second method, Predictive Entropy Search with Constraints (PESC), is highly effective and sufficiently flexible to address all problems in the general class of decoupled problems without any ad hoc modifications. The two weaknesses of PESC are its implementation difficulty and slow execution time. We address these issues by, respectively, providing a publicly available implementation within the popular Bayesian optimization software Spearmint, and developing an extension to PESC that achieves greater speed without significant performance losses. We demonstrate the effectiveness of these methods on real-world machine learning meta-optimization problems.},
author = {Gelbart, Michael Adam},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelbart - 2015 - Constrained Bayesian Optimization and Applications.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelbart - 2015 - Constrained Bayesian Optimization and Applications.html:html},
keywords = {Acquisition/InfillFxns,BayesOpt,TALAF},
language = {en},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,TALAF},
month = {may},
title = {{Constrained Bayesian Optimization and Applications}},
year = {2015}
}
@article{Pelikan2000,
author = {Goldberg, David E. and Pelikan, Martin},
file = {:home/brett/GoogleDrive/Education/Graduate/Mendeley/Goldberg, Pelikan/Genetic - 2000 - Bayesian Optimization Algorithm, Population Sizing, and Time to Convergence Martin Pelikan, David. E. Goldberg, and Eri.pdf:pdf},
journal = {Population (English Edition)},
keywords = {BayesOpt,NotRead,TALAF},
mendeley-groups = {BayesOpt},
mendeley-tags = {BayesOpt,NotRead,TALAF},
number = {2000001},
pages = {275--282},
title = {{Bayesian Optimization Algorithm, Population Sizing, and Time to Convergence Martin Pelikan, David. E. Goldberg, and Erick Cantu-Paz}},
year = {2000}
}
@article{Ginsbourger2010,
abstract = {The optimization of expensive-to-evaluate functions generally relies on metamodel-based exploration strategies. Many deterministic global optimization algorithms used in the field of computer experiments are based on Kriging (Gaussian process regression). Starting with a spatial predictor including a measure of uncertainty, they proceed by iteratively choosing the point maximizing a criterion which is a compromise between predicted performance and uncertainty. Distributing the evaluation of such numerically expensive objective functions on many processors is an appealing idea. Here we investigate a multi-points optimization criterion, the multipoints expected improvement ( q−EIq-$\backslash${\{}{\{}$\backslash$textbackslash{\}}mathbb E{\}}I ), aimed at choosing several points at the same time. An analytical expression of the q−EIq-$\backslash${\{}{\{}$\backslash$textbackslash{\}}mathbb E{\}}I is given when q = 2, and a consistent statistical estimate is given for the general case. We then propose two classes of heuristic strategies meant to approximately optimize the q−EIq-$\backslash${\{}{\{}$\backslash$textbackslash{\}}mathbb E{\}}I , and apply them to the classical Branin-Hoo test-case function. It is finally demonstrated within the covered example that the latter strategies perform as good as the best Latin Hypercubes and Uniform Designs ever found by simulation (2000 designs drawn at random for every q ∈ [1,10]).},
author = {Ginsbourger, David and Riche, Rodolphe Le and Carraro, Laurent},
doi = {10.1007/978-3-642-10701-6{\_}6},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ginsbourger, Riche, Carraro - 2010 - Kriging {\{}Is{\}} {\{}Well{\}}-{\{}Suited{\}} to {\{}Parallelize{\}} {\{}Optimization{\}}.pdf:pdf},
isbn = {978-3-642-10700-9, 978-3-642-10701-6},
journal = {Computational Intelligence in Expensive Optimization Problems},
keywords = {Acquisition/InfillFxns,Appl.Mathematics/Computational Methods of Enginee,Applications of Mathematics,Artificial Intelligence (incl. Robotics),BayesOpt,TALAF,qEI},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,TALAF,qEI},
number = {2},
pages = {131--162},
publisher = {Springer},
title = {{Kriging Is Well-Suited to Parallelize Optimization}},
year = {2010}
}
@article{Golovin2010,
abstract = {We tackle the fundamental problem of Bayesian active learning with noise, where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution. In the case of noise-free observations, a greedy algorithm called generalized binary search (GBS) is known to perform near-optimally. We show that if the observations are noisy, perhaps surprisingly, GBS can perform very poorly. We develop EC2, a novel, greedy active learning algorithm and prove that it is competitive with the optimal policy, thus obtaining the first competitiveness guarantees for Bayesian active learning with noisy observations. Our bounds rely on a recently discovered diminishing returns property called adaptive submodularity, generalizing the classical notion of submodular set functions to adaptive policies. Our results hold even if the tests have non–uniform cost and their noise is correlated. We also propose EffECXtive, a particularly fast approximation of EC2, and evaluate it on a Bayesian experimental design problem involving human subjects, intended to tease apart competing economic theories of how people make decisions under uncertainty.},
archivePrefix = {arXiv},
arxivId = {1010.3091},
author = {Golovin, Daniel and Krause, Andreas and Ray, Debajyoti},
eprint = {1010.3091},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Golovin, Krause, Ray - 2010 - Near-optimal Bayesian active learning with noisy observations.pdf:pdf},
isbn = {9781617823800},
journal = {Advances in Neural Information {\ldots}},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {1--9},
title = {{Near-optimal Bayesian active learning with noisy observations}},
year = {2010}
}
@inproceedings{Gonsalves2004,
abstract = {Recent military actions have demonstrated the need for addressing time-sensitive and time-critical targeting. The capabilities of precision guided munitions and the further development of strike warfare platforms and tactics portend a huge increase in effectiveness and lethality of air operations and achieving the vision of the US Air Force's Global Strike Task Force concept. To fully realize the benefits of these strike capable assets and to address the challenges inherent in time-sensitive targeting, decision support systems are needed to assist warfighters in optimal allocation and near real-time re-deployment of air assets, and to support predictive battlespace awareness. While meeting a specific operational need, additional benefits can accrue through the employment of such decision support systems for virtual and constructive simulation based training, experimentation, and Command and Control (C2) system evaluation and acquisition. Here, we detail a Software Toolkit for Optimizing Mission Plans (STOMP). STOMP integrates a genetic algorithm-based mechanism to rapidly generate, analyze, and visualize mission plans in tandem with software interoperability to provide the requisite interface and connectivity with Air Force C2 systems and synthetic battlespace environments. Copyright © 2004 by Charles River Analytics, Inc.},
annote = {Mainly looks at targeting. No metrics are discussed},
author = {Gonsalves, P.G. and Burge, J.E.},
booktitle = {Collection of Technical Papers - AIAA 1st Intelligent Systems Technical Conference},
doi = {doi:10.2514/6.2004-6279},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gonsalves, Burge - 2004 - Software Toolkit for Optimizing Mission Plans (STOMP).pdf:pdf},
isbn = {156347719X},
keywords = {TALAF},
language = {en},
mendeley-tags = {TALAF},
month = {Sep},
pages = {391--399},
publisher = {American Institute of Aeronautics and Astronautics},
title = {{Software Toolkit for Optimizing Mission Plans (STOMP)}},
volume = {1},
year = {2004}
}
@incollection{Goodrich1990,
annote = {This seems more related to aircraft improvement.},
author = {Goodrich, Kenneth and McManus, John},
booktitle = {Orbital Debris Conference: Technical Issues andFuture Directions},
doi = {doi:10.2514/6.1990-1287},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodrich, McManus - 1990 - An integrated environment for tactical guidance research and evaluation.pdf:pdf},
keywords = {TALAF},
language = {en},
mendeley-tags = {TALAF},
month = {apr},
publisher = {American Institute of Aeronautics and Astronautics},
title = {{An integrated environment for tactical guidance research and evaluation}},
year = {1990}
}
@article{Gopalan2014,
abstract = {Proceedings of the International Conference on Machine Learning 2014},
address = {New York ; London},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.0466v1},
author = {Gopalan, Aditya and Mannor, Shie and Mansour, Yishay},
editor = {Sammut, Claude and Webb, Geoffrey I.},
eprint = {arXiv:1311.0466v1},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gopalan, Mannor, Mansour - 2014 - Thompson Sampling for Complex Online Problems.pdf:pdf},
isbn = {9781634393973},
journal = {Icml},
keywords = {Acquisition/InfillFxns,BayesOpt,Machine learning,NotRead,TALAF,bandit problem},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,Machine learning,NotRead,TALAF},
pages = {1--9},
publisher = {Springer},
title = {{Thompson Sampling for Complex Online Problems}},
year = {2014}
}
@inproceedings{Guestrin2005,
author = {Guestrin, Carlos and Krause, Andreas and Singh, Ajit Paul},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guestrin, Krause, Singh - 2005 - Near-optimal sensor placements in gaussian processes.pdf:pdf},
keywords = {BayesOpt,GPs,NotRead,TALAF},
mendeley-tags = {BayesOpt,GPs,NotRead,TALAF},
pages = {265--272},
publisher = {ACM},
title = {{Near-optimal sensor placements in gaussian processes}},
year = {2005}
}
@inproceedings{Ha2015,
abstract = {This paper presents a game theory-based methodology to analyze multiple beyond-visual-range (BVR) air combat scenarios. The combat scenarios are formalized as a stochastic game consisting of a sequence of normal-form games with a continuous sub-game. The proposed game formulation improves the scalability by reducing the decision space while taking advantage of some underlying symmetry structures of the combat scenario. The equilibrium strategy and the value functions of the game are computed through some dynamic programming procedure; the impact of the aircraft's velocity and the cooperation scheme for the combat is analyzed based on the equilibrium strategies.},
author = {Ha, Jung-Su and Chae, Hyeok-Joo and Choi, Han-Lim},
booktitle = {American Control Conference},
doi = {10.1109/ACC.2015.7171909},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ha, Chae, Choi - 2015 - A Stochastic Game-Theoretic Approach for Analysis of Multiple Cooperative Air Combat.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ha, Chae, Choi - 2015 - A Stochastic Game-Theoretic Approach for Analysis of Multiple Cooperative Air Combat.html:html},
isbn = {9781479986866},
keywords = {Aircraft,BVR air combat scenarios,Games,Missiles,Nash equilibrium,NotRead,Resource management,Stochastic processes,TALAF,aircraft velocity,autonomous aerial vehicles,beyond-visual-range air combat scenarios,continuous subgame,cooperation scheme,decision space,dynamic programming,equilibrium strategies,equilibrium strategy,game formulation,game theory-based methodology,game value functions,military aircraft,multiple cooperative air combat analysis,normal-form games,stochastic game-theoretic approach,stochastic games,symmetry structures,unmanned air combat system,velocity control},
mendeley-tags = {Aircraft,BVR air combat scenarios,Games,Missiles,Nash equilibrium,NotRead,Resource management,Stochastic processes,TALAF,aircraft velocity,autonomous aerial vehicles,beyond-visual-range air combat scenarios,continuous subgame,cooperation scheme,decision space,dynamic programming,equilibrium strategies,equilibrium strategy,game formulation,game theory-based methodology,game value functions,military aircraft,multiple cooperative air combat analysis,normal-form games,stochastic game-theoretic approach,stochastic games,symmetry structures,unmanned air combat system,velocity control},
month = {jul},
pages = {3728--3733},
title = {{A Stochastic Game-Theoretic Approach for Analysis of Multiple Cooperative Air Combat}},
year = {2015}
}
@article{Hennig2012,
author = {Hennig, Philipp and Schuler, Christian J.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hennig, Schuler - 2012 - Entropy {\{}Search for Information-Efficient Global Optimization{\}}.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {1809--1837},
title = {{Entropy Search for Information-Efficient Global Optimization}},
volume = {13},
year = {2012}
}
@article{Hensman2013,
abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be vari- ationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our ap- proach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
archivePrefix = {arXiv},
arxivId = {1309.6835},
author = {Hensman, James and Sheffield, Uk and Fusi, Nicolo and Lawrence, Nd},
eprint = {1309.6835},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hensman et al. - 2013 - Gaussian Processes for Big Data.pdf:pdf},
journal = {Proceedings of UAI 29},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {282--290},
title = {{Gaussian Processes for Big Data}},
year = {2013}
}
@article{Herbrich2006,
abstract = {We present a new Bayesian skill rating system which can be viewed as a generalisation of the Elo system used in Chess. The new system tracks the uncertainty about player skills, explicitly models draws, can deal with any number of competing entities and can infer individual skills from team results. Inference is performed by approximate message passing on a factor graph representation of the model. We present experimental evidence on the increased accuracy and convergence speed of the system compared to Elo and report on our experience with the new rating system running in a large-scale commercial online gaming service under the name of TrueSkill.},
archivePrefix = {arXiv},
arxivId = {1011.1761},
author = {Herbrich, Ralf and Minka, Tom and Graepel, Thore},
doi = {10.2134/jeq2007.0177},
editor = {Sch{\"{o}}lkopf, B and Platt, J C and Hoffman, T},
eprint = {1011.1761},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Herbrich, Minka, Graepel - 2006 - TrueSkill A Bayesian Skill Rating System.html:html},
isbn = {1049-5258},
issn = {00472425},
journal = {Advances in Neural Information Processing Systems},
keywords = {NotRead,TALAF,bayesian learning,dynamic difficulty adjustment,match-making},
mendeley-tags = {NotRead,TALAF},
pages = {569--576},
pmid = {18268290},
publisher = {MIT Press},
shorttitle = {TrueSkill™},
title = {{TrueSkill: A Bayesian Skill Rating System}},
year = {2006}
}
@article{Hoffman2014,
author = {Hoffman, Matthew W. and Shahriari, Robak},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffman, Shahriari - 2014 - Modular mechanisms for Bayesian optimization.pdf:pdf},
journal = {NIPS Workshop on Bayesian Optimization},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {1--5},
title = {{Modular mechanisms for Bayesian optimization}},
year = {2014}
}
@article{Hoffman2011,
abstract = {Bayesian optimization with Gaussian pro- cesses has become an increasingly popular tool in the machine learning community. It is efficient and can be used when very little is known about the objective function, mak- ing it popular in expensive black-box optimization scenarios. It uses Bayesian methods to sample the objective efficiently using an acquisition function which incorporates the posterior estimate of the objective. However, there are several different parameterized acquisition functions in the literature, and it is often unclear which one to use. Instead of using a single acquisition function, we adopt a portfolio of acquisition functions governed by an online multi-armed bandit strategy. We propose several portfolio strategies, the best of which we call GP-Hedge, and show that this method outperforms the best individ- ual acquisition function. We also provide a theoretical bound on the algorithm’s performance.},
archivePrefix = {arXiv},
arxivId = {arXiv:1009.5419v1},
author = {Hoffman, Matthew and Brochu, Eric and Freitas, Nando De},
eprint = {arXiv:1009.5419v1},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffman, Brochu, Freitas - 2011 - Portfolio Allocation for Bayesian Optimization(2).pdf:pdf},
isbn = {978-0-9749039-7-2},
journal = {Conference on Uncertainty in Artificial Intelligence},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {327--336},
publisher = {Citeseer},
title = {{Portfolio Allocation for Bayesian Optimization}},
year = {2011}
}
@inproceedings{Holmgard2014a,
abstract = {This paper explores how evolved game playing agents can be used to represent a priori defined archetypical ways of playing a test-bed game, as procedural personas. The end goal of such procedural personas is substituting players when authoring game content manually, procedurally, or both (in a mixed-initiative setting). Building on previous work, we compare the performance of newly evolved agents to agents trained via Q-learning as well as a number of baseline agents. Comparisons are performed on the grounds of game playing ability, generalizability, and conformity among agents. Finally, all agents' decision making styles are matched to the decision making styles of human players in order to investigate whether the different methods can yield agents who mimic or differ from human decision making in similar ways. The experiments performed in this paper conclude that agents developed from a priori defined objectives can express human decision making styles and that they are more generalizable and versatile than Q-learning and hand-crafted agents.},
annote = {From Duplicate 1 (Evolving personas for player decision modeling - Holmgard, Christoffer; Liapis, Antonios; Togelius, Julian; Yannakakis, Georgios N.)

http://ieeexplore.ieee.org/ielx7/6919811/6932856/06932911.pdf?tp={\&}amp;arnumber=6932911{\&}amp;isnumber=6932856model human playing styles as {\&}quot;procedural persona{\&}quot;
Discusses {\&}quot;playing styles{\&}quot; that are classified by a perceptron.
evaluates the differences between {\&}quot;personas{\&}quot;(evolved AIs) and human players----------Holmgard et al{\_}2014{\_}Evolving personas for player decision modeling.pdfmodel human playing styles as {\&}quot;procedural persona{\&}quot;
Discusses {\&}quot;playing styles{\&}quot; that are classified by a perceptron.
evaluates the differences between {\&}quot;personas{\&}quot;(evolved AIs) and human players

From Duplicate 2 (Evolving personas for player decision modeling - Holmgard, Christoffer; Liapis, Antonios; Togelius, Julian; Yannakakis, Georgios N)

http://ieeexplore.ieee.org/ielx7/6919811/6932856/06932911.pdf?tp={\{}{\&}amp;{\}}amp;arnumber=6932911{\{}{\&}amp;{\}}amp;isnumber=6932856model human playing styles as {\{}{\&}amp;{\}}quot;procedural persona{\{}{\&}amp;{\}}quot;
Discusses {\{}{\&}amp;{\}}quot;playing styles{\{}{\&}amp;{\}}quot; that are classified by a perceptron.
evaluates the differences between {\{}{\&}amp;{\}}quot;personas{\{}{\&}amp;{\}}quot;(evolved AIs) and human players----------Holmgard et al{\{}{\_}{\}}2014{\{}{\_}{\}}Evolving personas for player decision modeling.pdfmodel human playing styles as {\{}{\&}amp;{\}}quot;procedural persona{\{}{\&}amp;{\}}quot;
Discusses {\{}{\&}amp;{\}}quot;playing styles{\{}{\&}amp;{\}}quot; that are classified by a perceptron.
evaluates the differences between {\{}{\&}amp;{\}}quot;personas{\{}{\&}amp;{\}}quot;(evolved AIs) and human players},
author = {Holmgard, Christoffer and Liapis, Antonios and Togelius, Julian and Yannakakis, Georgios N.},
booktitle = {IEEE Conference on Computatonal Intelligence and Games, CIG},
doi = {10.1109/CIG.2014.6932911},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holmgard et al. - 2014 - Evolving personas for player decision modeling.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holmgard et al. - 2014 - Evolving personas for player decision modeling.html:html},
isbn = {9781479935468},
issn = {23254289},
keywords = {Decision making,GameAI,Important,Navigation,Q-learning,TALAF,agents conformity,agents decision making styles,archetypical ways,authoring,baseline agents,computer games,evolved game playing agents,evolving personas,game content,game playing ability,game playing generalizability,hand-crafted agents,human decision making,human players,learning (artificial intelligence),multi-agent systems,player decision modeling,procedural personas,test-bed game playing},
mendeley-tags = {Decision making,GameAI,Important,Navigation,Q-learning,TALAF,agents conformity,agents decision making styles,archetypical ways,authoring,baseline agents,computer games,evolved game playing agents,evolving personas,game content,game playing ability,game playing generalizability,hand-crafted agents,human decision making,human players,learning (artificial intelligence),multi-agent systems,player decision modeling,procedural personas,test-bed game playing},
month = {Aug},
pages = {1--8},
title = {{Evolving personas for player decision modeling}},
year = {2014}
}
@phdthesis{Huang2005,
abstract = {Engineers in many industries routinely need to improve the product or process designs using data from the field, lab experiments, and computer experiments. Historically, designers have performed a calibration exercise to "fix" the lab system or computer model and then used an analysis method or optimization procedure that ignores the fact that systematic differences between products in the field and other environments necessarily exist. A new line of research is not based on the assumption that calibration is perfect and seeks to develop experimental planning and optimization schemes using data form multiple experimental sources. We use the term "fidelity" to refer to the extent to which a surrogate experimental system can reproduce results of the system of interest. For experimental planning, we present perhaps the first optimal designs for variable fidelity experimentation, using an extension of the Expected Integrated Mean Squared Error (EIMSE) criterion, where the Generalized Least Squares (GLS) method was used to generate the predictions. Numerical tests are used to compare the method performance with alternatives and to investigate the robustness to incorporated assumptions. The method is applied to automotive engine valve heat treatment process design in which real world data were mixed with data from two types of computer simulations. Sequential Kriging Optimization (SKO) is a method developed in recent years for solving expensive black-box problems in areas such as large-scale circuit board design and manufacturing process improvement. We propose an extension of the SKO method, named Multiple Fidelity Sequential Kriging Optimization (MFSKO), where surrogate systems are exploited to reduce the total evaluation cost. As a pre-step to MFSKO, we extended SKO to address stochastic black-box systems. In the empirical studies using numerical test functions, SKO compared favorably with alternatives in terms of consistency in finding global optima and efficiency as measured by number of evaluations. Also, in the presence of noise, the new expected improvement function for infill sample selection appears to achieve the desired balance between the need for global and local searches. In the proposed MFSKO method, data on all experimental systems are integrated to build a kriging meta-model that provides a global prediction of the system of interest and a measure of prediction uncertainty. The location and fidelity level of the next evaluation are selected by maximizing an augmented expected improvement function, which is connected with the evaluation costs. The proposed method was applied to test functions from the literature and metal-forming process design problems via Finite Element simulations. The method manifests sensible search patterns, robust performance, and appreciable reduction in total evaluation cost as compared to the original method.},
author = {Huang, Deng},
booktitle = {ProQuest Dissertations and Theses},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang - 2005 - Experimental planning and sequential kriging optimization using variable fidelity data.pdf:pdf},
isbn = {9780496944156; 0496944150},
keywords = {0546:Industrial engineering,0796:Operations research,Acquisition/InfillFxns,Applied sciences,BayesOpt,Engine valves,Industrial engineering,Kriging,Multiple fidelity,Operations research,TALAF},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,TALAF},
pages = {120--120 p.},
title = {{Experimental planning and sequential kriging optimization using variable fidelity data}},
year = {2005}
}
@article{Hutter2009,
author = {Hutter, Frank},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hutter - 2009 - Automated configuration of algorithms for solving hard computational problems.pdf:pdf},
keywords = {BayesOpt,GPs,NotRead,TALAF},
mendeley-tags = {BayesOpt,GPs,NotRead,TALAF},
title = {{Automated configuration of algorithms for solving hard computational problems}},
year = {2009}
}
@inproceedings{Hutter2011,
abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.},
annote = {Hutter et al. - 2011 - Sequential model-based optimization for general al.pdfRandom trees in Bayesian optimization},
author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25566-3{\_}40},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hutter, Hoos, Leyton-Brown - 2011 - Sequential model-based optimization for general algorithm configuration.pdf:pdf},
isbn = {9783642255656},
issn = {03029743},
keywords = {BayesOpt,Important,NotRead,TALAF},
mendeley-tags = {BayesOpt,Important,NotRead,TALAF},
pages = {507--523},
publisher = {Springer},
title = {{Sequential model-based optimization for general algorithm configuration}},
volume = {6683 LNCS},
year = {2011}
}
@inproceedings{Huynh1987,
author = {Huynh, HT T and Costes, Ph.},
booktitle = {. New York: AIAA Press, CA, AIAA-87},
doi = {doi:10.2514/6.1987-2392},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huynh, Costes - 1987 - Numerical optimization of air combat maneuvers.pdf:pdf},
keywords = {Important,TALAF},
language = {en},
mendeley-tags = {Important,TALAF},
month = {Aug},
pages = {647--658},
publisher = {American Institute of Aeronautics and Astronautics},
title = {{Numerical optimization of air combat maneuvers}},
year = {1987}
}
@book{Johansson2010,
abstract = {It is in military engagements the task of the air defense to protect valuable assets such as air bases from being destroyed by hostile aircrafts and missiles. In order to fulfill this mission, the ...},
annote = {PhD Thesis, mostly interested in threat detection. From a first look, doesn't really seem like there are metrics here},
author = {Johansson, Fredrik},
booktitle = {Orebro University 2010},
doi = {978-91-7668-761-1},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johansson - 2010 - Evaluating the performance of TEWA systems.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johansson - 2010 - Evaluating the performance of TEWA systems.html:html},
isbn = {9789176687611},
keywords = {TALAF},
language = {eng},
mendeley-tags = {TALAF},
title = {{Evaluating the performance of TEWA systems}},
year = {2010}
}
@misc{John,
abstract = {Someone asked me on Twitter Is there a trick to make an singular (non-invertible) matrix invertible? The only response I could think of in less than 140},
author = {John},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/John - Unknown - Making a singular matrix non-singular.html:html},
keywords = {Acquisition/InfillFxns,BayesOpt,TALAF},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,TALAF},
title = {{Making a singular matrix non-singular}},
}
@article{Jones1993,
author = {Jones, Donald R. and Law, Computer and Law, Computer},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jones, Law, Law - 1993 - Lipschitzian Optimization Without the Lipschitz Constant.pdf:pdf},
journal = {Journal of Optimization Theory and Applications},
keywords = {1staff research scientist,BayesOpt,DIRECT,Important,TALAF,covering,development center,general motors research and,global optimization,lipschitzian optimization,space,space partitioning,warren},
mendeley-tags = {BayesOpt,DIRECT,Important,TALAF},
number = {1},
pages = {157--181},
title = {{Lipschitzian Optimization Without the Lipschitz Constant}},
volume = {79},
year = {1993}
}
@article{Jones1998,
abstract = {Inmany engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
annote = {Introduced EI},
author = {Jones, Donald R. and Schonlau, Matthias and William, J},
doi = {10.1023/a:1008306431147},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jones, Schonlau, William - 1998 - Efficient Global Optimization of Expensive Black-Box Functions.pdf:pdf},
isbn = {0925-5001},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Acquisition/InfillFxns,BayesOpt,TALAF,bayesian global optimization,kriging,process,random function,response surface,stochastic,visualization},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,TALAF},
number = {4},
pages = {455--492},
pmid = {21858987},
title = {{Efficient Global Optimization of Expensive Black-Box Functions}},
volume = {13},
year = {1998}
}
@article{Kelly1988,
abstract = {Measurement and assessment of operator performance of complex tasks with decisional and psychomotor components of which only ultimate outcome measures are well defined provide difficult methodological challenges. One such task is the practice of air-to-air combat skills in simulated combat environments such as the Air Force Air Combat Maneuvering Instrumentation (ACMI) or ground-based visual flight simulators. Measurement and assessment of pilot performance in these environments are especially challenging because of the special cognitive and psychomotor skills involved in the freeplay and gamesmanship of two or more competing pilots. A review of the existing approaches to automated aircrew performance measurement was performed. Although the most common measurement models, using positional advantage or disadvantage, provide an adequate performance metric, other measures, such as control manipulation and the management of kinetic and potential energy, must be added to provide a refined performance measurement algorithm.},
annote = {From Duplicate 1 (Performance Measurement during Simulated Air-to-Air Combat - Kelly, Michael J)

Mainly concerned with development of ACM

* Relative aircraft positions
* tracking error
* time to first kill
* Aggressiveness
* Decisiveness
* Knowledge of weapons and tactics
* Knowledge of BFM
* Hands-on flying skills
* Situation Awareness
* Ability to maintain offensive position
* Ability to win engagements

Brictson et. al. developed event based measures:

* first radar contact
* first visual contact
* first shot
* first kill
* position recordings at weapons firing

Ciavarelli 1980 discusses broadly the philosophy and results of this research
1v1 tactics are on their way out. Need metrics for n vs. n engagements.
Awesome list of metrics with references!

From Duplicate 2 (Performance Measurement during Simulated Air-to-Air Combat - Kelly, Michael J.)

Mainly concerned with development of ACM

* Relative aircraft positions
* tracking error
* time to first kill
* Aggressiveness
* Decisiveness
* Knowledge of weapons and tactics
* Knowledge of BFM
* Hands-on flying skills
* Situation Awareness
* Ability to maintain offensive position
* Ability to win engagements

Brictson et. al. developed event based measures:

* first radar contact
* first visual contact
* first shot
* first kill
* position recordings at weapons firing

Ciavarelli 1980 discusses broadly the philosophy and results of this research
1v1 tactics are on their way out. Need metrics for n vs. n engagements.
Awesome list of metrics with references!
},
author = {Kelly, Michael J.},
doi = {10.1177/001872088803000410},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kelly - 1988 - Performance Measurement during Simulated Air-to-Air Combat.pdf:pdf},
issn = {00187208},
journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
keywords = {Important,NotRead,TALAF},
mendeley-tags = {Important,NotRead,TALAF},
number = {4},
pages = {495--506},
title = {{Performance Measurement during Simulated Air-to-Air Combat}},
volume = {30},
year = {1988}
}
@techreport{Kelly1979a,
abstract = {Due to the complex, dynamic and fast-moving nature of the air combat task, performance assessment during air-to-air combat provides many unique measurement problems. A combined analytical and empirical technical approach was used to develop a candidate measurement structure and algorithm for the measurement of pilot performance during one-versus-one air combat maneuvering. Nearly all of 28 candidate measures were found to discriminate between high and low skilled pilots during free engagements on the Simulator for Air-to-Air Combat. Discriminant analyses provided a measurement algorithm consisting of 13 measures which accounted for 51{\%} of the variance in the performance data and which predicted membership in high or low skill groups with 92{\%} accuracy.},
annote = {From Duplicate 1 (Air Combat Maneuvering Performance Measurement - Kelly, Michael J.; Wooldridge, Lee; Hennessy, Robert T.; Vreuls, Donald; Barnebey, Steve F.; Cotton, John C.; Reed, John C.)

This document outlines several equations for performance. They were able to get 92{\%} accuracy on identifying expert pilots from a group.

From Duplicate 2 (Air Combat Maneuvering Performance Measurement - Kelly, Michael J; Wooldridge, Lee; Hennessy, Robert T; Vreuls, Donald; Barnebey, Steve F; Cotton, John C; Reed, John C)

This document outlines several equations for performance. They were able to get 92{\{}{\%}{\}} accuracy on identifying expert pilots from a group.},
author = {Kelly, Michael J. and Wooldridge, Lee and Hennessy, Robert T. and Vreuls, Donald and Barnebey, Steve F. and Cotton, John C. and Reed, John C.},
booktitle = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
doi = {10.1177/107118137902300183},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kelly et al. - 1979 - Air Combat Maneuvering Performance Measurement(2).pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kelly et al. - 1979 - Air Combat Maneuvering Performance Measurement.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kelly et al. - 1979 - Air Combat Maneuvering Performance Measurement(2).html:html;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kelly et al. - 1979 - Air Combat Maneuvering Performance Measurement.html:html},
institution = {CANYON RESEARCH GROUP INC WESTLAKE VILLAGE CA, CANYON RESEARCH GROUP INC WESTLAKE VILLAGE CA},
issn = {1541-9312,},
keywords = {Important,NotRead,TALAF},
language = {en},
mendeley-tags = {Important,NotRead,TALAF},
month = {Sep},
number = {1},
pages = {324--328},
title = {{Air Combat Maneuvering Performance Measurement}},
volume = {23},
year = {1979}
}
@article{Kuindersma2013,
abstract = {We present new global and local policy search algorithms suitable for problems with policy-dependent cost variance (or risk), a property present in many robot control tasks. These algorithms exploit new techniques in non-parametric heteroscedastic regression to directly model the policy-dependent distribution of cost. For local search, the learned cost model can be used as a critic for performing risk-sensitive gradient descent. Alternatively, decision-theoretic criteria can be applied to globally select policies to balance exploration and exploitation in a principled way, or to perform greedy minimization with respect to various risk-sensitive criteria. This separation of learning and policy selection permits variable risk control, where risk-sensitivity can be flexibly adjusted and appropriate policies can be selected at runtime without relearning. We describe experiments in dynamic stabilization and manipulation with a mobile manipulator that demonstrate learning of flexible, risk-sensitive policies in very few trials.},
author = {Kuindersma, Scott R. and Grupen, Roderic A. and Barto, Andrew G.},
doi = {10.1177/0278364913476124},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuindersma, Grupen, Barto - 2013 - Variable risk control via stochastic optimization.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuindersma, Grupen, Barto - 2013 - Variable risk control via stochastic optimization.html:html},
isbn = {0278364913476},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {BayesOpt,Bayesian optimization,Important,NotRead,TALAF,bayesian optimization,dynamic mobile manipulation,policy search,risk-sensitive,robot learning},
language = {en},
mendeley-tags = {BayesOpt,Bayesian optimization,Important,NotRead,TALAF,dynamic mobile manipulation,policy search,risk-sensitive,robot learning},
month = {Jun},
number = {7},
pages = {806--825},
title = {{Variable risk control via stochastic optimization}},
volume = {32},
year = {2013}
}
@article{Kushner1964,
author = {Kushner, Harold J.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kushner - 1964 - A new method of locating the maximal point of an arbitrary multipeak curve in the presence of noise.pdf:pdf},
journal = {J. Basic Eng.},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
number = {1},
pages = {97--106},
title = {{A new method of locating the maximal point of an arbitrary multipeak curve in the presence of noise}},
volume = {86},
year = {1964}
}
@techreport{L3,
author = {L3},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/L3 - Unknown - WARFIGHTER READINESS SCIENCE Contract No . FA8650-05-D-6502 Task Order 0013 Operational Development and Validation of Co.pdf:pdf},
keywords = {AFRL,TALAF},
mendeley-tags = {AFRL,TALAF},
title = {{WARFIGHTER READINESS SCIENCE Contract No .: FA8650-05-D-6502 Task Order 0013 : Operational Development and Validation of Competency-Based Methods and Tools for Enhancing Human Performance in Air and Space Warfighting Systems Prepared by : AFRL-RH-WP-TR-20}}
}
@inproceedings{Lara-Cabrera2013a,
abstract = {Real-time strategy games offer a wide variety of fundamental AI research challenges. Most of these challenges have applications outside the game domain. This paper provides a review on computational intelligence in real-time strategy games (RTS). It starts with challenges in real-time strategy games, then it reviews different tasks to overcome this challenges. Later, it describes the techniques used to solve this challenges and it makes a relationship between techniques and tasks. Finally, it presents a set of different frameworks used as test-beds for the techniques employed. This paper is intended to be a starting point for future researchers on this topic.},
author = {Lara-Cabrera, Raul and Cotta, Carlos and Fernandez-Leiva, Antonio J.},
booktitle = {Proceedings of the 2013 IEEE Symposium on Foundations of Computational Intelligence, FOCI 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013},
doi = {10.1109/FOCI.2013.6602463},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lara-Cabrera, Cotta, Fernandez-Leiva - 2013 - A review of computational intelligence in RTS games.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lara-Cabrera, Cotta, Fernandez-Leiva - 2013 - A review of computational intelligence in RTS games.html:html},
isbn = {9781467359016},
keywords = {Buildings,Computational intelligence,Evolutionary computation,GameAI,Games,NotRead,Planning,RTS games,TALAF,artificial intelligence,computational intelligence,computer games,game domain,real time strategy games,real-time strategy games,real-time systems,review},
mendeley-tags = {Buildings,Evolutionary computation,GameAI,Games,NotRead,Planning,RTS games,TALAF,artificial intelligence,computational intelligence,computer games,game domain,real time strategy games,real-time strategy games,real-time systems,review},
month = {apr},
pages = {114--121},
title = {{A review of computational intelligence in RTS games}},
year = {2013}
}
@article{Laumanns2002,
author = {Laumanns, Marco and Ocenasek, Jiri},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laumanns, Ocenasek - 2002 - Bayesian Optimization Algorithms for Multi-Objective Optimization.pdf:pdf},
journal = {Parallel Prooblem Solving from Nature},
keywords = {BayesOpt,NotRead,TALAF},
mendeley-tags = {BayesOpt,NotRead,TALAF},
pages = {298--307},
publisher = {Springer},
title = {{Bayesian Optimization Algorithms for Multi-Objective Optimization}},
year = {2002}
}
@article{Liaw2013,
abstract = {Evolving game agents in a first-person shooter game is important to game developers and players. Choosing a proper set of parameters in a multiplayer game is not a straightforward process because consideration must be given to a large number of parameters, and therefore requires effort and thorough knowledge of the game. Thus, numerous artificial intelligence (AI) techniques are applied in the designing of game characters’ behaviors. This study applied a genetic algorithm to evolve a team in the mode of One Flag CTF in Quake III Arena to behave intelligently. The source code of the team AI is modified, and the progress of the game is represented as a finite state machine. A fitness function is used to evaluate the effect of a team's tactics in certain circumstances during the game. The team as a whole evolves intelligently, and consequently, effective strategies are discovered and applied in various situations. The experimental results have demonstrated that the proposed evolution method is capable of evolving a team's behaviors and optimizing the commands in a shooter game. The evolution strategy enhances the original game AI and assists game designers in tuning the parameters more effectively. In addition, this adaptive capability increases the variety of a game and makes gameplay more interesting and challenging.},
author = {Liaw, Chishyan and Wang, Wei-Hua and Tsai, Ching-Tsorng and Ko, Chao-Hui and Hao, Gorden},
doi = {10.1080/08839514.2013.768883},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liaw et al. - 2013 - Evolving a Team in a First-Person Shooter Game By Using a Genetic Algorithm.html:html;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liaw et al. - 2013 - Evolving a Team in a First-Person Shooter Game By Using a Genetic Algorithm.pdf:pdf},
issn = {0883-9514},
journal = {Applied Artificial Intelligence},
keywords = {NotRead,TALAF},
mendeley-tags = {NotRead,TALAF},
month = {mar},
number = {3},
pages = {199--212},
title = {{Evolving a Team in a First-Person Shooter Game By Using a Genetic Algorithm}},
volume = {27},
year = {2013}
}
@article{Lizotte2008,
abstract = {Global optimization of non-convex functions over real vector spaces is a problem of widespread theoretical and practical interest............ Examples: AIBO Gait parameters Stereo Camera Parameters},
address = {Edmonton, Alta., Canada},
author = {Lizotte, Daniel James},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lizotte - 2008 - Practical Bayesian Optimization.pdf:pdf},
isbn = {978-0-494-46365-9},
keywords = {Acquisition/InfillFxns,Bayes,BayesOpt,Important,Kernels,Learning,Optimization,Parameter optimization,TALAF,local search},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,Important,TALAF},
title = {{Practical Bayesian Optimization}},
year = {2008}
}
@article{Ma2015,
author = {Ma, Yifei and Sutherland, Dougal J. and Garnett, Roman and Schneider, Jeff},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ma et al. - 2015 - Active Pointillistic Pattern Search.pdf:pdf},
issn = {15337928},
journal = {Aistats},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {672--680},
title = {{Active Pointillistic Pattern Search}},
year = {2015}
}
@article{Macdonald2009,
abstract = {Sensitivity analysis is a key part of a comprehensive energy simulation study. Monte-Carlo techniques have been successfully applied to many simulation tools. Several sampling techniques have been proposed in the literature; however to date there has been no comparison of their performance for typical building simulation applications. This paper examines the performance of simple random, stratified and Latin Hypercube sampling when applied to a typical building simulation problem. An integrated natural ventilation problem was selected as it has an inexpensive calculation time thus allowing multiple sensitivity analyses to be undertaken, while being realistic as wind and temperature effects are both modeled. The research shows that compared to simple random sampling: LHS and stratified sampling produce results that are not significantly different (at a 5{\%} level) with increased robustness (less variance in the mean prediction). However, it should not be inferred from this that fewer simulation runs are required for LHS and stratified sampling. Given the results presented here and in previous work it would indicate that for practical purposes Monte-Carlo uncertainty analysis in typical building simulation applications should use about 100 runs and simple random sampling. INTRODUCTION The field of sensitivity analysis is becoming},
author = {Macdonald, Iain A.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Macdonald - 2009 - Comparison of sampling techniques on the performance of Monte-Carlo based sensitivity analysis.pdf:pdf},
journal = {BS2009: 11th Conference of International Building Performance Simulation Association, Glasgow, Scotland, July 27-30},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
number = {3},
pages = {992--999},
title = {{Comparison of sampling techniques on the performance of Monte-Carlo based sensitivity analysis}},
year = {2009}
}
@article{MacKay1999,
abstract = {I examine two approximate methods for computational implementation of Bayesian hierarchical models, that is, models that include unknown hyperparameters such as regularization constants and noise levels. In the evidence framework, the model parameters are integrated over, and the resulting evidence is maximized over the hyperparameters. The optimized hyperparameters are used to define a gaussian approximation to the posterior distribution. In the alternative MAP method, the true posterior probability is found by integrating over the hyperparameters. The true posterior is then maximized over the model parameters, and a gaussian approximation is made. The similarities of the two approaches and their relative merits are discussed, and comparisons are made with the ideal hierarchical Bayesian solution. In moderately ill-posed problems, integration over hyperparameters yields a probability distribution with a skew peak, which causes signifi-cant biases to arise in the MAP method. In contrast, the evidence fram...},
annote = {Referred here by Rasmussen Ch 5


I believe page 9 refers to the points that Rasmussen brings up.},
author = {MacKay, David J. C.},
doi = {10.1162/089976699300016331},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/MacKay - 1999 - Comparison of Approximate Methods for Handling Hyperparameters.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
number = {5},
pages = {1035--1068},
pmid = {80990000001},
title = {{Comparison of Approximate Methods for Handling Hyperparameters}},
volume = {11},
year = {1999}
}
@article{MacKay1992,
abstract = {Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed that measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness.},
author = {MacKay, David J. C.},
doi = {10.1162/neco.1992.4.4.590},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/MacKay - 1992 - Information-Based Objective Functions for Active Data Selection.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {Acquisition/InfillFxns,BayesOpt,TALAF},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,TALAF},
number = {4},
pages = {590--604},
title = {{Information-Based Objective Functions for Active Data Selection}},
volume = {4},
year = {1992}
}
@article{Mahendran2012,
abstract = {This paper proposes a new randomized strategy for adaptive MCMC using Bayesian optimization. This approach applies to non-differentiable objective functions and trades off exploration and exploitation to reduce the number of potentially costly objective function evaluations. We demonstrate the strategy in the complex setting of sampling from constrained, discrete and densely connected probabilistic graphical models where, for each variation of the problem, one needs to adjust the parameters of the proposal mechanism automatically to ensure efficient mixing of the Markov chains.},
archivePrefix = {arXiv},
arxivId = {1110.6497},
author = {Mahendran, Nimalan and Wang, Ziyu and Hamze, Firas and {De Freitas}, N.},
eprint = {1110.6497},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahendran et al. - 2012 - Adaptive MCMC with Bayesian optimization.pdf:pdf},
issn = {15337928},
journal = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {152},
title = {{Adaptive MCMC with Bayesian optimization}},
volume = {9},
year = {2012}
}
@inproceedings{Martinez-Cantin2007,
annote = {In high dimensions ({\&}gt; 10) DIRECT may not be the best choice},
author = {Martinez-Cantin, Ruben and Freitas, N De and Doucet, Arnaud and Castellanos, Jos{\'{e}} A.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martinez-Cantin et al. - 2007 - Active policy learning for robot planning and exploration under uncertainty.pdf:pdf},
isbn = {9780262524841},
issn = {2330765X},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {321--328},
title = {{Active policy learning for robot planning and exploration under uncertainty}},
year = {2007}
}
@techreport{McManus2003,
abstract = {A research program investigating the use of artificial intelligence (AI) techniques to aid in the development of a Tactical Decision Generator (TDG) for Within Visual Range (WVR) air combat engagements is discussed. The application of AI programming and problem solving methods in the development and implementation of a concurrent version of the Computerized Logic For Air-to-Air Warfare Simulations (CLAWS) program, a second generation TDG, is presented. Concurrent computing environments and programming approaches are discussed and the design and performance of a prototype concurrent TDG system are presented.},
author = {McManus, John W.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McManus - 2003 - A Parallel Distributed System for Aircraft Tactical Decision Generation.pdf:pdf},
institution = {NASA Langley Technical Report Server},
keywords = {NotRead,TALAF},
mendeley-tags = {NotRead,TALAF},
title = {{A Parallel Distributed System for Aircraft Tactical Decision Generation}},
year = {2003}
}
@techreport{McManus1990,
abstract = {A research program investigating the use of Artificial Intelligence (AI) techniques to aid in the development of a Tactical Decision Generator (TDG) for Within Visual Range (WVR) air combat engagements is discussed. The application of AI programming and problem solving methods in the development and implementation of the Computerized Logic For Air-to-Air Warfare Simulations (CLAWS), a second generation TDG, is presented. The Knowledge-Based Systems used by CLAWS to aid in the tactical decision-making process are outlined in detail, and the results of tests to evaluate the performance of CLAWS versus a baseline TDG developed in FORTRAN to run in real-time in the Langley Differential Maneuvering Simulator (DMS), are presented. To date, these test results have shown significant performance gains with respect to the TDG baseline in one-versus-one air combat engagements, and the AI-based TDG software has proven to be much easier to modify and maintain than the baseline FORTRAN TDG programs. Alternate computing environments and programming approaches, including the use of parallel algorithms and heterogeneous computer networks are discussed, and the design and performance of a prototype concurrent TDG system are presented.},
annote = {From Duplicate 1 (Artificial Intelligence (AI) Based Tactical Guidance For Fighter Aircraft - McManus, John W.; Goodrich, Kenneth H.)

* total time each airplan has its weapons locked on the opponent
* probability of survival using data from first metric
* {\&}quot;Lethal Time{\&}quot; advantage for each engagement (see paper for equation)
* Time on Offense

Some description of scenarios

From Duplicate 2 (Artificial Intelligence (AI) Based Tactical Guidance For Fighter Aircraft - McManus, John W; Goodrich, Kenneth H)

* total time each airplan has its weapons locked on the opponent
* probability of survival using data from first metric
* {\{}{\&}amp;{\}}quot;Lethal Time{\{}{\&}amp;{\}}quot; advantage for each engagement (see paper for equation)
* Time on Offense

Some description of scenarios},
author = {McManus, John W. and Goodrich, Kenneth H.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McManus, Goodrich - 1990 - Artificial Intelligence (AI) Based Tactical Guidance For Fighter Aircraft.pdf:pdf},
institution = {NASA Langley Technical Report Server},
keywords = {Important,TALAF},
mendeley-tags = {Important,TALAF},
title = {{Artificial Intelligence (AI) Based Tactical Guidance For Fighter Aircraft}},
year = {1990}
}
@techreport{Moore1979,
annote = {equation on page 27},
author = {Moore, Samuel B. and Madison, Walker G. and Sepp, George D. and Stracener, Jerrell T. and Coward, Robert E.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moore et al. - 1979 - Air Combat Training Good Stick Index Validation.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moore et al. - 1979 - Air Combat Training Good Stick Index Validation.html:html},
keywords = {*FLIGHT TRAINING,AERIAL WARFARE,Automation,FLIGHT MANEUVERS,FLIGHT SIMULATORS,GSI(GOOD STICK INDEX),Humanities and History,MATHEMATICAL PREDICTION,Measurement,PE62205F,PERFORMANCE TESTS,PERFORMANCE(HUMAN),PERSONNEL SELECTION,PILOTS,PROFICIENCY,SCORING,SKILLS,STATISTICAL ANALYSIS,STUDENTS,Statistics and Probability,TALAF,TEST CONSTRUCTION(PSYCHOLOGY),VALIDATION,WUAFHRL11231216},
language = {en},
mendeley-tags = {*FLIGHT TRAINING,AERIAL WARFARE,Automation,FLIGHT MANEUVERS,FLIGHT SIMULATORS,GSI(GOOD STICK INDEX),Humanities and History,MATHEMATICAL PREDICTION,Measurement,PE62205F,PERFORMANCE TESTS,PERFORMANCE(HUMAN),PERSONNEL SELECTION,PILOTS,PROFICIENCY,SCORING,SKILLS,STATISTICAL ANALYSIS,STUDENTS,Statistics and Probability,TALAF,TEST CONSTRUCTION(PSYCHOLOGY),VALIDATION,WUAFHRL11231216},
month = {Jun},
shorttitle = {Air Combat Training},
title = {{Air Combat Training: Good Stick Index Validation}},
year = {1979}
}
@inproceedings{Mora2012a,
abstract = {This work describes an evolutionary algorithm (EA) for evolving the constants, weights and probabilities of a rule-based decision engine of a bot designed to play the Planet Wars game. The evaluation of the individuals is based on the result of some non-deterministic combats, whose outcome depends on random draws as well as the enemy action, and is thus noisy. This noisy fitness is addressed in the EA and then, its effects are deeply analysed in the experimental section. The conclusions shows that reducing randomness via repeated combats and re-evaluations reduces the effect of the noisy fitness, making then the EA an effective approach for solving the problem.},
author = {Mora, Antonio M. and Fern{\'{a}}ndez-Ares, Antonio and Merelo-Guerv{\'{o}}s, Juan Juli{\'{a}}n and Garc{\'{\i}}a-S{\'{a}}nchez, Pablo},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29178-4{\{}{\_}{\}}24},
editor = {},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mora et al. - 2012 - Dealing with noisy fitness in the design of a RTS game bot.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mora et al. - 2012 - Dealing with noisy fitness in the design of a RTS game bot.html:html},
isbn = {9783642291777},
issn = {03029743},
keywords = {Artificial Intelligence (incl. Robotics),Computation by Abstract Devices,Computer Communication Networks,GameAI,Image Processing and Computer Vision,Math Applications in Computer Science,NotRead,Programming Techniques,TALAF},
language = {en},
mendeley-tags = {Artificial Intelligence (incl. Robotics),Computation by Abstract Devices,Computer Communication Networks,GameAI,Image Processing and Computer Vision,Math Applications in Computer Science,NotRead,Programming Techniques,TALAF},
month = {apr},
pages = {234--244},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Dealing with noisy fitness in the design of a RTS game bot}},
volume = {7248 LNCS},
year = {2012}
}
@inproceedings{Mulgund1998,
abstract = {Describes the development of a software tool for optimizing large-scale air combat tactics using stochastic genetic algorithms. The tool integrates four key components: (1) autonomous blue/red player agents, with their individual aircraft and tactics; (2) an engagement simulator used to play out a tactical scenario; (3) performance metrics reflecting engagement outcome and tactical advantage; and (4) a GA “engine” for performance-based optimization of blue team tactics. The tool's capabilities are demonstrated through the optimization of blue team formation and intercept geometry in a series of tactical engagements. The tactics implementation uses a hierarchical concept that builds large formation tactics from small conventional fighting units, facilitating the design of tactics compatible with existing air combat principles},
annote = {From Duplicate 1 (Air combat tactics optimization using stochastic genetic algorithms - Mulgund, S.; Harper, K.; Krishnakumar, K.; Zacharias, G.)

Mainly concerned with optimizing formations, they classify specific maneuvers as an {\&}quot;optimal control{\&}quot; problem.
They decide that formation tactics shouldn't be changed because those are fundamental to a pilots training.
Minimize J which is made up of:

* Fraction of blue members killed
* Fraction of red team members surviving
* Fraction of blue team members violating separation criteria
* Mean blue team realtive advantage assessment
* Standard deviation in blue team relative advantage
* Mean blue risk assessment

The mention using a Bayes net to quantify the advantage assessment. They say they used F-15 experts to build this net, but there is no reference that I can see.

From Duplicate 2 (Air combat tactics optimization using stochastic genetic algorithms - Mulgund, S; Harper, K; Krishnakumar, K; Zacharias, G)

Mainly concerned with optimizing formations, they classify specific maneuvers as an {\{}{\&}amp;{\}}quot;optimal control{\{}{\&}amp;{\}}quot; problem.
They decide that formation tactics shouldn't be changed because those are fundamental to a pilots training.
Minimize J which is made up of:

* Fraction of blue members killed
* Fraction of red team members surviving
* Fraction of blue team members violating separation criteria
* Mean blue team realtive advantage assessment
* Standard deviation in blue team relative advantage
* Mean blue risk assessment

The mention using a Bayes net to quantify the advantage assessment. They say they used F-15 experts to build this net, but there is no reference that I can see.},
author = {Mulgund, S. and Harper, K. and Krishnakumar, K. and Zacharias, G.},
booktitle = {SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)},
doi = {10.1109/ICSMC.1998.726484},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mulgund et al. - 1998 - Air combat tactics optimization using stochastic genetic algorithms.html:html;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mulgund et al. - 1998 - Air combat tactics optimization using stochastic genetic algorithms.pdf:pdf},
isbn = {0-7803-4778-1},
issn = {1062-922X},
keywords = {Aircraft,Algorithm design and analysis,Genetic algorithms,Important,Measurement,Optimization methods,Rivers,Search methods,Software tools,Stochastic processes,TALAF,USA Councils,air combat tactics optimization,autonomous blue/red player agents,engagement simulator,genetic algorithms,intercept geometry,military computing,performance metrics,stochastic genetic algorithms,tactical engagements,tactical scenario},
mendeley-tags = {Aircraft,Algorithm design and analysis,Important,Measurement,Optimization methods,Rivers,Search methods,Software tools,Stochastic processes,TALAF,USA Councils,air combat tactics optimization,autonomous blue/red player agents,engagement simulator,genetic algorithms,intercept geometry,military computing,performance metrics,stochastic genetic algorithms,tactical engagements,tactical scenario},
month = {Oct},
pages = {3136--3141},
title = {{Air combat tactics optimization using stochastic genetic algorithms}},
volume = {4},
year = {1998}
}
@article{Mulgund2001,
author = {Mulgund, Sandeep and Harper, Karen and Zacharias, Greg},
doi = {10.2514/2.4689},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mulgund, Harper, Zacharias - 2001 - Large-Scale Air Combat Tactics Optimization Using Genetic Algorithms.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mulgund, Harper, Zacharias - 2001 - Large-Scale Air Combat Tactics Optimization Using Genetic Algorithms.html:html;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mulgund, Harper, Zacharias - 2001 - Large-Scale Air Combat Tactics Optimization Using Genetic Algorithms(2).pdf:pdf},
issn = {0731-5090},
journal = {Journal of Guidance, Control, and Dynamics},
keywords = {TALAF},
language = {en},
mendeley-tags = {TALAF},
month = {Jan},
number = {1},
pages = {140--142},
title = {{Large-Scale Air Combat Tactics Optimization Using Genetic Algorithms}},
volume = {24},
year = {2001}
}
@article{Munos2014a,
abstract = {This work covers several aspects of the optimism in the face of uncertainty principle applied to large scale optimization problems under finite numerical budget. The initial motivation for the research reported here originated from the empirical success of the so-called Monte-Carlo Tree Search method popularized in computer-go and further extended to many other games as well as optimization and planning problems. Our objective is to contribute to the development of theoretical foundations of the field by characterizing the complexity of the underlying optimization problems and designing efficient algorithms with performance guarantees. The main idea presented here is that it is possible to decompose a complex decision making problem (such as an optimization problem in a large search space) into a sequence of elementary decisions, where each decision of the sequence is solved using a (stochastic) multi-armed bandit (simple mathematical model for decision making in stochastic environments). This so-called hierarchical bandit approach (where the reward observed by a bandit in the hierarchy is itself the return of another bandit at a deeper level) possesses the nice feature of starting the exploration by a quasi-uniform sampling of the space and then focusing progressively on the most promising area, at different scales, according to the evaluations observed so far, and eventually performing a local search around the global optima of the function. The performance of the method is assessed in terms of the optimality of the returned solution as a function of the number of function evaluations. Our main contribution to the field of function optimization is a class of hierarchical optimistic algorithms designed for general search spaces (such as metric spaces, trees, graphs, Euclidean spaces, ...) with different algorithmic instantiations depending on whether the evaluations are noisy or noiseless and whether some measure of the ''smoothness'' of the function is known or unknown. The performance of the algorithms depend on the local behavior of the function around its global optima expressed in terms of the quantity of near-optimal states measured with some metric. If this local smoothness of the function is known then one can design very efficient optimization algorithms (with convergence rate independent of the space dimension), and when it is not known, we can build adaptive techniques that can, in some cases, perform almost as well as when it is known.},
author = {Munos, R{\'{e}}mi},
doi = {10.1561/2200000038},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Munos - 2014 - From bandits to montecarlo tree search.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Munos - 2014 - From bandits to montecarlo tree search.html:html},
isbn = {9781601987662},
issn = {1935-8237},
keywords = {Bandit theory,GameAI,Important,Monte-Carlo Tree Search,NotRead,Optimism in the face of uncertainty,TALAF,Upper Confidence Bounds},
mendeley-tags = {Bandit theory,GameAI,Important,Monte-Carlo Tree Search,NotRead,Optimism in the face of uncertainty,TALAF,Upper Confidence Bounds},
shorttitle = {From Bandits to Monte-Carlo Tree Search},
title = {{From bandits to montecarlo tree search}},
year = {2014}
}
@article{Murray2010,
abstract = {The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using unknown hyperparameters. Integrating over these hyperparameters considers different possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However, with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes.},
archivePrefix = {arXiv},
arxivId = {1006.0868},
author = {Murray, Iain and Adams, Ryan Prescott},
editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
eprint = {1006.0868},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murray, Adams - 2010 - Slice sampling covariance hyperparameters of latent Gaussian models.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murray, Adams - 2010 - Slice sampling covariance hyperparameters of latent Gaussian models.html:html},
isbn = {9781617823800},
journal = {Advances in Neural Information Processing {\ldots}},
keywords = {Acquisition/InfillFxns,BayesOpt,NotRead,TALAF},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,NotRead,TALAF},
number = {1},
pages = {1732--1740},
publisher = {Curran Associates, Inc.},
title = {{Slice sampling covariance hyperparameters of latent Gaussian models}},
volume = {2},
year = {2010}
}
@article{Neal1993,
abstract = {Probabilistic inference is an attractive approach to uncertain reasoning and empirical learning in artificial intelligence. Computational difficulties arise, however, because probabilistic models with the necessary realism and flexibility lead to complex distributions over high-dimensional spaces. Related problems in other fields have been tackled using Monte Carlo methods based on sampling using Markov chains, providing a rich array of techniques that can be applied to problems in artificial intelligence. The "Metropolis algorithm" has been used to solve difficult problems in statistical physics for over forty years, and, in the last few years, the related method of "Gibbs sampling" has been applied to problems of statistical inference. Concurrently, an alternative method for solving problems in statistical physics by means of dynamical simulation has been developed as well, and has recently been unified with the Metropolis algorithm to produce the "hybrid Monte Carlo" method. In computer science, Markov chain sampling is the basis of the heuristic optimization technique of "simulated annealing", and has recently been used in randomized algorithms for approximate counting of large sets. In this review, I outline the role of probabilistic inference in artificial intelligence, present the theory of Markov chains, and describe various Markov chain Monte Carlo algorithms, along with a number of supporting techniques. I try to present a comprehensive picture of the range of methods that have been developed, including techniques from the varied literature that have not yet seen wide application in artificial intelligence, but which appear relevant. As illustrative examples, I use the problems of probabilistic inference in expert systems, discovery of latent classes from data, and Bayesian learning for neural networks.},
author = {Neal, Radford M.},
doi = {10.1016/j.neuroimage.2009.01.023},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neal - 1993 - Probabilistic Inference Using Markov Chain Monte Carlo Methods Acknowledgements.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$r1053-8119 (Linking)},
issn = {10959572},
journal = {Intelligence},
keywords = {BayesOpt,NotRead,TALAF},
mendeley-tags = {BayesOpt,NotRead,TALAF},
number = {September},
pages = {144},
pmid = {19349224},
title = {{Probabilistic Inference Using Markov Chain Monte Carlo Methods Acknowledgements}},
volume = {45},
year = {1993}
}
@article{Ng2013,
abstract = {This paper introduces a novel methodology for the optimization, analysis and decision support in production systems engineering. The methodology is based on the innovization procedure, originally introduced to unveil new and innovative design principles in engineering design problems. The innovization procedure stretches beyond an optimization task and attempts to discover new design/operational rules/principles relating to decision variables and objectives, so that a deeper understanding of the underlying problem can be obtained. By integrating the concept of innovization with simulation and data mining techniques, a new set of powerful tools can be developed for general systems analysis. The uniqueness of the approach introduced in this paper lies in that decision rules extracted from the multi-objective optimization using data mining are used to modify the original optimization. Hence, faster convergence to the desired solution of the decision-maker can be achieved. In other words, faster convergence and deeper knowledge of the relationships between the key decision variables and objectives can be obtained by interleaving the multi-objective optimization and data mining process. In this paper, such an interleaved approach is illustrated through a set of experiments carried out on a simulation model developed for a real-world production system analysis problem. © 2013 Springer-Verlag.},
address = {Berlin, Heidelberg},
author = {Ng, Amos H C and Dudas, Catarina and Bostr{\"{o}}m, Henrik and Deb, Kalyanmoy},
doi = {10.1007/978-3-642-44973-4},
editor = {Coello, Carlos A. Coello},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng et al. - 2013 - Learning and Intelligent Optimization.pdf:pdf},
isbn = {978-3-642-44972-7},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {BayesOpt,Data mining,Innovization,Multi-objective optimization,Production system simulation,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {1--18},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Learning and Intelligent Optimization}},
volume = {7997},
year = {2013}
}
@inproceedings{Ng1997,
abstract = {Suppose that, for a learning task, we have to select one hypothesis out of a set of hypotheses (that may, for example, have been generated by multiple applications of a randomized learning algorithm). A common approach is to evaluate each hypothesis in the set on some previously unseen cross-validation data, and then to select the hypothesis that had the lowest cross-validation error. But when the cross-validation data is partially corrupted such as by noise, and if the set of hypotheses we are selecting from is large, then "folklore" also warns about "overfitting" the crossvalidation data [Klockars and Sax, 1986, Tukey, 1949, Tukey, 1953]. In this paper, we explain how this "overfitting" really occurs, and show the surprising result that it can be overcome by selecting a hypothesis with a  higher cross-validation error, over others with lower cross-validation errors. We give reasons for not selecting the hypothesis with the lowest cross-validation error, and propose a new algorithm, L...},
annote = {Show that it is possible to do better than picking the hypothesis with the lowest CV error.},
author = {Ng, Andrew Y.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - 1997 - Preventing overfitting of cross-validation data.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - 1997 - Preventing Overfitting of Cross-Validation Data.html:html},
keywords = {BayesOpt,GPs,Important,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,Important,OptimizingHyperparameters,TALAF},
pages = {245--253},
publisher = {Morgan Kaufmann},
title = {{Preventing" overfitting" of cross-validation data}},
year = {1997}
}
@article{Nguyen2013,
abstract = {In this paper, we present an application of Monte Carlo tree search (MCTS) to control ghosts in the game called Ms. Pac-Man. Our proposed ghost team consists of a ghost controlled by rules and three ghosts controlled individually by different MCTS. Given a limited time response, in order to increase the reliability of MCTS results, we introduce a mechanism for predicting Ms. Pac-Man's future movements and use this mechanism for simulating Ms. Pac-Man during Monte Carlo simulations. Our ghost team won the first Ms. Pac-Man Versus Ghost Team Competition at the 2011 IEEE Congress on Evolutionary Computation (CEC). Its performances for a variety of design choices are also shown and discussed.},
author = {Nguyen, Kien Quang and Thawonmas, Ruck},
doi = {10.1109/TCIAIG.2012.2214776},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Thawonmas - 2013 - Monte carlo tree search for collaboration control of ghosts in Ms. Pac-Man.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Thawonmas - 2013 - Monte carlo tree search for collaboration control of ghosts in Ms. Pac-Man.html:html},
isbn = {978-1-4577-0259-4},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {2011 IEEE CEC,2011 IEEE Congress on Evolutionary Computation,Collaboration,Computers,Games,Ghosts,MCTS results reliability,Monte Carlo,Monte Carlo methods,Monte Carlo simulations,Monte Carlo tree search,Monte Carlo tree search (MCTS),Ms. Pac-Man,Ms. Pac-Man Versus Ghost Team Competition,Pac-Man,Prediction algorithms,Reliability,TALAF,Time factors,computer games,ghost collaboration control,groupware,movement prediction,tree searching,video game},
mendeley-tags = {2011 IEEE CEC,2011 IEEE Congress on Evolutionary Computation,Collaboration,Computers,Games,Ghosts,MCTS results reliability,Monte Carlo,Monte Carlo methods,Monte Carlo simulations,Monte Carlo tree search,Monte Carlo tree search (MCTS),Ms. Pac-Man,Ms. Pac-Man Versus Ghost Team Competition,Pac-Man,Prediction algorithms,Reliability,TALAF,Time factors,computer games,ghost collaboration control,groupware,movement prediction,tree searching,video game},
month = {mar},
number = {1},
pages = {57--68},
title = {{Monte carlo tree search for collaboration control of ghosts in Ms. Pac-Man}},
volume = {5},
year = {2013}
}
@article{OCallaghan2012,
author = {O'Callaghan, S. T. and Ramos, F. T.},
doi = {10.1177/0278364911421039},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Callaghan, Ramos - 2012 - Gaussian process occupancy maps.pdf:pdf},
issn = {0278-3649, 1741-3176},
journal = {The International Journal of Robotics Research},
keywords = {BayesOpt,GPs,TALAF},
language = {en},
mendeley-tags = {BayesOpt,GPs,TALAF},
month = {Jan},
number = {1},
pages = {42--62},
title = {{Gaussian process occupancy maps}},
volume = {31},
year = {2012}
}
@inproceedings{Othman2012a,
abstract = {The development of competent AI for real-time strategy games such as StarCraft is made difficult by the myriad of strategic and tactical reasonings which must be performed concurrently. A significant portion of StarCraft gameplay is in managing tactical conflict with opposing forces. We present a modular framework for simulating AI vs. AI conflicts through an XML specification, whereby the behavioural and tactical components for each force can be varied. Evolutionary computation can be employed on aspects of the scenario to yield superior solutions. Through evolution, a StarCraft AI tournament bot achieved a success rate of 68{\%} against its original version. We also demonstrate the use of evolutionary computation to yield a tactical attack path to maximise enemy casualties. We believe that our framework can be used to perform automatic refinement on AI bots in StarCraft.},
author = {Othman, Nasri and Decraene, James and Cai, Wentong and Hu, Nan and Low, Malcolm Yoke Hean and Gouaillard, Alexandre},
booktitle = {2012 IEEE Conference on Computational Intelligence and Games, CIG 2012},
doi = {10.1109/CIG.2012.6374182},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Othman et al. - 2012 - Simulation-based optimization of StarCraft tactical AI through evolutionary computation.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Othman et al. - 2012 - Simulation-based optimization of StarCraft tactical AI through evolutionary computation.html:html},
isbn = {9781467311922},
keywords = {AI bots,Computational modeling,Evolutionary computation,Force,GameAI,Games,NotRead,StarCraft gameplay,StarCraft tactical AI,TALAF,XML,XML specification,artificial intelligence,computer games,inference mechanisms,real-time strategy games,real-time systems,simulation-based optimization,software agents,strategic reasoning,tactical conflict management,tactical reasoning},
mendeley-tags = {AI bots,Computational modeling,Evolutionary computation,Force,GameAI,Games,NotRead,StarCraft gameplay,StarCraft tactical AI,TALAF,XML,XML specification,artificial intelligence,computer games,inference mechanisms,real-time strategy games,real-time systems,simulation-based optimization,software agents,strategic reasoning,tactical conflict management,tactical reasoning},
month = {Sep},
pages = {394--401},
title = {{Simulation-based optimization of StarCraft tactical AI through evolutionary computation}},
year = {2012}
}
@article{Paper2015a,
author = {Paper, Conference},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paper - 2015 - Opponent Modeling in Real-Time Strategy.pdf:pdf},
keywords = {GameAI,NotRead,TALAF},
mendeley-tags = {GameAI,NotRead,TALAF},
number = {September},
pages = {61--70},
title = {{Opponent Modeling in Real-Time Strategy}},
year = {2015}
}
@article{Paranjape2006,
author = {Paranjape, Aditya A. and Ananthkrishnan, N.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paranjape, Ananthkrishnan - 2006 - Combat aircraft agility metrics-a review.pdf:pdf},
journal = {Journal of Aerospace Sciences and Technologies},
keywords = {TALAF},
mendeley-tags = {TALAF},
number = {2},
pages = {143--154},
title = {{Combat aircraft agility metrics-a review}},
volume = {58},
year = {2006}
}
@article{Picheny2013,
abstract = {Responses of many real-world problems can only be evaluated perturbed by noise. In order to make an efficient optimization of these problems possible, intelligent optimization strategies successfully coping with noisy evaluations are required. In this article, a comprehensive review of existing kriging-based methods for the optimization of noisy functions is provided. In summary, ten methods for choosing the sequential samples are described using a unified formalism. They are compared on analytical benchmark problems, whereby the usual assumption of homoscedastic Gaussian noise made in the underlying models is meet. Different problem configurations (noise level, maximum number of observations, initial number of observations) and setups (covariance functions, budget, initial sample size) are considered. It is found that the choices of the initial sample size and the covariance function are not critical. The choice of the method, however, can result in significant differences in the performance. In particular, the three most intuitive criteria are found as poor alternatives. Although no criterion is found consistently more efficient than the others, two specialized methods appear more robust on average.},
author = {Picheny, Victor and Wagner, Tobias and Ginsbourger, David},
doi = {10.1007/s00158-013-0919-4},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Picheny, Wagner, Ginsbourger - 2013 - A benchmark of kriging-based infill criteria for noisy optimization.html:html;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Picheny, Wagner, Ginsbourger - 2013 - A benchmark of kriging-based infill criteria for noisy optimization.pdf:pdf},
isbn = {1615-147X$\backslash$r1615-1488},
issn = {1615147X},
journal = {Structural and Multidisciplinary Optimization},
keywords = {Acquisition/InfillFxns,BayesOpt,Computational Mathematics and Numerical Analysis,EGO,Engineering Design,Metamodeling,Noise,TALAF,Theoretical and Applied Mechanics},
language = {en},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,Computational Mathematics and Numerical Analysis,EGO,Engineering Design,Metamodeling,Noise,TALAF,Theoretical and Applied Mechanics},
month = {apr},
number = {3},
pages = {607--626},
title = {{A benchmark of kriging-based infill criteria for noisy optimization}},
volume = {48},
year = {2013}
}
@phdthesis{Pietilainen2010,
abstract = {This thesis examines three numerical approximations for the analytically intractable in- tegral over the posterior distribution of the hyperparameters in Gaussian processes. The properties of the approximations are studied, and their performance is compared to each other and to a method using a point-estimate. Traditionally the integral over the posterior of the hyperparameters is computed using Markov chain Monte Carlo (MCMC) -methods. However, MCMC methods suffer from a heavy computational burden of Gaussian processes, because the complexity of Gaussian process models grows with the amount of the data used. An alternative approach has been to use only a point estimate for the hyperparameters instead of integrating over their posterior distribution. This is a computationally attractive approach, but it ignores the uncertainty related to the hyperparameters. The approximations discussed in this thesis attempt to take the uncertainty in the hyper- parameters into consideration better than does a point estimate method, and to be compu- tationally lighter than MCMC methods. The results demonstrate that the integration over the hyperparameters is beneficial in particular conditions. In addition, it is shown that a point estimate method yields equally accurate results with the integration methods in other situations. The amount of the data and the use of the models determine the need for the integration methods and the determining conditions are discussed in this work},
annote = {Pietil{\~{A}}¤inen - 2010 - Approximations for Integration over the Hyperparam.pdf
Contents
* Introduction
Bayesian inference
Bayes{\&}amp;apos; rule
Marginalization
Prediction
Model Comparison
Markov chain Monte Carlo -methods
Gaussian processes
Definition of a Gaussian process
Prediction
Non-Gaussian likelihood
Effect of hyperparameters
Covariance functions
Squared exponential
The Mat{\'{e}}rn class of covariance functions
Normal approximation for the posterior distribution
Integration over the posterior distribution of the hyperparameters
Type II MAP estimate
Approximating the integral over the distribution of the hyperparameters
Grid search
Central composite design
Importance sampling with Student-t proposal distribution
Summary of the approximation methods
Results
Regression with a multimodal hyperparameter posterior distribution
Regression with a unimodal hyperparameter posterior distribution
Regression with Poisson observation model
Regression with precipitation data
Conclusion and future work},
author = {Pietil{\"{a}}inen, Ville},
keywords = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF},
pages = {47},
title = {{Approximations for Integration over the Hyperparameters in Gaussian Processes}},
year = {2010}
}
@inproceedings{Ponweiser2008,
abstract = {Surrogate model-based optimization is a well-known technique for optimizing expensive black-box functions. By applying this function approximation, the number of real problem evaluations can be reduced because the optimization is performed on the model. In this case two contradictory targets have to be achieved: increasing global model accuracy and exploiting potentially optimal areas. The key to these targets is the criterion for selecting the next point, which is then evaluated on the expensive black-box function - the dasiainfill sampling criterionpsila. Therefore, a novel approach - the dasiaClustered Multiple Generalized Expected Improvementpsila (CMGEI) - is introduced and motivated by an empirical study. Furthermore, experiments benchmarking its performance compared to the state of the art are presented.},
author = {Ponweiser, Wolfgang and Wagner, Tobias and Vincze, Markus},
booktitle = {2008 IEEE Congress on Evolutionary Computation, CEC 2008},
doi = {10.1109/CEC.2008.4631273},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ponweiser, Wagner, Vincze - 2008 - Clustered Multiple Generalized Expected Improvement A novel infill sampling criterion for surrogate m.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ponweiser, Wagner, Vincze - 2008 - Clustered Multiple Generalized Expected Improvement A novel infill sampling criterion for surrogate.html:html},
isbn = {9781424418237},
issn = {978-1-4244-1822-0},
keywords = {Acquisition/InfillFxns,BayesOpt,Fellows,Mathematical model,Neural networks,Optimization methods,Performance analysis,Performance evaluation,Robustness,Sampling methods,TALAF,Testing,clustered multiple generalized expected improvemen,expensive black-box functions,function approximation,infill sampling criterion,optimisation,surrogate model-based optimization},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,Fellows,Mathematical model,Neural networks,Optimization methods,Performance analysis,Performance evaluation,Robustness,Sampling methods,TALAF,Testing,clustered multiple generalized expected improvemen,expensive black-box functions,function approximation,infill sampling criterion,optimisation,surrogate model-based optimization},
month = {Jun},
pages = {3515--3522},
shorttitle = {Clustered multiple generalized expected improvemen},
title = {{Clustered Multiple Generalized Expected Improvement: A novel infill sampling criterion for surrogate models}},
year = {2008}
}
@article{Prechelt1998,
abstract = {Cross validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting ('early stopping'). The exact criterion used for cross validation based early stopping, however, is chosen in an ad- hoc fashion by most researchers or training is stopped interactively. To aid a more well-founded selection of the stopping criterion, 14 different automatic stopping criteria from three classes were evaluated empirically for their efficiency and effectiveness in 12 different classification and approximation tasks using multi-layer perceptrons with RPROP training. The experiments show that, on average, slower stopping criteria allow for small improvements in generalization (in the order of 4{\%}), but cost about a factor of 4 longer in training time.},
annote = {Study of 12 different {\&}quot;early-stopping{\&}quot; methods},
author = {Prechelt, Lutz},
doi = {10.1016/S0893-6080(98)00010-0},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Prechelt - 1998 - Automatic early stopping using cross validation Quantifying the criteria.pdf:pdf},
isbn = {1879-2782 (Electronic)$\backslash$n0893-6080 (Linking)},
issn = {08936080},
journal = {Neural Networks},
keywords = {BayesOpt,Cross validation,Early stopping,Empirical study,GPs,Generalization,OptimizingHyperparameters,Overfitting,Supervised learning,TALAF},
mendeley-tags = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
number = {4},
pages = {761--767},
pmid = {12662814},
shorttitle = {Automatic early stopping using cross validation},
title = {{Automatic early stopping using cross validation: Quantifying the criteria}},
volume = {11},
year = {1998}
}
@article{Qi2004,
abstract = {In many real-world classification problems the input contains a large $\backslash$r$\backslash$nnumber of potentially irrelevant features. This paper proposes a new Bayesian framework for determining the relevance of input features. This $\backslash$r$\backslash$napproach extends one of the$\backslash$r$\backslash$nmost successful Bayesian methods for feature selection and sparse $\backslash$r$\backslash$nlearning, known as Automatic Relevance Determination (ARD). ARD finds $\backslash$r$\backslash$nthe relevance of features by optimizing the model marginal likelihood, also$\backslash$r$\backslash$n known as the evidence. We show that this can lead to overfitting. To $\backslash$r$\backslash$naddress this problem, we propose Predictive ARD based on estimating $\backslash$r$\backslash$nthe predictive performance of the classifier. While the actual leave-one-out $\backslash$r$\backslash$npredictive performance is generally very costly to compute, the expectation$\backslash$r$\backslash$n propagation (EP) algorithm proposed by Minka provides an estimate of $\backslash$r$\backslash$nthis predictive performance as a side-effect of its iterations. We exploit this$\backslash$r$\backslash$n in our algorithm to do feature selection, and to select data points in a $\backslash$r$\backslash$nsparse Bayesian kernel classifier. Moreover, we provide two other $\backslash$r$\backslash$nimprovements to previous algorithms, by replacing Laplace's $\backslash$r$\backslash$napproximation with the generally more accurate EP, and by incorporating $\backslash$r$\backslash$nthe fast optimization algorithm proposed by Faul and Tipping. Our $\backslash$r$\backslash$nexperiments show that our method based on the EP estimate of predictive $\backslash$r$\backslash$nperformance is more accurate on test data than relevance determination by$\backslash$r$\backslash$noptimizing the evidence.},
address = {New York, NY, USA},
author = {Qi, Yuan (Alan) and Minka, Thomas P. and Picard, Rosalind W. and Ghahramani, Zoubin},
doi = {10.1145/1015330.1015418},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qi et al. - 2004 - Predictive automatic relevance determination by expectation propagation.pdf:pdf},
isbn = {1581138285},
journal = {Twenty-first international conference on Machine learning - ICML '04},
keywords = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF},
pages = {85},
publisher = {ACM},
series = {ICML '04},
title = {{Predictive automatic relevance determination by expectation propagation}},
year = {2004}
}
@article{Qian2008,
abstract = {Modeling experiments with qualitative and quantitative factors is an important issue in computer modeling. We propose a framework for building Gaussian process model,, that incorporate both types of factors. The key to the development of these new models is an approach for constructing correlation functions with qualitative and quantitative factors. An iterative estimation procedure is developed for the proposed models. Modern optimization techniques are used in the estimation to ensure the validity of the constructed correlation functions. The proposed method is illustrated with in example involving a known function and a real example for modeling the thermal distribution of a data center.},
author = {Qian, Peter Z. G. and Wu, Huaiqing and Wu, C. F. Jeff},
doi = {10.1198/004017008000000262},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian, Wu, Wu - 2008 - Gaussian Process Models for Computer Experiments With Qualitative and Quantitative Factors.html:html;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian, Wu, Wu - 2008 - Gaussian Process Models for Computer Experiments With Qualitative and Quantitative Factors.pdf:pdf},
isbn = {0040170615372723},
issn = {0040-1706},
journal = {Technometrics},
keywords = {BayesOpt,TALAF,cokriging,design of experiments,kriging,multivariate gaussian processes,semi-definite programming},
mendeley-tags = {BayesOpt,TALAF},
month = {Aug},
number = {October},
pages = {383--396},
title = {{Gaussian Process Models for Computer Experiments With Qualitative and Quantitative Factors}},
volume = {50},
year = {2008}
}
@article{Rao2006,
abstract = {Cross validation allows models to be tested using the full training set by means of repeated resampling; thus, maximizing the total number of points used for testing and potentially, helping to protect against overfitting. Improvements in computational power, recent reductions in the (computational) cost of classification algorithms, and the development of closed-form solutions (for performing cross validation in certain classes of learning algorithms) makes it possible to test thousand or millions of variants of learning models on the data. Thus, it is now possible to calculate cross validation performance on a much larger number of tuned models than would have been possible otherwise. However, we empirically show how under such large number of models the risk for overfitting increases and the performance estimated by cross validation is no longer an effective estimate of generalization; hence, this paper provides an empirical reminder of the dangers of cross validation. We use a closed-form solution that makes this evaluation possible for the cross validation problem of interest. In addition, through extensive experiments we expose and discuss the effects of the overuse/misuse of cross validation in various aspects, including model selection, feature selection, and data dimensionality. This is illustrated on synthetic, benchmark, and real-world data sets.},
annote = {* Whenever possible use a sequestered test set that is only used then the classifier has been {\&}quot;frozen{\&}quot;

No great answers here, only shows the issue.},
author = {Rao, R. Bharat and Fung, Glenn},
doi = {10.1137/1.9781611972788.54},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rao, Fung - 2006 - On the Dangers of Cross-Validation An Experimental Evaluation.pdf:pdf},
isbn = {9781605603179},
journal = {Solutions},
keywords = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
pages = {588--596},
publisher = {SIAM},
title = {{On the Dangers of Cross-Validation An Experimental Evaluation}},
year = {2006}
}
@article{Rasmussen2006a,
author = {Rasmussen, Carl Edward},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmussen - 2006 - Advances in Gaussian processes.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
keywords = {BayesOpt,GPs,TALAF},
mendeley-tags = {BayesOpt,GPs,TALAF},
title = {{Advances in Gaussian processes}},
volume = {19},
year = {2006}
}
@book{Rasmussen2004,
author = {Rasmussen, Carl Edward},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmussen - 2004 - Gaussian processes in machine learning.pdf:pdf},
keywords = {BayesOpt,GPs,TALAF},
mendeley-tags = {BayesOpt,GPs,TALAF},
pages = {63--71},
publisher = {Springer},
title = {{Gaussian processes in machine learning}},
year = {2004}
}
@misc{Rasmussen2006b,
abstract = {The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are specified by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classification. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace's method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks.},
archivePrefix = {arXiv},
arxivId = {026218253X},
author = {Rasmussen, Carl Edward and Williams, Christopher K I and ebrary Inc.},
booktitle = {Adaptive computation and machine learning},
doi = {10.1142/S0129065704001899},
eprint = {026218253X},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmussen, Williams, ebrary Inc. - 2006 - Gaussian processes for machine learning.pdf:pdf},
isbn = {026218253X},
issn = {0129-0657},
keywords = {BayesOpt,Gaussian processes Data processing.,Important,Machine learning Mathematical models.,TALAF},
mendeley-tags = {BayesOpt,Important,TALAF},
month = {Dec},
pages = {1 online resource xviii, 248 p.},
pmid = {15112367},
title = {{GPML Toolbox}},
volume = {11},
year = {2006}
}
@book{Rasmussen2006,
address = {Cambridge, Mass},
archivePrefix = {arXiv},
arxivId = {026218253X},
author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
doi = {10.1142/S0129065704001899},
eprint = {026218253X},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmussen, Williams - 2006 - Gaussian processes for machine learning.pdf:pdf},
isbn = {026218253X},
issn = {0129-0657},
keywords = {BayesOpt,Data processing,Folder - NIPS2015,GPs,Gaussian processes,Important,Machine learning,Mathematical models,TALAF,Variatinal Inference},
mendeley-tags = {BayesOpt,Data processing,Folder - NIPS2015,GPs,Gaussian processes,Important,Machine learning,Mathematical models,TALAF,Variatinal Inference},
pages = {248},
pmid = {15112367},
publisher = {MIT Press},
series = {Adaptive computation and machine learning},
title = {{Gaussian processes for machine learning}},
year = {2006}
}
@article{Reunanen2003,
abstract = {This paper addresses a common methodological flaw in the comparison of variable selection meth-ods. A practical approach to guide the search or the selection process is to compute cross-validation performance estimates of the different variable subsets. Used with computationally intensive search algorithms, these estimates may overfit and yield biased predictions. Therefore, they cannot be used reliably to compare two selection methods, as is shown by the empirical results of this paper. In-stead, like in other instances of the model selection problem, independent test sets should be used for determining the final performance. The claims made in the literature about the superiority of more exhaustive search algorithms over simpler ones are also revisited, and some of them infirmed.},
author = {Reunanen, Juha},
doi = {10.1162/153244303322753715},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reunanen - 2003 - Overfitting in Making Comparisons Between Variable Selection Methods.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Algorithm comparison,BayesOpt,Cross-validation,GPs,NotRead,OptimizingHyperparameters,Overfitting,TALAF,Variable selection,k nearest neighbors},
mendeley-tags = {BayesOpt,GPs,NotRead,OptimizingHyperparameters,TALAF},
month = {mar},
pages = {1371--1382},
title = {{Overfitting in Making Comparisons Between Variable Selection Methods}},
volume = {3},
year = {2003}
}
@article{Rodriguez2010,
abstract = {In the machine learning field, the performance of a classifier is usually measured in terms of prediction error. In most real-world problems, the error cannot be exactly calculated and it must be estimated. Therefore, it is important to choose an appropriate estimator of the error. This paper analyzes the statistical properties, bias and variance, of the kappa-fold cross-validation classification error estimator (kappa-cv). Our main contribution is a novel theoretical decomposition of the variance of the kappa-cv considering its sources of variance: sensitivity to changes in the training set and sensitivity to changes in the folds. The paper also compares the bias and variance of the estimator for different values of kappa. The experimental study has been performed in artificial domains because they allow the exact computation of the implied quantities and we can rigorously specify the conditions of experimentation. The experimentation has been performed for two classifiers (naive Bayes and nearest neighbor), different numbers of folds, sample sizes, and training sets coming from assorted probability distributions. We conclude by including some practical recommendation on the use of kappa-fold cross validation.},
author = {Rodr{\'{\i}}guez, Juan Diego and P{\'{e}}rez, Aritz and Lozano, Jose Antonio},
doi = {10.1109/TPAMI.2009.187},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodr{\'{\i}}guez, P{\'{e}}rez, Lozano - 2010 - Sensitivity analysis of kappa-fold cross validation in prediction error estimation.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodr{\'{\i}}guez, P{\'{e}}rez, Lozano - 2010 - Sensitivity analysis of kappa-fold cross validation in prediction error estimation.html:html},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Bayes methods,BayesOpt,GPs,Machine learning,NotRead,OptimizingHyperparameters,STATISTICAL ANALYSIS,TALAF,bias and variance,classification error estimator,decomposition of the variance,error estimation,estimation theory,k-fold cross validation,k-fold cross-validation,learning (artificial intelligence),naive Bayes method,nearest neighbor algorithm,pattern classification,prediction error,prediction error estimation,probability,probability distribution,sensitivity analysis,sources of sensitivity,statistical properties,supervised classification.},
mendeley-tags = {Bayes methods,BayesOpt,GPs,Machine learning,NotRead,OptimizingHyperparameters,STATISTICAL ANALYSIS,TALAF,bias and variance,classification error estimator,decomposition of the variance,error estimation,estimation theory,k-fold cross validation,k-fold cross-validation,learning (artificial intelligence),naive Bayes method,nearest neighbor algorithm,pattern classification,prediction error,prediction error estimation,probability,probability distribution,sensitivity analysis,sources of sensitivity,statistical properties,supervised classification.},
month = {mar},
number = {3},
pages = {569--575},
pmid = {20075479},
title = {{Sensitivity analysis of kappa-fold cross validation in prediction error estimation}},
volume = {32},
year = {2010}
}
@article{S??bester2005,
abstract = {Striking the correct balance between global exploration of search spaces and local exploitation of promising basins of attraction is one of the principal concerns in the design of global optimization algorithms. This is true in the case of techniques based on global response surface approximation models as well. After constructing such a model using some initial database of designs it is far from obvious how to select further points to examine so that the appropriate mix of exploration and exploitation is achieved. In this paper we propose a selection criterion based on the expected improvement measure, which allows relatively precise control of the scope of the search. We investigate its behavior through a set of artificial test functions and two structural optimization problems. We also look at another aspect of setting up search heuristics of this type: the choice of the size of the database that the initial approximation is built upon.},
author = {S??bester, Andr??s and Leary, Stephen J. and Keane, Andy J.},
doi = {10.1007/s10898-004-6733-1},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sbester, Leary, Keane - 2005 - On the design of optimization strategies based on global response surface approximation models.html:html},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Acquisition/InfillFxns,BayesOpt,Computer Science- general,Expected improvement,Gaussian kernels,Operations Research/Decision Theory,Optimization,Radial basis functions,Real Functions,TALAF,expected improvement},
language = {en},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,Computer Science- general,Gaussian kernels,Operations Research/Decision Theory,Optimization,Radial basis functions,Real Functions,TALAF,expected improvement},
month = {Sep},
number = {1},
pages = {31--59},
title = {{On the design of optimization strategies based on global response surface approximation models}},
volume = {33},
year = {2005}
}
@article{Schliep2012,
annote = {Schliep and Hoeting - 2012 - Multilevel latent Gaussian process model for mixed.pdf
Contents
* Erin M. Schliep and Jennifer A. Hoeting
1 Introduction
2 Motivating example
3 Model and inference
3.1 Multivariate mixed response data
3.2 Multilevel latency
3.3 Bayesian framework
3.4 Inference
4 Posterior inference
4.1 Posterior prediction
4.2 Multivariate correlation statistics
4.3 Model evaluation
5 Assessing wetland condition
5.1 Data and model specification
5.2 Model results
6 Discussion
References
7 Appendix
Appendix
7.1 Observed Data
7.2 Squared error loss
7.3 Simulation Study},
author = {Schliep, Erin M. and Hoeting, Jennifer A.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schliep, Hoeting - 2012 - Multilevel latent Gaussian process model for mixed discrete and continuous multivariate response data.pdf:pdf},
journal = {arXiv preprint arXiv:1205.4163},
keywords = {BayesOpt,Important,TALAF},
mendeley-tags = {BayesOpt,Important,TALAF},
title = {{Multilevel latent Gaussian process model for mixed discrete and continuous multivariate response data}},
year = {2012}
}
@article{Schreiber2006,
abstract = {Distributed Mission Operations (DMO) training consists of multiplayer networked environments enabling warfighting training on higher-order individual and team-oriented skills. Surprisingly, only sparse DMO training effectiveness literature can be found and very few studies contain objective data. The dataset used in this research represents the largest DMO effectiveness dataset known to exist today (76 teams/384 pilots on over 3,000 engagements), containing 33 months' worth of multi-faceted DMO data, including objective data from the simulators, multiple participant surveys, subject matter expert (SME) ratings of performance, and knowledge structure tests. Observed performance differences between the pre- and post-test mirror-image point-defense assessment sessions served as the primary basis for the evaluation. Results were dramatic: On the post-test, 58.33{\{}{\%}{\}} fewer enemy strikers reached their target and there were 54.77{\{}{\%}{\}} fewer F-16 mortalities. Furthermore, there were corroborating significant improvements from the numerous measured skill metrics (e.g., weapons employment), SME expert observer ratings, and participant self-report opinion ratings. These converging results provide substantial evidence that pilots become much more proficient on key aspects of combat mission objectives as a function of training within the simulator. Finding highly significant performance differences across multiple datasets between the pre- and post-tests with a combat-ready participant pool in a complex task/environment forms a formidable argument that DMO training yields considerable within-simulator warfighter competency improvement. In this report, we summarize the different dataset classes, overview the primary hypotheses and results associated with each, and discuss the convergence of the datasets to illustrate the 'big picture' DMO training effectiveness. As such, more detailed hypotheses, analyses, and discussions are contained in separate reports (Vols. II through V).},
annote = {From Duplicate 4 (Distributed mission operations within-simulator training effectiveness baseline study. Volume 1. Summary report. - Schreiber, Brian T; Bennett Jr, Winston)

AFRL-HE-AZ-TR-2006-0015{\{}{\_}{\}}Vol{\{}{\_}{\}}I - 1123AS03 - Bennett.pdfOutlines a study on Distrubuted Mission Operations (DMO's) training. This involves training on higher-order individual and team-oriented skills.

While DMO training is fairly common and exists to improve fighter capabilities, there are no studies that investigate its effectiveness.
page 13:

* enemy strikers reaching base
* closest distance achieved by strikers
* f-16 mortalities
* enemy striker and fighter mortalities
* weapons employment metrics (see volume V for more details)
* weapons engagement zone management metrics
* wingman formation metrics
* communication use

Table 2 (pg 20) is a summarized table of metrics from volume V.
2

From Duplicate 5 (Distributed mission operations within-simulator training effectiveness baseline study. Volume 1. Summary report. - Schreiber, Brian T.; Bennett Jr, Winston)

AFRL-HE-AZ-TR-2006-0015{\_}Vol{\_}I - 1123AS03 - Bennett.pdfOutlines a study on Distrubuted Mission Operations (DMO's) training. This involves training on higher-order individual and team-oriented skills.
 
While DMO training is fairly common and exists to improve fighter capabilities, there are no studies that investigate its effectiveness.
page 13:

* enemy strikers reaching base
* closest distance achieved by strikers
* f-16 mortalities
* enemy striker and fighter mortalities
* weapons employment metrics (see volume V for more details)
* weapons engagement zone management metrics
* wingman formation metrics
* communication use

Table 2 (pg 20) is a summarized table of metrics from volume V.
2},
author = {Schreiber, Brian T. and {Bennett Jr.}, Winston and {Bennett Jr}, Winston and {Bennett Jr.}, Winston and {Bennett Jr}, Winston},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schreiber, Bennett Jr - 2007 - Distributed mission operations within-simulator training effectiveness baseline study. Volume 1. Summa(2).pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schreiber, Bennett Jr - 2007 - Distributed mission operations within-simulator training effectiveness baseline study. Volume 1. Summary.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schreiber, Bennett Jr. - 2006 - Distributed Mission Operations Within-Simulator Training Effectiveness Baseline Study Summary Report.pdf:pdf},
institution = {DTIC Document},
issn = {1548-8837, 1548-8837},
journal = {STAR},
keywords = {10: Aerospace Engineering (General) (MT),99: General (AH),AFRL,Aero,Convergence,DMO,Distributed Mission Operations,Employment,Folder - AFRL{\{}{\_}{\}}Wink,Hypotheses,MEC,Mechanical {\&} Transportation Engineering (MT),Mechanical {\{}{\&}{\}} Transportation Engineering (MT),Military aircraft,Military planes,Mission Essential Competencies,Missions,Mortality,Networked environments,NotRead,Observers,Pilots,Pools,Ratings,Simulators,Skill acquisition,Surveys,TALAF,Training,Training effectiveness,Warfighter training,Weapons},
mendeley-tags = {AFRL,NotRead,TALAF},
number = {5},
title = {{Distributed mission operations within-simulator training effectiveness baseline study. Volume 1. Summary report.}},
volume = {45},
year = {2007}
}
@article{Schreiber2007a,
abstract = {The current work reports only the objective data from AFRL-HE-AZ-TR-2006-0015, Volume I, Distributed Mission Operations Within-Simulator Training Effectiveness: Summary Report, but here we expand the reporting of objective data both in depth and breadth.We examined F-16 pilots participating in week-long Distributed Mission Operation (DMO) training exercises and compared extensive computer-collected data between beginning-of-week and end-of-week pilot performance on mirror-image scenarios. The DMO research environment in Mesa, AZ consisted of four high-fidelity F-16 simulators and one high-fidelity Airborne Warning and Control System simulator. Participating F-16 teams flew over 40 total scenarios according to a five-day syllabus, book-ended on Monday and Friday by mirror-image point defense air combat benchmark scenarios. Seven mission outcome measures were found to be significantly better on Friday than Monday: A 58.33{\%} decrease in enemy strikers reaching their target, 38.10{\%} greater distance from the base the F-16s disposed of the strikers, 54.77{\%} fewer F-16 mortalities, 75.26{\%} more enemy striker kills (before reaching base), 6.82{\%} higher proportion of Viper Advanced Medium Range Air-to-Air Missile (AMRAAM) shots resulting in a kill, 51.60{\%} lower proportion of enemy Alamo missile shots resulting in a kill, and a highly impressive 314.21{\%} increase in an overall summary scoring scheme developed by subject matter experts. Significant trends were also found for a number of other metrics assessing skills. Of all the measures investigated in the current work, not a single offensive/defensive trade-off was observed, which significantly strengthens our conclusion that significant within-simulator learning took place.},
author = {Schreiber, Brian T. and Stock, William A. and {Bennett Jr}, Winston},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schreiber, Stock, Bennett Jr - 2007 - Distributed mission operations within-simulator training effectiveness baseline study. Volume 2(2).pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schreiber, Stock, Bennett Jr - 2007 - Distributed mission operations within-simulator training effectiveness baseline study. Volume 2. m.pdf:pdf},
institution = {DTIC Document},
issn = {1548-8837, 1548-8837},
journal = {STAR},
keywords = {10: Aerospace Engineering (General) (MT),99: General (AH),AFRL,Aero,Air combat,Benchmarking,Control systems,Folder - AFRL{\{}{\_}{\}}Wink,Important,Learning,Mechanical {\&} Transportation Engineering (MT),Mechanical {\{}{\&}{\}} Transportation Engineering (MT),Mesas,Military aircraft,Military planes,Missiles,Missions,Mortality,NotRead,Pilot performance,Pilots,Raw materials,Scoring,Shot,Simulators,TALAF,Tradeoffs,Training,Warning},
mendeley-tags = {AFRL,Important,NotRead,TALAF},
number = {5},
title = {{Distributed mission operations within-simulator training effectiveness baseline study. Volume 2. metric development and objectively quantifying the degree of learning.}},
volume = {45},
year = {2007}
}
@article{Serafino2014,
author = {Serafino, Loris},
doi = {10.1090/noti1140},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Serafino - 2014 - Optimizing Without Derivatives What Does the No Free Lunch Theorem Actually Say.pdf:pdf},
issn = {0002-9920, 1088-9477},
journal = {Notices of the Ams},
keywords = {BayesOpt,TALAF},
language = {en},
mendeley-tags = {BayesOpt,TALAF},
month = {Aug},
number = {7},
pages = {750--755},
shorttitle = {Optimizing Without Derivatives},
title = {{Optimizing Without Derivatives : What Does the No Free Lunch Theorem Actually Say?}},
volume = {61},
year = {2014}
}
@article{Shahriari2015a,
abstract = {—Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., rec-ommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable config-uration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
doi = {10.1109/JPROC.2015.2494218},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shahriari et al. - 2015 - Taking the Human Out of the Loop A Review of Bayesian Optimization.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Acquisition/InfillFxns,BayesOpt,Important,NotRead,TALAF},
mendeley-tags = {Acquisition/InfillFxns,BayesOpt,Important,NotRead,TALAF},
shorttitle = {Taking the Human Out of the Loop},
title = {{Taking the Human Out of the Loop: A Review of Bayesian Optimization}},
year = {2015}
}
@article{Shahriari2014,
abstract = {Bayesian optimization is a sample-efficient method for black-box global optimization. How- ever, the performance of a Bayesian optimization method very much depends on its exploration strategy, i.e. the choice of acquisition function, and it is not clear a priori which choice will result in superior performance. While portfolio methods provide an effective, principled way of combining a collection of acquisition functions, they are often based on measures of past performance which can be misleading. To address this issue, we introduce the Entropy Search Portfolio (ESP): a novel approach to portfolio construction which is motivated by information theoretic considerations. We show that ESP outperforms existing portfolio methods on several real and synthetic problems, including geostatistical datasets and simulated control tasks. We not only show that ESP is able to offer performance as good as the best, but unknown, acquisition function, but surprisingly it often gives better performance. Finally, over a wide range of conditions we find that ESP is robust to the inclusion of poor acquisition functions.},
archivePrefix = {arXiv},
arxivId = {1406.4625},
author = {Shahriari, Bobak and Wang, Ziyu and Hoffman, Matthew W. and Bouchard-C{\^{o}}t{\'{e}}, Alexandre and de Freitas, Nando},
eprint = {1406.4625},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shahriari et al. - 2014 - An Entropy Search Portfolio for Bayesian Optimization.pdf:pdf},
journal = {arXiv preprint arXiv:1406.4625},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {10},
title = {{An Entropy Search Portfolio for Bayesian Optimization}},
year = {2014}
}
@book{Shaw1985,
abstract = {This book is one of the best single-source volumes on the complexity of modern aircraft combat maneuvering. It is not light reading, but fighter aviation is deadly serious - high speed, three dimensional chess where the loss of the game is a very ugly death. The book begins with the basics (flight sim players might find it useful to consider his chapters "lesson plans" for practicing) and gradually take the reader into greater depth. Readers may find it useful to re-read some chapters - the text is fairly tight and there is much of value in here that might get overlooked.},
address = {Annapolis, Md},
annote = {Includes index},
author = {Shaw, Robert L.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shaw - 1985 - Fighter Combat Tactics and Maneuvering.pdf:pdf},
isbn = {978-0870210594},
keywords = {Fighter plane combat,TALAF},
mendeley-tags = {Fighter plane combat,TALAF},
pages = {428},
publisher = {Naval Institute Press},
shorttitle = {Fighter combat},
title = {{Fighter Combat: Tactics and Maneuvering}},
year = {1985}
}
@book{Shawe-Taylor2004,
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Shawe-Taylor, John and Cristianini, Nello},
booktitle = {Uma {\'{e}}tica para quantos?},
doi = {10.2277},
eprint = {9809069v1},
isbn = {0521813972},
issn = {0717-6163},
keywords = {BayesOpt,GPs,TALAF},
mendeley-tags = {BayesOpt,GPs,TALAF},
number = {2},
pages = {81--87},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
publisher = {Cambridge university press},
title = {{Kernel Methods for Pattern Analysis}},
volume = {XXXIII},
year = {2012}
}
@book{Shewchuk1994,
abstract = {The Conjugate Gradient Method is the most prominent iterative method for solving sparse systems of linear equations. Unfortunately, many textbook treatments of the topic are written with neither illustrations nor intuition, and their victims can be found to this day babbling senselessly in the corners of dusty libraries. For this reason, a deep, geometric understanding of the method has been reserved for the elite brilliant few who have painstakingly decoded the mumblings of their forebears. Nevertheless, the Conjugate Gradient Method is a composite of simple, elegant ideas that almost anyone can understand. Of course, a reader as intelligent as yourself will learn them almost effortlessly. The idea of quadratic forms is introduced and used to derive the methods of Steepest Descent, Conjugate Directions, and Conjugate Gradients. Eigenvectors are explained and used to examine the convergence of the Jacobi Method, Steepest Descent, and Conjugate Gradients. Other topics include preconditioning and the nonlinear Conjugate Gradient Method. I have taken pains to make this article easy to read. Sixty-six illustrations are provided. Dense prose is avoided. Concepts are explained in several differentways. Most equations are coupled with an intuitive interpretation.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Shewchuk, Jonathan Richard},
booktitle = {Science},
doi = {10.1.1.110.418},
eprint = {1102.0183},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shewchuk - 1994 - An Introduction to the Conjugate Gradient Method Without the Agonizing Pain.pdf:pdf},
isbn = {9781479928934},
issn = {14708728},
keywords = {1,2,5,BayesOpt,TALAF,agonizing pain,conjugate gradient method,convergence analysis,eigen do,eigenvalues,i try,jacobi iterations,preconditioning,thinking with eigenvectors},
mendeley-tags = {BayesOpt,TALAF},
number = {CS-94-125},
pages = {64},
pmid = {17348934},
publisher = {Carnegie-Mellon University. Department of Computer Science},
title = {{An Introduction to the Conjugate Gradient Method Without the Agonizing Pain}},
volume = {49},
year = {1994}
}
@article{Smith2011,
abstract = {“Player modeling” is a loose concept. It can equally apply to everything from a predictive model of player actions resulting from machine learning to a designer's description of a player's expected reactions in response to some piece of game content. This lack of a precise terminology prevents practitioners from quickly finding introductions to applicable modeling methods or determining viable alternatives to their own techniques. We introduce a vocabulary that distinguishes between the major existing player modeling applications and techniques. Four facets together define the kind for a model: the scope of application, the purpose of use, the domain of modeled details, and the source of a model's derivation or motivation. This vocabulary allows the identification of relevant player modeling methods for particular problems and clarifies the roles that a player model can take.},
annote = {From Duplicate 1 (An Inclusive View of Player Modeling - Smith, Adam M; Lewis, Chris; Hullett, Kenneth; Smith, Gillian; Sullivan, Anne)

Introduce {\{}{\&}amp;{\}}quot;facets{\{}{\&}amp;{\}}quot; for player models:
Scope, Purpose, Domain, Source
Give examples of games that use different facets.

From Duplicate 2 (An Inclusive View of Player Modeling - Smith, Adam M; Lewis, Chris; Hullett, Kenneth; Smith, Gillian; Sullivan, Anne)

Introduce {\&}quot;facets{\&}quot; for player models:
Scope, Purpose, Domain, Source
Give examples of games that use different facets.},
author = {Smith, Adam M and Lewis, Chris and Hullett, Kenneth and Smith, Gillian and Sullivan, Anne},
doi = {10.1145/2159365.2159419},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Smith et al. - 2011 - An Inclusive View of Player Modeling.-{\_}2011{\_}-{\_}a:-{\_}2011{\_}-{\_}a},
isbn = {9781450308045},
journal = {FDG '11 Proceedings of the 6th International Conference on Foundations of Digital Games},
keywords = {Important,TALAF,game design,games,player modeling,taxonomy},
mendeley-tags = {Important,TALAF},
pages = {301--303},
title = {{An Inclusive View of Player Modeling}},
year = {2011}
}
@article{Snelson2004,
abstract = {We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to significantly better performance than using a regular GP, or a GP with a fixed transformation.},
author = {Snelson, Edward and Rasmussen, Carl Edward and Ghahramani, Zoubin},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Snelson, Rasmussen, Ghahramani - 2004 - Warped Gaussian processes.pdf:pdf},
isbn = {0-262-20152-6},
issn = {1049-5258},
journal = {Adv Neural Inf Process Syst 16},
keywords = {BayesOpt,Computational,Important,Information-Theoretic Learning with,Learning/Statistics {\&} Optimisation,NotRead,TALAF,Theory {\&} Algorithms},
mendeley-tags = {BayesOpt,Important,NotRead,TALAF},
pages = {337--44},
title = {{Warped Gaussian processes}},
volume = {16},
year = {2004}
}
@inproceedings{Snoek2012,
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a {\{}$\backslash$textquoteleft{\}}{\{}$\backslash$textquoteleft{\}}black art{\{}$\backslash$textquoteright{\}}{\{}$\backslash$textquoteright{\}} requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm{\{}$\backslash$textquoteright{\}}s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
booktitle = {Adv. Neural Inf. Process. Syst. 25},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Snoek, Larochelle, Adams - 2012 - Practical Bayesian Optimization of Machine Learning Algorithms.pdf:pdf},
keywords = {BayesOpt,Important,TALAF,bayesian optimization,deep learning,gaussian process},
mendeley-tags = {BayesOpt,Important,TALAF},
pages = {2951--2959},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms}},
year = {2012}
}
@article{Sutton1999,
abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.},
author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
doi = {10.1016/S0004-3702(99)00052-1},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton, Precup, Singh - 1999 - Between MDPs and semi-MDPs A framework for temporal abstraction in reinforcement learning.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton, Precup, Singh - 1999 - Between MDPs and semi-MDPs A framework for temporal abstraction in reinforcement learning.html:html},
issn = {0004-3702},
journal = {Artificial Intelligence},
keywords = {BayesOpt,Hierarchical planning,Intra-option learning,Macroactions,Macros,Markov decision processes,NotRead,Options,Semi-Markov decision processes,Subgoals,TALAF,Temporal abstraction,reinforcement learning},
mendeley-tags = {BayesOpt,Hierarchical planning,Intra-option learning,Macroactions,Macros,Markov decision processes,NotRead,Options,Semi-Markov decision processes,Subgoals,TALAF,Temporal abstraction,reinforcement learning},
month = {Aug},
number = {1–2},
pages = {181--211},
shorttitle = {Between MDPs and semi-MDPs},
title = {{Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning}},
volume = {112},
year = {1999}
}
@inproceedings{Swersky2013,
author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Swersky, Snoek, Adams - 2013 - Multi-task bayesian optimization.pdf:pdf},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {2004--2012},
title = {{Multi-task bayesian optimization}},
year = {2013}
}
@incollection{Swiler2014,
annote = {How to mix discrete and continuous data using a GP},
author = {Swiler, Laura P. and Hough, Patricia D. and Qian, Peter and Xu, Xu and Storlie, Curtis and Lee, Herbert},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Swiler et al. - 2014 - Surrogate models for mixed discrete-continuous variables.pdf:pdf},
keywords = {BayesOpt,NotRead,TALAF},
mendeley-tags = {BayesOpt,NotRead,TALAF},
pages = {181--202},
publisher = {Springer},
title = {{Surrogate models for mixed discrete-continuous variables}},
year = {2014}
}
@misc{Synnaeve2012a,
abstract = {We describe a generative Bayesian model of tactical attacks in strategy games, which can be used both to predict attacks and to take tactical decisions. This model is designed to easily integrate and merge information from other (probabilistic) estimations and heuristics. In particular, it handles uncertainty in enemy units' positions as well as their probable tech tree. We claim that learning, being it supervised or through reinforcement, adapts to skewed data sources. We evaluated our approach on StarCraft1: the parameters are learned on a new (freely available) dataset of game states, deterministically re-created from replays, and the whole model is evaluated for prediction in realistic conditions. It is also the tactical decision-making component of a competitive StarCraft AI.},
author = {Synnaeve, Gabriel and Bessiere, Pierre},
booktitle = {Computer Games Workshop at ECAI 2012},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Synnaeve, Bessiere - 2012 - A Bayesian Tactician.html:html},
keywords = {Bayesian modeling,GameAI,RTS games,TALAF,game AI,machine learning,tactics},
mendeley-tags = {GameAI,TALAF},
pages = {pp. 114--125},
title = {{A Bayesian Tactician}},
year = {2012}
}
@inproceedings{Tamrat2004,
abstract = {Key historical fighter performance metrics were reviewed, and their shortcomings and strengths were discussed. Flight time histories of a typical air-to-air combat engagement of the X-31 against its F-18 adversary were assessed, and conclusions were drawn. Enough parameters were shown to help the reader make his/her own assessment as well. Given sufficient roll rate capability about flight path, the control of rate of descent was identified as key to X-31 superior close-in-combat performance. It was shown that open-loop turn reversal maneuvers, based on point-mass simulation, could be used to compare potential CIC out come of fighters. Based on this, the Dog-Fight-Metric (DFM) was formulated. DFM is a simple empirical metric that could be used to evaluate/ design CIC fighters. It emphasizes low wing-loading and high drag at moderate to high angles of attack. The use of this metric suggested that equal DFM could predict equal close- in-combat outcome. For example, the X-31 and the F-18 both flying at DFM 25, had corresponding angles of attack of 43 and 35 degrees, respectively. It is concluded that dissimilar fighters could be made to have similar CIC outcome (equal DFM) when flown at different angles of attack. Conversely, two similar fighters could be configured as adversaries via angle of attack limiting.},
author = {Tamrat, B F},
booktitle = {AIAA 2004-5173. AIAA Atmospheric Flight Mechanics Conference and Exhibit},
doi = {doi:10.2514/6.2004-5173},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tamrat - 2004 - The X-31 A Post-Stall Technology ( PST ) Fighter Close-In- Combat Results Assessment , And A Look At New CIC Performanc.pdf:pdf},
isbn = {1563477009},
keywords = {BayesOpt,NotRead,TALAF},
language = {en},
mendeley-tags = {BayesOpt,NotRead,TALAF},
month = {Aug},
number = {August},
pages = {1--21},
publisher = {American Institute of Aeronautics and Astronautics},
shorttitle = {The X-31},
title = {{The X-31 : A Post-Stall Technology ( PST ) Fighter Close-In- Combat Results Assessment , And A Look At New CIC Performance Evaluation Metrics}},
year = {2004}
}
@article{Teh2005,
abstract = {(x n , y n},
author = {Teh, Yw and Seeger, Matthias and Jordan, Michael},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Teh, Seeger, Jordan - 2005 - Semiparametric Latent Factor Models.pdf:pdf;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Teh, Seeger, Jordan - 2005 - Semiparametric Latent Factor Models.html:html},
isbn = {097273581X},
journal = {Proceedings of Tenth International Workshop on Artificial Inte lligence and Statistics},
keywords = {BayesOpt,NotRead,TALAF},
mendeley-tags = {BayesOpt,NotRead,TALAF},
title = {{Semiparametric Latent Factor Models}},
year = {2005}
}
@techreport{Thomas1989,
author = {Thomas, Gary S. and Miller, David C.},
booktitle = {Office},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas, Miller - 1989 - Performance Measurement Development for Air Combat.pdf:pdf},
institution = {DTIC Document},
keywords = {NotRead,TALAF},
mendeley-tags = {NotRead,TALAF},
title = {{Performance Measurement Development for Air Combat}},
year = {1989}
}
@article{Togelius2016a,
author = {Togelius, Julian},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Togelius - 2016 - Why video games are essential for inventing artificial intelligence.html:html},
keywords = {GameAI,TALAF},
mendeley-tags = {GameAI,TALAF},
month = {Jan},
shorttitle = {Togelius},
title = {{Why video games are essential for inventing artificial intelligence}},
year = {2016}
}
@article{Vallim2013,
abstract = {Player Modelling has been receiving much attention from the game community in the recent years. The ability to build accurate models of player behavior can be useful in many aspects of a game. One important aspect is the tracking of a player's behavior along time, informing every time a change is perceived. This way, the game Artificial Intelligence can adapt itself to better respond to this new behavior. In order to build models of player behavior, researchers frequently resort to Machine Learning techniques. Such methods work on previously recorded game metrics representing player's interactions with the game environment. However, if the player changes styles over time, the constructed models get out of date. In order to address this drawback, this work proposes the use of and incremental learning technique to track a player's behavior during his/her interaction with the game environment. Our approach attempts to automatically detect the moments in time when the player changes behavior. We apply a change detection technique from the area of Data Stream Mining that is based on incremental clustering and novelty detection. We also propose three modifications to the original technique, in order to formalize change detection, improve detection rate and reduce detection delay. Simulations were performed considering data produced by the Unreal Tournament game, showing the applicability of the method to online tracking of a player's behavior and informing whenever behavior changes occur. ?? 2013 Elsevier B.V. All rights reserved.},
annote = {From Duplicate 1 (Online behavior change detection in computer games - Vallim, Rosane M M; Andrade Filho, Jos{\'{e}} A; De Mello, Rodrigo F; De Carvalho, Andr?? C P L F)

http://www.sciencedirect.com/science/article/pii/S0957417413003576/pdfft?md5=625a875e5c8363832fff822c41f9341d{\{}{\&}amp;{\}}amp;pid=1-s2.0-S0957417413003576-main.pdf

Contents

* 1 Introduction

2 Player modeling

3 Micro-clustering DBScan
3.1 Clustering of incoming data

3.2 Novelty detection

3.3 Using entropy to detect novelties and behavior changes


4 Detecting changing player behavior
4.1 Applying M-DBScan

4.2 New restriction on novelty detection

4.3 Detecting a change

4.4 Reducing change detection delay


5 Experiments
5.1 Game environment and data collection

5.2 Experimental setup

5.3 Results

5.4 Discussion


6 Conclusions

Acknowledgment

References

----------
Vallim et al{\{}{\_}{\}}2013{\{}{\_}{\}}Online behavior change detection in computer games.pdf

Contents

* 1 Introduction

2 Player modeling

3 Micro-clustering DBScan
3.1 Clustering of incoming data

3.2 Novelty detection

3.3 Using entropy to detect novelties and behavior changes


4 Detecting changing player behavior
4.1 Applying M-DBScan

4.2 New restriction on novelty detection

4.3 Detecting a change

4.4 Reducing change detection delay


5 Experiments
5.1 Game environment and data collection

5.2 Experimental setup

5.3 Results

5.4 Discussion


6 Conclusions

Acknowledgment

References

From Duplicate 2 (Online behavior change detection in computer games - Vallim, Rosane M. M.; Andrade Filho, Jos{\'{e}} A.; De Mello, Rodrigo F.; De Carvalho, Andr?? C P L F)

http://www.sciencedirect.com/science/article/pii/S0957417413003576/pdfft?md5=625a875e5c8363832fff822c41f9341d{\&}amp;pid=1-s2.0-S0957417413003576-main.pdf

Contents

* 1 Introduction

2 Player modeling

3 Micro-clustering DBScan
3.1 Clustering of incoming data

3.2 Novelty detection

3.3 Using entropy to detect novelties and behavior changes


4 Detecting changing player behavior
4.1 Applying M-DBScan

4.2 New restriction on novelty detection

4.3 Detecting a change

4.4 Reducing change detection delay


5 Experiments
5.1 Game environment and data collection

5.2 Experimental setup

5.3 Results

5.4 Discussion


6 Conclusions

Acknowledgment

References

----------
Vallim et al{\_}2013{\_}Online behavior change detection in computer games.pdf

Contents

* 1 Introduction

2 Player modeling

3 Micro-clustering DBScan
3.1 Clustering of incoming data

3.2 Novelty detection

3.3 Using entropy to detect novelties and behavior changes


4 Detecting changing player behavior
4.1 Applying M-DBScan

4.2 New restriction on novelty detection

4.3 Detecting a change

4.4 Reducing change detection delay


5 Experiments
5.1 Game environment and data collection

5.2 Experimental setup

5.3 Results

5.4 Discussion


6 Conclusions

Acknowledgment

References},
author = {Vallim, Rosane M. M. and {Andrade Filho}, Jos{\'{e}} A. and {De Mello}, Rodrigo F. and {De Carvalho}, Andr?? C P L F},
doi = {10.1016/j.eswa.2013.05.059},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vallim et al. - 2013 - Online behavior change detection in computer games.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Behavior change detection,Data,Data Stream Mining,Mining,NotRead,Online player modeling,Stream,TALAF},
mendeley-tags = {Behavior change detection,Data Stream Mining,NotRead,Online player modeling,TALAF},
month = {nov},
number = {16},
pages = {6258--6265},
title = {{Online behavior change detection in computer games}},
volume = {40},
year = {2013}
}
@article{Wang2012b,
abstract = {Inference of human intention may be an essential step towards understanding human actions [21] and is hence$\backslash$nimportant for realizing efficient human-robot interaction. In this paper, we propose the Intention-Driven Dynamics Model (IDDM), a latent variable model for inferring unknown human intentions. We train the model based on observed human behaviors/actions and we introduce an approximate inference algorithm to efficiently infer the human's intention from an ongoing action.$\backslash$nWe verify the feasibility of the IDDM in two scenarios, i.e., target inference in robot table tennis and action recognition for interactive humanoid robots. In both tasks, the IDDM achieves substantial improvements over state-of-the-art regression and classification.},
address = {Cambridge, Mass},
annote = {From Duplicate 2 (Probabilistic Modeling of Human Movements for Intention Inference. - Wang, Z.; Deisenroth, Mp; Amor, Hb)

Why mixed teams?


Computer agents and humans have very different strengths and weaknesses. Humans are weak and the agents are strong :-)


Working in teams is a great way to get the best of both worlds. But as we all know team work, even among non-mixed teams is not an easy matter.

Some possible examples:


Consider situations in which autonomous agents and humans are working together. Some examples could be:

* TALAF problem

* A mixed human/bot team in a video game

* mixed human/UAV team performing search and rescue

* Tutoring systems (computer system helping a human to learn)

* this could be online (during the learning period) or offline (after some task has been completed)






In situations like these and others we need agents (systems able to act on their own) that are capable of interacting {\&}quot;intelligently{\&}quot; with humans.

Some of my research questions:


* How can autonomous agents learn optimal behaviors for given situations? We have already addressed this

* How can an agent act as a tutor, to improve human performance?

* How can an agent act as a supervisor, to improve human performance?

* Can agents adapt behavior in different scenarios?

* Can agent adapt behavior in different teams?



My Research problem:


Given a computer agent that can fly a combat mission to maximize some success metric:

* can that agent be an online advisor to a human pilot?

* A human advisor has expert knowledge that can be used to help someone who doesn't have as much expertise

* The computer agent must be able to make inferences regarding the enemy behavior






Article Notes:


Objective


Model the intention of a human interacting with a robot

* Propose a model that uses GP's

* method must be deployable in {\&}quot;real-time{\&}quot; application




* Demonstrate the utility of the approach through experiments



Key questions:


* Can GP's be used for this application?

* Does the algorithm run fast enough?

* Does it perform well enough compared to existing methods?



What is Novel?


Other work:


* Inspired by the idea that a human movement is inspired by a motivation -- Baker et al. 2009, Friesen an dRao 2011

* GP's have been used to model human dynamics -- Wang et al. 2008

* Other methods used so far:

* Intention inference

* HMM's -- Pentland and Liu 1999, Vasquez et al. 2008,2009

* Inverse Reinforcement Learning that infers the utility function that governs an agents behavior -- Liao et al. 2007.

* IRL to model goal-drected trajectories -- Ziebart

* GP's used to model gaze following Friesen and Rao 2011




* GPDM and extensions

* GPLVM (GP Latent Variable Model) marginalizes out function that maps from latent to observed space-- Lawrence 2004

* Approximate inference must be used and has been demonstrated by several authors









This paper:


* Assumes that the robot's decision does not influence the intention of the human and considers intention inference and decision making separately.

* In my problem we can assume the agent/human combo don't affect the adversary




* Propose the Intention Driven Dynamic Model (IDDM)

* Finds a latent state representation

* Models the dynamics driven by the intention




* Online algorithm to infer the human's intention

* Verify in two HRI scenarios



Approach:


IDDM


Measurement Model


* W in eqn.1 is for scaling (helps for measurements in different units)

* h is the unknown measurement function

* h is marginalized out during learning and inference.

* Use the same mean and cov fxns for every dimension of the function h




* The kernel for the measurement mapping from the state space to the observation space is chosen depending on the task



Transition Model


* First order Markov transition model (see (b) below



* Use Gaussian tensor-product kernel (dot-product) between observations -- see eqn. 9

* How is intention {\&}quot;distance{\&}quot; quantified? Seems like the distances are real distances in this paper, may be a challenge in my application






Learning the IDDM


* Training data is movements (i.e. one stroke of the opponent) and corresponding intentions

* Assume intention is available for training

* Equation 13 -- expression for the probability of Z given X(latent states)

* W are the scaling parameters

* Z are the observations

* alpha are hyperparameters for the intention/state GP

* beta are the hyperparameters for the measurement function GP

* g is the intention




* Equation 14 -- Sequence of latent states given g

* Equation 15 -- find the MAP estimates of the states

* I think the discussion about maximum likelihood overfitting is a little bit concerning. I don't like their solution much.

* Now use the model to infer the unobserved intention of a new movement...



Approximate intention inference


* p(g|z) -- marginalize out x

* Instead of EM they extend the inference by explicitly taking into account uncertainty about intention

* I don't Asquite understand Equation 20 yet...

* Bunches of math and we get a Gaussian approximation to the discrete/discretized intentions g

* Complexity:

* T - number of observations obtained

* K - number of discretized intentions

* N - number of training data

* Dx,Dz - dimensionality of state and observation




* See table 1 for the approximations



 

Results


Up to now no group has ever reach levels of a young child. Likely due to:

* human ability to predict hitting targets

* robustness of human hitting movements



This article focuses on the first issue. In the table tennis case the intention is only used to choose a hitting type.

Best results used a sliding window size of two.

 

* Used cross-validation to select Dx (dimension of the latent states), ended up being a linear kernel with 4d latent space},
author = {Wang, Zhikun and Deisenroth, Mp and Amor, Hb},
doi = {10.1177/0278364913478447},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Deisenroth, Amor - 2012 - Probabilistic Modeling of Human Movements for Intention Inference.pdf:pdf},
isbn = {978-0-262-19475-4},
issn = {00051098},
journal = {Robotics: Science and Systems (RSS)},
keywords = {Approximate inference,Folder - Spring2016,Gaussian process,Important,Kernel functions,Support vector machines,TALAF,intention inference},
language = {en},
mendeley-tags = {Approximate inference,Folder - Spring2016,Gaussian process,Important,Kernel functions,Support vector machines,TALAF,intention inference},
month = {Jun},
number = {2},
pages = {1--8},
publisher = {MIT Press},
series = {Adaptive computation and machine learning},
shorttitle = {Learning with kernels},
title = {{Probabilistic Modeling of Human Movements for Intention Inference.}},
volume = {32},
year = {2012}
}
@article{Wang2014a,
abstract = {Bayesian optimisation has gained great popularity as a tool for optimising the pa-rameters of machine learning algorithms and models. Somewhat ironically, setting up the hyper-parameters of Bayesian optimisation methods is notoriously hard. While reasonable practical solutions have been advanced, they can often fail to find the best optima. Surprisingly, there is little theoretical analysis of this crucial problem in the literature. To address this, we derive a cumulative regret bound for Bayesian optimisation with Gaussian processes and unknown kernel hyper-parameters in the stochastic setting. The bound, which applies to the expected improvement acquisition function and sub-Gaussian observation noise, provides us with guidelines on how to design hyper-parameter estimation methods. A sim-ple simulation demonstrates the importance of following these guidelines.},
annote = {Wang{\_}de Freitas{\_}2014{\_}Theoretical analysis of bayesian optimisation with unknown gaussian process.pdf

Contents

* 1 Introduction

2 Bayesian optimisation
2.1 Bayesian optimisation with Gaussian processes

2.2 An algorithm inspired by the theory


3 Theoretical analysis
3.1 Background: Regret

3.2 Background: Sub-Gaussian noise

3.3 Background: Information gain

3.4 Background: Reproducing kernel Hilbert spaces

3.5 Main result


4 Conclusion

A Proofs
A.1 Concentration

A.2 Supporting lemmas

A.3 Properties of the expected improvement acquisition function

A.4 Proof of main result},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.7758v1},
author = {Wang, Ziyu and Freitas, Nando De},
eprint = {arXiv:1406.7758v1},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Freitas - 2014 - Theoretical Analysis of Bayesian Optimisation with Unknown Gaussian Process Hyper-Parameters.pdf:pdf},
journal = {arXiv preprint arXiv:1406.7758},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {1--16},
title = {{Theoretical Analysis of Bayesian Optimisation with Unknown Gaussian Process Hyper-Parameters}},
year = {2014}
}
@article{Wang2013a,
abstract = {In this paper we address the widely-experienced difficulty in tuning Hamiltonian-based Monte Carlo samplers. We develop an algorithm that allows for the adaptation of Hamiltonian and Riemann manifold Hamiltonian Monte Carlo samplers using Bayesian optimization that allows for infinite adaptation of the parameters of these samplers. We show that the resulting sampling algorithms are ergodic, and that the use of our adaptive algorithms makes it easy to obtain more efficient samplers, in some cases precluding the need for more complex solutions. Hamiltonian-based Monte Carlo samplers are widely known to be an excellent choice of MCMC method, and we aim with this paper to remove a key obstacle towards the more widespread use of these samplers in practice.},
archivePrefix = {arXiv},
arxivId = {1302.6182},
author = {Wang, Ziyu and Mohamed, Shakir and de Freitas, Nando},
eprint = {1302.6182},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Mohamed, de Freitas - 2013 - Adaptive Hamiltonian and Riemann Manifold Monte Carlo Samplers.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
keywords = {BayesOpt,TALAF},
mendeley-tags = {BayesOpt,TALAF},
pages = {10},
title = {{Adaptive Hamiltonian and Riemann Manifold Monte Carlo Samplers}},
volume = {28},
year = {2013}
}
@article{Wang2013,
abstract = {Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high-dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. We present a thorough theoretical analysis of REMBO, including regret bounds that only depend on the problem's intrinsic dimensionality. Empirical results confirm that REMBO can effectively solve problems with billions of dimensions, provided the intrinsic dimensionality is low. They also show that REMBO achieves state-of-the-art performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver.},
annote = {Wang et al{\_}2013{\_}Bayesian optimization in high dimensions via random embeddings.pdf

Contents

* Introduction

Bayesian Optimization

REMBO
Choice of Kernel


Experiments
Bayesian Optimization in a Billion Dimensions

Automatic Configuration of a Mixed Integer Linear Programming Solver


Conclusion},
archivePrefix = {arXiv},
arxivId = {1301.1942},
author = {Wang, Ziyu and Zoghi, Masrour and Hutter, Frank and Matheson, David and Freitas, Nando De},
eprint = {1301.1942},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2013 - Bayesian Optimization in High Dimensions via Random Embeddings.pdf:pdf},
isbn = {978-1-57735-633-2},
issn = {10450823},
journal = {Proceedings of the Twenty-Third international joint conference on Artificial Intelligence.},
keywords = {BayesOpt,Important,TALAF},
mendeley-tags = {BayesOpt,Important,TALAF},
pages = {1778--1784},
publisher = {AAAI Press},
title = {{Bayesian Optimization in High Dimensions via Random Embeddings}},
year = {2013}
}
@article{Weber2011a,
abstract = {A big challenge for creating human-level game AI is building agents capable of operating in imperfect information environments. In real-time strategy games the technological progress of an opponent and locations of enemy units are partially observable. To overcome this limitation, we explore a particle-based approach for estimating the location of enemy units that have been encountered. We represent state estimation as an optimization problem, and automatically learn parameters for the particle model by mining a corpus of expert StarCraft replays. The particle model tracks opponent units and provides conditions for activating tactical behaviors in our StarCraft bot. Our results show that incorporating a learned particle model improves the performance of EISBot by 10{\{}{\%}{\}} over baseline approaches.},
annote = {From Duplicate 1 (A Particle Model for State Estimation in Real-Time Strategy Games. - Weber, Ben; Mateas, Michael; Jhala, Arnav)

http://www.aaai.org/ocs/index.php/AIIDE/AIIDE11/paper/download/4051/4420

Partially observable approach to

----------
Weber et al. - 2011 - A Particle Model for State Estimation in Real-Time.pdf

Partially observable approach to

From Duplicate 2 (A Particle Model for State Estimation in Real-Time Strategy Games. - Weber, Ben; Mateas, Michael; Jhala, Arnav)

http://www.aaai.org/ocs/index.php/AIIDE/AIIDE11/paper/download/4051/4420

Partially observable approach to

----------
Weber et al. - 2011 - A Particle Model for State Estimation in Real-Time.pdf

Partially observable approach to},
author = {Weber, Ben and Mateas, Michael and Jhala, Arnav},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weber, Mateas, Jhala - 2011 - A Particle Model for State Estimation in Real-Time Strategy Games.pdf:pdf},
journal = {Aiide},
keywords = {GameAI,Real-Time Strategy Games,TALAF},
language = {en},
mendeley-tags = {GameAI,TALAF},
month = {Oct},
number = {Isla 2006},
pages = {103--108},
title = {{A Particle Model for State Estimation in Real-Time Strategy Games.}},
year = {2011}
}
@article{Williams1998,
abstract = {We consider the problem of assigning an input vector to one of m classes by predicting P(c|x) for c=1,...,m. For a two-class problem, the probability of class one given x is estimated by $\sigma$(y(x)), where $\sigma$(y)=1/(1+e-y). A Gaussian process prior is placed on y(x), and is combined with the training data to obtain predictions for new x points. We provide a Bayesian treatment, integrating over uncertainty in y and in the parameters that control the Gaussian process prior the necessary integration over y is carried out using Laplace's approximation. The method is generalized to multiclass problems (m>2) using the softmax function. We demonstrate the effectiveness of the method on a number of datasets},
author = {Williams, C.K.I. and Barber, D.},
doi = {10.1109/34.735807},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams, Barber - 1998 - Bayesian classification with Gaussian processes.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Bayes methods,BayesOpt,Bayesian classification,Bayesian methods,Computer Society,Gaussian noise,Gaussian processes,Important,Laplace approximation,Logistics,Markov chain,Markov processes,Monte Carlo method,Monte Carlo methods,NotRead,Process control,TALAF,Training data,Uncertain systems,Uncertainty,input vector,multiclass problems,optimisation,parameter uncertainty,pattern classification,probability,softmax,two-class problem},
mendeley-tags = {Bayes methods,BayesOpt,Bayesian classification,Bayesian methods,Computer Society,Gaussian noise,Gaussian processes,Important,Laplace approximation,Logistics,Markov chain,Markov processes,Monte Carlo method,Monte Carlo methods,NotRead,Process control,TALAF,Training data,Uncertain systems,Uncertainty,input vector,multiclass problems,optimisation,parameter uncertainty,pattern classification,probability,softmax,two-class problem},
month = {Dec},
number = {12},
pages = {1342--1351},
title = {{Bayesian classification with Gaussian processes}},
volume = {20},
year = {1998}
}
@techreport{Wooldridge1982,
author = {Wooldridge, Lee and Kelly, Michael J. and Obermayer, Richard W. and Vreuls, Donald and Nelson, William H.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wooldridge et al. - 1982 - Air Combat Maneuvering Performance Measurement State Space Analysis.pdf:pdf},
institution = {VREULS RESEARCH CORP THOUSAND OAKS CA, VREULS RESEARCH CORP THOUSAND OAKS CA},
keywords = {NotRead,TALAF},
mendeley-tags = {NotRead,TALAF},
month = {Oct},
title = {{Air Combat Maneuvering Performance Measurement State Space Analysis}},
year = {1982}
}
@inproceedings{Wu2005,
abstract = {The Decision-Making (DM) problem is investigated for Cooperative Multiple Target Attack in air combat. It is to search for a proper attack assignment of M friendly fighters, with multiple target attack capability, to N hostile fighters called targets to achieve an optimal missile-target attack effect. Thus, Missile-Target Assignment (MTA) is regarded as the main part of the DM problem and has to be solved firstly. Then, the DM solution is derived from the optimal MTA solution. To the MTA problem, a Heuristic Adaptive Genetic Algorithm (HAGA) is proposed to search for its optimal solution. The HAGA utilizes specific heuristic knowledge to improve the search capability of the Adaptive Genetic Algorithm (AGA). Simulation results show that the HAGA is effective and has much better performance than the AGA.},
annote = {This is looking mainly at Missile targeting...},
author = {Wu, Wen-Hai and Branch, Qingdao},
booktitle = {Proceedings of the 4th International Conference on Machine Learning and Cybernetics},
doi = {10.1109/ICMLC.2005.1526992},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Branch - 2005 - Air Combat Decision-making for Cooperative Multiple Target Attack using Heuristic Adaptive Genetic Algorithm.html:html;:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Branch - 2005 - Air Combat Decision-making for Cooperative Multiple Target Attack using Heuristic Adaptive Genetic Algorithm.pdf:pdf},
isbn = {0780390911},
keywords = {Aerospace engineering,Automation,Decision making,Delta modulation,Educational institutions,Genetic engineering,Heuristic algorithms,Missiles,Multiple target attack,Neural networks,TALAF,cooperative,cooperative air combat,decision-making,genetic algorithm,genetic algorithms,heuristic,multiple target attack},
mendeley-tags = {Aerospace engineering,Automation,Decision making,Delta modulation,Educational institutions,Genetic engineering,Heuristic algorithms,Missiles,Multiple target attack,Neural networks,TALAF,cooperative air combat,decision-making,genetic algorithm,genetic algorithms,heuristic},
month = {Aug},
pages = {473--478},
title = {{Air Combat Decision-making for Cooperative Multiple Target Attack using Heuristic Adaptive Genetic Algorithm}},
volume = {1},
year = {2005}
}
@article{Yannakakis2009a,
abstract = {A methodology for optimizing player satisfaction in games on the "playware" physical interactive platform is demonstrated in this paper. Previously constructed artificial neural network user models, reported in the literature, map individual playing characteristics to reported entertainment preferences for augmented-reality game players. An adaptive mechanism then adjusts controllable game parameters in real time in order to improve the entertainment value of the game for the player. The basic approach presented here applies gradient ascent to the user model to suggest the direction of parameter adjustment that leads toward games of higher entertainment value. A simple rule set exploits the derivative information to adjust specific game parameters to augment the entertainment value. Those adjustments take place frequently during the game with interadjustment intervals that maintain the user model's accuracy. Performance of the adaptation mechanism is evaluated using a game survey experiment. Results indicate the efficacy and robustness of the mechanism in adapting the game according to a user's individual playing features and enhancing the gameplay experience. The limitations and the use of the methodology as an effective adaptive mechanism for entertainment capture and augmentation are discussed.},
annote = {From Duplicate 1 (Real-Time Game Adaptation for Optimizing Player Satisfaction - Yannakakis, G N; Hallam, J)

http://ieeexplore.ieee.org/ielx5/4804728/5200752/05067382.pdf?tp={\{}{\&}amp;{\}}amp;arnumber=5067382{\{}{\&}amp;{\}}amp;isnumber=5200752

uses player preference model to adapt the game to maximize player satisfaction.

----------
Yannakakis and Hallam - 2009 - Real-Time Game Adaptation for Optimizing Player Sa.pdf

uses player preference model to adapt the game to maximize player satisfaction.

From Duplicate 2 (Real-Time Game Adaptation for Optimizing Player Satisfaction - Yannakakis, G.N.; Hallam, J.)

http://ieeexplore.ieee.org/ielx5/4804728/5200752/05067382.pdf?tp={\&}amp;arnumber=5067382{\&}amp;isnumber=5200752

uses player preference model to adapt the game to maximize player satisfaction.

----------
Yannakakis and Hallam - 2009 - Real-Time Game Adaptation for Optimizing Player Sa.pdf

uses player preference model to adapt the game to maximize player satisfaction.},
author = {Yannakakis, G.N. N and Hallam, J.},
doi = {10.1109/TCIAIG.2009.2024533},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yannakakis, Hallam - 2009 - Real-Time Game Adaptation for Optimizing Player Satisfaction.pdf:pdf},
issn = {1943-068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Augmented-reality games,GameAI,TALAF,artificial intelligence,artificial neural network user model,augmented reality,augmented-reality game player,computer games,controllable game parameter,game survey experiment,gradient ascent,neural nets,neuro-evolution,player satisfaction,player satisfaction optimisation,playware physical interactive platform,real-time adaptation,real-time game adaptation,user modeling},
mendeley-tags = {Augmented-reality games,GameAI,TALAF,artificial intelligence,artificial neural network user model,augmented reality,augmented-reality game player,computer games,controllable game parameter,game survey experiment,gradient ascent,neural nets,neuro-evolution,player satisfaction,player satisfaction optimisation,playware physical interactive platform,real-time adaptation,real-time game adaptation,user modeling},
month = {Jun},
number = {2},
pages = {121--133},
title = {{Real-Time Game Adaptation for Optimizing Player Satisfaction}},
volume = {1},
year = {2009}
}
@article{Yannakakis2014a,
abstract = {This paper attempts to give a high-level overview 4of the field of artificial and computational intelligence (AI/CI) in games, with particular reference to how the different core research areas within this field inform and interact with each other, both actually and potentially. We identify ten main research areas within this field: NPC behavior learning, search and planning, player modeling, games as AI benchmarks, procedural content generation, computational narrative, believable agents, AI-assisted game design, general game artificial intelligence and AI in commercial games. We view and analyze the areas from three key perspectives: (1) the dominant AI method(s) used under each area; (2) the relation of each area with respect to the end (human) user; and (3) the placement of each area within a human-computer (player-game) interaction perspective. In addition, for each of these areas we consider how it could inform or interact with each of the other areas; in those cases where we find that meaningful interaction either exists or is possible, we describe the character of that interaction and provide references to published studies, if any. We believe that this paper improves understanding of the current nature of the game AI/CI research field and the interdependences between its core areas by providing a unifying overview. We also believe that the discussion of potential interactions between research areas provides a pointer to many interesting future research projects and unexplored subfields.},
annote = {It looks like behavior evaluation for players might be a new area in game AI (as of December 2015)},
archivePrefix = {arXiv},
arxivId = {1312.3903},
author = {Yannakakis, Georgios N and Togelius, Julian},
doi = {10.1109/TCIAIG.2014.2339221},
eprint = {1312.3903},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yannakakis, Togelius - 2014 - A Panorama of Artificial and Computational Intelligence in Games.pdf:pdf},
isbn = {1943-068X},
issn = {1943-068X},
journal = {Computational Intelligence and AI in Games, IEEE Transactions on},
keywords = {AI benchmarks,AI-assisted game design,Artificial intelligence,Computational modeling,Evolutionary computation,GameAI,Games,Important,NPC behavior learning,Planning,Seminars,TALAF,artificial intelligence,commercial games,computational intelligence,computational narrative believable agents,computer games,human computer interaction,human-computer interaction,player modeling,player-game interaction,procedural content generation,search and planning},
mendeley-tags = {AI benchmarks,AI-assisted game design,Computational modeling,Evolutionary computation,GameAI,Games,Important,NPC behavior learning,Planning,Seminars,TALAF,artificial intelligence,commercial games,computational intelligence,computational narrative believable agents,computer games,human computer interaction,human-computer interaction,player modeling,player-game interaction,procedural content generation,search and planning},
month = {Dec},
number = {99},
pages = {1},
pmid = {8356185},
title = {{A Panorama of Artificial and Computational Intelligence in Games}},
volume = {PP},
year = {2014}
}
@article{Zhou2011,
abstract = {We propose a flexible yet computationally efficient approach for building Gaussian process models for computer experiments with both qualitative and quantitative factors. This approach uses the hypersphere parameterization to model the correlations of the qualitative factors, thus avoiding the need of directly solving optimization problems with positive definite constraints. The effectiveness of the proposed method is successfully illustrated by several examples.},
author = {Zhou, Qiang and Qian, Peter Z. G. and Zhou, Shiyu},
doi = {10.1198/TECH.2011.10025},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Qian, Zhou - 2011 - A Simple Approach to Emulation for Computer Models With Qualitative and Quantitative Factors.pdf:pdf},
isbn = {10.1198/TECH.2011.10025},
issn = {0040-1706},
journal = {Technometrics},
keywords = {BayesOpt,Important,TALAF,computer experiment,hypersphere decomposition,kriging},
mendeley-tags = {BayesOpt,Important,TALAF},
month = {Aug},
number = {3},
pages = {266--273},
title = {{A Simple Approach to Emulation for Computer Models With Qualitative and Quantitative Factors}},
volume = {53},
year = {2011}
}
@misc{Mennen_plt,
abstract = {Finding the nearest positive definite matrix},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - plt - File Exchange - MATLAB Central.html:html},
keywords = {TALAF},
mendeley-tags = {TALAF},
title = {{plt - File Exchange - MATLAB Central}},
urldate = {2015-11-25}
}
@misc{SE_hyp_opt,
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - optimization - Hyperparameter estimation in Gaussian process - Cross Validated.html:html},
keywords = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
title = {{optimization - Hyperparameter estimation in Gaussian process - Cross Validated}},
urldate = {2015-11-04}
}
@misc{rmgarnett_gpml_extensions,
abstract = {gpml{\_}extensions - Provides various extensions to the GPML toolbox for Gaussian process inference in MATLAB.},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - rmgarnettgpml{\_}extensions.html:html},
keywords = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
title = {rmgarnett/gpml{\_}extensions},
urldate = {2015-11-04}
}
@misc{bayes_hyper,
abstract = {I was checking the paper, [Practical Bayesian Optimization of Machine Learning](http://arxiv.org/pdf/1206.2944) and i was wondering if anyone here...},
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Experiences with bayesian hyperparameter optimization • rMachineLearning.html:html},
keywords = {BayesOpt,NotRead,TALAF},
mendeley-tags = {BayesOpt,NotRead,TALAF},
shorttitle = {Experiences with bayesian hyperparameter optimizat},
title = {{Experiences with bayesian hyperparameter optimization?}},
urldate = {2015-12-03}
}
@misc{SE_GP_overfit,
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - machine learning - How can you detect if a Gaussian process is over-fitting - Cross Validated.html:html},
keywords = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
shorttitle = {machine learning - How can you detect if a Gaussia},
title = {{machine learning - How can you detect if a Gaussian process is over-fitting? - Cross Validated}},
urldate = {2015-11-04}
}
@misc{SE_GPML_MAP,
file = {:home/brett/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - machine learning - Marginalization of GP regression hyperparameters with Laplace approximation - Cross Validated.html:html},
keywords = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
mendeley-tags = {BayesOpt,GPs,OptimizingHyperparameters,TALAF},
title = {{machine learning - Marginalization of GP regression hyperparameters with Laplace approximation - Cross Validated}},
urldate = {2015-11-04}
}
