% Generated by Paperpile. Check out http://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Kuhn1997-qc,
  title    = "Communicating Uncertainty: Framing Effects on Responses to Vague
              Probabilities",
  author   = "Kuhn, Kristine M",
  abstract = "Most real-world risky decisions are based on imprecise
              probabilities. Although people generally demonstrate vagueness
              aversion, behaving as if vaguely specified probabilities are
              worse than comparable precisely specified probabilities,
              vagueness seeking also occurs. Previous explanations of vagueness
              preferences have been based on individual differences and
              regressive beliefs about extreme probabilities, and little
              research has examined the effect of changes in the way vagueness
              is communicated to the decision maker. The present study
              demonstrates that gain/loss framing, moderated by the
              operationalization of vagueness, influences how people respond to
              vagueness about a probability estimate. Subjects read scenarios
              describing consumer purchases, organizational marketing
              decisions, and medical treatments, and expressed preference
              between options having either precisely or vaguely described
              probabilities. Vagueness was operationalized either as a range of
              possible values or as verbal qualification of a single point
              estimate. Negative framing was associated with greater preference
              for vague prospects, unless vagueness was described by a
              numerical range with the higher value presented first, indicating
              a substantial primacy order effect. A second experiment
              demonstrates that negative framing led people to make more
              favorable inferences about the likelihoods of vague
              probabilities.",
  journal  = "Organ. Behav. Hum. Decis. Process.",
  volume   =  71,
  number   =  1,
  pages    = "55--83",
  month    =  "1~" # jul,
  year     =  1997,
  keywords = "Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Montavon2017-qu,
  title    = "Explaining nonlinear classification decisions with deep Taylor
              decomposition",
  author   = "Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder,
              Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert",
  abstract = "Abstract Nonlinear methods such as Deep Neural Networks (DNNs)
              are the gold standard for various challenging machine learning
              problems such as image recognition. Although these methods
              perform impressively well, they have a significant disadvantage,
              the lack of transparency, limiting the interpretability of the
              solution and thus the scope of application in practice.
              Especially DNNs act as black boxes due to their multilayer
              nonlinear structure. In this paper we introduce a novel
              methodology for interpreting generic multilayer neural networks
              by decomposing the network classification decision into
              contributions of its input elements. Although our focus is on
              image classification, the method is applicable to a broad set of
              input data, learning tasks and network architectures. Our method
              called deep Taylor decomposition efficiently utilizes the
              structure of the network by backpropagating the explanations from
              the output to the input layer. We evaluate the proposed method
              empirically on the MNIST and ILSVRC data sets.",
  journal  = "Pattern Recognit.",
  volume   =  65,
  pages    = "211--222",
  year     =  2017,
  keywords = "Deep neural networks; Heatmapping; Taylor decomposition;
              Relevance propagation; Image recognition;explain;Mendeley Import
              (Jan 17)/Assurances"
}

@INPROCEEDINGS{Wang2015-ww,
  title      = "Falling Rule Lists",
  booktitle  = "Artificial Intelligence and Statistics",
  author     = "Wang, Fulton and Rudin, Cynthia",
  abstract   = "Falling rule lists are classification models consisting of an
                ordered list of if-then rules, where (i) the order of rules
                determines which example should be classified by each rule, and
                (ii) the estimated probability of success decreases
                monotonically down the list. These kinds of rule lists are
                inspired by healthcare applications where patients would be
                stratified into risk sets and the highest at-risk patients
                should be considered first. We provide a Bayesian framework for
                learning falling rule lists that does not rely on traditional
                greedy decision tree learning methods.",
  pages      = "1013--1022",
  month      =  "21~" # feb,
  year       =  2015,
  keywords   = "
                trust\_informal\_treatment;assurance\_explicit;interp\_models;Mendeley
                Import (Jan 17)/Assurances",
  language   = "en",
  conference = "Artificial Intelligence and Statistics"
}

@INPROCEEDINGS{Szafir2014-ok,
  title     = "Communication of Intent in Assistive Free Flyers",
  booktitle = "Proceedings of the 2014 {ACM/IEEE} International Conference on
               Human-robot Interaction",
  author    = "Szafir, Daniel and Mutlu, Bilge and Fong, Terrence",
  publisher = "ACM",
  pages     = "358--365",
  series    = "HRI '14",
  year      =  2014,
  address   = "New York, NY, USA",
  keywords  = "assistive free-flyer (aff), design, flying robot, human factors,
               intent, micro air vehicle (mav), motion, usability"
}

@ARTICLE{Lacher2014-yc,
  title     = "Autonomy, trust, and transportation",
  author    = "Lacher, A and Grabowski, R and Cook, S",
  abstract  = "Abstract Automation in transportation (rail, air, road, etc.) is
               becoming increasingly complex and interconnected. Ensuring that
               these sophisticated non-deterministic software systems can be
               trusted and remain resilient is a community concern. As
               technology evolves, systems",
  journal   = "2014 AAAI Spring Symposium Series",
  publisher = "aaai.org",
  year      =  2014,
  keywords  = "trust\_formal\_treatment;Mendeley Import (Jan 17)/Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Yamagishi2005-cp,
  title     = "Separating trust from cooperation in a dynamic relationship
               prisoner’s dilemma with variable dependence",
  author    = "Yamagishi, Toshio and Kanazawa, Satoshi and Mashima, Rie and
               Terai, Shigeru",
  journal   = "Ration. Soc.",
  publisher = "Sage Publications",
  volume    =  17,
  number    =  3,
  pages     = "275--308",
  year      =  2005,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Ghosh2016-dl,
  title    = "Trusted Machine Learning for Probabilistic Models",
  author   = "Ghosh, Shalini and Lincoln, Patrick and Tiwari, Ashish and Zhu,
              Xiaojin and Edu, Wisc",
  journal  = "Reliable Machine Learning in the Wild at ICML",
  year     =  2016,
  keywords = "
              trust\_informal\_treatment;assurance\_explicit;model\_check;Mendeley
              Import (Jan 17)/Assurances"
}

@ARTICLE{Wickens1999-la,
  title     = "Unreliable Automated Attention Cueing for {Air-Ground} Targeting
               and Traffic Maneuvering",
  author    = "Wickens, Christopher D and Conejo, Rena and Gempler, Keith",
  abstract  = "We report two experiments in which pilots' attention is
               occasionally directed to inappropriate or inaccurate locations
               in space, replicating the effects of imperfect automation. A
               general taxonomy of human performance costs in these situations
               is presented. In Experiment 1, pilots are engaged in an
               air-ground targeting scenario. Target cueing, based upon
               semi-reliable sensor information, sometimes directs attention
               away from the true target. Yet pilots follow such guidance, even
               knowing its unreliability, a result of the difficulty of the
               unaided task. In Experiment 2, pilots in a free flight
               simulation are engaged in a series of traffic conflict avoidance
               maneuvers, using a cockpit display of traffic information
               (CDTI). On rare trials the CDTI knowledge of the traffic
               intruder's intentions, reflected in a predictor symbol, is
               unreliable and does not correspond with the actual aircraft
               behavior. Yet pilots' avoidance behavior is governed by the
               predictor symbol, and a display manipulation that calls
               attention to the inaccuracy of the predictor does little to
               influence pilots' reliance upon the predictor symbol although it
               does reduce visual workload. The data are interpreted in terms
               of appropriate trust calibration.",
  journal   = "Proc. Hum. Fact. Ergon. Soc. Annu. Meet.",
  publisher = "SAGE Publications",
  volume    =  43,
  number    =  1,
  pages     = "21--25",
  month     =  "1~" # sep,
  year      =  1999,
  keywords  = "
               human\_study;trust\_formal\_treatment;assurance\_implicit;assurance\_structural;assurance\_normality;in\_paper;Mendeley
               Import (Jan 17)/Assurances"
}

@MISC{Benioff2016-tc,
  title        = "The {AI} revolution is coming fast. But without a revolution
                  in trust, it will fail",
  booktitle    = "World Economic Forum",
  author       = "Benioff, Mark",
  abstract     = "Before we can benefit from any of the latest innovations and
                  technologies, we need a revolution in trust.",
  month        =  "26~" # aug,
  year         =  2016,
  howpublished = "\url{https://www.weforum.org/agenda/2016/08/the-digital-revolution-is-here-but-without-a-revolution-in-trust-it-will-fail/}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_corporate\_conversation"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Rudnitsky2017-in,
  title        = "How Can We Trust {AI} Companies? -- Contextual Wisdom --- The
                  Official Neura Blog -- Medium",
  booktitle    = "Medium",
  author       = "Rudnitsky, Fade",
  abstract     = "When building a company or a product, a lot of times you
                  happen to be at a crossroads, where you are tested against
                  your integrity and your commitment to become a company which
                  people can trust. If…",
  publisher    = "Contextual Wisdom --- The Official Neura Blog",
  month        =  "2~" # may,
  year         =  2017,
  howpublished = "\url{https://medium.com/the-official-neura-blog/how-can-we-trust-ai-companies-28a6ac635e45}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_corporate\_conversation"
}

@MISC{Moody2017-vd,
  title        = "How {CHROs} can handle emerging {AI} and build company trust",
  booktitle    = "{HR} Dive",
  author       = "Moody, Kathryn",
  abstract     = "Artificial intelligence is on its way. Before full adoption,
                  CHROs can brace their departments for change.",
  month        =  "22~" # mar,
  year         =  2017,
  howpublished = "\url{http://www.hrdive.com/news/how-chros-can-handle-emerging-ai-and-build-company-trust/438602/}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_corporate\_conversation"
}

@MISC{Spectrum2016-jv,
  title        = "Can We Trust Robots?",
  booktitle    = "{IEEE} Spectrum: Technology, Engineering, and Science News",
  author       = "Spectrum, Ieee",
  abstract     = "Robots will soon have the power of life and death over human
                  beings. Are they ready? Are we?",
  publisher    = "IEEE Spectrum",
  month        =  "31~" # may,
  year         =  2016,
  howpublished = "\url{http://spectrum.ieee.org/robotics/artificial-intelligence/can-we-trust-robots}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_public\_conversation"
}

@MISC{Danks_undated-sb,
  title        = "Can we trust self-driving cars?",
  booktitle    = "Salon",
  author       = "Danks, David",
  abstract     = "Trust is complex and varied, and so are the technologies in
                  question",
  howpublished = "\url{http://www.salon.com/2017/01/08/can-we-trust-self-driving-cars_partner/}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_public\_conversation"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Cassel2017-tn,
  title        = "The Big Question from 2016: Can We Trust Our Technologies? -
                  The New Stack",
  booktitle    = "The New Stack",
  author       = "Cassel, David",
  abstract     = "As a new year begins, it’s a good time to cast one last,
                  lingering look back at 2016. I’ve called this the ultimate
                  big data question: what just happened over the last 12
                  months? If every moment is its own container filled with
                  memories, here’s an attempt to tease out some meaning from
                  that great …",
  month        =  "1~" # jan,
  year         =  2017,
  howpublished = "\url{https://thenewstack.io/big-question-2016-can-trust-technologies/}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_public\_conversation"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Cranz2017-yh,
  title        = "Why We Don't Fully Trust Technology",
  booktitle    = "Gizmodo Australia",
  author       = "Cranz, Alex and Elderkin, Beth",
  abstract     = "We’ve all been there. The presenter is about to begin, but
                  then disaster strikes: the computer technology fails. Perhaps
                  the computer has fallen asleep,...",
  month        =  "14~" # jun,
  year         =  2017,
  howpublished = "\url{https://www.gizmodo.com.au/2015/10/why-we-dont-fully-trust-technology/}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_public\_conversation"
}

@MISC{DeSteno2014-cq,
  title        = "Can You Trust Technology?",
  booktitle    = "{HuffPost}",
  author       = "DeSteno, David",
  publisher    = "HuffPost",
  month        =  "29~" # jan,
  year         =  2014,
  howpublished = "\url{http://www.huffingtonpost.com/david-desteno/can-you-trust-technology_b_4683614.html}",
  note         = "Accessed: 2017-6-14",
  keywords     = "trust\_popular\_media;trust\_public\_conversation"
}

@PHDTHESIS{Ganjali2016-pb,
  title    = "Efficient Reinforcement Learning with Bayesian Optimization",
  author   = "Ganjali, Danyan",
  editor   = "Sideris, Athanasios",
  abstract = "A probabilistic reinforcement learning algorithm is presented for
              finding control policies in continuous state and action spaces
              without a prior knowledge of the dynamics. The objective of this
              algorithm is to learn from minimal amount of interaction with the
              environment in order to maximize a notion of reward, i.e. a
              numerical measure of the quality of the resulting state
              trajectories. Experience from the interactions are used to
              construct a set of probabilistic Gaussian process (GP) models
              that predict the resulting state trajectories and the reward from
              executing a policy on the system. These predictions are used with
              a technique known as Bayesian optimization to search for policies
              that promise higher rewards. As more experience is gathered,
              predictions are made with more confidence and the search for
              better policies relies less on new interactions with the
              environment. The computational demand of a GP makes it eventually
              impractical to use as the number of observations from interacting
              with the environment increase. Moreover, using a single GP to
              model different regions that may exhibit disparate behaviors can
              produce unsatisfactory representations and predictions. One way
              of mitigating these issues is by partitioning the observation
              points into different regions each represented by a local GP.
              With the sequential arrival of the observation points from new
              experiences, it is necessary to have an adaptive clustering
              method that can partition the data into an appropriate number of
              regions. This led to the development of EM+ algorithm presented
              in the second part of this work, which is an extension to the
              Expectation Maximization (EM) for the Gaussian mixture models,
              that assumes no prior knowledge of the number of components.
              Lastly, an application of the EM+ algorithm to filtering problems
              is presented. We propose a filtering algorithm that combines the
              advantages of the well-known particle filter and the mixture of
              Gaussian filter, while avoiding their issues.",
  year     =  2016,
  school   = "UC Irvine",
  keywords = "Mendeley Import (Jan 17)/ReinforcementLearning;Mendeley Import
              (Jan 17)/BayesOpt"
}

@MISC{Foley2017-qj,
  title        = "A pioneering computer scientist wants algorithms to be
                  regulated like cars, banks, and drugs",
  booktitle    = "Quartz",
  author       = "Foley, Katherine Ellen",
  abstract     = "It's convenient when Facebook can tag your friends in photos
                  for you, and it's fun when Snapchat can apply a filter to
                  your face. Both are examples of algorithms that have been
                  trained to recognize eyes, noses, and mouths with consistent
                  accuracy. When these programs are wrong---like when Facebook
                  mistakes you for your sibling or...",
  publisher    = "Quartz",
  month        =  "3~" # jun,
  year         =  2017,
  howpublished = "\url{https://qz.com/998131}",
  note         = "Accessed: 2017-6-5",
  keywords     = "trust\_popular\_media;trust\_academic\_conversation;Mendeley
                  Import (Jan 17)/Assurances"
}

@ARTICLE{Lacave2004-gq,
  title     = "A review of explanation methods for heuristic expert systems",
  author    = "Lacave, Carmen and Diez, Francisco J",
  journal   = "Knowl. Eng. Rev.",
  publisher = "Cambridge Univ Press",
  volume    =  19,
  number    =  02,
  pages     = "133--146",
  year      =  2004
}

@BOOK{Halpern2017-zl,
  title     = "Reasoning about Uncertainty",
  author    = "Halpern, Joseph Y",
  abstract  = "In order to deal with uncertainty intelligently, we need to be
               able to represent it and reason about it. In this book, Joseph
               Halpern examines formal ways of representing uncertainty and
               considers various logics for reasoning about it. While the ideas
               presented are formalized in terms of definitions and theorems,
               the emphasis is on the philosophy of representing and reasoning
               about uncertainty. Halpern surveys possible formal systems for
               representing uncertainty, including probability measures,
               possibility measures, and plausibility measures; considers the
               updating of beliefs based on changing information and the
               relation to Bayes' theorem; and discusses qualitative,
               quantitative, and plausibilistic Bayesian networks.This second
               edition has been updated to reflect Halpern's recent research.
               New material includes a consideration of weighted probability
               measures and how they can be used in decision making; analyses
               of the Doomsday argument and the Sleeping Beauty problem;
               modeling games with imperfect recall using the runs-and-systems
               approach; a discussion of complexity-theoretic considerations;
               the application of first-order conditional logic to security.
               Reasoning about Uncertainty is accessible and relevant to
               researchers and students in many fields, including computer
               science, artificial intelligence, economics (particularly game
               theory), mathematics, philosophy, and statistics.",
  publisher = "MIT Press",
  month     =  "7~" # apr,
  year      =  2017,
  keywords  = "Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

@ARTICLE{Reeves1997-ad,
  title    = "The Media Equation: How People Treat Computers, Television, ? New
              Media Like Real People ? Places",
  author   = "Reeves, B and Nass, C",
  journal  = "Comput. Math. Appl.",
  volume   =  5,
  number   =  33,
  pages    = "128",
  year     =  1997,
  keywords = "NotRead;Mendeley Import (Jan 17)/Assurances",
  language = "en"
}

@ARTICLE{Thatcher2011-si,
  title    = "The Role of Trust in Postadoption {IT} Exploration: An Empirical
              Examination of Knowledge Management Systems",
  author   = "Thatcher, J B and McKnight, D H and Baker, E W and Arsal, R E and
              Roberts, N H",
  abstract = "In this study, we examine trust in information technology's (IT)
              relationship with postadoption exploration of knowledge
              management systems (KMS). We introduce and distinguish between
              trust in IT and trust in IT support staff as object-specific
              beliefs that influence technology's infusion into organizations.
              We suggest that these object-specific beliefs' influence on
              intention to explore KMS is mediated by behavioral beliefs about
              IT (e.g., perceived usefulness and perceived ease of use). To
              test the model, we completed two studies. Study 1 examined users'
              perceptions of a knowledge portal. Study 2 examined IT
              professionals' perceptions of KMS. Across studies, our analysis
              suggests that trust in IT exerts direct effects on behavioral
              beliefs leading to intentions to explore KMS. Further, when
              compared to trust in IT support, we found that trust in IT played
              a more central role in shaping behavioral beliefs leading to
              exploration of IT. Implications for research and practice are
              offered.",
  journal  = "IEEE Trans. Eng. Manage.",
  volume   =  58,
  number   =  1,
  pages    = "56--70",
  month    =  feb,
  year     =  2011,
  keywords = "belief maintenance;information technology;knowledge
              management;organisational aspects;security of data;behavioral
              beliefs;information technology;knowledge management
              system;knowledge portal;professional perception;technology
              infusion;Intention to explore information technology
              (IT);knowledge management system (KMS);technology
              diffusion;technology infusion;trust;trust in
              technology;NotRead;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Lippert2008-dv,
  title     = "Assessing post-adoption utilisation of an information technology
               within a supply chain management context",
  author    = "Lippert, Susan K",
  abstract  = "Information Technology (IT) has generated profound effects on
               Supply Chain Management (SCM) activities related to
               problem-solving, information sharing, and cost reduction
               initiatives. The influences of individual-level antecedents on
               post-adoption utilisation of a specialised IT within an SCM
               context were examined. Data were collected from 272 first-tier
               supply chain members of the second largest US automotive
               service-parts logistics operation using a new supply chain
               technology. Twelve hypotheses were tested through a structural
               equation model. The results suggest that in supply chains where
               usage is mandated, individual-level determinants can increase
               utilisation. Study implications and suggestions for future
               research are discussed.",
  journal   = "Int. J. Inf. Technol. Manage.",
  publisher = "Inderscience Publishers",
  volume    =  7,
  number    =  1,
  pages     = "36--59",
  month     =  "1~" # jan,
  year      =  2008,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{2003-lu,
  title     = "Consumer Acceptance of Electronic Commerce: Integrating Trust
               and Risk with the Technology Acceptance Model",
  author    = "{Paul A Pavlou}",
  abstract  = "This paper aims to predict consumer acceptance of e-commerce by
               proposing a set of key drivers for engaging consumers in on-line
               transactions. The primary constructs for capturing consumer
               acceptance of e-commerce are intention to transact and on-line
               transaction behavior. Following the theory of reasoned action
               (TRA) as applied to a technology-driven environment, technology
               acceptance model (TAM) variables (perceived usefulness and ease
               of use) are posited as key drivers of e-commerce acceptance. The
               practical utility of TAM stems from the fact that e-commerce is
               technology-driven. The proposed model integrates trust and
               perceived risk, which are incorporated given the implicit
               uncertainty of the e-commerce environment. The proposed
               integration of the hypothesized independent variables is
               justified by placing all the variables under the nomological TRA
               structure and proposing their interrelationships. The resulting
               research model is tested using data from two empirical studies.
               The first, exploratory study comprises three experiential
               scenarios with 103 students. The second, confirmatory study uses
               a sample of 155 on-line consumers. Both studies strongly support
               the e-commerce acceptance model by validating the proposed
               hypotheses. The paper discusses the implications for e-commerce
               theory, research, and practice, and makes several suggestions
               for future research.",
  journal   = "International Journal of Electronic Commerce",
  publisher = "Routledge",
  volume    =  7,
  number    =  3,
  pages     = "101--134",
  month     =  "1~" # apr,
  year      =  2003,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Orlikowski2006-eg,
  title     = "Desperately seeking the {‘IT’in} {IT} research: a call to
               theorizing the {IT} artifact",
  author    = "Orlikowski, Wanda J and Iacono, C Suzanne",
  journal   = "Information systems: The state of the field",
  publisher = "Chichester, UK: John Wiley \& Sons Ltd",
  pages     = "19--42",
  year      =  2006,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Luhmann1982-jx,
  title     = "Trust and Power",
  author    = "Luhmann, Niklas",
  journal   = "Stud. Sov. Thought",
  publisher = "Springer",
  volume    =  23,
  number    =  3,
  pages     = "266--270",
  year      =  1982,
  keywords  = "trust\_definition;distrust\_definition;Mendeley Import (Jan
               17)/Assurances"
}

@INCOLLECTION{McKnight2001-hm,
  title     = "Trust and Distrust Definitions: One Bite at a Time",
  booktitle = "Trust in Cyber-societies",
  author    = "McKnight, D H and Chervany, Norman L",
  abstract  = "Researchers have remarked and recoiled at the literature
               confusion regarding the meanings of trust and distrust. The
               problem involves both the proliferation of narrow
               intra-disciplinary research definitions of trust and the
               multiple meanings the word trust possesses in everyday use. To
               enable trust researchers to more easily compare empirical
               results, we define a cohesive set of conceptual and measurable
               constructs that captures the essence of trust and distrust
               definitions across several disciplines. This chapter defines
               disposition to trust (and -distrust) constructs from psychology
               and economics, institution-based trust (and -distrust)
               constructs from sociology, and trusting/distrusting beliefs,
               trusting/distrusting intentions, and trust/distrust-related
               behavior constructs from social psychology and other
               disciplines. Distrust concepts are defined as separate and
               opposite from trust concepts. We conclude by discussing the
               importance of viewing trust and distrust as separate,
               simultaneously operating concepts.",
  publisher = "Springer, Berlin, Heidelberg",
  pages     = "27--54",
  year      =  2001,
  keywords  = "distrust\_definition;trust\_definition;Mendeley Import (Jan
               17)/Assurances",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Mcknight1996-ng,
  title    = "The Meanings of Trust",
  author   = "Mcknight, D H and Chervany, N L",
  abstract = "CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep
              Teregowda): Our trust conceptualizations have benefited from
              discussions with Ellen Berscheid and Larry Cummings of the
              University of Minnesota. The authors also thank three anonymous
              reviewers from the Organizational Behavior division of the 1996
              meeting of the Academy of Management for their comments on an
              earlier version of this paper. THE MEANINGS OF TRUST What does
              the word ‘trust ’ mean? Scholars continue to express concern
              regarding their collective lack of consensus about trust’s
              meaning. Conceptual confusion on trust makes comparing one trust
              study to another problematic. To facilitate cumulative trust
              research, the authors propose two kinds of trust typologies: (a)
              a classification system for types of trust, and (b) definitions
              of six related trust types that form a model. Some of the model’s
              implications for management are also outlined. 2 THE MEANINGS OF
              TRUST ``...trust is a term with many meanings. '' (Williamson,
              1993: 453) ``Trust is itself a term for a clustering of
              perceptions. '' (White, 1992: 174) Scholars and practitioners
              widely acknowledge trust's importance. Trust makes cooperative
              endeavors happen (e.g., Arrow, 1974; Deutsch, 1973; Gambetta,
              1988). Trust is a key to positive interpersonal relationships in",
  year     =  1996,
  keywords = "trust\_definition;business;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Phillips2006-ix,
  title    = "A Model of User Distrust of Information Systems",
  author   = "Phillips, Brandis and McKnight, D H",
  journal  = "MWAIS 2006 Proceedings",
  pages    = "4",
  year     =  2006,
  keywords = "trust\_in\_technology;NotRead;Mendeley Import (Jan 17)/Assurances"
}

@INCOLLECTION{McKnight2007-qw,
  title     = "An Extended Trust Building Model: Comparing Experiential and
               {Non-Experiential} Factors",
  booktitle = "Emerging Information Resources Management and Technologies",
  author    = "McKnight, D H and Chervany, Norman L",
  abstract  = "An Extended Trust Building Model: Comparing Experiential and
               Non-Experiential Factors: 10.4018/978-1-59904-286-2.ch008: This
               study examines a model of factors influencing system
               troubleshooter trust in their supervisors, contrasting
               experiential and nonexperiential factors.",
  publisher = "IGI Global",
  pages     = "176--199",
  year      =  2007,
  keywords  = "Normal Accident Theory; computer security; terrorism; Y2K;
               computer privacy;human\_study;trust\_in\_technology;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@ARTICLE{Lankton2008-ct,
  title    = "Do People Trust Facebook as a Technology or as a`` Person''?
              Distinguishing Technology Trust from Interpersonal Trust",
  author   = "Lankton, Nancy K and McKnight, D Harrison",
  journal  = "AMCIS 2008 Proceedings",
  pages    = "375",
  year     =  2008,
  keywords = "
              trust\_in\_technology;very\_similar\_to\_mine;trust\_formal\_treatment;human\_study;assurance\_implicit;in\_paper;Mendeley
              Import (Jan 17)/Assurances"
}

@ARTICLE{Tripp2011-cq,
  title    = "Degrees of Humanness in Technology: What Type of Trust Matters?",
  author   = "Tripp, John and McKnight, D Harrison and Lankton, Nancy K",
  abstract = "Berkeley Electronic Press Selected Works",
  year     =  2011,
  keywords = "
              trust\_in\_technology;very\_similar\_to\_mine;trust\_formal\_treatment;assurance\_implicit;in\_paper;Mendeley
              Import (Jan 17)/Assurances"
}

@ARTICLE{Mcknight2011-gv,
  title     = "Trust in a Specific Technology: An Investigation of Its
               Components and Measures",
  author    = "Mcknight, D Harrison and Carter, Michelle and Thatcher, Jason
               Bennett and Clay, Paul F",
  journal   = "ACM Trans. Manage. Inf. Syst.",
  publisher = "ACM",
  volume    =  2,
  number    =  2,
  pages     = "12:1--12:25",
  month     =  jul,
  year      =  2011,
  address   = "New York, NY, USA",
  keywords  = "Trust, construct development, trust in
               technology;trust\_in\_technology;very\_similar\_to\_mine;assurances;human\_study;trust\_formal\_treatment;assurance\_implicit;in\_paper;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{McKnight2006-ce,
  title     = "Reflections on an initial trust-building model",
  author    = "McKnight, D Harrison and Chervany, Norman L",
  journal   = "Handbook of trust research",
  publisher = "Edward Elgar Cheltenham, UK",
  pages     = "29--51",
  year      =  2006,
  keywords  = "
               trust\_definition;distrust\_definition;e-commerce;business;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{McKnight1998-ty,
  title    = "Initial Trust Formation in New Organizational Relationships",
  author   = "McKnight, D Harrison and Cummings, Larry L and Chervany, Norman L",
  abstract = "Arguably, the most critical time frame for organizational
              participants to develop trust is at the beginning of their
              relationship. Using primarily a cognitive approach, we address
              factors and processes that enable two organizational parties to
              form relatively high trust initially. We propose a model of
              specific relationships among several trust-related constructs and
              two cognitive processes. The model helps explain the paradoxical
              finding of high initial trust levels in new organizational
              relationships.",
  journal  = "Acad. Manage. Rev.",
  volume   =  23,
  number   =  3,
  pages    = "473--490",
  month    =  "1~" # jul,
  year     =  1998,
  keywords = "trust\_definition;trust\_model;business;Mendeley Import (Jan
              17)/Assurances"
}

@ARTICLE{McKnight2004-vv,
  title     = "Dispositional Trust And Distrust Distinctions in Predicting
               High- and {Low-Risk} Internet Expert Advice Site Perceptions",
  author    = "McKnight, D H and Kacmar, Charles J and Choudhury, Vivek",
  abstract  = "This study examines whether some types of dispositional
               trust/distrust concepts are better than other types at inducing
               consumers to trust a Web advice provider. We propose and test a
               model in which dispositional trust and distrust concepts are
               given separate roles. This unique approach is based on the
               growing, but untested theoretical consensus that trust and
               distrust are separate concepts that co-exist yet differ in terms
               of their emotional makeup. While trust concepts tend to be calm
               and collected, distrust concepts embody significant levels of
               fear and insecurity. Based on this difference, we propose that
               dispositional distrust concepts will be better predictors of
               high-risk Internet legal advice site perceptions, while the
               corresponding trust concepts will be better predictors of
               low-risk Internet legal advice site perceptions. As proposed,
               the study finds that dispositional trust better predicts
               low-risk perceptions, while dispositional distrust better
               predicts high-risk perceptions. For e-commerce advice site
               research, the findings of this article suggest that perhaps
               scholars should Not only study dispositional trust but also
               dispositional distrust.",
  journal   = "e-Service Journal",
  publisher = "Indiana University Press",
  volume    =  3,
  number    =  2,
  pages     = "35--55",
  year      =  2004,
  keywords  = "
               trust\_definition;trust\_model;e-commerce;human\_study;distrust\_definition;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{McKnight2001-gz,
  title    = "While trust is cool and collected, distrust is fiery and
              frenzied: A model of distrust concepts",
  author   = "McKnight, D Harrison and Chervany, Norman",
  journal  = "Amcis 2001 Proceedings",
  pages    = "171",
  year     =  2001,
  keywords = "trust\_definition;trust\_model;e-commerce;Mendeley Import (Jan
              17)/Assurances"
}

@ARTICLE{Feltz_undated-yn,
  title    = "{Self'-Confidence} and Sports Performance",
  author   = "Feltz, Deborah L",
  keywords = "self-confidence;sport;ai\_general;assurances;Mendeley Import (Jan
              17)/Assurances"
}

@ARTICLE{Rempel1985-sg,
  title     = "Trust in close relationships",
  author    = "Rempel, John K and Holmes, John G and Zanna, Mark P",
  abstract  = "Tested a theoretical model of interpersonal trust in close
               relationships with 47 dating, cohabiting, or married couples
               (mean ages were 31 yrs for males and 29 yrs for females). The
               validity of the model's 3 dimensions of trust---predictability,
               dependability, and faith---was examined. Ss completed scales
               designed to measure liking and loving, trust, and motivation for
               maintaining the relationship. An analysis of the instrument
               measuring trust was consistent with the notion that the
               predictability, dependability, and faith components represent
               distinct and coherent dimensions. The perception of intrinsic
               motives in a partner emerged as a dimension, as did instrumental
               and extrinsic motives. As expected, love and happiness were
               closely tied to feelings of faith and the attribution of
               intrinsic motivation to both self and partner. Women appeared to
               have more integrated, complex views of their relationships than
               men: All 3 forms of trust were strongly related, and
               attributions of instrumental motives in their partners seemed to
               be self-affirming. There was a tendency for Ss to view their own
               motives as less self-centered and more exclusively intrinsic
               than their partner's motives. (25 ref) (PsycINFO Database Record
               (c) 2016 APA, all rights reserved)",
  journal   = "J. Pers. Soc. Psychol.",
  publisher = "American Psychological Association",
  volume    =  49,
  number    =  1,
  pages     = "95",
  month     =  jul,
  year      =  1985,
  keywords  = "validity of predictability \& dependability \& faith as
               dimensions of model of interpersonal trust, dating vs cohabiting
               vs married couples;NotRead;Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

@BOOK{Wiener1964-ex,
  title     = "God and golem",
  author    = "Wiener, Norbert",
  publisher = "Massachusetts Institute of Technology",
  year      =  1964,
  keywords  = "NotRead;Textbook;Mendeley Import (Jan 17)/TextBooks"
}

@ARTICLE{Tribe1972-zp,
  title     = "Technology assessment and the fourth discontinuity: The limits
               of instrumental rationality",
  author    = "Tribe, Laurence H",
  journal   = "South. Calif. Law Rev.",
  publisher = "HeinOnline",
  volume    =  46,
  pages     = "617",
  year      =  1972
}

@INCOLLECTION{Crum2004-xy,
  title     = "Certification Challenges for Autonomous Flight Control Systems",
  booktitle = "{AIAA} Guidance, Navigation, and Control Conference and Exhibit",
  author    = "Crum, Vincent and Homan, David and Bortner, Raymond",
  publisher = "American Institute of Aeronautics and Astronautics",
  series    = "Guidance, Navigation, and Control and Co-located Conferences",
  month     =  "16~" # aug,
  year      =  2004,
  keywords  = "V\&V;Mendeley Import (Jan 17)/Assurances"
}

@TECHREPORT{Huang2016-vd,
  title       = "Reasoning about cognitive trust in stochastic multiagent
                 systems",
  author      = "Huang, Xiaowei and Kwiatkowska, Marta",
  institution = "Technical Report CS-RR-16-02, Department of Computer Science,
                 University of Oxford",
  year        =  2016,
  keywords    = "Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Patrick2002-ga,
  title     = "Building trustworthy software agents",
  author    = "Patrick, A S",
  abstract  = "As agents become more active and sophisticated, the implications
               of their actions become more serious. With today's GUIs, user
               and software errors can often be easily fixed or undone. An
               agent performing actions on behalf of a user could make errors
               that are very difficult to ``undo'', and, depending on the
               agent's complexity, it might not be clear what went wrong.
               Moreover, for agents to operate effectively and truly act on
               their users' behalf, they might need confidential or sensitive
               information. This includes financial details and personal
               contact information. Thus, along with the excitement about
               agents and what they can do, there is concern about the
               resulting security and privacy issues. It is not enough to
               assume that well-designed software agents will provide the
               security and privacy users need; assurances and assumptions
               about security and privacy need to be made explicit. This
               article proposes a model of the factors that determine agent
               acceptance, based on earlier work on user attitudes toward
               e-commerce transactions, in which feelings of trust and
               perceptions of risk combine in opposite directions to determine
               a user's final acceptance of an agent technology.",
  journal   = "IEEE Internet Comput.",
  publisher = "ieeexplore.ieee.org",
  volume    =  6,
  number    =  6,
  pages     = "46--53",
  month     =  nov,
  year      =  2002,
  keywords  = "data privacy;electronic commerce;knowledge engineering;security
               of data;software agents;confidential information;e-commerce
               transactions;privacy;risk perceptions;security;trust;trustworthy
               software agent building;user attitudes;Computer errors;Computer
               interfaces;Computer networks;Distributed computing;Graphical
               user interfaces;Information security;Intelligent
               agent;Mice;Privacy;Software agents;NotRead;Mendeley Import (Jan
               17)/Assurances"
}

@ARTICLE{Cheshire2011-zx,
  title       = "Online trust, trustworthiness, or assurance?",
  author      = "Cheshire, Coye",
  affiliation = "Univ. of California, Berkeley.",
  abstract    = "Every day, individuals around the world retrieve, share, and
                 exchange information on the Internet. We interact online to
                 share personal information, find answers to questions, make
                 financial transactions, play social games, and maintain
                 professional and personal relationships. Sometimes our online
                 interactions take place between two or more humans. In other
                 cases, we rely on computers to manage information on our
                 behalf. In each scenario, risk and uncertainty are essential
                 for determining possible actions and outcomes. This essay
                 highlights common deficiencies in our understanding of key
                 concepts such as trust, trustworthiness, cooperation, and
                 assurance in online environments. Empirical evidence from
                 experimental work in computer-mediated environments
                 underscores the promises and perils of overreliance on
                 security and assurance structures as replacements for
                 interpersonal trust. These conceptual distinctions are
                 critical because the future shape of the Internet will depend
                 on whether we build assurance structures to limit and control
                 ambiguity or allow trust to emerge in the presence of risk and
                 uncertainty.",
  journal     = "Daedalus",
  publisher   = "MIT Press",
  volume      =  140,
  number      =  4,
  pages       = "49--58",
  year        =  2011,
  keywords    = "NotRead;Mendeley Import (Jan 17)/Assurances",
  language    = "en"
}

@INPROCEEDINGS{Aitken2016-fb,
  title     = "Assurances and machine self-confidence for enhanced trust in
               autonomous systems",
  booktitle = "{RSS} 2016 Workshop on Social Trust in Autonomous Systems",
  author    = "Aitken, Matthew and Ahmed, Nisar and Lawrence, Dale and Argrow,
               Brian and Frew, Eric",
  abstract  = "This work investigates a model-based approach to understanding
               how user trust evolves in systems consisting of a supervising
               user and an autonomous agent. This model consists of a
               multivariate model for user trust , and a feedback connection
               between user and agent.",
  publisher = "qav.comlab.ox.ac.uk",
  year      =  2016,
  keywords  = "self-confidence;Mendeley Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Ahmed2016-wz,
  title     = "Collaborative autonomous sensing with Bayesians in the loop",
  booktitle = "{SPIE} Security + Defence",
  author    = "Ahmed, Nisar",
  abstract  = "abstract There is a strong push to develop intelligent unmanned
               autonomy that complements human reasoning for applications as
               diverse as wilderness search and rescue, military surveillance,
               and robotic space exploration. More than just replacing",
  publisher = "International Society for Optics and Photonics",
  pages     = "99860B--99860B--15",
  month     =  "21~" # oct,
  year      =  2016,
  keywords  = "Algorithms; Defense and security; Information fusion;
               Interfaces; Machine learning; Modeling; Robotics; Sensors;
               Surveillance; Robots; Search and rescue"
}

@ARTICLE{Nahavandi2017-mo,
  title     = "Trusted Autonomy Between Humans and Robots: Toward
               {Human-on-the-Loop} in Robotics and Autonomous Systems",
  author    = "Nahavandi, S",
  abstract  = "Systems that can change their behavior in response to unexpected
               conditions and events during operation are known as autonomous
               [1]. Autonomy refers to the capability of a machine to perform a
               task, or part of it, with no-or substantially reduced-human
               intervention. Over the years, autonomous systems have appeared
               and sometimes dominated various aspects of human daily
               activities, such as in robot-controlled operations.",
  journal   = "IEEE Systems, Man, and Cybernetics Magazine",
  publisher = "ieeexplore.ieee.org",
  volume    =  3,
  number    =  1,
  pages     = "10--17",
  month     =  jan,
  year      =  2017,
  keywords  = "Autonomous systems;Cybernetics;Decision making;Man-machine
               systems;Robots;Service robots;Weapons"
}

@ARTICLE{Dassonville1996-hu,
  title    = "Trust between man and machine in a teleoperation system",
  author   = "Dassonville, I and Jolly, D and Desodt, A M",
  abstract = "The work we present deals with the trust of man in a
              teleoperation system. Trust is important because it is linked to
              stress which modifies human reliability. We are trying to
              quantify trust. In this paper, we'll present the theory of trust
              in relationships, and its extension for a man-machine system.
              Then, we explain the links between trust and human reliability.
              Then, we introduce our experimental process and the first results
              concerning selfconfidence.",
  journal  = "Reliab. Eng. Syst. Saf.",
  volume   =  53,
  number   =  3,
  pages    = "319--325",
  month    =  "1~" # sep,
  year     =  1996
}

@TECHREPORT{Sheridan1984-kx,
  title       = "Research and modeling of supervisory control behavior. Report
                 of a workshop",
  author      = "Sheridan, Thomas B and Hennessy, Robert T",
  institution = "DTIC Document",
  year        =  1984,
  keywords    = "
                 automation;assurances;visionary\_paper;trust\_formal\_treatment;assurance\_implicit;Mendeley
                 Import (Jan 17)/Assurances"
}

@ARTICLE{Lewis1985-pr,
  title     = "Trust as a Social Reality",
  author    = "Lewis, J David and Weigert, Andrew",
  abstract  = "Although trust is an underdeveloped concept in sociology,
               promising theoretical formulations are available in the recent
               work of Luhmann and Barber. This sociological version
               complements the psychological and attitudinal conceptualizations
               of experimental and survey researchers. Trust is seen to include
               both emotional and cognitive dimensions and to function as a
               deep assumption underwriting social order. Contemporary examples
               such as lying, family exchange, monetary attitudes, and
               litigation illustrate the centrality of trust as a sociological
               reality.",
  journal   = "Soc. Forces",
  publisher = "Oxford University Press",
  volume    =  63,
  number    =  4,
  pages     = "967--985",
  month     =  "1~" # jun,
  year      =  1985,
  keywords  = "trust\_definition;NotRead;Mendeley Import (Jan 17)/Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wachter2016-kz,
  title    = "Why a Right to Explanation of Automated {Decision-Making} Does
              Not Exist in the General Data Protection Regulation",
  author   = "Wachter, Sandra and Mittelstadt, Brent and Floridi, Luciano",
  abstract = "Since approval of the EU General Data Protection Regulation
              (GDPR) in 2016, it has been widely and repeatedly claimed that
              the GDPR will legally mandate a ‘righ",
  month    =  "28~" # dec,
  year     =  2016,
  keywords = "artificial intelligence, algorithms, automated decision-making,
              data protection, General Data Protection Regulation, right to
              explanation, right of
              access;NotRead;trust\_popular\_media;Mendeley Import (Jan
              17)/Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Lipton2017-zw,
  title        = "{NYU} Law’s Algorithms and Explanations",
  booktitle    = "Approximately Correct",
  author       = "Lipton, Zachary",
  month        =  "5~" # aug,
  year         =  2017,
  howpublished = "\url{http://approximatelycorrect.com/2017/05/08/nyu-laws-algorithms-and-explanations/}",
  note         = "Accessed: 2017-8-5",
  keywords     = "decision\_support;ai\_reasoning;assurances;NotRead;Mendeley
                  Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Azzopardi2003-bg,
  title     = "Investigating the Relationship Between Language Model Perplexity
               and {IR} Precision-recall Measures",
  booktitle = "Proceedings of the 26th Annual International {ACM} {SIGIR}
               Conference on Research and Development in Informaion Retrieval",
  author    = "Azzopardi, Leif and Girolami, Mark and van Risjbergen, Keith",
  publisher = "ACM",
  pages     = "369--370",
  series    = "SIGIR '03",
  year      =  2003,
  address   = "New York, NY, USA",
  keywords  = "language model;NotRead"
}

@ARTICLE{Heinrich2008-nf,
  title    = "Parameter estimation for text analysis",
  author   = "Heinrich, Gregor",
  journal  = "University of Leipzig, Tech. Rep",
  year     =  2008,
  keywords = "NotRead"
}

@INPROCEEDINGS{Zhang2014-he,
  title     = "Predicting failures of vision systems",
  booktitle = "Proceedings of the {IEEE} Conference on Computer Vision and
               Pattern Recognition",
  author    = "Zhang, Peng and Wang, Jiuling and Farhadi, Ali and Hebert,
               Martial and Parikh, Devi",
  pages     = "3566--3573",
  year      =  2014,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;classification;perf\_prediction;Mendeley
               Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Churchill2015-ei,
  title     = "Know your limits: Embedding localiser performance models in
               teach and repeat maps",
  booktitle = "2015 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Churchill, W and Tong, Chi Hay and Gur{\u a}u, C and Posner, I
               and Newman, P",
  abstract  = "This paper is about building maps which not only contain the
               traditional information useful for localising - such as point
               features - but also embeds a spatial model of expected localiser
               performance. This often overlooked second-order information
               provides vital context when it comes to map use and planning.
               Our motivation here is to improve the performance of the popular
               Teach and Repeat paradigm [1] which has been shown to enable
               truly large-scale field operation. When using the taught route
               for localisation, it is often assumed the robot is following
               exactly, or is sufficiently close to, the original path,
               enabling successful localisation. However, what happens if it is
               not possible, or not desirable to exactly follow the mapped
               path? How far off the beaten track can the robot travel before
               it gets lost? We present an approach for assessing this
               localisation area around a taught route, which we refer to as
               the localisation envelope. Using a combination of physical
               sampling and a Gaussian Process model, we are able to accurately
               predict the localisation performance at unseen points.",
  pages     = "4238--4244",
  month     =  may,
  year      =  2015,
  keywords  = "Gaussian processes;path planning;robots;sampling
               methods;Gaussian process model;localisation envelope;localiser
               performance models;map building;physical sampling;point
               features;robot;second-order information;teach and repeat
               paradigm;Gaussian processes;Planning;Robots;Splines
               (mathematics);Trajectory;Uncertainty;Visualization;trust\_informal\_treatment;assurance\_explicit;perf\_prediction;Mendeley
               Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Tellex2012-hn,
  title     = "Toward Information Theoretic {Human-Robot} Dialog",
  booktitle = "Robotics: Science and Systems",
  author    = "Tellex, Stefanie and Thaker, Pratiksha and Deits, Robin and
               Kollar, Thomas and Roy, Nicholas",
  volume    =  2,
  pages     = "3",
  year      =  2012,
  keywords  = "
               ai\_learning;ai\_interaction;assurance\_competence;assurance\_predictability;robotics;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Paul2011-vr,
  title     = "Self help: Seeking out perplexing images for ever improving
               navigation",
  booktitle = "2011 {IEEE} International Conference on Robotics and Automation",
  author    = "Paul, R and Newman, P",
  abstract  = "This paper is a demonstration of how a robot can, through
               introspection and then targeted data retrieval, improve its own
               performance. It is a step in the direction of lifelong learning
               and adaptation and is motivated by the desire to build robots
               that have plastic competencies which are not baked in. They
               should react to and benefit from use. We consider a particular
               instantiation of this problem in the context of place
               recognition. Based on a topic based probabilistic model of
               images, we use a measure of perplexity to evaluate how well a
               working set of background images explain the robot's online view
               of the world. Offline, the robot then searches an external
               resource to seek out additional background images that bolster
               its ability to localise in its environment when used next. In
               this way the robot adapts and improves performance through use.",
  pages     = "445--451",
  month     =  may,
  year      =  2011,
  keywords  = "SLAM (robots);image retrieval;mobile robots;path
               planning;probability;robot vision;FAB-MAP algorithm;data
               retrieval;image probabilistic model;introspection;lifelong
               learning;navigation improvement;perplexing image seeking
               out;place recognition;self help;Biological system
               modeling;Convergence;Databases;Mathematical
               model;Redundancy;Robots;Visualization;image\_classification;assurance\_competence;assurance\_normality;ai\_perception;ai\_knowledge\_rep;ai\_learning;assurance\_implicit;trust\_informal\_treatment;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Freund1997-ig,
  title     = "Selective Sampling Using the Query by Committee Algorithm",
  author    = "Freund, Yoav and Sebastian Seung, H and Shamir, Eli and Tishby,
               Naftali",
  abstract  = "We analyze the ``query by committee'' algorithm, a method for
               filtering informative queries from a random stream of inputs. We
               show that if the two-member committee algorithm achieves
               information gain with positive lower bound, then the prediction
               error decreases exponentially with the number of queries. We
               show that, in particular, this exponential decrease holds for
               query learning of perceptrons.",
  journal   = "Mach. Learn.",
  publisher = "Kluwer Academic Publishers",
  volume    =  28,
  number    = "2-3",
  pages     = "133--168",
  month     =  "1~" # aug,
  year      =  1997,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

@INPROCEEDINGS{Weiyu_Zhang_undated-jx,
  title           = "Power {SVM}: Generalization with exemplar classification
                     uncertainty",
  booktitle       = "2012 {IEEE} Conference on Computer Vision and Pattern
                     Recognition",
  author          = "{Weiyu Zhang} and Yu, S X and {Shang-Hua Teng}",
  publisher       = "IEEE",
  pages           = "2144--2151",
  keywords        = "meh..;Mendeley Import (Jan 17)/Assurances",
  conference      = "2012 IEEE Conference on Computer Vision and Pattern
                     Recognition (CVPR)"
}

@ARTICLE{Tong2001-di,
  title    = "Support Vector Machine Active Learning with Applications to Text
              Classification",
  author   = "Tong, Simon and Koller, Daphne",
  journal  = "J. Mach. Learn. Res.",
  volume   =  2,
  number   = "Nov",
  pages    = "45--66",
  year     =  2001,
  keywords = "classification;meh..;Mendeley Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Holub2008-pe,
  title     = "Entropy-based active learning for object recognition",
  booktitle = "2008 {IEEE} Computer Society Conference on Computer Vision and
               Pattern Recognition Workshops",
  author    = "Holub, A and Perona, P and Burl, M C",
  abstract  = "Most methods for learning object categories require large
               amounts of labeled training data. However, obtaining such data
               can be a difficult and time-consuming endeavor. We have
               developed a novel, entropy-based ldquoactive learningrdquo
               approach which makes significant progress towards this problem.
               The main idea is to sequentially acquire labeled data by
               presenting an oracle (the user) with unlabeled images that will
               be particularly informative when labeled. Active learning
               adaptively prioritizes the order in which the training examples
               are acquired, which, as shown by our experiments, can
               significantly reduce the overall number of training examples
               required to reach near-optimal performance. At first glance this
               may seem counter-intuitive: how can the algorithm know whether a
               group of unlabeled images will be informative, when, by
               definition, there is no label directly associated with any of
               the images? Our approach is based on choosing an image to label
               that maximizes the expected amount of information we gain about
               the set of unlabeled images. The technique is demonstrated in
               several contexts, including improving the efficiency of Web
               image-search queries and open-world visual learning by an
               autonomous agent. Experiments on a large set of 140 visual
               object categories taken directly from text-based Web image
               searches show that our technique can provide large improvements
               (up to 10 x reduction in the number of training examples needed)
               over baseline techniques.",
  pages     = "1--8",
  month     =  jun,
  year      =  2008,
  keywords  = "entropy;learning (artificial intelligence);object
               recognition;Web image-search queries;autonomous
               agent;entropy-based active learning;object categories;object
               recognition;open-world visual learning;text-based Web
               image;unlabeled images;Autonomous
               agents;Entropy;Histograms;Image
               sampling;Labeling;Laboratories;Object
               recognition;Propulsion;Scattering;Training
               data;image\_classification;classification;assurance\_competence;ai\_reasoning;ai\_learning;assurance\_predictability;supervised\_learning;assurance\_implicit;trust\_informal\_treatment;Mendeley
               Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Joshi2009-ws,
  title     = "Multi-class active learning for image classification",
  booktitle = "2009 {IEEE} Conference on Computer Vision and Pattern
               Recognition",
  author    = "Joshi, A J and Porikli, F and Papanikolopoulos, N",
  abstract  = "One of the principal bottlenecks in applying learning techniques
               to classification problems is the large amount of labeled
               training data required. Especially for images and video,
               providing training data is very expensive in terms of human time
               and effort. In this paper we propose an active learning approach
               to tackle the problem. Instead of passively accepting random
               training examples, the active learning algorithm iteratively
               selects unlabeled examples for the user to label, so that human
               effort is focused on labeling the most ``useful'' examples. Our
               method relies on the idea of uncertainty sampling, in which the
               algorithm selects unlabeled examples that it finds hardest to
               classify. Specifically, we propose an uncertainty measure that
               generalizes margin-based uncertainty to the multi-class case and
               is easy to compute, so that active learning can handle a large
               number of classes and large data sizes efficiently. We
               demonstrate results for letter and digit recognition on datasets
               from the UCI repository, object recognition results on the
               Caltech-101 dataset, and scene categorization results on a
               dataset of 13 natural scene categories. The proposed method
               gives large reductions in the number of training examples
               required over random selection to achieve similar classification
               accuracy, with little computational overhead.",
  pages     = "2372--2379",
  month     =  jun,
  year      =  2009,
  keywords  = "character recognition;image classification;learning (artificial
               intelligence);object recognition;uncertainty handling;digit
               recognition;image classification;labeling examples;letter
               recognition;multi-class active learning;object
               recognition;uncertainty sampling;Humans;Image
               classification;Iterative algorithms;Labeling;Layout;Measurement
               uncertainty;Object recognition;Sampling methods;Size
               measurement;Training
               data;image\_classification;classification;assurance\_predictability;assurance\_competence;ai\_reasoning;ai\_learning;supervised\_learning;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Kapoor2010-cy,
  title     = "Gaussian Processes for Object Categorization",
  author    = "Kapoor, Ashish and Grauman, Kristen and Urtasun, Raquel and
               Darrell, Trevor",
  abstract  = "Discriminative methods for visual object category recognition
               are typically non-probabilistic, predicting class labels but not
               directly providing an estimate of uncertainty. Gaussian
               Processes (GPs) provide a framework for deriving regression
               techniques with explicit uncertainty models; we show here how
               Gaussian Processes with covariance functions defined based on a
               Pyramid Match Kernel (PMK) can be used for probabilistic object
               category recognition. Our probabilistic formulation provides a
               principled way to learn hyperparameters, which we utilize to
               learn an optimal combination of multiple covariance functions.
               It also offers confidence estimates at test points, and
               naturally allows for an active learning paradigm in which points
               are optimally selected for interactive labeling. We show that
               with an appropriate combination of kernels a significant boost
               in classification performance is possible. Further, our
               experiments indicate the utility of active learning with
               probabilistic predictive models, especially when the amount of
               training data labels that may be sought for a category is
               ultimately very small.",
  journal   = "Int. J. Comput. Vis.",
  publisher = "Springer US",
  volume    =  88,
  number    =  2,
  pages     = "169--188",
  month     =  "1~" # jun,
  year      =  2010,
  keywords  = "
               assurance\_competence;assurance\_normality;ai\_reasoning;image\_classification;classification;GPs;assurance\_implicit;trust\_informal\_treatment;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@BOOK{Riley1996-qm,
  title     = "Operator reliance on automation: Theory and data",
  author    = "Riley, Victor",
  abstract  = "until recently, little has been known about what factors
               influence the decision to use or not use automation and what
               types of bias to which this decision may be subject / a better
               understanding of these factors and biases may help system
               developers anticipate the conditions under which operators may
               underrely or overrely on automation and guide the development of
               training methods and user interfaces to encourage rational
               automation use / the decision has been a critical link in the
               chains of events that have led to many incidents and accidents
               in aircraft, railroad, ship, process control, medical, and power
               plant operations an investigation of factors that influence
               automation reliance using a simple computer game (PsycINFO
               Database Record (c) 2016 APA, all rights reserved)",
  publisher = "Lawrence Erlbaum Associates, Inc",
  year      =  1996,
  keywords  = "theory of computer games for investigation of factors that
               influence automation usage patterns \&
               reliance;human\_study;automation;trust\_formal\_treatment;assurance\_implicit;in\_paper;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bainbridge2011-pl,
  title     = "The Benefits of Interactions with Physically Present Robots over
               {Video-Displayed} Agents",
  author    = "Bainbridge, Wilma A and Hart, Justin W and Kim, Elizabeth S and
               Scassellati, Brian",
  abstract  = "This paper explores how a robot’s physical presence affects
               human judgments of the robot as a social partner. For this
               experiment, participants collaborated on simple book-moving
               tasks with a humanoid robot that was either physically present
               or displayed via a live video feed. Multiple tasks individually
               examined the following aspects of social interaction: greetings,
               cooperation, trust, and personal space. Participants readily
               greeted and cooperated with the robot whether present physically
               or in live video display. However, participants were more likely
               both to fulfill an unusual request and to afford greater
               personal space to the robot when it was physically present, than
               when it was shown on live video. The same was true when the live
               video displayed robot’s gestures were augmented with
               disambiguating 3-D information. Questionnaire data support these
               behavioral findings and also show that participants had an
               overall more positive interaction with the physically present
               robot.",
  journal   = "Adv. Robot.",
  publisher = "Springer Netherlands",
  volume    =  3,
  number    =  1,
  pages     = "41--52",
  month     =  "1~" # jan,
  year      =  2011,
  keywords  = "
               human\_study;trust\_formal\_treatment;assurance\_implicit;in\_paper;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@ARTICLE{Corritore2003-gx,
  title    = "On-line trust: concepts, evolving themes, a model",
  author   = "Corritore, Cynthia L and Kracher, Beverly and Wiedenbeck, Susan",
  abstract = "Trust is emerging as a key element of success in the on-line
              environment. Although considerable research on trust in the
              offline world has been performed, to date empirical study of
              on-line trust has been limited. This paper examines on-line
              trust, specifically trust between people and informational or
              transactional websites. It begins by analysing the definitions of
              trust in previous offline and on-line research. The relevant
              dimensions of trust for an on-line context are identified, and a
              definition of trust between people and informational or
              transactional websites is presented. We then turn to an
              examination of the causes of on-line trust. Relevant findings in
              the human--computer interaction literature are identified. A
              model of on-line trust between users and websites is presented.
              The model identifies three perceptual factors that impact on-line
              trust: perception of credibility, ease of use and risk. The model
              is discussed in detail and suggestions for future applications of
              the model are presented.",
  journal  = "Int. J. Hum. Comput. Stud.",
  volume   =  58,
  number   =  6,
  pages    = "737--758",
  year     =  2003,
  keywords = "On-line trust; Trust; Internet trust; User trust; Website trust;
              Credibility; Risk; Ease of
              use;trust\_definition;e-commerce;trust\_model;Mendeley Import
              (Jan 17)/Assurances;Mendeley Import (Jan 17)/Assurances/Trust
              Background"
}

@BOOK{Barber1983-yc,
  title    = "The logic and limits of trust",
  author   = "Barber, Bernard",
  year     =  1983,
  keywords = "Mendeley Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Kaniarasu2012-mo,
  title     = "Potential Measures for Detecting Trust Changes",
  booktitle = "Proceedings of the Seventh Annual {ACM/IEEE} International
               Conference on {Human-Robot} Interaction",
  author    = "Kaniarasu, Poornima and Steinfeld, Aaron and Desai, Munjal and
               Yanco, Holly",
  publisher = "ACM",
  pages     = "241--242",
  series    = "HRI '12",
  year      =  2012,
  address   = "New York, NY, USA",
  keywords  = "automation, experiments,
               trust;human\_study;assurance\_competence;assurance\_predictability;ai\_reasoning;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Rouse1986-dz,
  title    = "Design and evaluation of computer-based decision support systems",
  author   = "Rouse, William B",
  journal  = "Microcomputer decision support systems",
  year     =  1986,
  keywords = "
              decision\_support;assurances;ai\_general;trust\_informal\_treatment;assurance\_explicit;explain;in\_paper;Mendeley
              Import (Jan 17)/Assurances;Mendeley Import (Jan
              17)/Assurances/Trust Background"
}

@ARTICLE{Sheridan1983-nd,
  title    = "Adapting automation to man, culture and society",
  author   = "Sheridan, T B and V{\'a}mos, T and Aida, S",
  abstract = "Anxiety about the effects of automation on workers and society is
              at least 150 years old. The recent explosion of microelectronics
              and robotic applications has sharpened our understanding of both
              the gains and the risks: mismatch to human physiological,
              psychological and cultural characteristics; alienation from
              fulfillment and dignity in work; widening of the gap between
              skilled and unskilled workers and between technologically
              developed and underdeveloped communities; decrement in individual
              security. Attention to these problems can ensure that automation
              results in a better society. The control engineer, who is
              responsible for enlarging the scale of automation, should also
              play a role in adapting it to people. For the time being,
              technology should be individually designed to each culture.",
  journal  = "Automatica",
  volume   =  19,
  number   =  6,
  pages    = "605--612",
  month    =  "1~" # nov,
  year     =  1983,
  keywords = "Automation; social impacts; work; productivity; culture and
              robots;meh..;visionary\_paper;automation;Mendeley Import (Jan
              17)/Assurances;Mendeley Import (Jan 17)/Assurances/Trust
              Background"
}

@ARTICLE{Sheridan1980-px,
  title    = "Computer control and human alienation",
  author   = "Sheridan, Thomas B",
  journal  = "Technol. Rev.",
  volume   =  83,
  number   =  1,
  pages    = "60--73",
  year     =  1980,
  keywords = "automation;meh..;Mendeley Import (Jan 17)/Assurances;Mendeley
              Import (Jan 17)/Assurances/Trust Background"
}

@ARTICLE{Muir1996-gt,
  title    = "Trust in automation. Part {II}. Experimental studies of trust and
              human intervention in a process control simulation",
  author   = "Muir, B M and Moray, N",
  abstract = "Two experiments are reported which examined operators' trust in
              and use of the automation in a simulated supervisory process
              control task. Tests of the integrated model of human trust in
              machines proposed by Muir (1994) showed that models of
              interpersonal trust capture some important aspects of the nature
              and dynamics of human-machine trust. Results showed that
              operators' subjective ratings of trust in the automation were
              based mainly upon their perception of its competence. Trust was
              significantly reduced by any sign of incompetence in the
              automation, even one which had no effect on overall system
              performance. Operators' trust changed very little with
              experience, with a few notable exceptions. Distrust in one
              function of an automatic component spread to reduce trust in
              another function of the same component, but did not generalize to
              another independent automatic component in the same system, or to
              other systems. There was high positive correlation between
              operators' trust in and use of the automation; operators used
              automation they trusted and rejected automation they distrusted,
              preferring to do the control task manually. There was an inverse
              relationship between trust and monitoring of the automation.
              These results suggest that operators' subjective ratings of trust
              and the properties of the automation which determine their trust,
              can be used to predict and optimize the dynamic allocation of
              functions in automated systems.",
  journal  = "Ergonomics",
  volume   =  39,
  number   =  3,
  pages    = "429--460",
  month    =  mar,
  year     =  1996,
  keywords = "
              human\_study;very\_similar\_to\_mine;assurances;automation;trust\_formal\_treatment;assurance\_implicit;Mendeley
              Import (Jan 17)/Assurances/Trust Background",
  language = "en"
}

@ARTICLE{Muir1994-ow,
  title    = "Trust in automation: Part I. Theoretical issues in the study of
              trust and human intervention in automated systems",
  author   = "Muir, Bonnie M",
  abstract = "Abstract Today many systems are highly automated. The human
              operator's role in these systems is to supervise the automation
              and intervene to take manual control when necessary. The
              operator's choice of automatic or manual control has important
              consequences for system performance, and therefore it is
              important to understand and optimize this decision process. One
              important determinant of operators' choice of manual or automatic
              control may be their degree of trust in the automation. However,
              there have been no experimental tests of this hypothesis until
              recently, nor is there a model of human trust in machines to form
              a theoretical foundation for empirical studies. In this paper a
              model of human trust in machines is developed, taking models of
              trust between people as a starting point, and extending them to
              the human-machine relationship. The resulting model defines human
              trust in machines and specifies how trust changes with experience
              on a system, providing a framework for experimental research on
              trust and human intervention in automated systems.",
  journal  = "Ergonomics",
  volume   =  37,
  number   =  11,
  pages    = "1905--1922",
  year     =  1994,
  keywords = "
              trust\_definition;assurances;very\_similar\_to\_mine;automation;trust\_formal\_treatment;assurance\_explicit;in\_paper;Mendeley
              Import (Jan 17)/Assurances/Trust Background;Mendeley Import (Jan
              17)/Assurances"
}

@INPROCEEDINGS{Salem2015-md,
  title     = "Would You Trust a (Faulty) Robot?: Effects of Error, Task Type
               and Personality on {Human-Robot} Cooperation and Trust",
  booktitle = "Proceedings of the Tenth Annual {ACM/IEEE} International
               Conference on {Human-Robot} Interaction",
  author    = "Salem, Maha and Lakatos, Gabriella and Amirabdollahian, Farshid
               and Dautenhahn, Kerstin",
  publisher = "ACM",
  pages     = "141--148",
  series    = "HRI '15",
  year      =  2015,
  address   = "New York, NY, USA",
  keywords  = "cooperation, social human-robot interaction,
               trust;human\_study;ai\_planning;ai\_motion\_manipulation;ai\_interaction;ai\_general;assurance\_competence;assurance\_structural;assurance\_predictability;assurances;trust\_formal\_treatment;assurance\_implicit;in\_paper;Mendeley
               Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Desai2012-rc,
  title     = "Effects of changing reliability on trust of robot systems",
  booktitle = "2012 7th {ACM/IEEE} International Conference on {Human-Robot}
               Interaction ({HRI})",
  author    = "Desai, M and Medvedev, M and V{\'a}zquez, M and McSheehy, S and
               Gadea-Omelchenko, S and Bruggeman, C and Steinfeld, A and Yanco,
               H",
  abstract  = "Prior work in human-autonomy interaction has focused on plant
               systems that operate in highly structured environments. In
               contrast, many human-robot interaction (HRI) tasks are dynamic
               and unstructured, occurring in the open world. It is our belief
               that methods developed for the measurement and modeling of trust
               in traditional automation need alteration in order to be useful
               for HRI. Therefore, it is important to characterize the factors
               in HRI that influence trust. This study focused on the influence
               of changing autonomy reliability. Participants experienced a set
               of challenging robot handling scenarios that forced autonomy use
               and kept them focused on autonomy performance. The
               counterbalanced experiment included scenarios with different low
               reliability windows so that we could examine how drops in
               reliability altered trust and use of autonomy. Drops in
               reliability were shown to affect trust, the frequency and timing
               of autonomy mode switching, as well as participants'
               self-assessments of performance. A regression analysis on a
               number of robot, personal, and scenario factors revealed that
               participants tie trust more strongly to their own actions rather
               than robot performance.",
  pages     = "73--80",
  month     =  mar,
  year      =  2012,
  keywords  = "control engineering computing;human-robot interaction;regression
               analysis;reliability;HRI;autonomy mode switching;autonomy
               performance;changing reliability;human-autonomy
               interaction;human-robot interaction;participant
               self-assessments;regression analysis;reliability windows;robot
               systems;Automation;Reliability;Robot sensing
               systems;Switches;USA Councils;Unified modeling
               language;Trust;automation;experiments;human\_study;ai\_motion\_manipulation;ai\_general;assurances;human-robot
               team;trust\_formal\_treatment;assurance\_implicit;in\_paper;Mendeley
               Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Freedy2007-sg,
  title     = "Measurement of trust in human-robot collaboration",
  booktitle = "2007 International Symposium on Collaborative Technologies and
               Systems",
  author    = "Freedy, A and DeVisser, E and Weltman, G and Coeyman, N",
  abstract  = "We describe a collaborative performance model that captures the
               critical performance attributes of the distinctive human-robotic
               decision and control environment. The literature and our initial
               experimental studies show that the element of trust in
               human-robot collaboration is an extremely important factor in
               the performance model, and accordingly we have focused much of
               our attention on deriving suitable and practical measures of
               this variable. In this paper we describe the formulation of a
               decision-analytical based measure of trust as well as the
               results of two initial experiments designed to examine trust in
               a tactical human-robot collaborative task performed in our new
               mixed initiative team performance assessment system (MITPAS)
               simulation environment.",
  pages     = "106--114",
  month     =  may,
  year      =  2007,
  keywords  = "man-machine systems;robots;collaborative performance
               model;control environment;human-robot collaboration;mixed
               initiative team performance assessment system;trust
               measurement;Adaptation
               model;Automation;Collaboration;Firing;Robot
               kinematics;Robots;Training;Human Robot Collaboration;Human-Robot
               Performance Modeling;Measurement of Trust;Mixed-Initiative
               Systems;human\_study;trust\_definition;human-robot
               team;ai\_motion\_manipulation;assurance\_competence;assurances;very\_similar\_to\_mine;trust\_formal\_treatment;assurance\_implicit;in\_paper;Mendeley
               Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Grimmett2013-gj,
  title     = "Knowing when we don't know: Introspective classification for
               mission-critical decision making",
  booktitle = "2013 {IEEE} International Conference on Robotics and Automation",
  author    = "Grimmett, H and Paul, R and Triebel, R and Posner, I",
  abstract  = "Classification precision and recall have been widely adopted by
               roboticists as canonical metrics to quantify the performance of
               learning algorithms. This paper advocates that for robotics
               applications, which often involve mission-critical decision
               making, good performance according to these standard metrics is
               desirable but insufficient to appropriately characterise system
               performance. We introduce and motivate the importance of a
               classifier's introspective capacity: the ability to mitigate
               potentially overconfident classifications by an appropriate
               assessment of how qualified the system is to make a judgement on
               the current test datum. We provide an intuition as to how this
               introspective capacity can be achieved and systematically
               investigate it in a selection of classification frameworks
               commonly used in robotics: support vector machines, LogitBoost
               classifiers and Gaussian Process classifiers (GPCs). Our
               experiments demonstrate that for common robotics tasks a
               framework such as a GPC exhibits a superior introspective
               capacity while maintaining commensurate classification
               performance to more popular, alternative approaches.",
  pages     = "4531--4538",
  month     =  may,
  year      =  2013,
  keywords  = "Gaussian processes;decision making;image classification;robot
               vision;support vector machines;GPC;Gaussian process
               classifiers;LogitBoost classifiers;introspective
               capacity;introspective classification;mission-critical decision
               making;potentially overconfident classification
               mitigation;robotics tasks;support vector machines;Educational
               institutions;introspection;classification;image\_classification;ai\_reasoning;ai\_learning;assurance\_competence;assurance\_normality;Safety\_AI;robotics;GPs;supervised\_learning;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Triebel2013-ku,
  title           = "Confidence Boosting: Improving the Introspectiveness of a
                     Boosted Classifier for Efficient Learning",
  booktitle       = "Workshop on Autonomous Learing.",
  author          = "Triebel, Rudolph and Grimmett, Hugo and Posner, Ingmar",
  month           =  may,
  year            =  2013,
  keywords        = "
                     introspection;ai\_reasoning;ai\_learning;assurance\_competence;classification;image\_classification;Safety\_AI;robotics;supervised\_learning;trust\_informal\_treatment;assurance\_implicit;Mendeley
                     Import (Jan 17)/Assurances",
  conference      = "IEEE International Conference on Robotics and Automation
                     (ICRA)"
}

@INPROCEEDINGS{Triebel2013-ow,
  title     = "Introspective active learning for scalable semantic mapping",
  booktitle = "Workshop. Robotics Science and Systems ({RSS})",
  author    = "Triebel, Rudolph and Grimmett, Hugo and Paul, Rohan and Posner,
               Ingmar",
  pages     = "809--816",
  year      =  2013,
  keywords  = "
               introspection;Safety\_AI;robotics;image\_classification;ai\_learning;assurance\_competence;GPs;supervised\_learning;assurance\_implicit;trust\_informal\_treatment;Mendeley
               Import (Jan 17)/Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Triebel2016-kj,
  title     = "Driven Learning for Driving: How Introspection Improves Semantic
               Mapping",
  booktitle = "Robotics Research",
  author    = "Triebel, Rudolph and Grimmett, Hugo and Paul, Rohan and Posner,
               Ingmar",
  editor    = "Inaba, Masayuki and Corke, Peter",
  abstract  = "This paper explores the suitability of commonly employed
               classification methods to action-selection tasks in robotics,
               and argues that a classifier’s introspective capacity is a vital
               but as yet largely under-appreciated attribute. As illustration
               we propose an active learning framework for semantic mapping in
               mobile robotics and demonstrate it in the context of autonomous
               driving. In this framework, data are selected for label
               disambiguation by a human supervisor using uncertainty sampling.
               Intuitively, an introspective classification framework---i.e.
               one which moderates its predictions by an estimate of how well
               it is placed to make a call in a particular situation---is
               particularly well suited to this task. To achieve an efficient
               implementation we extend the notion of introspection to a
               particular sparse Gaussian Process Classifier, the Informative
               Vector Machine (IVM). Furthermore, we leverage the
               information-theoretic nature of the IVM to formulate a
               principled mechanism for forgetting stale data, thereby bounding
               memory use and resulting in a truly life-long learning system.
               Our evaluation on a publicly available dataset shows that an
               introspective active learner asks more informative questions
               compared to a more traditional non-introspective approach like a
               Support Vector Machine (SVM) and in so doing, outperforms the
               SVM in terms of learning rate while retaining efficiency for
               practical use.",
  publisher = "Springer International Publishing",
  pages     = "449--465",
  series    = "Springer Tracts in Advanced Robotics",
  year      =  2016,
  keywords  = "
               introspection;Safety\_AI;robotics;image\_classification;ai\_learning;assurance\_competence;GPs;supervised\_learning;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@INPROCEEDINGS{Berczi2015-rd,
  title     = "Learning to assess terrain from human demonstration using an
               introspective Gaussian-process classifier",
  booktitle = "2015 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Berczi, L P and Posner, I and Barfoot, T D",
  abstract  = "This paper presents an approach to learning robot terrain
               assessment from human demonstration. An operator drives a robot
               for a short period of time, supervising the gathering of
               traversable and untraversable terrain data. After this initial
               training period, the robot can then predict the traversability
               of new terrain based on its experiences. We improve on current
               methods in two ways: first, we maintain a richer
               (higher-dimensional) representation of the terrain that is
               better able to distinguish between different training examples.
               Second, we use a Gaussian-process classifier for terrain
               assessment due to its superior introspective abilities (leading
               to better uncertainty estimates) when compared to other
               classifier methods in the literature. Our method is tested on
               real data and shown to outperform current methods both in
               classification accuracy and uncertainty estimation.",
  pages     = "3178--3185",
  month     =  may,
  year      =  2015,
  keywords  = "Gaussian processes;image classification;learning (artificial
               intelligence);mobile robots;robot vision;terrain mapping;human
               demonstration;initial training period;introspective
               Gaussian-process classifier;introspective abilities;mobile
               robots;robot terrain assessment learning;terrain
               representation;traversability prediction;traversable terrain
               data gathering;untraversable terrain data
               gathering;Labeling;Learning systems;Robot sensing
               systems;Training;Uncertainty;introspection;robotics;ai\_perception;ai\_learning;supervised\_learning;image\_classification;assurance\_competence;GPs;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Grimmett2016-yc,
  title    = "Introspective classification for robot perception",
  author   = "Grimmett, Hugo and Triebel, Rudolph and Paul, Rohan and Posner,
              Ingmar",
  abstract = "In robotics, the use of a classification framework which produces
              scores with inappropriate confidences will ultimately lead to the
              robot making dangerous decisions. In order to select a framework
              which will make the best decisions, we should pay careful
              attention to the ways in which it generates scores. Precision and
              recall have been widely adopted as canonical metrics to quantify
              the performance of learning algorithms, but for robotics
              applications involving mission-critical decision making, good
              performance in relation to these metrics is insufficient. We
              introduce and motivate the importance of a classifier’s
              introspective capacity: the ability to associate an appropriate
              assessment of confidence with any test case. We propose that a
              key ingredient for introspection is a framework’s potential to
              increase its uncertainty with the distance between a test datum
              its training data. We compare the introspective capacities of a
              number of commonly used classification frameworks in both
              classification and detection tasks, and show that better
              introspection leads to improved decision making in the context of
              tasks such as autonomous driving or semantic map generation.",
  journal  = "Int. J. Rob. Res.",
  volume   =  35,
  number   =  7,
  pages    = "743--762",
  year     =  2016,
  keywords = "
              introspection;Safety\_AI;image\_classification;assurance\_competence;ai\_learning;ai\_reasoning;ai\_knowledge\_rep;robotics;GPs;supervised\_learning;assurance\_implicit;trust\_informal\_treatment;Mendeley
              Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Dequaire2016-kh,
  title     = "Off the beaten track: Predicting localisation performance in
               visual teach and repeat",
  booktitle = "2016 {IEEE} International Conference on Robotics and Automation
               ({ICRA})",
  author    = "Dequaire, J and Tong, C H and Churchill, W and Posner, I",
  abstract  = "This paper proposes an appearance-based approach to estimating
               localisation performance in the context of visual teach and
               repeat. Specifically, it aims to estimate the likely corridor
               around a taught trajectory within which a vision-based
               localisation system is still able to localise itself. In
               contrast to prior art, our system is able to predict this
               localisation envelope for trajectories in similar, yet
               geographically distant locations where no repeat runs have yet
               been performed. Thus, by characterising the localisation
               performance in one region, we are able to predict performance in
               another. To achieve this, we leverage a Gaussian Process
               regressor to estimate the likely number of feature matches for
               any keyframe in the teach run, based on a combination of
               trajectory properties such as curvature and an appearance model
               of the keyframe. Using data from real traversals, we demonstrate
               that our approach performs as well as prior art when it comes to
               interpolating localisation performance based on a number of
               repeat runs, while also performing well at generalising
               performance estimation to freshly taught trajectories.",
  pages     = "795--800",
  month     =  may,
  year      =  2016,
  keywords  = "Gaussian processes;interpolation;robot vision;Gaussian process
               regressor;appearance-based approach;interpolation;vision-based
               localisation;vision-based localisation system;Cameras;Gaussian
               processes;Planning;Robots;Training;Trajectory;Visualization;ai\_planning;ai\_learning;ai\_motion\_manipulation;robotics;image\_classification;assurance\_competence;ai\_perception;GPs;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Gurau2016-hs,
  title           = "Fit for Purpose? Predicting Perception Performance Based
                     on Past Experience",
  booktitle       = "2016 International Symposium on Experimental Robotics",
  author          = "Gur{\u a}u, Corina and Tong, Chi Hay and Posner, Ingmar",
  abstract        = "This paper explores the idea of predicting the likely
                     performance of a robot’s perception system based on past
                     experience in the same workspace. In particular, we
                     propose to build a place-specific model of perception
                     performance from observations gathered over time. We
                     evaluate our method in a classical decision making
                     scenario in which the robot must choose when and where to
                     drive autonomously in 60 km of driving data from an urban
                     environment. We demonstrate that leveraging visual
                     appearance within a state-of-the-art navigation framework
                     increases the accuracy of our performance predictions.",
  publisher       = "Springer, Cham",
  pages           = "454--464",
  month           =  "3~" # oct,
  year            =  2016,
  keywords        = "ai\_reasoning;ai\_perception;human-robot
                     team;assurance\_competence;GPs;supervised\_learning;trust\_informal\_treatment;assurance\_explicit;classification;perf\_prediction;Mendeley
                     Import (Jan 17)/Assurances",
  language        = "en",
  conference      = "International Symposium on Experimental Robotics"
}

@ARTICLE{Morrison2011-yf,
  title         = "Optimal Data Split Methodology for Model Validation",
  author        = "Morrison, Rebecca and Bryant, Corey and Terejanu, Gabriel
                   and Miki, Kenji and Prudhomme, Serge",
  abstract      = "The decision to incorporate cross-validation into validation
                   processes of mathematical models raises an immediate
                   question - how should one partition the data into
                   calibration and validation sets? We answer this question
                   systematically: we present an algorithm to find the optimal
                   partition of the data subject to certain constraints. While
                   doing this, we address two critical issues: 1) that the
                   model be evaluated with respect to predictions of a given
                   quantity of interest and its ability to reproduce the data,
                   and 2) that the model be highly challenged by the validation
                   set, assuming it is properly informed by the calibration
                   set. This framework also relies on the interaction between
                   the experimentalist and/or modeler, who understand the
                   physical system and the limitations of the model; the
                   decision-maker, who understands and can quantify the cost of
                   model failure; and the computational scientists, who strive
                   to determine if the model satisfies both the modeler's and
                   decision maker's requirements. We also note that our
                   framework is quite general, and may be applied to a wide
                   range of problems. Here, we illustrate it through a specific
                   example involving a data reduction model for an ICCD camera
                   from a shock-tube experiment located at the NASA Ames
                   Research Center (ARC).",
  month         =  "30~" # aug,
  year          =  2011,
  keywords      = "NotRead",
  archivePrefix = "arXiv",
  primaryClass  = "physics.data-an",
  eprint        = "1108.6043"
}

@ARTICLE{Morrison2016-fz,
  title         = "Representing model inadequacy: A stochastic operator
                   approach",
  author        = "Morrison, Rebecca E and Oliver, Todd A and Moser, Robert D",
  abstract      = "Mathematical models of physical systems are subject to many
                   uncertainties such as measurement errors and uncertain
                   initial and boundary conditions. After accounting for these
                   uncertainties, it is often revealed that discrepancies
                   between the model output and the observations remain; if so,
                   the model is said to be inadequate. In practice, the
                   inadequate model may be the best that is available or
                   tractable, and so despite its inadequacy the model may be
                   used to make predictions of unobserved quantities. In this
                   case, a representation of the inadequacy is necessary, so
                   the impact of the observed discrepancy can be determined. We
                   investigate this problem in the context of chemical kinetics
                   and propose a new technique to account for model inadequacy
                   that is both probabilistic and physically meaningful. A
                   stochastic inadequacy operator $\mathcal\{S\}$ is introduced
                   which is embedded in the ODEs describing the evolution of
                   chemical species concentrations and which respects certain
                   physical constraints such as conservation laws. The
                   parameters of $\mathcal\{S\}$ are governed by probability
                   distributions, which in turn are characterized by a set of
                   hyperparameters. The model parameters and hyperparameters
                   are calibrated using high-dimensional hierarchical Bayesian
                   inference. We apply the method to a typical problem in
                   chemical kinetics---the reaction mechanism of hydrogen
                   combustion.",
  month         =  "6~" # apr,
  year          =  2016,
  keywords      = "
                   assurance\_competence;reactions;ai\_learning;assurance\_predictability;trust\_informal\_treatment;assurance\_explicit;interp\_models;Mendeley
                   Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CE",
  eprint        = "1604.01651"
}

@INPROCEEDINGS{Eggensperger2013-ho,
  title     = "Towards an Empirical Foundation for Assessing Bayesian
               Optimization of Hyperparameters",
  booktitle = "{NIPS} workshop on Bayesian Optimization in Theory and Practice",
  author    = "Eggensperger, Katharina and Feurer, Matthias and Hutter, Frank
               and Bergstra, James and Snoek, Jasper and Hoos, Holger H and
               Leyton-Brown, Kevin",
  abstract  = "Progress in practical Bayesian optimization is hampered by the
               fact that the only available standard benchmarks are artificial
               test functions that are not representative of practical
               applications. To alleviate this problem, we introduce a library
               of benchmarks from the prominent application of hyperparameter
               optimization and use it to compare Spearmint, TPE, and SMAC,
               three recent Bayesian optimization methods for hyperparameter
               optimization.",
  pages     = "1--5",
  year      =  2013,
  keywords  = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;Mendeley Import
               (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters"
}

@ARTICLE{Bradshaw2013-ck,
  title     = "The Seven Deadly Myths of`` Autonomous Systems''",
  author    = "Bradshaw, Jeffrey M and Hoffman, Robert R and Woods, David D and
               Johnson, Matthew",
  journal   = "IEEE Intell. Syst.",
  publisher = "IEEE",
  volume    =  28,
  number    =  3,
  pages     = "54--61",
  year      =  2013,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances;Mendeley Import (Jan
               17)/Assurances/Trust Background"
}

@BOOK{Mittu2016-ia,
  title     = "Robust Intelligence and Trust in Autonomous Systems:",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  publisher = "Springer US",
  year      =  2016,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@BOOK{Luger2008-vf,
  title     = "Artificial Intelligence: Structures and Strategies for Complex
               Problem Solving",
  author    = "Luger, George F",
  publisher = "Addison-Wesley Publishing Company",
  edition   = "6th",
  year      =  2008,
  address   = "USA",
  keywords  = "AI;Textbook;Mendeley Import (Jan 17)/TextBooks"
}

@BOOK{Nilsson2009-rp,
  title     = "The Quest for Artificial Intelligence",
  author    = "Nilsson, Nils J",
  abstract  = "Artificial intelligence (AI) is a field within computer science
               that is attempting to build enhanced intelligence into computer
               systems. This book traces the history of the subject, from the
               early dreams of eighteenth-century (and earlier) pioneers to the
               more successful work of today's AI engineers. AI is becoming
               more and more a part of everyone's life. The technology is
               already embedded in face-recognizing cameras, speech-recognition
               software, Internet search engines, and health-care robots, among
               other applications. The book's many diagrams and
               easy-to-understand descriptions of AI programs will help the
               casual reader gain an understanding of how these and other AI
               systems actually work. Its thorough (but unobtrusive)
               end-of-chapter notes containing citations to important source
               materials will be of great use to AI scholars and researchers.
               This book promises to be the definitive history of a field that
               has captivated the imaginations of scientists, philosophers, and
               writers for centuries.",
  publisher = "Cambridge University Press",
  month     =  "30~" # oct,
  year      =  2009,
  keywords  = "AI;Textbook;Mendeley Import (Jan 17)/TextBooks",
  language  = "en"
}

@BOOK{Poole2010-pv,
  title     = "Artificial Intelligence: Foundations of Computational Agents",
  author    = "Poole, David L and Mackworth, Alan K",
  abstract  = "Recent decades have witnessed the emergence of artificial
               intelligence as a serious science and engineering discipline.
               This textbook, aimed at junior to senior undergraduate students
               and first-year graduate students, presents artificial
               intelligence (AI) using a coherent framework to study the design
               of intelligent computational agents. By showing how basic
               approaches fit into a multidimensional design space, readers can
               learn the fundamentals without losing sight of the bigger
               picture. The book balances theory and experiment, showing how to
               link them intimately together, and develops the science of AI
               together with its engineering applications. Although structured
               as a textbook, the book's straightforward, self-contained style
               will also appeal to a wide audience of professionals,
               researchers, and independent learners. AI is a rapidly
               developing field: this book encapsulates the latest results
               without being exhaustive and encyclopedic. The text is supported
               by an online learning environment, AIspace, http://aispace.org,
               so that students can experiment with the main AI algorithms plus
               problems, animations, lecture slides, and a knowledge
               representation system, AIlog, for experimentation and problem
               solving.",
  publisher = "Cambridge University Press",
  month     =  "19~" # apr,
  year      =  2010,
  keywords  = "AI;Textbook;Mendeley Import (Jan 17)/TextBooks",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Russell2010-wv,
  title     = "Artificial Intelligence: A Modern Approach",
  author    = "Russell, Stuart Jonathan and Norvig, Peter",
  abstract  = "Artificial Intelligence: A Modern Approach, 3e offers the most
               comprehensive, up-to-date introduction to the theory and
               practice of artificial intelligence. Number one in its field,
               this textbook is ideal for one or two-semester, undergraduate or
               graduate-level courses in Artificial Intelligence. Dr. Peter
               Norvig, contributing Artificial Intelligence author and
               Professor Sebastian Thrun, a Pearson author are offering a free
               online course at Stanford University on artificial intelligence.
               According to an article in The New York Times , the course on
               artificial intelligence is ``one of three being offered
               experimentally by the Stanford computer science department to
               extend technology knowledge and skills beyond this elite campus
               to the entire world.'' One of the other two courses, an
               introduction to database software, is being taught by Pearson
               author Dr. Jennifer Widom. Artificial Intelligence: A Modern
               Approach, 3e is available to purchase as an eText for your
               Kindle™, NOOK™, and the
               iPhone\textregistered{}/iPad\textregistered{}. To learn more
               about the course on artificial intelligence, visit
               http://www.ai-class.com. To read the full New York Times
               article, click here.",
  publisher = "Prentice Hall",
  series    = "Artificial Intelligence",
  edition   = "Third",
  year      =  2010,
  keywords  = "AI;Textbook;Mendeley Import (Jan 17)/TextBooks",
  language  = "en"
}

@INPROCEEDINGS{Franklin1996-nx,
  title       = "Is it an Agent, or just a Program?: A Taxonomy for Autonomous
                 Agents",
  booktitle   = "International Workshop on Agent Theories, Architectures, and
                 Languages",
  author      = "Franklin, Stan and Graesser, Art",
  abstract    = "Abstract The advent of software agents gave rise to much
                 discussion of just what such an agent is, and of how they
                 differ from programs in general. Here we propose a formal
                 definition of an autonomous agent which clearly distinguishes
                 a software agent from just any",
  publisher   = "Springer",
  pages       = "21--35",
  institution = "Springer",
  year        =  1996,
  keywords    = "AI;visionary\_paper"
}

@ARTICLE{Montague1999-ns,
  title     = "Reinforcement Learning: An Introduction, by Sutton, {R.S}. and
               Barto, {A.G}",
  author    = "Montague, P Read",
  abstract  = "The present book is an excellent entry point for someone who
               wants to understand intuitively the ideas of reinforcement
               learning and the general connection between its parts. It is
               not, however, a mathematical 'how-to'book, replete with proofs
               and pointers to unsolved",
  journal   = "Trends Cogn. Sci.",
  publisher = "Elsevier",
  volume    =  3,
  number    =  9,
  pages     = "360",
  month     =  "1~" # sep,
  year      =  1999,
  keywords  = "Neuroscience; Reward; Dynamic programming; Reinforcement
               learning; Computational learning;AI",
  language  = "en"
}

@ARTICLE{Wooldridge1995-sh,
  title     = "Intelligent agents: Theory and practice",
  author    = "Wooldridge, Michael and Jennings, Nicholas R",
  abstract  = "Abstract The concept of an agent has become important in both
               artificial intelligence (AT) and mainstream computer science.
               Our aim in this paper is to point the reader at what we perceive
               to be the most important theoretical and practical issues
               associated with the design",
  journal   = "Knowl. Eng. Rev.",
  publisher = "Cambridge Univ Press",
  volume    =  10,
  number    =  02,
  pages     = "115--152",
  year      =  1995,
  keywords  = "AI;NotRead"
}

@INPROCEEDINGS{Da_Veiga2012-gh,
  title     = "Gaussian process modeling with inequality constraints",
  booktitle = "Annales de la Facult{\'e} des Sciences de Toulouse",
  author    = "Da Veiga, S{\'e}bastien and Marrel, Amandine",
  volume    =  21,
  pages     = "529--555",
  year      =  2012,
  keywords  = "
               assurance\_predictability;ai\_knowledge\_rep;ai\_reasoning;GPs;assurance\_normality;assurance\_implicit;trust\_informal\_treatment;Mendeley
               Import (Jan 17)/BayesOpt;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Tallis1961-ar,
  title     = "The Moment Generating Function of the Truncated Multi-normal
               Distribution",
  author    = "Tallis, G M",
  abstract  = "In this paper the moment generating function (m.g.f.) of the
               truncated n-dimensional normal distribution is obtained. From
               the m.g.f., formulae for E(Xi) and E(Xi Xj) are derived, and are
               used to investigate certain special cases. Some applications of
               these results to statistical genetics are also discussed.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "[Royal Statistical Society, Wiley]",
  volume    =  23,
  number    =  1,
  pages     = "223--229",
  year      =  1961,
  keywords  = "Mendeley Import (Jan 17)/BayesOpt"
}

@ARTICLE{Alcala-Fdez2009-fu,
  title     = "{KEEL}: a software tool to assess evolutionary algorithms for
               data mining problems",
  author    = "Alcal{\'a}-Fdez, J and S{\'a}nchez, L and Garc{\'\i}a, S and del
               Jesus, M J and Ventura, S and Garrell, J M and Otero, J and
               Romero, C and Bacardit, J and Rivas, V M and Fern{\'a}ndez, J C
               and Herrera, F",
  abstract  = "This paper introduces a software tool named KEEL which is a
               software tool to assess evolutionary algorithms for Data Mining
               problems of various kinds including as regression,
               classification, unsupervised learning, etc. It includes
               evolutionary learning algorithms based on different approaches:
               Pittsburgh, Michigan and IRL, as well as the integration of
               evolutionary learning techniques with different pre-processing
               techniques, allowing it to perform a complete analysis of any
               learning model in comparison to existing software tools.
               Moreover, KEEL has been designed with a double goal: research
               and educational.",
  journal   = "Soft Comput",
  publisher = "Springer-Verlag",
  volume    =  13,
  number    =  3,
  pages     = "307--318",
  month     =  "1~" # feb,
  year      =  2009,
  keywords  = "meh..",
  language  = "en"
}

@ARTICLE{Guyon2003-fj,
  title     = "An Introduction to Variable and Feature Selection",
  author    = "Guyon, Isabelle and Elisseeff, Andr{\'e}",
  abstract  = "Abstract Variable and feature selection have become the focus of
               much research in areas of application for which datasets with
               tens or hundreds of thousands of variables are available. These
               areas include text processing of internet documents, gene
               expression array analysis, and combinatorial chemistry. The
               objective of variable selection is three-fold: improving the
               prediction performance of the predictors, providing faster and
               more cost-effective ...",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  volume    =  3,
  number    = "Mar",
  pages     = "1157--1182",
  year      =  2003,
  keywords  = "
               interpretability;assurances;ai\_learning;ai\_reasoning;assurance\_implicit;trust\_informal\_treatment;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Lewicki1995-sx,
  title     = "Trust in relationships",
  author    = "Lewicki, Roy J and Bunker, Barbara Benedict",
  abstract  = "Roy J. Lewicki, Barbara Benedict Bunker s trust easier to
               destroy than it is to build? The presumption that the answer to
               this question is a resounding yes has been prevalent in
               cooperation and conflicr management research for over thirty
               years. Observation of the",
  journal   = "Adm. Sci. Q.",
  publisher = "researchgate.net",
  volume    =  5,
  pages     = "583--601",
  year      =  1995,
  keywords  = "trust\_definition;business;Mendeley Import (Jan
               17)/Assurances/Trust Background;Mendeley Import (Jan
               17)/Assurances"
}

@ARTICLE{McKnight2002-qx,
  title     = "Developing and Validating Trust Measures for e-Commerce: An
               Integrative Typology",
  author    = "McKnight, D Harrison and Choudhury, Vivek and Kacmar, Charles",
  abstract  = "Evidence suggests that consumers often hesitate to transact with
               Web-based vendors because of uncertainty about vendor behavior
               or the perceived risk of having personal information stolen by
               hackers. Trust plays a central role in helping consumers
               overcome perceptions of risk and insecurity. Trust makes
               consumers comfortable sharing personal information, making
               purchases, and acting on Web vendor advice---behaviors essential
               to widespread adoption of e-commerce. Therefore, trust is
               critical to both researchers and practitioners. Prior research
               on e-commerce trust has used diverse, incomplete, and
               inconsistent definitions of trust, making it difficult to
               compare results across studies. This paper contributes by
               proposing and validating measures for a multidisciplinary,
               multidimensional model of trust in e-commerce. The model
               includes four high-level constructs---disposition to trust,
               institution-based trust, trusting beliefs, and trusting
               intentions---which are further delineated into 16 measurable,
               literature-grounded subconstructs. The psychometric properties
               of the measures are demonstrated through use of a hypothetical,
               legal advice Web site. The results show that trust is indeed a
               multidimensional concept. Proposed relationships among the trust
               constructs are tested (for internal nomological validity), as
               are relationships between the trust constructs and three other
               e-commerce constructs (for external nomological validity), as
               Web experience, personal innovativeness, and Web site quality.
               Suggestions for future research as well as implications for
               practice are discussed.",
  journal   = "Information Systems Research",
  publisher = "pubsonline.informs.org",
  volume    =  13,
  number    =  3,
  pages     = "334--359",
  year      =  2002,
  keywords  = "trust\_definition;e-commerce;Mendeley Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Van_Belle2013-ph,
  title     = "Research directions in interpretable machine learning models",
  booktitle = "{ESANN}",
  author    = "Van Belle, Vanya and Lisboa, Paulo",
  year      =  2013,
  keywords  = "
               interpretability;trust\_informal\_treatment;assurance\_explicit;interp\_models;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Van_Belle2012-dt,
  title       = "A mathematical model for interpretable clinical decision
                 support with applications in gynecology",
  author      = "Van Belle, Vanya M C A and Van Calster, Ben and Timmerman,
                 Dirk and Bourne, Tom and Bottomley, Cecilia and Valentin, Lil
                 and Neven, Patrick and Van Huffel, Sabine and Suykens, Johan A
                 K and Boyd, Stephen",
  affiliation = "Department of Electrical Engineering (ESAT-SCD), Katholieke
                 Universiteit Leuven, Leuven, Belgium.
                 vanya.vanbelle@esat.kuleuven.be",
  abstract    = "BACKGROUND: Over time, methods for the development of clinical
                 decision support (CDS) systems have evolved from interpretable
                 and easy-to-use scoring systems to very complex and
                 non-interpretable mathematical models. In order to accomplish
                 effective decision support, CDS systems should provide
                 information on how the model arrives at a certain decision. To
                 address the issue of incompatibility between performance,
                 interpretability and applicability of CDS systems, this paper
                 proposes an innovative model structure, automatically leading
                 to interpretable and easily applicable models. The resulting
                 models can be used to guide clinicians when deciding upon the
                 appropriate treatment, estimating patient-specific risks and
                 to improve communication with patients. METHODS AND FINDINGS:
                 We propose the interval coded scoring (ICS) system, which
                 imposes that the effect of each variable on the estimated risk
                 is constant within consecutive intervals. The number and
                 position of the intervals are automatically obtained by
                 solving an optimization problem, which additionally performs
                 variable selection. The resulting model can be visualised by
                 means of appealing scoring tables and color bars. ICS models
                 can be used within software packages, in smartphone
                 applications, or on paper, which is particularly useful for
                 bedside medicine and home-monitoring. The ICS approach is
                 illustrated on two gynecological problems: diagnosis of
                 malignancy of ovarian tumors using a dataset containing 3,511
                 patients, and prediction of first trimester viability of
                 pregnancies using a dataset of 1,435 women. Comparison of the
                 performance of the ICS approach with a range of prediction
                 models proposed in the literature illustrates the ability of
                 ICS to combine optimal performance with the interpretability
                 of simple scoring systems. CONCLUSIONS: The ICS approach can
                 improve patient-clinician communication and will provide
                 additional insights in the importance and influence of
                 available variables. Future challenges include extensions of
                 the proposed methodology towards automated detection of
                 interaction effects, multi-class decision support systems,
                 prognosis and high-dimensional data.",
  journal     = "PLoS One",
  volume      =  7,
  number      =  3,
  pages       = "e34312",
  month       =  "29~" # mar,
  year        =  2012,
  keywords    = "
                 interpretability;trust\_informal\_treatment;assurance\_explicit;classification;interp\_models;review;Mendeley
                 Import (Jan 17)/Assurances",
  language    = "en"
}

@ARTICLE{Freitas2006-qo,
  title     = "Are we really discovering interesting knowledge from data",
  author    = "Freitas, Alex A",
  journal   = "Expert Update (the BCS-SGAI magazine)",
  publisher = "The British Computer Society",
  volume    =  9,
  number    =  1,
  pages     = "41--47",
  year      =  2006,
  keywords  = "
               interpretability;assurance\_explicit;human\_study;trust\_informal\_treatment;human\_involved;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Sugiyama2013-ci,
  title     = "Learning under nonstationarity: covariate shift and
               class-balance change",
  author    = "Sugiyama, Masashi and Yamada, Makoto and du Plessis, Marthinus
               Christoffel",
  abstract  = "The goal of supervised learning such as regression and
               classification is to learn an input-- output dependency from
               input--output paired training samples so that test output y for
               unseen test input x can be accurately estimated. Various
               supervised learning algorithms were developed thus far, and they
               have been demonstrated to be useful in a wide range of
               applications. Most of the popular machine-learning algorithms
               assume that training and ...",
  journal   = "Wiley Interdiscip. Rev. Comput. Stat.",
  publisher = "Wiley Online Library",
  volume    =  5,
  number    =  6,
  pages     = "465--477",
  year      =  2013,
  keywords  = "
               Safety\_AI;supervised\_learning;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances"
}

@BOOK{Sugiyama2012-mo,
  title     = "Machine Learning in Non-stationary Environments: Introduction to
               Covariate Shift Adaptation",
  author    = "Sugiyama, Masashi and Kawanabe, Motoaki",
  abstract  = "As the power of computing has grown over the past few decades,
               the field of machine learning has advanced rapidly in both
               theory and practice. Machine learning methods are usually based
               on the assumption that the data generation mechanism does not
               change over time. Yet real-world applications of machine
               learning, including image recognition, natural language
               processing, speech recognition, robot control, and
               bioinformatics, often violate this common assumption. Dealing
               with non-stationarity is one of modern machine learning's
               greatest challenges. This book focuses on a specific
               non-stationary environment known as covariate shift, in which
               the distributions of inputs (queries) change but the conditional
               distribution of outputs (answers) is unchanged, and presents
               machine learning theory, algorithms, and applications to
               overcome this variety of non-stationarity. After reviewing the
               state-of-the-art research in the field, the authors discuss
               topics that include learning under covariate shift, model
               selection, importance estimation, and active learning. They
               describe such real world applications of covariate shift
               adaption as brain-computer interface, speaker identification,
               and age prediction from facial images. With this book, they aim
               to encourage future research in machine learning, statistics,
               and engineering that strives to create truly autonomous learning
               machines able to learn under non-stationarity.",
  publisher = "MIT Press",
  year      =  2012,
  keywords  = "Safety\_AI;NotRead;Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

@ARTICLE{Garcia2015-rs,
  title     = "A comprehensive survey on safe reinforcement learning",
  author    = "Garc{\i}a, J and Fern{\'a}ndez, F",
  abstract  = "Abstract Safe Reinforcement Learning can be defined as the
               process of learning policies that maximize the expectation of
               the return in problems in which it is important to ensure
               reasonable system performance and/or respect safety constraints
               during the learning and/or",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  year      =  2015,
  keywords  = "
               Safety\_AI;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Zycinski2012-jj,
  title       = "Discriminant functional gene groups identification with
                 machine learning and prior knowledge",
  booktitle   = "{ESANN}",
  author      = "Zycinski, Grzegorz and Squillario, Margherita and Barla,
                 Annalisa and Sanavia, Tiziana and Verri, Alessandro and Di
                 Camillo, Barbara",
  institution = "Citeseer",
  year        =  2012,
  keywords    = "
                 interpretability;trust\_informal\_treatment;assurance\_explicit;interp\_models;Mendeley
                 Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Kastner2012-yu,
  title     = "Integration of Structural Expert Knowledge about Classes for
               Classification Using the Fuzzy Supervised Neural Gas",
  booktitle = "{ESANN}",
  author    = "K{\"a}stner, Marika and Hermann, Wieland and Villmann, Thomas
               and Mittweida, Saxonia-Germany and Zwickau, Saxonia-Germany",
  year      =  2012,
  keywords  = "in\_table"
}

@BOOK{Venna2007-yj,
  title     = "Dimensionality reduction for visual exploration of similarity
               structures",
  author    = "Venna, Jarkko",
  publisher = "Helsinki University of Technology",
  year      =  2007,
  keywords  = "
               interpretability;trust\_informal\_treatment;assurance\_explicit;classification;vis\_dr;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Park2016-ld,
  title         = "{ACDC}: {$\alpha$-Carving} Decision Chain for Risk
                   Stratification",
  author        = "Park, Yubin and Ho, Joyce and Ghosh, Joydeep",
  abstract      = "In many healthcare settings, intuitive decision rules for
                   risk stratification can help effective hospital resource
                   allocation. This paper introduces a novel variant of
                   decision tree algorithms that produces a chain of decisions,
                   not a general tree. Our algorithm, $\alpha$-Carving Decision
                   Chain (ACDC), sequentially carves out ``pure'' subsets of
                   the majority class examples. The resulting chain of decision
                   rules yields a pure subset of the minority class examples.
                   Our approach is particularly effective in exploring large
                   and class-imbalanced health datasets. Moreover, ACDC
                   provides an interactive interpretation in conjunction with
                   visual performance metrics such as Receiver Operating
                   Characteristics curve and Lift chart.",
  month         =  "16~" # jun,
  year          =  2016,
  keywords      = "
                   medical;assurance\_predictability;ai\_knowledge\_rep;ai\_learning;ai\_perception;ai\_motion\_manipulation;ai\_interaction;decision\_tree;trust\_informal\_treatment;assurance\_explicit;interp\_models;Mendeley
                   Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1606.05325"
}

@INPROCEEDINGS{Turner2016-jq,
  title     = "A model explanation system",
  booktitle = "2016 {IEEE} 26th International Workshop on Machine Learning for
               Signal Processing ({MLSP})",
  author    = "Turner, R",
  abstract  = "We propose a new methodology for explaining the predictions of
               black box classifiers. We use the motivating paradigm that
               predictive performance is of primary importance but human
               analysts (e.g., in fraud detection) desire a classifier's
               predictions to be augmented with useful explanations. To be
               truly general and principled, we derive a scoring system for
               finding explanations based on formal requirements. In this
               system, the explanations are assumed to take the form of simple
               logical statements. We derive an efficient Monte Carlo algorithm
               to find explanations for black box classifiers with finite
               sample guarantees. The methodology is then applied to
               interesting examples in facial recognition and credit data.",
  publisher = "ieeexplore.ieee.org",
  pages     = "1--6",
  year      =  2016,
  keywords  = "Monte Carlo methods;pattern classification;Monte Carlo
               algorithm;black box classifiers;classifier predictions;credit
               data;facial recognition;finite sample guarantees;formal
               requirements;logical statements;model explanation
               system;predictive performance;scoring system;Computational
               modeling;Credit cards;Data models;Decision trees;Face
               recognition;History;Monte Carlo
               methods;finance;assurance\_predictability;ai\_learning;monte\_carlo;image\_classification;trust\_informal\_treatment;assurance\_explicit;classification;explain;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Turner2016-ao,
  title         = "A Model Explanation System: Latest Updates and Extensions",
  author        = "Turner, Ryan",
  abstract      = "We propose a general model explanation system (MES) for
                   ``explaining'' the output of black box classifiers. This
                   paper describes extensions to Turner (2015), which is
                   referred to frequently in the text. We use the motivating
                   example of a classifier trained to detect fraud in a credit
                   card transaction history. The key aspect is that we provide
                   explanations applicable to a single prediction, rather than
                   provide an interpretable set of parameters. We focus on
                   explaining positive predictions (alerts). However, the
                   presented methodology is symmetrically applicable to
                   negative predictions.",
  month         =  "30~" # jun,
  year          =  2016,
  keywords      = "interpretability;legal;classification;Mendeley Import (Jan
                   17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1606.09517"
}

@MISC{Goodrum_2016-fm,
  title        = "Finding Balance: Model Accuracy vs. Interpretability in
                  Regulated Environments",
  author       = "Goodrum, Will",
  abstract     = "Blog by Elder Research Data Scientist Will Goodrum highlights
                  the trade off of predictive model interpretability vs. model
                  accuracy in regulated environments.",
  month        =  "4~" # nov,
  year         =  2016,
  howpublished = "\url{http://www.elderresearch.com/company/blog/predictive-model-accuracy-versus-interpretability}",
  note         = "Accessed: 2017-3-16",
  keywords     = "
                  interpretability;trust\_popular\_media;in\_table;trust\_academic\_conversation;Mendeley
                  Import (Jan 17)/Assurances"
}

@ARTICLE{Obermann2016-xp,
  title     = "Interpretable Multiclass Models for Corporate Credit Rating
               Capable of Expressing Doubt",
  author    = "Obermann, Lennart and Waack, Stephan",
  abstract  = "Corporate credit rating is a process to classify commercial
               enterprises based on their creditworthiness. Machine learning
               algorithms can construct classification models, but in general
               they do not tend to be 100\% accurate. Since they can be used as
               decision support",
  journal   = "Frontiers in Applied Mathematics and Statistics",
  publisher = "Frontiers",
  volume    =  2,
  pages     = "16",
  year      =  2016,
  keywords  = "interpretability;transparency;in\_table"
}

@ARTICLE{Vellido2012-nm,
  title     = "Making machine learning models interpretable",
  author    = "Vellido, A and Mart{\'\i}n-Guerrero, J D and Lisboa, Pjg",
  abstract  = "Abstract. Data of different levels of complexity and of ever
               growing diversity of characteristics are the raw materials that
               machine learning practitioners try to model using their wide
               palette of methods and tools. The obtained models are meant to
               be a synthetic representation of the available, observed data
               that captures some of their intrinsic regularities or patterns.
               Therefore, the use of machine learning techniques for data
               analysis can be understood as ...",
  journal   = "ESANN",
  publisher = "Citeseer",
  year      =  2012,
  keywords  = "
               interpretability;ML\_theory;trust\_informal\_treatment;assurance\_explicit;vis\_dr;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Jovanovic2016-gw,
  title    = "Building interpretable predictive models for pediatric hospital
              readmission using {Tree-Lasso} logistic regression",
  author   = "Jovanovic, Milos and Radovanovic, Sandro and Vukicevic, Milan and
              Van Poucke, Sven and Delibasic, Boris",
  abstract = "AbstractObjectives Quantification and early identification of
              unplanned readmission risk have the potential to improve the
              quality of care during hospitalization and after discharge.
              However, high dimensionality, sparsity, and class imbalance of
              electronic health data and the complexity of risk quantification,
              challenge the development of accurate predictive models.
              Predictive models require a certain level of interpretability in
              order to be applicable in real settings and create actionable
              insights. This paper aims to develop accurate and interpretable
              predictive models for readmission in a general pediatric patient
              population, by integrating a data-driven model (sparse logistic
              regression) and domain knowledge based on the international
              classification of diseases 9th---revision clinical modification
              (ICD-9-CM) hierarchy of diseases. Additionally, we propose a way
              to quantify the interpretability of a model and inspect the
              stability of alternative solutions. Materials and methods The
              analysis was conducted on >66,000 pediatric hospital discharge
              records from California, State Inpatient Databases, Healthcare
              Cost and Utilization Project between 2009 and 2011. We
              incorporated domain knowledge based on the ICD-9-CM hierarchy in
              a data driven, Tree-Lasso regularized logistic regression model,
              providing the framework for model interpretation. This approach
              was compared with traditional Lasso logistic regression resulting
              in models that are easier to interpret by fewer high-level
              diagnoses, with comparable prediction accuracy. Results The
              results revealed that the use of a Tree-Lasso model was as
              competitive in terms of accuracy (measured by area under the
              receiver operating characteristic curve---AUC) as the traditional
              Lasso logistic regression, but integration with the ICD-9-CM
              hierarchy of diseases provided more interpretable models in terms
              of high-level diagnoses. Additionally, interpretations of models
              are in accordance with existing medical understanding of
              pediatric readmission. Best performing models have similar
              performances reaching AUC values 0.783 and 0.779 for traditional
              Lasso and Tree-Lasso, respectfully. However, information loss of
              Lasso models is 0.35 bits higher compared to Tree-Lasso model.
              Conclusions We propose a method for building predictive models
              applicable for the detection of readmission risk based on
              Electronic Health records. Integration of domain knowledge (in
              the form of ICD-9-CM taxonomy) and a data-driven, sparse
              predictive algorithm (Tree-Lasso Logistic Regression) resulted in
              an increase of interpretability of the resulting model. The
              models are interpreted for the readmission prediction problem in
              general pediatric population in California, as well as several
              important subpopulations, and the interpretations of models
              comply with existing medical understanding of pediatric
              readmission. Finally, quantitative assessment of the
              interpretability of the models is given, that is beyond simple
              counts of selected low-level features.",
  journal  = "Artif. Intell. Med.",
  volume   =  72,
  pages    = "12--21",
  year     =  2016,
  keywords = "Lasso regression; Tree Lasso regression; Model interpretability;
              Hospital readmission
              prediction;interpretability;medical;in\_table;trust\_informal\_treatment;assurance\_explicit;classification;interp\_models;Mendeley
              Import (Jan 17)/Assurances"
}

@ARTICLE{Choi2016-by,
  title         = "{RETAIN}: An Interpretable Predictive Model for Healthcare
                   using Reverse Time Attention Mechanism",
  author        = "Choi, Edward and Bahadori, Mohammad Taha and Kulas, Joshua A
                   and Schuetz, Andy and Stewart, Walter F and Sun, Jimeng",
  abstract      = "Accuracy and interpretability are two dominant features of
                   successful predictive models. Typically, a choice must be
                   made in favor of complex black box models such as recurrent
                   neural networks (RNN) for accuracy versus less accurate but
                   more interpretable traditional models such as logistic
                   regression. This tradeoff poses challenges in medicine where
                   both accuracy and interpretability are important. We
                   addressed this challenge by developing the REverse Time
                   AttentIoN model (RETAIN) for application to Electronic
                   Health Records (EHR) data. RETAIN achieves high accuracy
                   while remaining clinically interpretable and is based on a
                   two-level neural attention model that detects influential
                   past visits and significant clinical variables within those
                   visits (e.g. key diagnoses). RETAIN mimics physician
                   practice by attending the EHR data in a reverse time order
                   so that recent clinical visits are likely to receive higher
                   attention. RETAIN was tested on a large health system EHR
                   dataset with 14 million visits completed by 263K patients
                   over an 8 year period and demonstrated predictive accuracy
                   and computational scalability comparable to state-of-the-art
                   methods such as RNN, and ease of interpretability comparable
                   to traditional models.",
  month         =  "19~" # aug,
  year          =  2016,
  keywords      = "
                   medical;assurance\_predictability;assurance\_competence;ai\_reasoning;trust\_informal\_treatment;assurance\_explicit;interp\_models;Mendeley
                   Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1608.05745"
}

@ARTICLE{Swartout1983-ko,
  title    = "{XPLAIN}: a system for creating and explaining expert consulting
              programs",
  author   = "Swartout, William R",
  abstract = "Traditional methods for explaining programs provide explanations
              by converting the code of the program or traces of its execution
              to English. While such methods can sometimes adequately explain
              program behavior, they typically cannot provide justifications
              for that behavior. That is, such systems cannot tell why what the
              system is doing is a reasonable thing to be doing. The problem is
              that the knowledge required to provide these justifications was
              used to produce the program but is itself not recorded as part of
              the code, and hence is unavailable. The XPLAIN system uses an
              automatic programmer to generate a consulting program by
              refinement from abstract goals. The automatic programmer uses a
              domain model, consisting of descriptive facts about the
              application domain, and a set of domain principles which
              prescribe behavior and drive the refinement process forward. By
              examining the refinement structure created by the automatic
              programmer, XPLAIN provides justifications of the code. XPLAIN
              has been used to re-implement major portions of a Digitalis
              Therapy Advisor and provides superior explanations of its
              behavior.",
  journal  = "Artif. Intell.",
  volume   =  21,
  number   =  3,
  pages    = "285--325",
  month    =  "1~" # sep,
  year     =  1983,
  keywords = "
              interpretability;medical;trust\_informal\_treatment;assurance\_explicit;explain;in\_paper;Mendeley
              Import (Jan 17)/Assurances"
}

@ARTICLE{Lipton2016-dq,
  title         = "Combating Reinforcement Learning's Sisyphean Curse with
                   Intrinsic Fear",
  author        = "Lipton, Zachary C and Gao, Jianfeng and Li, Lihong and Chen,
                   Jianshu and Deng, Li",
  abstract      = "To use deep reinforcement learning in the wild, we might
                   hope for an agent that can avoid catastrophic mistakes.
                   Unfortunately, even in simple environments, the popular deep
                   Q-network (DQN) algorithm is doomed by a Sisyphean curse.
                   Owing to the use of function approximation, these agents
                   eventually forget experiences as they become exceedingly
                   unlikely under a new policy. Consequently, for as long as
                   they continue to train, DQNs may periodically relive
                   catastrophic mistakes. In this paper, we demonstrate
                   unacceptable performance of DQNs on two toy problems. We
                   then introduce intrinsic fear, a new method that mitigates
                   these problems by avoiding states deemed dangerous. Our
                   approach incorporates a second model trained via supervised
                   learning to predict the probability of catastrophe within a
                   short number of steps. This score then acts to penalize the
                   Q-learning objective, shaping the reward function away from
                   catastrophic states.",
  month         =  "3~" # nov,
  year          =  2016,
  keywords      = "
                   Safety\_AI;trust\_informal\_treatment;assurance\_implicit;Mendeley
                   Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1611.01211"
}

@INCOLLECTION{Mikolov2013-lt,
  title     = "Distributed Representations of Words and Phrases and their
               Compositionality",
  booktitle = "Advances in Neural Information Processing Systems 26",
  author    = "Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado,
               Greg S and Dean, Jeff",
  editor    = "Burges, C J C and Bottou, L and Welling, M and Ghahramani, Z and
               Weinberger, K Q",
  publisher = "Curran Associates, Inc.",
  pages     = "3111--3119",
  year      =  2013,
  keywords  = "
               interpretability;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Karpathy2017-xv,
  title    = "Deep {Visual-Semantic} Alignments for Generating Image
              Descriptions",
  author   = "Karpathy, Andrej and Fei-Fei, Li",
  abstract = "We present a model that generates natural language descriptions
              of images and their regions. Our approach leverages datasets of
              images and their sentence descriptions to learn about the
              inter-modal correspondences between language and visual data. Our
              alignment model is based on a novel combination of Convolutional
              Neural Networks over image regions, bidirectional Recurrent
              Neural Networks (RNN) over sentences, and a structured objective
              that aligns the two modalities through a multimodal embedding. We
              then describe a Multimodal Recurrent Neural Network architecture
              that uses the inferred alignments to learn to generate novel
              descriptions of image regions. We demonstrate that our alignment
              model produces state of the art results in retrieval experiments
              on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the
              generated descriptions outperform retrieval baselines on both
              full images and on a new dataset of region-level annotations.
              Finally, we conduct large-scale analysis of our RNN language
              model on the Visual Genome dataset of 4.1 million captions and
              highlight the differences between image and region-level caption
              statistics.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  39,
  number   =  4,
  pages    = "664--676",
  month    =  apr,
  year     =  2017,
  keywords = "
              interpretability;trust\_informal\_treatment;assurance\_explicit;Mendeley
              Import (Jan 17)/Assurances",
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Huysmans2011-th,
  title    = "An empirical evaluation of the comprehensibility of decision
              table, tree and rule based predictive models",
  author   = "Huysmans, Johan and Dejaeger, Karel and Mues, Christophe and
              Vanthienen, Jan and Baesens, Bart",
  abstract = "An important objective of data mining is the development of
              predictive models. Based on a number of observations, a model is
              constructed that allows the analysts to provide classifications
              or predictions for new observations. Currently, most research
              focuses on improving the accuracy or precision of these models
              and comparatively little research has been undertaken to increase
              their comprehensibility to the analyst or end-user. This is
              mainly due to the subjective nature of ‘comprehensibility’, which
              depends on many factors outside the model, such as the user's
              experience and his/her prior knowledge. Despite this influence of
              the observer, some representation formats are generally
              considered to be more easily interpretable than others. In this
              paper, an empirical study is presented which investigates the
              suitability of a number of alternative representation formats for
              classification when interpretability is a key requirement. The
              formats under consideration are decision tables, (binary)
              decision trees, propositional rules, and oblique rules. An
              end-user experiment was designed to test the accuracy, response
              time, and answer confidence for a set of problem-solving tasks
              involving the former representations. Analysis of the results
              reveals that decision tables perform significantly better on all
              three criteria, while post-test voting also reveals a clear
              preference of users for decision tables in terms of ease of use.",
  journal  = "Decis. Support Syst.",
  volume   =  51,
  number   =  1,
  pages    = "141--154",
  month    =  "1~" # apr,
  year     =  2011,
  keywords = "Data mining; Classification; Knowledge representation;
              Comprehensibility; Decision
              tables;interpretability;trust\_informal\_treatment;assurance\_explicit;human\_study;interp\_models;Mendeley
              Import (Jan 17)/Assurances"
}

@ARTICLE{Ridgeway1998-lv,
  title     = "Interpretable Boosted Na{\"\i}ve Bayes Classification",
  author    = "Ridgeway, G and Madigan, D and Richardson, T and O'Kane, J",
  abstract  = "Abstract Voting methods such as boosting and bagging provide
               substantial improvements in classification performance in many
               problem domains. However, the resulting predictions can prove
               inscrutable to end-users. This is especially problematic in
               domains such as medicine, where end-user acceptance often
               depends on the ability of a classifier to explain its reasoning.
               Here we propose a variant of the boosted na{\"\i}ve Bayes
               classifier that facilitates ...",
  journal   = "KDD",
  publisher = "aaai.org",
  year      =  1998,
  keywords  = "
               interpretability;trust\_informal\_treatment;assurance\_explicit;interp\_models;Mendeley
               Import (Jan 17)/Assurances"
}

@PHDTHESIS{Kim2015-rj,
  title     = "Interactive and interpretable machine learning models for human
               machine collaboration",
  author    = "Kim, Been",
  abstract  = "I envision a system that enables successful collaborations
               between humans and machine learning models by harnessing the
               relative strength to accomplish what neither can do alone.
               Machine learning techniques and humans have skills that
               complement each other - machine learning techniques are good at
               computation on data at the lowest level of granularity, whereas
               people are better at abstracting knowledge from their
               experience, and transferring the knowledge across domains. The
               goal of this thesis is to develop a framework for
               human-in-the-loop machine learning that enables people to
               interact effectively with machine learning models to make better
               decisions, without requiring in-depth knowledge about machine
               learning techniques. Many of us interact with machine learning
               systems everyday. Systems that mine data for product
               recommendations, for example, are ubiquitous. However these
               systems compute their output without end-user involvement, and
               there are typically no life or death consequences in the case
               the machine learning result is not acceptable to the user. In
               contrast, domains where decisions can have serious consequences
               (e.g., emergency response panning, medical decision-making),
               require the incorporation of human experts' domain knowledge.
               These systems also must be transparent to earn experts' trust
               and be adopted in their workflow. The challenge addressed in
               this thesis is that traditional machine learning systems are not
               designed to extract domain experts' knowledge from natural
               workflow, or to provide pathways for the human domain expert to
               directly interact with the algorithm to interject their
               knowledge or to better understand the system output. For machine
               learning systems to make a real-world impact in these important
               domains, these systems must be able to communicate with highly
               skilled human experts to leverage their judgment and expertise,
               and share useful information or patterns from the data. In this
               thesis, I bridge this gap by building human-in-the-loop machine
               learning models and systems that compute and communicate machine
               learning results in ways that are compatible with the human
               decision-making process, and that can readily incorporate human
               experts' domain knowledge. I start by building a machine
               learning model that infers human teams' planning decisions from
               the structured form of natural language of team meetings. I show
               that the model can infer a human teams' final plan with 86\%
               accuracy on average. I then design an interpretable machine
               learning model then ``makes sense to humans'' by exploring and
               communicating patterns and structure in data to support human
               decision-making. Through human subject experiments, I show that
               this interpretable machine learning model offers statistically
               significant quantitative improvements in interpretability while
               preserving clustering performance. Finally, I design a machine
               learning model that supports transparent interaction with humans
               without requiring that a user has expert knowledge of machine
               learning technique. I build a human-in-the-loop machine learning
               system that incorporates human feedback and communicates its
               internal states to humans, using an intuitive medium for
               interaction with the machine learning model. I demonstrate the
               application of this model for an educational domain in which
               teachers cluster programming assignments to streamline the
               grading process.",
  publisher = "Massachusetts Institute of Technology",
  year      =  2015,
  school    = "Massachusetts Institute of Technology",
  keywords  = "Aeronautics and Astronautics.;
               Thesis;interpretability;assurance\_explicit;trust\_formal\_treatment;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@INPROCEEDINGS{Caruana2015-za,
  title     = "Intelligible Models for {HealthCare}: Predicting Pneumonia Risk
               and Hospital 30-day Readmission",
  booktitle = "Proceedings of the 21th {ACM} {SIGKDD} International Conference
               on Knowledge Discovery and Data Mining",
  author    = "Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul
               and Sturm, Marc and Elhadad, Noemie",
  publisher = "ACM",
  pages     = "1721--1730",
  series    = "KDD '15",
  year      =  2015,
  address   = "New York, NY, USA",
  keywords  = "additive models, classification, healthcare, intelligibility,
               interaction detection, logistic regression, risk
               prediction;interpretability;medical;in\_table;trust\_informal\_treatment;assurance\_explicit;interp\_models;Mendeley
               Import (Jan 17)/Assurances"
}

@BOOK{Fukuyama1995-un,
  title     = "Trust: The social virtues and the creation of prosperity",
  author    = "Fukuyama, F",
  abstract  = "Why are some nations richer than others? Rand Corp. political
               scientist Francis Fukuyama argues in his latest book that some
               societies are able to develop cultural norms, such as hard work
               and mutual trust, more than others. Such positive cultural
               values foster economic",
  publisher = "mpls.frb.org",
  year      =  1995,
  keywords  = "trust\_definition;NotRead;Mendeley Import (Jan
               17)/Assurances/Trust Background"
}

@ARTICLE{Fung1999-bi,
  title    = "{EC-trust} (trust in electronic commerce): exploring the
              antecedent factors",
  author   = "Fung, Raymond and Lee, Matthew",
  journal  = "AMCIS 1999 Proceedings",
  pages    = "179",
  year     =  1999,
  keywords = "trust\_definition;meh..;Mendeley Import (Jan 17)/Assurances/Trust
              Background"
}

@ARTICLE{Baier1986-im,
  title     = "Trust and Antitrust",
  author    = "Baier, Annette",
  journal   = "Ethics",
  publisher = "University of Chicago Press",
  volume    =  96,
  number    =  2,
  pages     = "231--260",
  year      =  1986,
  keywords  = "trust\_definition;in\_table;visionary\_paper;Mendeley Import
               (Jan 17)/Assurances/Trust Background"
}

@BOOK{Gambetta1988-pi,
  title     = "Trust: Making and Breaking Cooperative Relations",
  author    = "Gambetta, Diego",
  publisher = "Blackwell",
  year      =  1988,
  keywords  = "in\_table;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@ARTICLE{Lewicki2006-hj,
  title     = "Models of interpersonal trust development: Theoretical
               approaches, empirical evidence, and future directions",
  author    = "Lewicki, Roy J and Tomlinson, Edward C and Gillespie, Nicole",
  journal   = "J. Manage.",
  publisher = "Sage Publications",
  volume    =  32,
  number    =  6,
  pages     = "991--1022",
  year      =  2006,
  keywords  = "in\_table;trust\_definition;business;Mendeley Import (Jan
               17)/Assurances/Trust Background;Mendeley Import (Jan
               17)/Assurances"
}

@ARTICLE{Lewicki2006-gp,
  title     = "Trust, trust development, and trust repair",
  author    = "Lewicki, Roy J and Wiethoff, Carolyn",
  journal   = "The handbook of conflict resolution: Theory and practice",
  publisher = "Jossey-Bas Publishers San Francisco",
  pages     = "92--119",
  year      =  2006,
  keywords  = "trust\_definition;in\_table;business;Mendeley Import (Jan
               17)/Assurances/Trust Background;Mendeley Import (Jan
               17)/Assurances"
}

@ARTICLE{Lewicki1998-ox,
  title     = "Trust and Distrust: New Relationships and Realities",
  author    = "Lewicki, Roy J and McAllister, Daniel J and Bies, Robert J",
  abstract  = "We propose a new theoretical framework for understanding
               simultaneous trust and distrust within relationships, grounded
               in assumptions of multidimensionality and the inherent tensions
               of relationships, and we separate this research from prior work
               grounded in assumptions of unidimensionality and balance.
               Drawing foundational support for this new framework from recent
               research on simultaneous positive and negative sentiments and
               ambivalence, we explore the theoretical and practical
               significance of the framework for future work on trust and
               distrust relationships within organizations.",
  journal   = "Acad. Manage. Rev.",
  publisher = "Academy of Management",
  volume    =  23,
  number    =  3,
  pages     = "438--458",
  year      =  1998,
  keywords  = "trust\_definition;business;Mendeley Import (Jan
               17)/Assurances/Trust Background;Mendeley Import (Jan
               17)/Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Rasmussen2006-ne,
  title     = "Gaussian processes for machine learning",
  author    = "Rasmussen, Carl Edward",
  abstract  = "CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep
               Teregowda): Abstract. We give a basic introduction to Gaussian
               Process regression models. We focus on understanding the role of
               the stochastic process and how it is used to define a
               distribution over functions. We present the simple equations for
               incorporating training data and examine how to learn the
               hyperparameters using the marginal likelihood. We explain the
               practical advantages of Gaussian Process and end with
               conclusions and a look at the current trends in GP work.
               Supervised learning in the form of regression (for continuous
               outputs) and classification (for discrete outputs) is an
               important constituent of statistics and machine learning, either
               for analysis of data sets, or as a subgoal of a more complex
               problem. Traditionally parametric 1 models have been used for
               this purpose. These have a possible advantage in ease of
               interpretability, but for complex data sets, simple parametric
               models may lack expressive power, and their more complex
               counterparts (such as feed forward neural networks) may not be
               easy to work with in practice. The advent of kernel machines,
               such as Support Vector Machines and Gaussian Processes has
               opened the possibility of flexible models which are practical to
               work with. In this short tutorial we present the basic idea on
               how Gaussian Process models can be used to formulate a Bayesian
               framework for regression. We will focus on understanding the
               stochastic process and how it is used in supervised learning.
               Secondly, we will discuss practical matters regarding the role
               of hyperparameters in the covariance function, the marginal
               likelihood and the automatic Occam’s razor. For broader
               introductions to Gaussian processes, consult [1], [2]. 1
               Gaussian Processes In this section we define Gaussian Processes
               and show how they can very naturally be used to define
               distributions over functions. In the following section we
               continue to show how this distribution is updated in the light
               of training examples. 1 By a parametric model, we here mean a
               model which during training ``absorbs '' the information from
               the training data into the parameters; after training the data
               can be discarded.",
  publisher = "Citeseer",
  year      =  2006,
  keywords  = "GPs;interpretability;Textbook;Mendeley Import (Jan 17)/TextBooks"
}

@ARTICLE{Ishibuchi2007-yn,
  title    = "Analysis of interpretability-accuracy tradeoff of fuzzy systems
              by multiobjective fuzzy genetics-based machine learning",
  author   = "Ishibuchi, Hisao and Nojima, Yusuke",
  abstract = "This paper examines the interpretability-accuracy tradeoff in
              fuzzy rule-based classifiers using a multiobjective fuzzy
              genetics-based machine learning (GBML) algorithm. Our GBML
              algorithm is a hybrid version of Michigan and Pittsburgh
              approaches, which is implemented in the framework of evolutionary
              multiobjective optimization (EMO). Each fuzzy rule is represented
              by its antecedent fuzzy sets as an integer string of fixed
              length. Each fuzzy rule-based classifier, which is a set of fuzzy
              rules, is represented as a concatenated integer string of
              variable length. Our GBML algorithm simultaneously maximizes the
              accuracy of rule sets and minimizes their complexity. The
              accuracy is measured by the number of correctly classified
              training patterns while the complexity is measured by the number
              of fuzzy rules and/or the total number of antecedent conditions
              of fuzzy rules. We examine the interpretability-accuracy tradeoff
              for training patterns through computational experiments on some
              benchmark data sets. A clear tradeoff structure is visualized for
              each data set. We also examine the interpretability-accuracy
              tradeoff for test patterns. Due to the overfitting to training
              patterns, a clear tradeoff structure is not always obtained in
              computational experiments for test patterns.",
  journal  = "Int. J. Approx. Reason.",
  volume   =  44,
  number   =  1,
  pages    = "4--31",
  month    =  "1~" # jan,
  year     =  2007,
  keywords = "Classification; Fuzzy systems; Fuzzy data mining; Multiobjective
              optimization; Genetic algorithms; Genetics-based machine
              learning;interpretability;in\_table;Mendeley Import (Jan
              17)/Assurances"
}

@ARTICLE{Muller2008-sp,
  title    = "Machine learning for real-time single-trial {EEG-analysis}: From
              brain--computer interfacing to mental state monitoring",
  author   = "M{\"u}ller, Klaus-Robert and Tangermann, Michael and Dornhege,
              Guido and Krauledat, Matthias and Curio, Gabriel and Blankertz,
              Benjamin",
  abstract = "Machine learning methods are an excellent choice for compensating
              the high variability in EEG when analyzing single-trial data in
              real-time. This paper briefly reviews preprocessing and
              classification techniques for efficient EEG-based brain--computer
              interfacing (BCI) and mental state monitoring applications. More
              specifically, this paper gives an outline of the Berlin
              brain--computer interface (BBCI), which can be operated with
              minimal subject training. Also, spelling with the novel
              BBCI-based Hex-o-Spell text entry system, which gains
              communication speeds of 6--8 letters per minute, is discussed.
              Finally the results of a real-time arousal monitoring experiment
              are presented.",
  journal  = "J. Neurosci. Methods",
  volume   =  167,
  number   =  1,
  pages    = "82--90",
  month    =  "15~" # jan,
  year     =  2008,
  keywords = "EEG; Sensorimotor rhythms; $\alpha$ -Rhythm; Single-trial
              EEG-analysis; Real-time; Machine learning; Mental state
              monitoring;interpretability;in\_table;Mendeley Import (Jan
              17)/Assurances"
}

@ARTICLE{Haury2011-zi,
  title       = "The influence of feature selection methods on accuracy,
                 stability and interpretability of molecular signatures",
  author      = "Haury, Anne-Claire and Gestraud, Pierre and Vert,
                 Jean-Philippe",
  affiliation = "Mines ParisTech, Centre for Computational Biology,
                 Fontainebleau, France. anne-claire.haury@mines-paristech.fr",
  abstract    = "Biomarker discovery from high-dimensional data is a crucial
                 problem with enormous applications in biology and medicine. It
                 is also extremely challenging from a statistical viewpoint,
                 but surprisingly few studies have investigated the relative
                 strengths and weaknesses of the plethora of existing feature
                 selection methods. In this study we compare 32 feature
                 selection methods on 4 public gene expression datasets for
                 breast cancer prognosis, in terms of predictive performance,
                 stability and functional interpretability of the signatures
                 they produce. We observe that the feature selection method has
                 a significant influence on the accuracy, stability and
                 interpretability of signatures. Surprisingly, complex wrapper
                 and embedded methods generally do not outperform simple
                 univariate feature selection methods, and ensemble feature
                 selection has generally no positive effect. Overall a simple
                 Student's t-test seems to provide the best results.",
  journal     = "PLoS One",
  volume      =  6,
  number      =  12,
  pages       = "e28210",
  month       =  "21~" # dec,
  year        =  2011,
  keywords    = "
                 interpretability;trust\_informal\_treatment;assurance\_implicit;Mendeley
                 Import (Jan 17)/Assurances",
  language    = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Garcia2009-gd,
  title     = "A study of statistical techniques and performance measures for
               genetics-based machine learning: accuracy and interpretability",
  author    = "Garc{\'\i}a, S and Fern{\'a}ndez, A and Luengo, J and Herrera, F",
  abstract  = "The experimental analysis on the performance of a proposed
               method is a crucial and necessary task to carry out in a
               research. This paper is focused on the statistical analysis of
               the results in the field of genetics-based machine Learning. It
               presents a study involving a set of techniques which can be used
               for doing a rigorous comparison among algorithms, in terms of
               obtaining successful classification models. Two accuracy
               measures for multi-class problems have been employed:
               classification rate and Cohen’s kappa. Furthermore, two
               interpretability measures have been employed: size of the rule
               set and number of antecedents. We have studied whether the
               samples of results obtained by genetics-based classifiers, using
               the performance measures cited above, check the necessary
               conditions for being analysed by means of parametrical tests.
               The results obtained state that the fulfillment of these
               conditions are problem-dependent and indefinite, which supports
               the use of non-parametric statistics in the experimental
               analysis. In addition, non-parametric tests can be
               satisfactorily employed for comparing generic classifiers over
               various data-sets considering any performance measure. According
               to these facts, we propose the use of the most powerful
               non-parametric statistical tests to carry out multiple
               comparisons. However, the statistical analysis conducted on
               interpretability must be carefully considered.",
  journal   = "Soft Comput",
  publisher = "Springer-Verlag",
  volume    =  13,
  number    =  10,
  pages     = "959",
  month     =  "1~" # aug,
  year      =  2009,
  keywords  = "interpretability;medical;classification;assurance\_predictability;ai\_learning;ai\_reasoning;decision\_tree",
  language  = "en"
}

@INPROCEEDINGS{Yu2017-se,
  title           = "Towards Deep Interpretability ({MUS-ROVER} {II)}: Learning
                     Hierarchical Representations of Tonal Music",
  booktitle       = "Proceedings of the 5th International Conference on
                     Learning Representations ({ICLR})",
  author          = "Yu, Haizi and Varshney, Lav R",
  abstract        = "Music theory studies the regularity of patterns in music
                     to capture concepts underlying music styles and composers'
                     decisions. This paper continues the study of building
                     \textbackslashemph\{automatic theorists\} (rovers) to
                     learn and represent music concepts that lead to human
                     interpretable knowledge and further lead to materials for
                     educating people. Our previous work took a first step in
                     algorithmic concept learning of tonal music, studying
                     high-level representations (concepts) of symbolic music
                     (scores) and extracting interpretable rules for
                     composition. This paper further studies the representation
                     \textbackslashemph\{hierarchy\} through the learning
                     process, and supports \textbackslashemph\{adaptive\} 2D
                     memory selection in the resulting language model. This
                     leads to a deeper-level interpretability that expands from
                     individual rules to a dynamic system of rules, making the
                     entire rule learning process more cognitive. The outcome
                     is a new rover, MUS-ROVER \textbackslashRN\{2\}, trained
                     on Bach's chorales, which outputs customizable syllabi for
                     learning compositional rules. We demonstrate comparable
                     results to our music pedagogy, while also presenting the
                     differences and variations. In addition, we point out the
                     rover's potential usages in style recognition and
                     synthesis, as well as applications beyond music.",
  year            =  2017,
  keywords        = "interpretability",
  conference      = "International Conference on Learning Representations"
}

@ARTICLE{Karaletsos2015-zx,
  title         = "Bayesian representation learning with oracle constraints",
  author        = "Karaletsos, Theofanis and Belongie, Serge and R{\"a}tsch,
                   Gunnar",
  abstract      = "Representation learning systems typically rely on massive
                   amounts of labeled data in order to be trained to high
                   accuracy. Recently, high-dimensional parametric models like
                   neural networks have succeeded in building rich
                   representations using either compressive, reconstructive or
                   supervised criteria. However, the semantic structure
                   inherent in observations is oftentimes lost in the process.
                   Human perception excels at understanding semantics but
                   cannot always be expressed in terms of labels. Thus,
                   \textbackslashemph\{oracles\} or
                   \textbackslashemph\{human-in-the-loop systems\}, for example
                   crowdsourcing, are often employed to generate similarity
                   constraints using an implicit similarity function encoded in
                   human perception. In this work we propose to combine
                   \textbackslashemph\{generative unsupervised feature
                   learning\} with a \textbackslashemph\{probabilistic
                   treatment of oracle information like triplets\} in order to
                   transfer implicit privileged oracle knowledge into explicit
                   nonlinear Bayesian latent factor models of the observations.
                   We use a fast variational algorithm to learn the joint model
                   and demonstrate applicability to a well-known image dataset.
                   We show how implicit triplet information can provide rich
                   information to learn representations that outperform
                   previous metric learning approaches as well as generative
                   models without this side-information in a variety of
                   predictive tasks. In addition, we illustrate that the
                   proposed approach compartmentalizes the latent spaces
                   semantically which allows interpretation of the latent
                   variables.",
  month         =  "16~" # jun,
  year          =  2015,
  keywords      = "interpretability",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1506.05011"
}

@TECHREPORT{Sheridan1978-be,
  title       = "Human and computer control of undersea teleoperators",
  author      = "Sheridan, Thomas B and Verplank, William L",
  institution = "DTIC Document",
  year        =  1978,
  keywords    = "automation"
}

@INPROCEEDINGS{Freedman2016-gr,
  title     = "Safety in {AI-HRI}: Challenges Complementing User Experience
               Quality",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Freedman, Richard G and Zilberstein, Shlomo",
  abstract  = "Contemporary research in human-robot interaction (HRI)
               predominantly focuses on the user's experience while controlling
               a robot. However, with the increased deployment of artificial
               intelligence (AI) techniques, robots are quickly becoming more
               autonomous in both academic and industrial experimental
               settings. In addition to improving the user's interactive
               experience with AI-operated robots through personalization,
               dialogue, emotions, and dynamic behavior, there is also a
               growing need to consider the safety of the interaction. AI may
               not account for the user's less likely responses, making it
               possible for an unaware user to be injured by the robot if they
               have a collision. Issues of trust and acceptance may also come
               into play if users cannot always understand the robot's thought
               process, creating a potential for emotional harm. We identify
               challenges that will need to be addressed in safe AI-HRI and
               provide an overview of approaches to consider for them, many
               stemming from the contemporary research.",
  month     =  "28~" # sep,
  year      =  2016,
  keywords  = "Safety\_AI;NotRead;Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Maeda2016-tt,
  title     = "Anticipative Interaction Primitives for {Human-Robot}
               Collaboration",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Maeda, Guilherme and Maloo, Aayush and Ewerton, Marco and
               Lioutikov, Rudolf and Peters, Jan",
  abstract  = "This paper introduces our initial investigation on the problem
               of providing a semi-autonomous robot collaborator with
               anticipative capabilities to predict human actions. Anticipative
               robot behavior is a desired characteristic of robot
               collaborators that lead to fluid, proactive interactions. We are
               particularly interested in improving reactive methods that rely
               on human action recognition to activate the corresponding robot
               action. Action recognition invariably causes delay in the
               robot’s response, and the goal of our method is to eliminate
               this delay by predicting the next human action. Prediction is
               achieved by using a lookup table containing variations of
               assembly sequences, previously demonstrated by different users.
               The method uses the nearest neighbor sequence in the table that
               matches the actual sequence of human actions. At the movement
               level, our method uses a probabilistic representation of
               interaction primitives to generate robot trajectories. The
               method is demonstrated using a 7 degree-of-freedom lightweight
               arm equipped with a 5-finger hand on an assembly task consisting
               of 17 steps.",
  month     =  "28~" # sep,
  year      =  2016,
  keywords  = "HRI;Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

@INPROCEEDINGS{Curran2016-ij,
  title     = "{POMDPs} for {Risk-Aware} Autonomy",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Curran, William and Bowie, Cameron and Smart, William D",
  abstract  = "Although we would like our robots to have completely autonomous
               behavior, this is often not possible. Some parts of a task might
               be hard to automate, perhaps due to hard-to-interpret sensor
               information, or a complex environment. In this case, using
               shared autonomy or teleoperation is preferable to an error-prone
               autonomous approach. However, the question of which parts of a
               task to allocate to the human, and which to the robot can often
               be tricky. In this work, we introduce A 3 P, a risk-aware
               task-level reinforcement learning algorithm. A 3 P represents a
               task-level state machine as a POMDP. In this paper, we introduce
               A 3 P, a risk-aware algorithm that discovers when to hand off
               subtasks to a human assistant. A 3 P models the task as a
               Partially Observably Markov Decision Process (POMDP) and
               explicitly represents failures as additional state-action pairs.
               Based on the model, the algorithm allows the user to allocate
               subtasks the robot or the human in such a way as to manage the
               worst-case performance time for the overall task.",
  month     =  "28~" # sep,
  year      =  2016,
  keywords  = "
               HRI;assurance\_competence;ai\_planning;robotics;POMDP;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@INPROCEEDINGS{Chadalavada2016-ze,
  title           = "Empirical evaluation of human trust in an expressive
                     mobile robot",
  booktitle       = "Proceedings of {RSS} Workshop`` Social Trust in Autonomous
                     Robots 2016'', June 19, 2016",
  author          = "Chadalavada, Ravi Teja and Lilienthal, Achim and
                     Andreasson, Henrik and Krug, Robert",
  year            =  2016,
  keywords        = "HRI;Mendeley Import (Jan 17)/Assurances",
  conference      = "RSS 2016"
}

@INPROCEEDINGS{Chadalavada2015-wx,
  title     = "That's on my mind! robot to human intention communication
               through on-board projection on shared floor space",
  booktitle = "2015 European Conference on Mobile Robots ({ECMR})",
  author    = "Chadalavada, R T and Andreasson, H and Krug, R and Lilienthal, A
               J",
  abstract  = "The upcoming new generation of autonomous vehicles for
               transporting materials in industrial environments will be more
               versatile, flexible and efficient than traditional AGVs, which
               simply follow pre-defined paths. However, freely navigating
               vehicles can appear unpredictable to human workers and thus
               cause stress and render joint use of the available space
               inefficient. Here we address this issue and propose on-board
               intention projection on the shared floor space for communication
               from robot to human. We present a research prototype of a
               robotic fork-lift equipped with a LED projector to visualize
               internal state information and intents. We describe the
               projector system and discuss calibration issues. The robot's
               ability to communicate its intentions is evaluated in realistic
               situations where test subjects meet the robotic forklift. The
               results show that already adding simple information, such as the
               trajectory and the space to be occupied by the robot in the near
               future, is able to effectively improve human response to the
               robot.",
  pages     = "1--6",
  year      =  2015,
  keywords  = "mobile robots;LED projector;autonomous vehicles;human intention
               communication;human workers;industrial environments;internal
               state information;onboard projection;robotic forklift;shared
               floor space;Calibration;Cameras;Service
               robots;Standards;Three-dimensional
               displays;Vehicles;HRI;assurance\_explicit;trust\_formal\_treatment;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Broad2017-jb,
  title    = "Trust Adaptation Leads to Lower Control Effort in Shared Control
              of Crane Automation",
  author   = "Broad, A and Schultz, J and Derry, M and Murphey, T and Argall, B",
  abstract = "We present a shared-control framework predicated on a measure of
              trust in the operator, that is calculated automatically based on
              the quality of the interactions between a human and autonomous
              system. This measure of trust is built upon a control-theoretic
              foundation that rewards stable operation of the system to give
              more trusted users additional control authority. The level of
              control authority is used to modify the human input, and as a
              result, we observe a minimization of the required effort of the
              controller. We validate this work within a planar crane
              environment with a receding horizon controller to assist with the
              regulation of the system dynamics. The human defines the
              reference trajectory for the controller. In an experimental study
              users navigate a suspended payload through a set of maze
              configurations. We find that adaptation of the trust metric over
              time provides the benefit of substantially ( $p <; 0.01$)
              improving the automated system's ability to modulate the user's
              input, resulting in stable reference trajectories that require
              less effort to track. In effect, the human and automation spend
              less time fighting each other during task execution, suggesting
              that the automated system and user each have a better
              understanding of the other's ability.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  2,
  number   =  1,
  pages    = "239--246",
  month    =  jan,
  year     =  2017,
  keywords = "cranes;path planning;trajectory control;control authority;crane
              automation control;maze configuration;planar crane
              environment;receding horizon controller;reference
              trajectory;suspended payload navigation;trust adaptation;trust
              measure;trust
              metric;Aerodynamics;Measurement;Optimization;Robots;Trajectory;Vehicle
              dynamics;Control architectures and programming;human factors and
              human-in-the-loop;physically assistive
              devices;HRI;automation;Mendeley Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Smith2016-bv,
  title     = "Interdependence quantification for compositional control
               synthesis with an application in vehicle safety systems",
  booktitle = "2016 {IEEE} 55th Conference on Decision and Control ({CDC})",
  author    = "Smith, S W and Nilsson, P and Ozay, N",
  abstract  = "Composing controllers designed individually for interacting
               subsystems, while preserving the guarantees that each controller
               provides for each subsystem, is a challenging task. Motivated by
               this challenge, we consider in this paper the problem of
               synthesizing safety controllers for linear parameter-varying
               subsystems, where the system matrices of each subsystem depend
               (possibly nonlinearly) on the states of the other subsystems. In
               particular, we propose a method for synthesis of controlled
               invariant sets and associated controllers, that is robust
               against affine parametric uncertainties in the system matrices.
               Then we show for certain classes of parameter dependencies how
               to quantify the uncertainty imposed on the other subsystems by
               convexifying, with an affine map, the effects of these
               parameters. An analysis of this quantification is provided. In
               the second part of the paper, we focus on an application of this
               method to vehicle safety systems. We demonstrate how controllers
               for lane-keeping and adaptive cruise control can be synthesized
               in a compositional way using the proposed method. Our
               simulations illustrate how these controllers keep their
               individual safety guarantees when implemented simultaneously, as
               the theory suggests.",
  pages     = "5700--5707",
  month     =  dec,
  year      =  2016,
  keywords  = "adaptive control;control system synthesis;linear parameter
               varying systems;matrix algebra;road safety;road
               vehicles;uncertain systems;adaptive cruise control;affine
               map;affine parametric uncertainties;compositional control
               synthesis;controlled invariant sets;interacting
               subsystems;interdependence quantification;lane-keeping;linear
               parameter-varying subsystems;safety controllers;safety
               guarantees;system matrices;vehicle safety systems;Approximation
               algorithms;Robustness;Uncertainty;Vehicle dynamics;Vehicle
               safety;Vehicles;V\&V;Mendeley Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Wang2016-vt,
  title     = "{Trust-Based} Symbolic Robot Motion Planning with
               {Human-in-the-Loop}",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Wang, Yue",
  abstract  = "Autonomous robots are becoming increasingly popular and such
               systems has led to complex design and analysis which brings the
               necessity of validation and verification. In particular,
               symbolic robot motion planning based on formal methods is
               verifiably correct. It is the process of specifying and planning
               robot tasks in a discrete space, then carrying them out in a
               continuous space in a manner that preserves the discrete-level
               task specifications. Despite progress in symbolic motion
               planning, many challenges remain, including addressing
               scalability for multi-robot systems and improving solutions by
               incorporating human intelligence in an adaptive fashion. On the
               other hand, extant works in human-robot interaction (HRI) often
               lack quantitative models and real-time analytical approaches.
               Here, we summarize our recent works on symbolic robot motion
               planning with human-in-the-loop as a step toward addressing
               these challenges. We specially focus on human trust in
               autonomous robots and embed trust analysis into the symbolic
               robot motion planning.",
  month     =  "28~" # sep,
  year      =  2016,
  keywords  = "V\&V;Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Da_Silva2016-qb,
  title     = "Extended Abstract: Formal Design of Cooperative {Multi-Agent}
               Systems",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "da Silva, Rafael Rodrigues and Wu, Bo and Dai, Jin and Lin, Hai",
  abstract  = "We propose a formal design framework to automatically synthesize
               coordination and control schemes for cooperative multi-agent
               systems by combining a top-down mission planning with a
               bottom-up motion planning. The multi-agent system is assigned a
               global mission, specified as regular languages over all the
               agents’ capabilities, whereas basic motion controllers for each
               agent shall be designed with respect to given environment
               description. On one hand, a mission planning layer sits on the
               top of the proposed framework, decomposing the global mission
               into local tasks that are in consistency with each agent’s
               individual capabilities, and compositionally verifying the joint
               effort of the agents via an assume guarantee paradigm. On the
               other hand, corresponding to these local missions, motion plans
               associated with each agent are synthesized by composing basic
               motion primitives, which are verified safe by differential
               dynamic logic (dL), through a Satisfiability Modulo Theories
               (SMT) solver that searches feasible solutions in face of
               constraints due to local task requirements and the environment
               description. It is shown that the proposed framework can handle
               changing environments as the motion primitives are reactive in
               nature, making the motion planning adaptive to local
               environmental changes. Furthermore, on-line mission
               reconfiguration can be triggered by the motion planning layer
               once no feasible solutions can be found through the SMT solver.
               The effectiveness of the overall design framework is
               demonstrated by an automated warehouse case study.",
  month     =  "28~" # sep,
  year      =  2016,
  keywords  = "V\&V;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@INPROCEEDINGS{Nishi2016-zq,
  title     = "Reduction of the State Observation Problem to an Identifiability
               Problem",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Nishi, Masataka",
  abstract  = "Data integrity is a property which a world state interpreted
               with a world model is consistent with the real operating
               environment. Even a formally verified safety claim of an
               autonomous system is prone to a malfunction caused by loss of
               data integrity. From a first-person viewpoint in a congested
               environment, some components of measurable part of the world
               state may become transiently deficient or unavailable because of
               the limited capability of sensor devices. If the system could
               get into a situation where the world state becomes suddenly
               unobservable, existing estimation methods may get unstable.
               These methods can hardly detect the loss of data integrity and
               produce an incorrect estimate without any notice. Our insight is
               that we can merge the original concept of observer theory with
               that of automated reasoning. Firstly, we propose a new way of
               unifying them into a problem of checking satisfiability of a
               formula that consists of predicates regarding the world model
               and decision variables regarding unmeasurable part of the world
               state. We can detect a loss of data integrity by checking if the
               problem is unsatisfiable. Secondly, we replace the idea of
               observability in control theory with identifiability with
               respect to a measure of tolerance and a world model. We show a
               procedure of estimating the world state with a bounded
               uncertainty specified by the measure of tolerance. Third, we
               show that a problem of sensor fusion, a problem of reasoning a
               world state of discrete and enumerated type, and a decision
               problem under uncertainty in the world state are formulated as
               an identifiability problem. The proposal presents a constructive
               basis for supporting the degree of confidence in the estimated
               world state.",
  month     =  "28~" # sep,
  year      =  2016,
  keywords  = "V\&V;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@INPROCEEDINGS{Lahijanian2016-nd,
  title     = "Social Trust: a Major Challenge for the Future of Autonomous
               Systems",
  booktitle = "{AAAI} Fall Symposium on {Cross-Disciplinary} Challenges for
               Autonomous Systems, {AAAI} Fall Symposium. {AAAI}",
  author    = "Lahijanian, Morteza and Kwiatkowska, Marta",
  abstract  = "The immense technological advancements in the past decade have
               enabled robots to enjoy high levels of autonomy, paving their
               way into our society. The recent catastrophic accidents
               involving autonomous systems (e.g., Tesla fatal car accident),
               however, show that sole engineering progress in the technology
               is not enough to guarantee a safe and productive partnership
               between a human and a robot. In this paper we argue that we also
               need to advance our understanding of the role of social trust
               within human-robot relationships, and formulate a theory for
               expressing and reasoning about trust in the context of decisions
               affecting collaboration or competition between humans and
               robots. Therefore, we call for cross-disciplinary collaborations
               to study the formalization of social trust in the context of
               human-robot relationship. We lay the groundwork for such a study
               in this paper.",
  year      =  2016,
  keywords  = "V\&V;trust\_definition;Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

@INPROCEEDINGS{Joshi2016-og,
  title     = "{ALDA}: Cognitive Assistant for Legal Document Analytics",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Joshi, Karuna P and Gupta, Aditi and Mittal, Sudip and Pearce,
               Claudia and Joshi, Anupam and Finin, Tim",
  abstract  = "In recent times, there has been an exponential growth in
               digitization of legal documents such as case records,
               contracts,terms of services, regulations, privacy documents and
               compliance guidelines. Courts have been digitizing their
               archivedcases and also making it available for e-discovery. On
               theother hand, businesses are now maintaining large data setsof
               legal contracts that they have signed with their
               employees,customers and contractors. Large public sector
               organizationsare often bound by complex legal legislation and
               statutes.Hence, there is a need of a cognitive assistant to
               analyze andreason over these legal rules and help people make
               decisions.Today the process of monitoring an ever increasing
               datasetof legal contracts and ensuring regulations and
               complianceis still very manual and labour intensive. This can
               prove tobe a bottleneck in the smooth functioning of an
               enterprise.Automating these digital workflows is quite hard
               because theinformation is available as text documents but it is
               not represented in a machine understandable way. With the
               advancements in cognitive assistance technologies, it is now
               possibleto analyze these digitized legal documents efficiently.
               In thispaper, we discuss ALDA, a legal cognitive assistant to
               analyze digital legal documents. We also present some of
               thepreliminary results we have obtained by analyzing legal
               documents using techniques such as semantic web, text miningand
               graph analysis.",
  month     =  "28~" # sep,
  year      =  2016,
  keywords  = "CA;Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

@INPROCEEDINGS{Gutfreund2016-xe,
  title     = "Automatic Arguments Construction --- From Search Engine to
               Research Engine",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Gutfreund, Dan and Katz, Yoav and Slonim, Noam",
  abstract  = "While discussing a concrete controversial topic, most humans
               will find it challenging to swiftly raise a diverse set of
               convincing and relevant arguments. In this paper we present a
               system that, given a point of view about a controversial topic,
               automatically generates arguments supporting and contesting it.
               This is achieved by breaking the task of automatic argument
               construction into a pipeline of successive modules, each is
               responsible for a specific tangible task such as documents
               retrieval, identifying building blocks of arguments within a
               document, and analyzing whether these building blocks support or
               contest the point of view. By providing an interface for humans
               to interact and intervene at different points in the pipeline,
               we present an interactive research tool which, for a given topic
               and a corpus of documents such as Wikipedia or newspaper
               archive, provides a more comprehensive view and deeper insights
               than can be obtained using standard search engines.",
  month     =  "28~" # sep,
  year      =  2016,
  keywords  = "
               CA;trust\_informal\_treatment;assurance\_implicit;assurance\_predictability;assurance\_competence;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@INPROCEEDINGS{Estes2016-zp,
  title     = "Digital Copilot: Cognitive Assistance for Pilots",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Estes, Steven and Burns, Kevin and Helleberg, John and Long,
               Kevin and Stein, Jeffrey and Pollack, Matthew",
  abstract  = "Over the past several years, pilot-oriented mobile applications
               have seen widespread adoption among recreational pilots. Pilots
               have reported they provide significant workload savings by
               eliminating the need to manage paper charts, manuals, and
               checklists in the cockpit. The pilot, nonetheless, still must go
               looking for the information when it is required, increasing
               accident risk by diverting attention away from control of the
               aircraft. In this paper, we provide an overview of a cognitive
               assistant that determines when information is required based on
               flight context and automatically provides it to the pilot at the
               appropriate time. In addition to an overview of the concept, a
               recent evaluation is discussed alongside future plans to
               evaluate the safety of the Digital Copilot.",
  month     =  "28~" # sep,
  year      =  2016,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Wu2016-ei,
  title     = "Trust and Cooperation in {Human-Robot} Decision Making",
  booktitle = "2016 {AAAI} Fall Symposium Series",
  author    = "Wu, Jane and Paeng, Erin and Linder, Kari and Valdesolo,
               Piercarlo and Boerkoel, James C",
  abstract  = "Trust plays a key role in social interactions, particularly when
               the decisions we make depend on the people we face. In this
               paper, we use game theory to explore whether a person’s
               decisions are influenced by the type of agent they interact
               with:human or robot. By adopting a coin entrustment game, we
               quantitatively measure trust and cooperation to see if such
               phenomena emerge differently when a person believes they are
               playing a robot rather than another human. We found that while
               people cooperate with other humans and robots at a similar rate,
               they grow to trust robots more completely than humans. As a
               possible explanation for these differences, our survey results
               suggest that participants perceive humans as having faculty for
               feelings and sympathy, whereas they perceive robots as being
               more precise and reliable.",
  month     =  "28~" # sep,
  year      =  2016,
  keywords  = "
               HRI;human\_study;trust\_formal\_treatment;assurance\_predictability;in\_paper;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@MISC{Kumar2016-yw,
  title      = "Theory-guided Data Science",
  author     = "Kumar, Vipin",
  month      =  "17~" # nov,
  year       =  2016,
  keywords   = "TGDS;Mendeley Import (Jan 17)/Assurances",
  conference = "AAAI Fall Symposium on Accelerating Science"
}

@ARTICLE{Faghmous2014-og,
  title       = "A Big Data Guide to Understanding Climate Change: The Case for
                 {Theory-Guided} Data Science",
  author      = "Faghmous, James H and Kumar, Vipin",
  affiliation = "Department of Computer Science and Engineering, The University
                 of Minnesota-Twin Cities Minneapolis, Minnesota. Department of
                 Computer Science and Engineering, The University of
                 Minnesota-Twin Cities Minneapolis, Minnesota.",
  abstract    = "Global climate change and its impact on human life has become
                 one of our era's greatest challenges. Despite the urgency,
                 data science has had little impact on furthering our
                 understanding of our planet in spite of the abundance of
                 climate data. This is a stark contrast from other fields such
                 as advertising or electronic commerce where big data has been
                 a great success story. This discrepancy stems from the complex
                 nature of climate data as well as the scientific questions
                 climate science brings forth. This article introduces a data
                 science audience to the challenges and opportunities to mine
                 large climate datasets, with an emphasis on the nuanced
                 difference between mining climate data and traditional big
                 data approaches. We focus on data, methods, and application
                 challenges that must be addressed in order for big data to
                 fulfill their promise with regard to climate science
                 applications. More importantly, we highlight research showing
                 that solely relying on traditional big data techniques results
                 in dubious findings, and we instead propose a theory-guided
                 data science paradigm that uses scientific theory to constrain
                 both the big data techniques as well as the
                 results-interpretation process to extract accurate insight
                 from large climate data.",
  journal     = "Big Data",
  volume      =  2,
  number      =  3,
  pages       = "155--163",
  month       =  "1~" # sep,
  year        =  2014,
  keywords    = "
                 TGDS;interpretability;assurance\_predictability;ai\_learning;supervised\_learning;Computer
                 Science -
                 Learning;trust\_informal\_treatment;assurance\_implicit;Mendeley
                 Import (Jan 17)/Assurances",
  language    = "en"
}

@ARTICLE{Parasuraman2000-js,
  title       = "A model for types and levels of human interaction with
                 automation",
  author      = "Parasuraman, R and Sheridan, T B and Wickens, C D",
  affiliation = "Cognitive Science Laboratory, The Catholic University of
                 America, Washington, DC 20064, USA.",
  abstract    = "Technical developments in computer hardware and software now
                 make it possible to introduce automation into virtually all
                 aspects of human-machine systems. Given these technical
                 capabilities, which system functions should be automated and
                 to what extent? We outline a model for types and levels of
                 automation that provides a framework and an objective basis
                 for making such choices. Appropriate selection is important
                 because automation does not merely supplant but changes human
                 activity and can impose new coordination demands on the human
                 operator. We propose that automation can be applied to four
                 broad classes of functions: 1) information acquisition; 2)
                 information analysis; 3) decision and action selection; and 4)
                 action implementation. Within each of these types, automation
                 can be applied across a continuum of levels from low to high,
                 i.e., from fully manual to fully automatic. A particular
                 system can involve automation of all four types at different
                 levels. The human performance consequences of particular types
                 and levels of automation constitute primary evaluative
                 criteria for automation design using our model. Secondary
                 evaluative criteria include automation reliability and the
                 costs of decision/action consequences, among others. Examples
                 of recommended types and levels of automation are provided to
                 illustrate the application of the model to automation design.",
  journal     = "IEEE Trans. Syst. Man Cybern. A Syst. Hum.",
  volume      =  30,
  number      =  3,
  pages       = "286--297",
  month       =  may,
  year        =  2000,
  keywords    = "NASA Discipline Space Human Factors; Non-NASA
                 Center;automation;interesting\_ideas;Mendeley Import (Jan
                 17)/Assurances",
  language    = "en"
}

@MISC{Ericsson1991-md,
  title        = "Introspection and verbal reports on cognitive processes---Two
                  approaches to the study of thinking: A response to Howe -
                  {ScienceDirect}",
  author       = "Ericsson, K Anders and Crutcher, Robert J",
  abstract     = "... A detailed model of the cognitive processes mediating
                  introspection is needed, in which the introspective processes
                  extracting additional information about thinking are
                  specified and are shown to be consistent with our knowledge
                  about human cognition . ...",
  publisher    = "Elsevier",
  year         =  1991,
  howpublished = "\url{http://www.sciencedirect.com/science/article/pii/0732118X9190041J}",
  note         = "Accessed: 2017-3-2",
  keywords     = "Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Hayes-Roth1979-wj,
  title     = "A cognitive model of planning",
  author    = "Hayes-Roth, Barbara and Hayes-Roth, Frederick",
  journal   = "Cogn. Sci.",
  publisher = "Wiley Online Library",
  volume    =  3,
  number    =  4,
  pages     = "275--310",
  year      =  1979,
  keywords  = "Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{McKnight2001-fa,
  title    = "What Trust Means in {E-Commerce} Customer Relationships: An
              Interdisciplinary Conceptual Typology",
  author   = "McKnight, D H and Chervany, N L",
  abstract = "Trust is a vital relationship concept that needs clarification
              because researchers across disciplines have defined it in so many
              different ways. A typology of trust types would make it easier to
              compare and communicate results, and would be especially valuable
              if the types of trust related to one other. The typology should
              be interdisciplinary because many disciplines research
              e-commerce. This paper justifies a parsimonious interdisciplinary
              typology and relates trust constructs to e-commerce consumer
              actions, defining both conceptual-level and operational-level
              trust constructs. Conceptual-level constructs consist of
              disposition to trust (primarily from psychology),
              institution-based trust (from sociology), and trusting beliefs
              and trusting intentions (primarily from social psychology). Each
              construct is decomposed into measurable subconstructs, and the
              typology shows how trust constructs relate to already existing
              Internet relationship constructs. The effects of Web vendor
              interventions on consumer behaviors are posited to be partially
              mediated by consumer trusting beliefs and trusting intentions in
              the e-vendor.",
  journal  = "International Journal of Electronic Commerce",
  volume   =  6,
  number   =  2,
  pages    = "35--59",
  year     =  2001,
  keywords = "e-commerce;trust\_model;Mendeley Import (Jan 17)/Assurances/Trust
              Background"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Groom2007-bz,
  title     = "Can robots be teammates?: Benchmarks in human--robot teams",
  author    = "Groom, Victoria and Nass, Clifford",
  abstract  = "The team has become a popular model to organize joint
               human--robot behavior. Robot teammates are designed with
               high-levels of autonomy and well-developed coordination skills
               to aid humans in unpredictable environments. In this paper, we
               challenge the assumption that robots will succeed as teammates
               alongside humans. Drawing from the literature on human teams, we
               evaluate robots’ potential to meet the requirements of
               successful teammates. We argue that lacking humanlike mental
               models and a sense of self, robots may prove untrustworthy and
               will be rejected from human teams. Benchmarks for evaluating
               human--robot teams are included, as are guidelines for defining
               alternative structures for human--robot groups.",
  journal   = "Interact. Stud.",
  publisher = "John Benjamins",
  volume    =  8,
  number    =  3,
  pages     = "483--500",
  month     =  "1~" # jan,
  year      =  2007,
  keywords  = "humanrobot interaction; robot teams; teamwork; psychological
               benchmarks; social robotics; trust;NotRead;Mendeley Import (Jan
               17)/Assurances/Trust Background;Mendeley Import (Jan
               17)/Assurances"
}

@ARTICLE{Munjal_Desai2009-en,
  title    = "Creating Trustworthy Robots: Lessons and Inspirations from
              Automated Systems",
  author   = "Munjal Desai, University of Massachusetts-Lowell and Kristen
              Stubbs, University of Massachusetts-Lowell and Aaron Steinfeld,
              Carnegie Mellon University and Holly Yanco, University of
              Massachusetts-Lowell and {Authors}",
  abstract = "One of the most significant challenges of human-robot interaction
              research is designing systems which foster an appropriate level
              of trust in their users: in order to use a robot effectively and
              safely, a user must place neither too little nor too much trust
              in the system. In order to better understand the factors which
              influence trust in a robot, we present a survey of prior work on
              trust in automated systems. We also discuss issues specific to
              robotics which pose challenges not addressed in the automation
              literature, particularly related to reliability, capability, and
              adjustable autonomy. We conclude with the results of a
              preliminary web-based questionnaire which illustrate some of the
              biases which autonomous robots may need to overcome in order to
              promote trust in users.",
  series   = "Robotics Institute",
  year     =  2009,
  keywords = "
              trust\_formal\_treatment;human\_study;assurance\_implicit;in\_paper;Mendeley
              Import (Jan 17)/Assurances/Trust Background;Mendeley Import (Jan
              17)/Assurances"
}

@TECHREPORT{Chen2014-dk,
  title       = "Situation awareness-based agent transparency",
  author      = "Chen, Jessie Y and Procci, Katelyn and Boyce, Michael and
                 Wright, Julia and Garcia, Andre and Barnes, Michael",
  institution = "DTIC Document",
  year        =  2014,
  keywords    = "
                 very\_similar\_to\_mine;trust\_formal\_treatment;assurance\_explicit;in\_paper;Mendeley
                 Import (Jan 17)/Assurances/Trust Background;Mendeley Import
                 (Jan 17)/Assurances"
}

@ARTICLE{Parasuraman1997-co,
  title     = "Humans and automation: Use, misuse, disuse, abuse",
  author    = "Parasuraman, Raja and Riley, Victor",
  abstract  = "Abstract This paper addresses theoretical, empirical, and
               analytical studies pertaining to human use, misuse, disuse, and
               abuse of automation technology. Use refers to the voluntary
               activation or disengagement of automation by human operators.
               Trust, mental workload, and",
  journal   = "Human Factors: The Journal of the Human Factors and Ergonomics
               Society",
  publisher = "SAGE Publications",
  volume    =  39,
  number    =  2,
  pages     = "230--253",
  year      =  1997,
  keywords  = "automation;interesting\_ideas;Mendeley Import (Jan
               17)/Assurances/Trust Background;Mendeley Import (Jan
               17)/Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Castelfranchi2007-gb,
  title     = "The role of beliefs in goal dynamics: prolegomena to a
               constructive theory of intentions",
  author    = "Castelfranchi, Cristiano and Paglieri, Fabio",
  abstract  = "In this article we strive to provide a detailed and principled
               analysis of the role of beliefs in goal processing---that is,
               the cognitive transition that leads from a mere desire to a
               proper intention. The resulting model of belief-based goal
               processing has also relevant consequences for the analysis of
               intentions, and constitutes the necessary core of a constructive
               theory of intentions, i.e. a framework that not only analyzes
               what an intention is, but also explains how it becomes what it
               is. We discuss similarities and differences between our approach
               and other standard accounts of intention, in particular
               Bratman’s planning theory. The aim here is to question and
               refine the conceptual foundations of many theories of
               intentional action: as a consequence, although our analysis is
               not formal in itself, it is ultimately meant to have deep
               consequences for formal models of intentional agency.",
  journal   = "Synthese",
  publisher = "Kluwer Academic Publishers",
  volume    =  155,
  number    =  2,
  pages     = "237--263",
  month     =  "1~" # mar,
  year      =  2007,
  keywords  = "Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Pinyol2013-uy,
  title     = "Computational trust and reputation models for open multi-agent
               systems: a review",
  author    = "Pinyol, Isaac and Sabater-Mir, Jordi",
  abstract  = "In open environments, agents depend on reputation and trust
               mechanisms to evaluate the behavior of potential partners. The
               scientific research in this field has considerably increased,
               and in fact, reputation and trust mechanisms have been already
               considered a key elements in the design of multi-agent systems.
               In this paper we provide a survey that, far from being
               exhaustive, intends to show the most representative models that
               currently exist in the literature. For this enterprise we
               consider several dimensions of analysis that appeared in three
               existing surveys, and provide new dimensions that can be
               complementary to the existing ones and that have not been
               treated directly. Moreover, besides showing the original
               classification that each one of the surveys provide, we also
               classify models that where not taken into account by the
               original surveys. The paper illustrates the proliferation in the
               past few years of models that follow a more cognitive approach,
               in which trust and reputation representation as mental attitudes
               is as important as the final values of trust and reputation.
               Furthermore, we provide an objective definition of trust, based
               on Castelfranchi’s idea that trust implies a decision to rely on
               someone.",
  journal   = "Artif Intell Rev",
  publisher = "Springer Netherlands",
  volume    =  40,
  number    =  1,
  pages     = "1--25",
  month     =  "1~" # jun,
  year      =  2013,
  language  = "en"
}

@INCOLLECTION{Kessler2017-bw,
  title     = "A Comparison of Trust Measures in {Human--Robot} Interaction
               Scenarios",
  booktitle = "Advances in Human Factors in Robots and Unmanned Systems",
  author    = "Kessler, Theresa T and Larios, Cintya and Walker, Tiffani and
               Yerdon, Valarie and Hancock, P A",
  abstract  = "When studying Human--Robot Interaction (HRI), we often employ
               measures of trust. Trust is essential in HRI, as inappropriate
               levels of trust result in misuse, abuse, or disuse of that
               robot. Some measures of trust specifically target automation,
               while others specifically target HRI. Although robots are a type
               of automation, it is unclear which of the broader factors that
               define automation are shared by robots. However, measurements of
               trust in automation and trust in robots should theoretically
               still yield similar results. We examined an HRI scenario using
               (1) an automation trust scale and (2) a robotic trust scale.
               Findings indicated conflicting results coming from these
               respective trust scales. It may well be that these two trust
               scales examine separate constructs and are therefore not
               interchangeable. This discord shows us that future evaluations
               are required to identify scale appropriate context applications
               for either automation or robotic operations.",
  publisher = "Springer, Cham",
  pages     = "353--364",
  year      =  2017,
  keywords  = "Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@INPROCEEDINGS{Wang2016-id,
  title     = "Trust Calibration Within a {Human-Robot} Team: Comparing
               Automatically Generated Explanations",
  booktitle = "The Eleventh {ACM/IEEE} International Conference on Human Robot
               Interaction",
  author    = "Wang, Ning and Pynadath, David V and Hill, Susan G",
  abstract  = "... If the robots are less suited, then we want the humans to
               appropriately gauge the ... Human teammates will learn the
               correctness of the robot's decisions upon entering the buildings
               ... We modified items on interpersonal trust to measure trust in
               the robot's ability, benevolence and ...",
  publisher = "IEEE Press",
  pages     = "109--116",
  series    = "HRI '16",
  year      =  2016,
  address   = "Piscataway, NJ, USA",
  keywords  = "explainable a.i., human-robot interaction, pomdp,
               trust;assurance\_predictability;human\_study;POMDP;assurance\_competence;ai\_planning;very\_similar\_to\_mine;trust\_formal\_treatment;assurance\_explicit;in\_paper;Mendeley
               Import (Jan 17)/Assurances/Trust Background;Mendeley Import (Jan
               17)/Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Xu2016-rz,
  title     = "Towards Modeling {Real-Time} Trust in Asymmetric {Human--Robot}
               Collaborations",
  booktitle = "Robotics Research",
  author    = "Xu, Anqi and Dudek, Gregory",
  editor    = "Inaba, Masayuki and Corke, Peter",
  abstract  = "We are interested in enhancing the efficiency of human--robot
               collaborations, especially in ``supervisor-worker'' settings
               where autonomous robots work under the supervision of a human
               operator. We believe that trust serves a critical role in
               modeling the interactions within these teams, and also in
               streamlining their efficiency. We propose an operational
               formulation of human--robot trust on a short interaction time
               scale, which is tailored to a practical tele-robotics setting.
               We also report on a controlled user study that collected
               interaction data from participants collaborating with an
               autonomous robot to perform visual navigation tasks. Our
               analyses quantify key correlations between real-time
               human--robot trust assessments and diverse factors, including
               properties of failure events reflecting causal trust
               attribution, as well as strong influences from each user’s
               personality. We further construct and optimize a predictive
               model of users’ trust responses to discrete events, which
               provides both insights on this fundamental aspect of real-time
               human--machine interaction, and also has pragmatic significance
               for designing trust-aware robot agents.",
  publisher = "Springer International Publishing",
  pages     = "113--129",
  series    = "Springer Tracts in Advanced Robotics",
  year      =  2016,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Charalambous2016-si,
  title     = "The Development of a Scale to Evaluate Trust in Industrial
               Human-robot Collaboration",
  author    = "Charalambous, George and Fletcher, Sarah and Webb, Philip",
  abstract  = "Trust has been identified as a key element for the successful
               cooperation between humans and robots. However, little research
               has been directed at understanding trust development in
               industrial human-robot collaboration (HRC). With industrial
               robots becoming increasingly integrated into production lines as
               a means for enhancing productivity and quality, it will not be
               long before close proximity industrial HRC becomes a viable
               concept. Since trust is a multidimensional construct and heavily
               dependent on the context, it is vital to understand how trust
               develops when shop floor workers interact with industrial
               robots. To this end, in this study a trust measurement scale
               suitable for industrial HRC was developed in two phases. In
               phase one, an exploratory study was conducted to collect
               participants’ opinions qualitatively. This led to the
               identification of trust related themes relevant to the
               industrial context and a related pool of questionnaire items was
               generated. In the second phase, three human-robot trials were
               carried out in which the questionnaire items were applied to
               participants using three different types of industrial robots.
               The results were statistically analysed to identify the key
               factors impacting trust and from these generate a trust
               measurement scale for industrial HRC.",
  journal   = "Adv. Robot.",
  publisher = "Springer Netherlands",
  volume    =  8,
  number    =  2,
  pages     = "193--209",
  month     =  "1~" # apr,
  year      =  2016,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@TECHREPORT{Schaefer2014-dn,
  title       = "A meta-analysis of factors influencing the development of
                 trust in automation: Implications for human-robot interaction",
  author      = "Schaefer, Kristin E and Billings, Deborah R and Szalma, James
                 L and Adams, Jeffrey K and Sanders, Tracy L and Chen, Jessie Y
                 and Hancock, Peter A",
  abstract    = "... For example, within robotics , unmanned aerial systems are
                 designed to be used in ... environment for application),
                 management (ie, coordination of the actions of humans and
                 robots ... robot teaming, de Visser and Parasuraman (2011)
                 found that human - robot teams benefited from ...",
  publisher   = "DTIC Document",
  institution = "DTIC Document",
  year        =  2014,
  keywords    = "NotRead;automation;Mendeley Import (Jan 17)/Assurances/Trust
                 Background"
}

@INPROCEEDINGS{Schaefer2012-ng,
  title       = "Classification of robot form: factors predicting perceived
                 trustworthiness",
  booktitle   = "Proceedings of the Human Factors and Ergonomics Society Annual
                 Meeting",
  author      = "Schaefer, Kristin E and Sanders, Tracy L and Yordon, Ryan E
                 and Billings, Deborah R and Hancock, P A",
  abstract    = "... For example, in social interactions between humans ,
                 attractiveness leads to more positive appraisals (Calvert,
                 1988). ... Anthropomorphism of robotic forms: A response to
                 affordances? Proc. ... Proceedings of the 5th ACM/IEEE
                 International Conference on Human Robot Interaction . ...",
  publisher   = "pro.sagepub.com",
  volume      =  56,
  pages       = "1548--1552",
  institution = "SAGE Publications",
  year        =  2012,
  keywords    = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Van_den_Brule2014-hf,
  title     = "Do Robot Performance and Behavioral Style affect Human Trust?",
  author    = "van den Brule, Rik and Dotsch, Ron and Bijlstra, Gijsbert and
               Wigboldus, Daniel H J and Haselager, Pim",
  abstract  = "An important aspect of a robot’s social behavior is to convey
               the right amount of trustworthiness. Task performance has shown
               to be an important source for trustworthiness judgments. Here,
               we argue that factors such as a robot’s behavioral style can
               play an important role as well. Our approach to studying the
               effects of a robot’s performance and behavioral style on human
               trust involves experiments with simulated robots in video
               human--robot interaction (VHRI) and immersive virtual
               environments (IVE). Although VHRI and IVE settings cannot
               substitute for the genuine interaction with a real robot, they
               can provide useful complementary approaches to experimental
               research in social human robot interaction. VHRI enables rapid
               prototyping of robot behaviors. Simulating human--robot
               interaction in IVEs can be a useful tool for measuring human
               responses to robots and help avoid the many constraints caused
               by real-world hardware. However, there are also difficulties
               with the generalization of results from one setting (e.g., VHRI)
               to another (e.g. IVE or the real world), which we discuss. In
               this paper, we use animated robot avatars in VHRI to rapidly
               identify robot behavioral styles that affect human trust
               assessment of the robot. In a subsequent study, we use an IVE to
               measure behavioral interaction between humans and an animated
               robot avatar equipped with behaviors from the VHRI experiment.
               Our findings reconfirm that a robot’s task performance
               influences its trustworthiness, but the effect of the behavioral
               style identified in the VHRI study did not influence the robot’s
               trustworthiness in the IVE study.",
  journal   = "Adv. Robot.",
  publisher = "Springer Netherlands",
  volume    =  6,
  number    =  4,
  pages     = "519--531",
  month     =  "1~" # nov,
  year      =  2014,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@PHDTHESIS{Schaefer2013-uu,
  title     = "The perception and measurement of human-robot trust",
  author    = "Schaefer, Kristin E",
  abstract  = "... HRI Human Robot Interaction IRB Institutional Review Board
               ... PI Perceived Intelligence RIVET Robotic Interactive
               Visualization \& Exploration Technology ... In the past two
               decades especially, we have seen a rapid influx of robotics into
               many everyday social environments. ...",
  publisher = "etd.fcla.edu",
  year      =  2013,
  school    = "University of Central Florida Orlando, Florida",
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@ARTICLE{Yagoda2012-ox,
  title     = "You Want Me to Trust a {ROBOT}? The Development of a
               {Human--Robot} Interaction Trust Scale",
  author    = "Yagoda, Rosemarie E and Gillan, Douglas J",
  abstract  = "Trust plays a critical role when operating a robotic system in
               terms of both acceptance and usage. Considering trust is a
               multidimensional context dependent construct, the differences
               and common themes were examined to identify critical
               considerations within human--robot interaction (HRI). In order
               to examine the role of trust within HRI, a measurement tool was
               generated based on five attributes: team configuration, team
               processes, context, task, and system (Yagoda in Human Factors
               and Ergonomics Society Annual Meeting, San Francisco, CA, pp.
               304--308, 2010). The HRI trust scale was developed based on two
               studies. The first study conducts a content validity assessment
               of preliminary items generated, based on a review of previous
               research within HRI and automation, using subject matter experts
               (SMEs). The second study assesses the quality of each trust
               scale item derived from the first study. The results were then
               compiled to generate the HRI trust measurement tool.",
  journal   = "Adv. Robot.",
  publisher = "Springer Netherlands",
  volume    =  4,
  number    =  3,
  pages     = "235--248",
  month     =  "1~" # aug,
  year      =  2012,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@INPROCEEDINGS{Lomas2012-ie,
  title     = "Explaining Robot Actions",
  booktitle = "Proceedings of the Seventh Annual {ACM/IEEE} International
               Conference on {Human-Robot} Interaction",
  author    = "Lomas, Meghann and Chevalier, Robert and Cross, II, Ernest
               Vincent and Garrett, Robert Christopher and Hoare, John and
               Kopack, Michael",
  publisher = "ACM",
  pages     = "187--188",
  series    = "HRI '12",
  year      =  2012,
  address   = "New York, NY, USA",
  keywords  = "explanations, human-robot partnering, natural communications,
               robotic actions,
               trust;human\_study;assurance\_explicit;trust\_informal\_treatment;explain;Mendeley
               Import (Jan 17)/Assurances"
}

@MISC{Ahmed_undated-ie,
  title     = "Explaining Intelligent Autonomy through Adaptive Dialog and
               Machine {Self-Confidence}",
  author    = "Ahmed, Nisar and Frew, Eric and Lawrence, Dale",
  publisher = "DARPA",
  number    = "BAA 16-53",
  keywords  = "NotRead;needs\_classification;Mendeley Import (Jan
               17)/Assurances"
}

@INCOLLECTION{Lawless2016-vy,
  title     = "The Intersection of Robust Intelligence and Trust: Hybrid Teams,
               Firms and Systems",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Lawless, W F and Sofge, Donald",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "We are developing the physics of interdependent uncertainty
               relations to efficiently and effectively control interdependence
               for autonomous hybrid teams (i.e., arbitrary combinations of
               humans, robots and machines), which cannot be done presently.
               Uncertainty is created in states of interdependence between
               social objects: at one extreme, interdependence reduces to
               independent agents and certainty but with asocial, low-power
               solutions generating little meaning or understanding in social
               contexts; oppositely, the length of interdependence increases
               across a group, de-individuating its members until individual
               identity dissolves (e.g., cults, gangs, well-run teams),
               increasing power, efficiency and meaning internal to a group,
               but also the chances of mal-adaptation (e.g., tragic mistakes).
               We focus on how interdependence increases the robust
               intelligence of a group by increasing its autonomy while
               decreasing its entropy, but requiring external control to be
               indirect. For humans, teamwork is an unsolved theoretical
               problem; solving it should generalize to the effective
               computational control of hybrid teams, a path forward for the
               users of a team to trust it to operate safely in hostile
               environments. Present theories of interdependence, like game
               theory or social science, are inadequate to formulate strategies
               to control teams; alternative theories like machine learning can
               control swarms with pattern formations, but not states of
               interdependence, such as multi-tasking operations. While
               alternative theories cannot be used to model teams,
               decision-making or social conflict at the same time (hostile
               mergers; checks and balances), ours can.",
  publisher = "Springer US",
  pages     = "255--270",
  year      =  2016,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@INCOLLECTION{Yanco2016-eu,
  title     = "Methods for Developing Trust Models for Intelligent Systems",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Yanco, Holly A and Desai, Munjal and Drury, Jill L and
               Steinfeld, Aaron",
  editor    = "Mittu, Ranjiv and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  publisher = "Springer",
  pages     = "219--254",
  year      =  2016,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@INCOLLECTION{Gao2016-da,
  title     = "Designing for Robust and Effective Teamwork in {Human-Agent}
               Teams",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Gao, Fei and Cummings, M L and Solovey, Erin",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "We investigated the impact of team structure, task uncertainty,
               and information-sharing tools on team coordination and team
               performance in human-agent teams. In applications such as search
               and rescue, command and control, and air traffic control,
               operators in the future will likely need to work in teams
               together with robots. It is critical to understand how these
               teams could be robust against uncertainty and what influences
               team performance. We conducted two experiments in which teams of
               three operators controlled simulated heterogeneous robots on the
               same testbed. Experiment 1 investigated the impact of team
               structure and uncertainty of task arrival processes on team
               coordination and performance. Experiment 2 explored the usage of
               information-sharing tools under different uncertainty levels. In
               Experiment 1, it was found that divisional teams were more
               robust against the uncertainty on task arrival processes.
               However, this robustness was achieved with an overall worse
               performance compared to functional teams. Three reasons for the
               degraded performance were identified, namely duplication on task
               assignment, under-utilization of vehicles, and infrequent
               communication. In Experiment 2, it was found that
               information-sharing tools reduced the duplication on task
               assignments, improved overall task performance, and reduced
               workload. These results provide insights for achieving robust
               and effective teamwork. This goal can be achieved by using a
               team structure that could adapt to uncertainties together with
               effective information-sharing tools. These findings could inform
               the design of robust teams and the development of
               information-sharing tools to improve teamwork.",
  publisher = "Springer US",
  pages     = "167--190",
  year      =  2016,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@INCOLLECTION{Robinette2016-ys,
  title     = "Investigating {Human-Robot} Trust in Emergency Scenarios:
               Methodological Lessons Learned",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Robinette, Paul and Wagner, Alan R and Howard, Ayanna M",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "The word ``trust'' has many definitions that vary based on
               context and culture, so asking participants if they trust a
               robot is not as straightforward as one might think. The
               perceived risk involved in a scenario and the precise wording of
               a question can bias the outcome of a study in ways that the
               experimenter did not intend. This chapter presents the lessons
               we have learned about trust while conducting human-robot
               experiments with 770 human subjects. We discuss our work
               developing narratives that describe trust situations as well as
               interactive human-robot simulations. These experimental
               paradigms have guided our research exploring the meaning of
               trust, trust loss, and trust repair. By using crowdsourcing to
               locate and manage experiment participants, considerable
               diversity of opinion is found; there are, however, several
               considerations that must be included. Conclusions drawn from
               these experiments demonstrate the types of biases that
               participants are prone to as well as techniques for mitigating
               these biases.",
  publisher = "Springer US",
  pages     = "143--166",
  year      =  2016,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@INCOLLECTION{Sadrfaridpour2016-di,
  title     = "Modeling and Control of Trust in {Human-Robot} Collaborative
               Manufacturing",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Sadrfaridpour, Behzad and Saeidi, Hamed and Burke, Jenny and
               Madathil, Kapil and Wang, Yue",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "Human-Robot Collaboration (HRC) on the factory floor has opened
               a new realm of manufacturing in real-world settings. In such
               applications, a human and robot work together with each other as
               coworkers while HRC plays a critical role in safety,
               productivity, and flexibility. In particular, human-robot trust
               determines his/her acceptance and hence allocation of autonomy
               to a robot, which alter the overall task efficiency and human
               workload. Inspired by well-known human factors research, we
               develop a time-series trust model for human-robot collaboration
               tasks, which is a function of prior trust, robot performance,
               and human performance. The robot performance is evaluated by its
               flexibility to keep pace with the human coworker and is molded
               as the difference between human and robot speed. The human
               performance in doing physical tasks is directly related to
               his/her muscle fatigue level. We use the muscle fatigue and
               recovery dynamics to capture the fatigue level of the human body
               when performing repetitive kinesthetic tasks, which are typical
               types of human motions in manufacturing. The robot speed can be
               controlled in three different modes: manually by the associate,
               autonomously through robust intelligence algorithms, or
               collaboratively by the combination of manual and autonomous
               inputs. We first simulate a typical 9-h work day for human robot
               collaborative tasks and implement the proposed trust model and
               the three control schemes. Furthermore, we experimentally
               validate our model and control schemes by conducting a series of
               human-in-the-loop experiments using the Rethink Robotics Baxter
               robot.",
  publisher = "Springer US",
  pages     = "115--141",
  year      =  2016,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Palmer2016-ks,
  title     = "The ``Trust V'': Building and Measuring Trust in Autonomous
               Systems",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Palmer, Gari and Selwyn, Anne and Zwillinger, Dan",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "For systems to be used they must be trusted. They must engender
               both system trust (system meets specifications) and operational
               trust (system meets user expectations). Simple systems are
               easily trustable. A light switch has only a few possible states.
               Complicated systems typically demonstrate trustability through
               established methods during their requirements confirmation
               processes (such as Test \& Evaluation and Verification \&
               Validation). While autonomous systems can use these same
               processes to establish trust, different methods within these
               processes are needed to address specific autonomous attributes
               such as ``adaptive'' or ``self-directed''. We have created a
               framework that identifies methods for engendering trust in
               automated and autonomous systems. These methods are used
               throughout a product’s lifecycle. This paradigm led us to invent
               new methods that create both systems and operational trust for
               autonomous systems. Several examples of these new methods, that
               are useful for all systems, are given.",
  publisher = "Springer US",
  pages     = "55--77",
  year      =  2016,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@INCOLLECTION{Floyd2016-da,
  title     = "Learning Trustworthy Behaviors Using an Inverse Trust Metric",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Floyd, Michael W and Drinkwater, Michael and Aha, David W",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "The addition of a robot to a human team can be beneficial if the
               robot can perform important tasks, provide additional skills, or
               otherwise help the team achieve its goals. However, if the human
               team members do not trust the robot they may underutilize it or
               excessively monitor its behavior. We present an algorithm that
               allows a robot to estimate its trustworthiness based on
               interactions with a team member and adapt its behavior in an
               attempt to increase its trustworthiness. The robot is able to
               learn as it performs behavior adaptation and increase the
               efficiency of future adaptation. We compare our approach for
               inverse trust estimation and behavior adaptation to a variant
               that does not learn. Our results, in a simulated robotics
               environment, show that both approaches can identify trustworthy
               behaviors but the learning approach does so significantly
               faster.",
  publisher = "Springer US",
  pages     = "33--53",
  year      =  2016,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Taylor2016-vh,
  title     = "Towards Modeling the Behavior of Autonomous Systems and Humans
               for Trusted Operations",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Taylor, Gavin and Mittu, Ranjeev and Sibley, Ciara and Coyne,
               Joseph",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "Greater unmanned system autonomy will lead to improvements in
               mission outcomes, survivability and safety. However, an increase
               in platform autonomy increases system complexity. For example,
               flexible autonomous platforms deployed in a range of
               environments place a burden on humans to understand evolving
               behaviors. More importantly, when problems arise within complex
               systems, they need to be managed without increasing operator
               workload. A supervisory control paradigm can reduce workload and
               allow a single human to manage multiple autonomous platforms.
               However, this requires consideration of the human as an
               integrated part of the overall system, not just as a central
               controller. This paradigm can benefit from novel and intuitive
               techniques that isolate and predict anomalous situations or
               state trajectories within complex autonomous systems in terms of
               mission context to allow efficient management of aberrant
               behavior. This information will provide the user with improved
               feedback about system behavior, which will in turn lead to more
               relevant and effective prescriptions for interaction,
               particularly during emergency procedures. This, in turn, will
               enable proper trust calibration. We also argue that by
               understanding the context of the user’s decisions or system’s
               actions (seamless integration of the human), the autonomous
               platform can provide more appropriate information to the user.",
  publisher = "Springer US",
  pages     = "11--31",
  year      =  2016,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@ARTICLE{De_Visser2017-mq,
  title       = "A Little Anthropomorphism Goes a Long Way",
  author      = "de Visser, Ewart J and Monfort, Samuel S and Goodyear,
                 Kimberly and Lu, Li and O'Hara, Martin and Lee, Mary R and
                 Parasuraman, Raja and Krueger, Frank",
  affiliation = "George Mason University, Fairfax, Virginia. George Mason
                 University, Fairfax, Virginia. George Mason University,
                 Fairfax, Virginia. Brown University, Providence, Rhode Island.
                 George Mason University, Fairfax, Virginia. George Mason
                 University, Fairfax, Virginia. George Mason University,
                 Fairfax, Virginia. Virginia Hospital Center, Fairfax Hospital,
                 Arlington, Virginia. George Mason University, Fairfax,
                 Virginia. National Institute on Alcohol Abuse and Alcoholism,
                 Bethesda, Maryland. George Mason University, Fairfax,
                 Virginia. George Mason University, Fairfax, Virginia. George
                 Mason University, Fairfax, Virginia.",
  abstract    = "OBJECTIVE: We investigated the effects of exogenous oxytocin
                 on trust, compliance, and team decision making with agents
                 varying in anthropomorphism (computer, avatar, human) and
                 reliability (100\%, 50\%). BACKGROUND: Authors of recent work
                 have explored psychological similarities in how people trust
                 humanlike automation compared with how they trust other
                 humans. Exogenous administration of oxytocin, a neuropeptide
                 associated with trust among humans, offers a unique
                 opportunity to probe the anthropomorphism continuum of
                 automation to infer when agents are trusted like another human
                 or merely a machine. METHOD: Eighty-four healthy male
                 participants collaborated with automated agents varying in
                 anthropomorphism that provided recommendations in a pattern
                 recognition task. RESULTS: Under placebo, participants
                 exhibited less trust and compliance with automated aids as the
                 anthropomorphism of those aids increased. Under oxytocin,
                 participants interacted with aids on the extremes of the
                 anthropomorphism continuum similarly to placebos but increased
                 their trust, compliance, and performance with the avatar, an
                 agent on the midpoint of the anthropomorphism continuum.
                 CONCLUSION: This study provides the first evidence that
                 administration of exogenous oxytocin affected trust,
                 compliance, and team decision making with automated agents.
                 These effects provide support for the premise that oxytocin
                 increases affinity for social stimuli in automated aids.
                 APPLICATION: Designing automation to mimic basic human
                 characteristics is sufficient to elicit behavioral trust
                 outcomes that are driven by neurological processes typically
                 observed in human-human interactions. Designers of automated
                 systems should consider the task, the individual, and the
                 level of anthropomorphism to achieve the desired outcome.",
  journal     = "Hum. Factors",
  volume      =  59,
  number      =  1,
  pages       = "116--133",
  month       =  feb,
  year        =  2017,
  keywords    = "autonomous agents; compliance and reliance; human--automation
                 interaction; neuroergonomics; oxytocin; trust in automation;
                 virtual humans;NotRead;Mendeley Import (Jan
                 17)/Assurances/Trust Background",
  language    = "en"
}

@MISC{Cai_undated-kx,
  title    = "Tuning trust using cognitive cues for better human-machine
              collaboration",
  author   = "Cai, Hua and Lin, Yingzi",
  journal  = "PsycEXTRA Dataset",
  keywords = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@ARTICLE{Inagaki1998-cl,
  title    = "Trust self-confidence and authority in human-machine systems",
  author   = "Inagaki, T and Moray, N and Itoh, M",
  journal  = "Proceedings of the IFAC man-machine systems",
  pages    = "431--436",
  year     =  1998,
  keywords = "human\_study;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@ARTICLE{Jian2000-tp,
  title    = "Foundations for an Empirically Determined Scale of Trust in
              Automated Systems",
  author   = "Jian, Jiun-Yin and Bisantz, Ann M and Drury, Colin G",
  abstract = "One component in the successful use of automated systems is the
              extent to which people trust the automation to perform
              effectively. In order to understand the relationship between
              trust in computerized systems and the use of those systems, we
              need to be able to effectively measure trust. Although
              questionnaires regarding trust have been used in prior studies,
              these questionnaires were theoretically rather than empirically
              generated and did not distinguish between three potentially
              different types of trust: human-human trust, human-machine trust,
              and trust in general. A 3-phased experiment, comprising a word
              elicitation study, a questionnaire study, and a paired comparison
              study, was performed to better understand similarities and
              differences in the concepts of trust and distrust, and among the
              different types of trust. Results indicated that trust and
              distrust can be considered opposites, rather than different
              concepts. Components of trust, in terms of words related to
              trust, were similar across the three types of trust. Results
              obtained from a cluster analysis were used to identify 12
              potential factors of trust between people and automated systems.
              These 12 factors were then used to develop a proposed scale to
              measure trust in automation.",
  journal  = "Int. J. Cogn. Ergon.",
  volume   =  4,
  number   =  1,
  pages    = "53--71",
  year     =  2000,
  keywords = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@ARTICLE{Lee1992-hx,
  title       = "Trust, control strategies and allocation of function in
                 human-machine systems",
  author      = "Lee, J and Moray, N",
  affiliation = "Department of Mechanical and Industrial Engineering,
                 University of Illinois, Urbana-Champaign 61801.",
  abstract    = "As automated controllers supplant human intervention in
                 controlling complex systems, the operators' role often changes
                 from that of an active controller to that of a supervisory
                 controller. Acting as supervisors, operators can choose
                 between automatic and manual control. Improperly allocating
                 function between automatic and manual control can have
                 negative consequences for the performance of a system.
                 Previous research suggests that the decision to perform the
                 job manually or automatically depends, in part, upon the trust
                 the operators invest in the automatic controllers. This paper
                 reports an experiment to characterize the changes in
                 operators' trust during an interaction with a semi-automatic
                 pasteurization plant, and investigates the relationship
                 between changes in operators' control strategies and trust. A
                 regression model identifies the causes of changes in trust,
                 and a 'trust transfer function' is developed using time series
                 analysis to describe the dynamics of trust. Based on a
                 detailed analysis of operators' strategies in response to
                 system faults we suggest a model for the choice between manual
                 and automatic control, based on trust in automatic controllers
                 and self-confidence in the ability to control the system
                 manually.",
  journal     = "Ergonomics",
  volume      =  35,
  number      =  10,
  pages       = "1243--1270",
  month       =  oct,
  year        =  1992,
  keywords    = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language    = "en"
}

@ARTICLE{Lucas2010-dt,
  title     = "Should We Trust Experiments on Trust?",
  author    = "Lucas, A J and Lewis, C",
  journal   = "Hum. Dev.",
  publisher = "Karger Publishers",
  volume    =  53,
  number    =  4,
  pages     = "167--172",
  month     =  "27~" # sep,
  year      =  2010,
  keywords  = "meh..;Mendeley Import (Jan 17)/Assurances/Trust
               Background;Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

@ARTICLE{Castelfranchi2000-xr,
  title    = "Trust and control: A dialectic link",
  author   = "Castelfranchi, Cristiano and Falcone, Rino",
  abstract = "The relationship between trust and control is quite relevant both
              for the very notion of trust and for modelling and implementing
              trust-control relations with autonomous systems, but it is not
              trivial at all. On the one side, it is true that where / when
              there is control there is no trust, and vice versa. However, this
              refers to a restricted notion of trust: i.e., ``trust in y,''
              which is just a part, a component of the global trust needed for
              relying on the action of another agent. It is claimed that
              control is antagonistic of this strict form of trust; but also
              that it completes and complements it for arriving to a global
              trust. In other words, putting control and guarantees is
              trust-building; it produces a sufficient trust, when trust in y's
              autonomous willingness and competence would not be enough. It is
              also argued that control requires new forms of trust: trust in
              the control itself or in the controller, trust in y as for being
              monitored and controlled; trust in possible authorities, etc.
              Finally, it is shown that, paradoxically, control could not be
              antagonistic of strict trust in y, but it can even create and
              increase it by making y more willing or more effective. In
              conclusion, depending on the circumstances, control makes y more
              reliable or less reliable; control can either decrease or
              increase trust. Two kinds of control are also analyzed,
              characterized by two different functions: ``pushing or
              influencing control'' aimed at preventing violations or mistakes,
              versus ``safety, correction, or adjustment control'' aimed at
              preventing failure or damages after a violation or a mistake. A
              good theory of trust cannot be complete without a theory of
              control.",
  journal  = "Appl. Artif. Intell.",
  volume   =  14,
  number   =  8,
  pages    = "799--823",
  year     =  2000,
  keywords = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@ARTICLE{Schillo2000-ap,
  title    = "Using trust for detecting deceitful agents in artificial
              societies",
  author   = "Schillo, Michael and Funk, Petra and Rovatsos, Michael",
  abstract = "Trust is one of the most important concepts guiding
              decision-making and contracting in human societies. In artificial
              societies, this concept has been neglected until recently. The
              inherent benevolence assumption implemented in many multiagent
              systems can have hazardous consequences when dealing with deceit
              in open systems. The aim of this paper is to establish a
              mechanism that helps agents to cope with environments inhabited
              by both selfish and cooperative entities. This is achieved by
              enabling agents to evaluate trust in others. A formalization and
              an algorithm for trust are presented so that agents can
              autonomously deal with deception and identify trustworthy parties
              in open systems. The approach is twofold: agents can observe the
              behavior of others and thus collect information for establishing
              an initial trust model. In order to adapt quickly to a new or
              rapidly changing environment, one enables agents to also make use
              of observations from other agents. The practical relevance of
              these ideas is demonstrated by means of a direct mapping from a
              scenario to electronic commerce.",
  journal  = "Appl. Artif. Intell.",
  volume   =  14,
  number   =  8,
  pages    = "825--848",
  year     =  2000,
  keywords = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@ARTICLE{Muir1987-mk,
  title    = "Trust between humans and machines, and the design of decision
              aids",
  author   = "Muir, Bonnie M",
  abstract = "A problem in the design of decision aids is how to design them so
              that decision makers will trust them and therefore use them
              appropriately. This problem is approached in this paper by taking
              models of trust between humans as a starting point, and extending
              these to the human-machine relationship. A definition and model
              of human-machine trust are proposed, and the dynamics of trust
              between humans and machines are examined. Based upon this
              analysis, recommendations are made for calibrating users' trust
              in decision aids.",
  journal  = "Int. J. Man. Mach. Stud.",
  volume   =  27,
  number   =  5,
  pages    = "527--539",
  month    =  "12~" # nov,
  year     =  1987,
  keywords = "
              very\_similar\_to\_mine;assurances;decision\_support;automation;trust\_formal\_treatment;assurance\_explicit;in\_paper;Mendeley
              Import (Jan 17)/Assurances/Trust Background;Mendeley Import (Jan
              17)/Assurances"
}

@ARTICLE{Zacharia2000-ur,
  title    = "Trust management through reputation mechanisms",
  author   = "Zacharia, Giorgos and Maes, Pattie",
  abstract = "The members of electronic communities are often unrelated to each
              other; they may have never met and have no information on each
              other's reputation. This kind of information is vital in
              electronic commerce interactions, where the potential
              counterpart's reputation can be a significant factor in the
              negotiation strategy. Two complementary reputation mechanisms are
              investigated which rely on collaborative rating and personalized
              evaluation of the various ratings assigned to each user. While
              these reputation mechanisms are developed in the context of
              electronic commerce, it is believed that they may have
              applicability in other types of electronic communities such as
              chatrooms, newsgroups, mailing lists, etc.",
  journal  = "Appl. Artif. Intell.",
  volume   =  14,
  number   =  9,
  pages    = "881--907",
  year     =  2000,
  keywords = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@ARTICLE{Sabater2005-wr,
  title     = "Review on Computational Trust and Reputation Models",
  author    = "Sabater, Jordi and Sierra, Carles",
  abstract  = "The scientific research in the area of computational mechanisms
               for trust and reputation in virtual societies is a recent
               discipline oriented to increase the reliability and performance
               of electronic communities. Computer science has moved from the
               paradigm of isolated machines to the paradigm of networks and
               distributed computing. Likewise, artificial intelligence is
               quickly moving from the paradigm of isolated and non-situated
               intelligence to the paradigm of situated, social and collective
               intelligence. The new paradigm of the so called intelligent or
               autonomous agents and multi-agent systems (MAS) together with
               the spectacular emergence of the information society
               technologies (specially reflected by the popularization of
               electronic commerce) are responsible for the increasing interest
               on trust and reputation mechanisms applied to electronic
               societies. This review wants to offer a panoramic view on
               current computational trust and reputation models.",
  journal   = "Artif Intell Rev",
  publisher = "Kluwer Academic Publishers",
  volume    =  24,
  number    =  1,
  pages     = "33--60",
  month     =  "1~" # sep,
  year      =  2005,
  keywords  = "Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@INPROCEEDINGS{Wagner_undated-bk,
  title           = "Exploring human-robot trust: Insights from the first 1000
                     subjects",
  booktitle       = "2015 International Conference on Collaboration
                     Technologies and Systems ({CTS})",
  author          = "Wagner, Alan R",
  publisher       = "IEEE",
  pages           = "485--486",
  keywords        = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust
                     Background",
  conference      = "2015 International Conference on Collaboration
                     Technologies and Systems (CTS)"
}

@INPROCEEDINGS{Ximenes_undated-wd,
  title           = "Extreme human-robot interfaces: Increasing trust and
                     assurance around robots",
  booktitle       = "The 23rd {IEEE} International Symposium on Robot and Human
                     Interactive Communication",
  author          = "Ximenes, Bianca H and Moreira, Icaro M and Kelner, Judith",
  publisher       = "IEEE",
  pages           = "1006--1011",
  keywords        = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust
                     Background",
  conference      = "2014 RO-MAN: The 23rd IEEE International Symposium on
                     Robot and Human Interactive Communication"
}

@INPROCEEDINGS{Billings2012-vk,
  title     = "Human-robot interaction: Developing trust in robots",
  booktitle = "2012 7th {ACM/IEEE} International Conference on {Human-Robot}
               Interaction ({HRI})",
  author    = "Billings, D R and Schaefer, K E and Chen, J Y C and Hancock, P A",
  abstract  = "In all human-robot interaction, trust is an important element to
               consider because the presence or absence of trust certainly
               impacts the ultimate outcome of that interaction. Limited
               research exists that delineates the development and maintenance
               of this trust in various operational contexts. Our own prior
               research has investigated theoretical and empirically supported
               antecedents of human-robot trust. Here, we describe progress to
               date relating to the development of a comprehensive human-robot
               trust model based on our ongoing program of research.",
  pages     = "109--110",
  month     =  mar,
  year      =  2012,
  keywords  = "human-robot interaction;human-robot interaction;human-robot
               trust model;trust development;Collaboration;Current
               measurement;Educational institutions;Human
               factors;Humans;Robots;Training;Human-Robot
               Interaction;Trust;NotRead;Mendeley Import (Jan
               17)/Assurances/Trust Background"
}

@INPROCEEDINGS{Kaniarasu_undated-ek,
  title           = "Robot confidence and trust alignment",
  booktitle       = "2013 8th {ACM/IEEE} International Conference on
                     {Human-Robot} Interaction ({HRI})",
  author          = "Kaniarasu, Poornima and Steinfeld, Aaron and Desai, Munjal
                     and Yanco, Holly",
  publisher       = "IEEE",
  pages           = "155--156",
  keywords        = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust
                     Background",
  conference      = "2013 8th ACM/IEEE International Conference on Human-Robot
                     Interaction (HRI)"
}

@INPROCEEDINGS{Sanders2011-kz,
  title       = "A Model of human-robot trust: Theoretical model development",
  booktitle   = "Proceedings of the Human Factors and Ergonomics Society Annual
                 Meeting",
  author      = "Sanders, Tracy and Oleson, Kristin E and Billings, D R and
                 Chen, Jessie Y C and Hancock, P A",
  volume      =  55,
  pages       = "1432--1436",
  institution = "SAGE Publications Sage CA: Los Angeles, CA",
  year        =  2011,
  keywords    = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@ARTICLE{Hancock2011-dh,
  title       = "A meta-analysis of factors affecting trust in human-robot
                 interaction",
  author      = "Hancock, Peter A and Billings, Deborah R and Schaefer, Kristin
                 E and Chen, Jessie Y C and de Visser, Ewart J and Parasuraman,
                 Raja",
  affiliation = "University of Central Florida, Orlando, FL 32816, USA.",
  abstract    = "OBJECTIVE: We evaluate and quantify the effects of human,
                 robot, and environmental factors on perceived trust in
                 human-robot interaction (HRI). BACKGROUND: To date, reviews of
                 trust in HRI have been qualitative or descriptive. Our
                 quantitative review provides a fundamental empirical
                 foundation to advance both theory and practice. METHOD:
                 Meta-analytic methods were applied to the available literature
                 on trust and HRI. A total of 29 empirical studies were
                 collected, of which 10 met the selection criteria for
                 correlational analysis and 11 for experimental analysis. These
                 studies provided 69 correlational and 47 experimental effect
                 sizes. RESULTS: The overall correlational effect size for
                 trust was r = +0.26,with an experimental effect size of d =
                 +0.71. The effects of human, robot, and environmental
                 characteristics were examined with an especial evaluation of
                 the robot dimensions of performance and attribute-based
                 factors. The robot performance and attributes were the largest
                 contributors to the development of trust in HRI. Environmental
                 factors played only a moderate role. CONCLUSION: Factors
                 related to the robot itself, specifically, its performance,
                 had the greatest current association with trust, and
                 environmental factors were moderately associated. There was
                 little evidence for effects of human-related factors.
                 APPLICATION: The findings provide quantitative estimates of
                 human, robot, and environmental factors influencing HRI trust.
                 Specifically, the current summary provides effect size
                 estimates that are useful in establishing design and training
                 guidelines with reference to robot-related factors of HRI
                 trust. Furthermore, results indicate that improper trust
                 calibration may be mitigated by the manipulation of robot
                 design. However, many future research needs are identified.",
  journal     = "Hum. Factors",
  volume      =  53,
  number      =  5,
  pages       = "517--527",
  month       =  oct,
  year        =  2011,
  keywords    = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language    = "en"
}

@ARTICLE{Laursen2013-ul,
  title    = "Robot to human: ``Trust me''",
  author   = "Laursen, L",
  abstract = "In a crisis control center, several teams of firefighters in
              Montelibretti, Italy, used laptops to guide a robotic ground
              vehicle into a smoke-filled highway tunnel. Inside, overturned
              motorcycles, errant cars, and spilled pallets impeded the
              robot{\^A}\?\`s progress. The rover, equipped with a video camera
              and autonomous navigation software, was capable of crawling
              through the wreckage unguided while humans monitored the video
              footage for accident victims. But most of the time the
              firefighters took manual control once the robot was a few meters
              into the tunnel.",
  journal  = "IEEE Spectrum",
  volume   =  50,
  number   =  3,
  pages    = "18--18",
  month    =  mar,
  year     =  2013,
  keywords = "Accidents;Cameras;Control systems;Disasters;Fires;Mobile
              robots;NotRead;Mendeley Import (Jan 17)/Assurances/Trust
              Background"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Schaefer2016-yq,
  title     = "Measuring Trust in Human Robot Interactions: Development of the
               ``Trust Perception {Scale-HRI''}",
  booktitle = "Robust Intelligence and Trust in Autonomous Systems",
  author    = "Schaefer, Kristin E",
  editor    = "Mittu, Ranjeev and Sofge, Donald and Wagner, Alan and Lawless, W
               F",
  abstract  = "As robots penetrate further into the everyday environments,
               trust in these robots becomes a crucial issue. The purpose of
               this work was to create and validate a reliable scale that could
               measure changes in an individual’s trust in a robot. Assessment
               of current trust theory identified measurable antecedents
               specific to the human, the robot, and the environment. Six
               experiments subsumed the development of the 40 item trust scale.
               Scale development included the creation of a 156 item pool. Two
               experiments identified the robot features and perceived
               functional characteristics that were related to the
               classification of a machine as a robot for this item pool. Item
               pool reduction techniques and subject matter expert (SME)
               content validation were used to reduce the scale to 42 items.
               The two final experiments were then conducted to validate the
               scale. The finalized 40 item pre-post interaction trust scale
               was designed to measure trust perceptions specific to
               human-robot interaction. The scale measures trust on a 0--100 \%
               rating scale and provides a percentage trust score. A 14 item
               sub-scale of this final version of the test recommended by SMEs
               may be sufficient for some HRI tasks, and the implications of
               this proposition are discussed.",
  publisher = "Springer US",
  pages     = "191--218",
  year      =  2016,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@INPROCEEDINGS{Kaniarasu_undated-gj,
  title           = "Effects of blame on trust in human robot interaction",
  booktitle       = "The 23rd {IEEE} International Symposium on Robot and Human
                     Interactive Communication",
  author          = "Kaniarasu, Poornima and Steinfeld, Aaron M",
  publisher       = "IEEE",
  pages           = "850--855",
  keywords        = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust
                     Background",
  conference      = "2014 RO-MAN: The 23rd IEEE International Symposium on
                     Robot and Human Interactive Communication"
}

@INCOLLECTION{Wang2014-wk,
  title     = "{Human-Robot} Mutual Trust in (Semi)autonomous Underwater Robots",
  booktitle = "Cooperative Robots and Sensor Networks 2014",
  author    = "Wang, Yue and Shi, Zhenwu and Wang, Chuanfeng and Zhang, Fumin",
  editor    = "Koubaa, Anis and Khelil, Abdelmajid",
  abstract  = "It is envisioned that a human operator is able to monitor and
               control one or more (semi)autonomous underwater robots
               simultaneously in future marine operations. To enable such
               operations, a human operator must trust the capability of a
               robot to perform tasks autonomously, and the robot must
               establish its trust to the human operator based on human
               performance and follow guidance accordingly. Therefore, we seek
               to i model the mutual trust between humans and robots
               (especially (semi)autonomous underwater robots in this chapter),
               and ii) develop a set of trust-based algorithms to control the
               human-robot team so that the mutual trust level can be
               maintained at a desired level. We propose a time series based
               mutual trust model that takes into account robot performance,
               human performance and overall human-robot system fault rates.
               The robot performance model captures the performance evolution
               of a robot under autonomous mode and teleoperated mode,
               respectively. Furthermore, we specialize the robot performance
               model of a YSI EcoMapper autonomous underwater robot based on
               its distance to a desired waypoint. The human performance model
               is inspired by the Yerkes-Dodson law in psychology, which
               describes the relationship between human arousal and
               performance. Based on the mutual trust model, we first study a
               simple case of one human operator controlling a single robot and
               propose a trust-triggered control strategy depending on the
               limit conditions of the desired trust region. The method is then
               enhanced for the case of one human operator controlling a swarm
               of robots. In this framework, a periodic trust-based control
               strategy with a highest-trust-first scheduling algorithm is
               proposed. Matlab simulation results are provided to validate the
               proposed model and control strategies that guarantee effective
               real-time scheduling of teleoperated and autonomous controls in
               both one human one underwater robot case and one human multiple
               underwater robots case.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "115--137",
  series    = "Studies in Computational Intelligence",
  year      =  2014,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@INPROCEEDINGS{Kaniarasu2013-ho,
  title     = "Robot Confidence and Trust Alignment",
  booktitle = "Proceedings of the 8th {ACM/IEEE} International Conference on
               Human-robot Interaction",
  author    = "Kaniarasu, Poornima and Steinfeld, Aaron and Desai, Munjal and
               Yanco, Holly",
  publisher = "IEEE Press",
  pages     = "155--156",
  series    = "HRI '13",
  year      =  2013,
  address   = "Piscataway, NJ, USA",
  keywords  = "automation, experiments, robot confidence,
               trust;human\_study;assurance\_competence;ai\_motion\_manipulation;very\_similar\_to\_mine;trust\_formal\_treatment;assurance\_explicit;in\_paper;Mendeley
               Import (Jan 17)/Assurances/Trust Background;Mendeley Import (Jan
               17)/Assurances"
}

@ARTICLE{McDermott2011-ok,
  title     = "Adaptive aiding of human-robot teaming: Effects of imperfect
               automation on performance, trust, and workload",
  author    = "McDermott, Patricia L and Riley, Jennifer M and Gillan, Douglas
               J and Cuevas, Haydee M and de Visser, Ewart and Parasuraman,
               Raja",
  journal   = "Journal of Cognitive Engineering and Decision Making",
  publisher = "SAGE Publications Sage CA: Los Angeles, CA",
  volume    =  5,
  number    =  2,
  pages     = "209--231",
  year      =  2011,
  keywords  = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@TECHREPORT{Billings2012-ao,
  title       = "Human-animal trust as an analog for human-robot trust: A
                 review of current evidence",
  author      = "Billings, Deborah R and Schaefer, Kristin E and Chen, Jessie Y
                 and Kocsis, Vivien and Barrera, Maria and Cook, Jacquelyn and
                 Ferrer, Michelle and Hancock, Peter A",
  institution = "DTIC Document",
  year        =  2012,
  keywords    = "NotRead;Mendeley Import (Jan 17)/Assurances/Trust Background"
}

@INPROCEEDINGS{Oleson2011-mw,
  title     = "Antecedents of trust in human-robot collaborations",
  booktitle = "2011 {IEEE} International {Multi-Disciplinary} Conference on
               Cognitive Methods in Situation Awareness and Decision Support
               ({CogSIMA})",
  author    = "Oleson, K E and Billings, D R and Kocsis, V and Chen, J Y C and
               Hancock, P A",
  abstract  = "Robotic systems are being introduced into military echelons to
               extend warfighter capabilities in complex, dynamic environments.
               While these systems are designed to complement human
               capabilities (e.g., aiding in battlefield situation awareness
               and decision making, etc), they are often misused or disused
               because the user does not have an appropriate level of trust in
               his or her robotic counterpart(s). We describe a continuing body
               of research that identifies factors impacting a human's level of
               trust in a robotic teammate. The factors identified to date can
               be categorized as human influences (e.g., individual differences
               in terms of personality, experience, culture), machine
               influences (e.g., robotic platform, robot performance in terms
               of levels of automation, failure rates, false alarms), and
               environmental influences (e.g. task type, operational
               environment, shared mental models). A framework for human-robot
               team trust was constructed, which is evolving into a working
               model contingent upon the results of an on-going meta-analysis.",
  pages     = "175--178",
  month     =  feb,
  year      =  2011,
  keywords  = "human factors;human-robot interaction;military
               systems;environmental influence;human capability;human-robot
               collaboration;human-robot team trust;meta analysis;military
               echelon;robotic teammate;warflghter
               capability;Automation;Collaboration;Humans;Robot
               kinematics;Service robots;Training;Human-robot interaction;human
               factors;human-robot teams;trust;NotRead;Mendeley Import (Jan
               17)/Assurances/Trust Background"
}

@INPROCEEDINGS{Carter2003-ni,
  title     = "Value centric trust in multiagent systems",
  booktitle = "Proceedings {IEEE/WIC} International Conference on Web
               Intelligence ({WI} 2003)",
  author    = "Carter, J and Ghorbani, A A",
  abstract  = "We focus on the design and implementation of a new model of
               trust based on the formalizations of reputation, self-esteem,
               and similarity within an agent. We universalize reputation
               through the use of values found within all multiagent systems.
               The following values are manifested within multiagent systems:
               responsibility, honesty, independence, obedience, ambition,
               helpfulness, capability, knowledgability, and cost-efficiency.
               Manifestations of these values lead to a more universalized
               approach to formalizing reputation. This new model of trust is
               examined within the context of an e-commerce framework. It is
               analyzed with respect to stability, scalability, accuracy in
               attaining e-commerce objectives, and general effectiveness in
               discouraging untrustworthy behavior. Based on the experiments,
               the model is scalable and stable dependent upon the agent
               population of buyers and sellers. It achieves its primary
               objective of discouraging untrustworthy behavior as measured
               through the acceleration of Gross Domestic Product growth over
               time.",
  pages     = "3--9",
  month     =  oct,
  year      =  2003,
  keywords  = "economic indicators;electronic commerce;multi-agent
               systems;security of data;Gross Domestic
               Product;cost-efficiency;e-commerce;knowledgability;multiagent
               systems;responsibility;untrustworthy behavior;value centric
               trust;Accelerated aging;Computer science;Context
               modeling;Electronic commerce;Humans;Multiagent
               systems;Niobium;Scalability;Stability analysis;Time
               measurement;NotRead;Mendeley Import (Jan 17)/Assurances/Trust
               Background"
}

@INPROCEEDINGS{Castelfranchi2003-ue,
  title     = "Trust in Information Sources As a Source for Trust: A Fuzzy
               Approach",
  booktitle = "Proceedings of the Second International Joint Conference on
               Autonomous Agents and Multiagent Systems",
  author    = "Castelfranchi, Cristiano and Falcone, Rino and Pezzulo, Giovanni",
  publisher = "ACM",
  pages     = "89--96",
  series    = "AAMAS '03",
  year      =  2003,
  address   = "New York, NY, USA",
  keywords  = "beliefs, fuzzy cognitive maps, medical house assistance
               scenarios, sources of beliefs, trust;Mendeley Import (Jan
               17)/Assurances/Trust Background"
}

@INCOLLECTION{Falcone2001-jc,
  title     = "Social Trust: A Cognitive Approach",
  booktitle = "Trust and Deception in Virtual Societies",
  author    = "Falcone, Rino and Castelfranchi, Cristiano",
  editor    = "Castelfranchi, Cristiano and Tan, Yao-Hua",
  abstract  = "As it was been written in the call of the original workshop ``In
               recent research on electronic commerce'' trust has been
               recognized as one of the key factors for successful electronic
               commerce adoption. In electronic commerce problems of trust are
               magnified, because agents reach out far beyond their familiar
               trade environments. Also it is far from obvious whether existing
               paper-based techniques for fraud detection and prevention are
               adequate to establish trust in an electronic network environment
               where you usually never meet your trade partner face to face,
               and where messages can be read or copied a million times without
               leaving any trace. With the growing impact of electronic
               commerce distance trust building becomes more and more
               important, and better models of trust and deception are needed.
               One trend is that in electronic communication channels extra
               agents, the so called Trusted Third Parties, are introduced in
               an agent community that take care of trust building among the
               other agents in the network. But in fact different kind of trust
               are needed and should be modelled and supported: trust in the
               environment and in the infrastructure (the socio-technical
               system); trust in your agent and in mediating agents; trust in
               the potential partners; trust in the warrantors and authorities
               (if any).",
  publisher = "Springer Netherlands",
  pages     = "55--90",
  year      =  2001,
  keywords  = "Mendeley Import (Jan 17)/Assurances/Trust Background",
  language  = "en"
}

@ARTICLE{Granatyr2015-hy,
  title     = "Trust and Reputation Models for Multiagent Systems",
  author    = "Granatyr, Jones and Botelho, Vanderson and Lessing, Otto Robert
               and Scalabrin, Edson Em{\'\i}lio and Barth{\`e}s, Jean-Paul and
               Enembreck, Fabr{\'\i}cio",
  journal   = "ACM Comput. Surv.",
  publisher = "ACM",
  volume    =  48,
  number    =  2,
  pages     = "27:1--27:42",
  month     =  oct,
  year      =  2015,
  address   = "New York, NY, USA",
  keywords  = "Trust, reputation, trust model;Mendeley Import (Jan
               17)/Assurances/Trust Background"
}

@MISC{Lloyd_undated-bb,
  title        = "Automatic Statistician",
  booktitle    = "Automatic Statistician",
  author       = "Lloyd, James Robert",
  howpublished = "\url{https://www.automaticstatistician.com/examples/}",
  note         = "Accessed: 2017-2-16",
  keywords     = "trust\_popular\_media;trust\_academic\_conversation;Mendeley
                  Import (Jan 17)/Assurances"
}

@ARTICLE{Ghahramani2015-yq,
  title       = "Probabilistic machine learning and artificial intelligence",
  author      = "Ghahramani, Zoubin",
  affiliation = "Department of Engineering, University of Cambridge,
                 Trumpington Street, Cambridge CB2 1PZ, UK.",
  abstract    = "How can a machine learn from experience? Probabilistic
                 modelling provides a framework for understanding what learning
                 is, and has therefore emerged as one of the principal
                 theoretical and practical approaches for designing machines
                 that learn from data acquired through experience. The
                 probabilistic framework, which describes how to represent and
                 manipulate uncertainty about models and predictions, has a
                 central role in scientific data analysis, machine learning,
                 robotics, cognitive science and artificial intelligence. This
                 Review provides an introduction to this framework, and
                 discusses some of the state-of-the-art advances in the field,
                 namely, probabilistic programming, Bayesian optimization, data
                 compression and automatic model discovery.",
  journal     = "Nature",
  volume      =  521,
  number      =  7553,
  pages       = "452--459",
  month       =  "28~" # may,
  year        =  2015,
  keywords    = "trust\_popular\_media;trust\_academic\_conversation;Mendeley
                 Import (Jan 17)/Assurances",
  language    = "en"
}

@ARTICLE{Castelvecchi2016-mr,
  title    = "Can we open the black box of {AI}?",
  author   = "Castelvecchi, Davide",
  journal  = "Nature",
  volume   =  538,
  number   =  7623,
  pages    = "20--23",
  month    =  "6~" # oct,
  year     =  2016,
  keywords = "trust\_popular\_media;trust\_academic\_conversation;Mendeley
              Import (Jan 17)/Assurances",
  language = "en"
}

@MISC{Khosravi2016-ke,
  title        = "Will You Trust {AI} To Be Your New Doctor?",
  booktitle    = "Forbes",
  author       = "Khosravi, Bijan",
  abstract     = "Whatever the case, AI has always had the appeal of a
                  futuristic spectacle; fascinating but not likely to take
                  place in our lifetimes. That image is about to change,
                  particularly in the healthcare industry.",
  month        =  "24~" # mar,
  year         =  2016,
  howpublished = "\url{http://www.forbes.com/sites/bijankhosravi/2016/03/24/will-you-trust-ai-to-be-your-new-doctor-a-five-year-outcome/}",
  note         = "Accessed: 2017-2-16",
  keywords     = "
                  trust\_popular\_media;trust\_corporate\_conversation;Mendeley
                  Import (Jan 17)/Assurances"
}

@MISC{Banavar2016-nm,
  title        = "What It Will Take for Us to Trust {AI}",
  booktitle    = "Harvard Business Review",
  author       = "Banavar, Guru",
  abstract     = "Ethics and accountability.",
  month        =  "29~" # nov,
  year         =  2016,
  howpublished = "\url{https://hbr.org/2016/11/what-it-will-take-for-us-to-trust-ai}",
  note         = "Accessed: 2017-2-16",
  keywords     = "
                  trust\_popular\_media;trust\_corporate\_conversation;Mendeley
                  Import (Jan 17)/Assurances"
}

@ARTICLE{Cohn1996-fd,
  title         = "Active Learning with Statistical Models",
  author        = "Cohn, D A and Ghahramani, Z and Jordan, M I",
  abstract      = "For many types of machine learning algorithms, one can
                   compute the statistically `optimal' way to select training
                   data. In this paper, we review how optimal data selection
                   techniques have been used with feedforward neural networks.
                   We then show how the same principles may be used to select
                   data for two alternative, statistically-based learning
                   architectures: mixtures of Gaussians and locally weighted
                   regression. While the techniques for neural networks are
                   computationally expensive and approximate, the techniques
                   for mixtures of Gaussians and locally weighted regression
                   are both efficient and accurate. Empirically, we observe
                   that the optimality criterion sharply decreases the number
                   of training examples the learner needs in order to achieve
                   good performance.",
  month         =  "1~" # mar,
  year          =  1996,
  keywords      = "NotRead",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "cs/9603104"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Hutter2006-ak,
  title      = "Performance Prediction and Automated Tuning of Randomized and
                Parametric Algorithms",
  booktitle  = "Principles and Practice of Constraint Programming - {CP} 2006",
  author     = "Hutter, Frank and Hamadi, Youssef and Hoos, Holger H and
                Leyton-Brown, Kevin",
  editor     = "Benhamou, Fr{\'e}d{\'e}ric",
  abstract   = "Machine learning can be used to build models that predict the
                run-time of search algorithms for hard combinatorial problems.
                Such empirical hardness models have previously been studied for
                complete, deterministic search algorithms. In this work, we
                demonstrate that such models can also make surprisingly
                accurate predictions of the run-time distributions of
                incomplete and randomized search methods, such as stochastic
                local search algorithms. We also show for the first time how
                information about an algorithm’s parameter settings can be
                incorporated into a model, and how such models can be used to
                automatically adjust the algorithm’s parameters on a
                per-instance basis in order to optimize its performance.
                Empirical results for Novelty + and SAPS on structured and
                unstructured SAT instances show very good predictive
                performance and significant speedups of our automatically
                determined parameter settings when compared to the default and
                best fixed distribution-specific parameter settings.",
  publisher  = "Springer Berlin Heidelberg",
  pages      = "213--228",
  series     = "Lecture Notes in Computer Science",
  month      =  "25~" # sep,
  year       =  2006,
  keywords   = "assurance\_implicit;trust\_informal\_treatment;Mendeley Import
                (Jan 17)/Assurances",
  language   = "en",
  conference = "International Conference on Principles and Practice of
                Constraint Programming"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Kuter2015-qh,
  title     = "Computational Mechanisms to Support Reporting of Self Confidence
               of {Automated/Autonomous} Systems",
  booktitle = "2015 {AAAI} Fall Symposium Series",
  author    = "Kuter, Ugur and Miller, Chris",
  abstract  = "This paper describes a new candidate method of computing
               autonomous ``self confidence.'' We describe how to analyze a
               plan for possible but unexpected break down cases and how to
               adapt the plan to circumvent those conditions. We view the
               result plan as more stable than the original one. The ability of
               achieving such plan stability is the core of how we propose to
               compute a system’s self confidence in its decisions and plans.
               This paper summarizes this approach and presents a preliminary
               evaluation that shows our approach is promising.",
  month     =  "23~" # sep,
  year      =  2015,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;perf\_prediction;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{OCallaghan2005-wa,
  title      = "Generating Corrective Explanations for Interactive Constraint
                Satisfaction",
  booktitle  = "Principles and Practice of Constraint Programming - {CP} 2005",
  author     = "O’Callaghan, Barry and O’Sullivan, Barry and Freuder, Eugene C",
  editor     = "van Beek, Peter",
  abstract   = "Interactive tasks such as online configuration and e-commerce
                can be modelled as constraint satisfaction problems (CSPs).
                These can be solved interactively by a user assigning values to
                variables. The user may require advice and explanations from a
                system to help him/her find a satisfactory solution.
                Explanations of failure in constraint programming tend to focus
                on conflict. However, what is really desirable is an
                explanation that is corrective in the sense that it provides
                the basis for moving forward in the problem-solving process.
                More specifically, when faced with a dead-end, or when a
                desirable value has been removed from a domain, we need to
                compute alternative assignments for a subset of the assigned
                variables that enables the user to move forward. This paper
                defines this notion of corrective explanation, and proposes an
                algorithm to generate such explanations. The approach is shown
                to perform well on both real-world configuration benchmarks and
                randomly generated problems.",
  publisher  = "Springer Berlin Heidelberg",
  pages      = "445--459",
  series     = "Lecture Notes in Computer Science",
  month      =  "1~" # oct,
  year       =  2005,
  keywords   = "meh..;Mendeley Import (Jan 17)/Assurances",
  language   = "en",
  conference = "International Conference on Principles and Practice of
                Constraint Programming"
}

@INPROCEEDINGS{Wallace2001-fm,
  title     = "Explanations for whom",
  booktitle = "{CP01} Workshop on {User-Interaction} in Constraint Satisfaction",
  author    = "Wallace, Richard J and Freuder, Eugene C",
  year      =  2001,
  keywords  = "trust\_informal\_treatment;assurance\_explicit;explain;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Hadfield-Menell2016-ws,
  title         = "The {Off-Switch} Game",
  author        = "Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter
                   and Russell, Stuart",
  abstract      = "It is clear that one of the primary tools we can use to
                   mitigate the potential risk from a misbehaving AI system is
                   the ability to turn the system off. As the capabilities of
                   AI systems improve, it is important to ensure that such
                   systems do not adopt subgoals that prevent a human from
                   switching them off. This is a challenge because many
                   formulations of rational agents create strong incentives for
                   self-preservation. This is not caused by a built-in
                   instinct, but because a rational agent will maximize
                   expected utility and cannot achieve whatever objective it
                   has been given if it is dead. Our goal is to study the
                   incentives an agent has to allow itself to be switched off.
                   We analyze a simple game between a human H and a robot R,
                   where H can press R's off switch but R can disable the off
                   switch. A traditional agent takes its reward function for
                   granted: we show that such agents have an incentive to
                   disable the off switch, except in the special case where H
                   is perfectly rational. Our key insight is that for R to want
                   to preserve its off switch, it needs to be uncertain about
                   the utility associated with the outcome, and to treat H's
                   actions as important observations about that utility. (R
                   also has no incentive to switch itself off in this setting.)
                   We conclude that giving machines an appropriate level of
                   uncertainty about their objectives leads to safer designs,
                   and we argue that this setting is a useful generalization of
                   the classical AI paradigm of rational agents.",
  month         =  "24~" # nov,
  year          =  2016,
  keywords      = "
                   Safety\_AI;trust\_informal\_treatment;assurance\_implicit;Mendeley
                   Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1611.08219"
}

@ARTICLE{Leyton-Brown2009-yr,
  title     = "Empirical Hardness Models: Methodology and a Case Study on
               Combinatorial Auctions",
  author    = "Leyton-Brown, Kevin and Nudelman, Eugene and Shoham, Yoav",
  journal   = "J. ACM",
  publisher = "ACM",
  volume    =  56,
  number    =  4,
  pages     = "22:1--22:52",
  month     =  jul,
  year      =  2009,
  address   = "New York, NY, USA",
  keywords  = "Empirical analysis of algorithms, algorithm portfolios,
               combinatorial auctions, runtime
               prediction;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances"
}

@MASTERSTHESIS{Aitken2016-cv,
  title    = "Assured {Human-Autonomy} Interaction through Machine
              {Self-Confidence}",
  author   = "Aitken, Matthew",
  year     =  2016,
  school   = "UNIVERSITY OF COLORADO AT BOULDER",
  keywords = "assurance\_explicit;trust\_formal\_treatment;in\_paper;Mendeley
              Import (Jan 17)/Assurances"
}

@BOOK{Back1996-jp,
  title     = "Evolutionary Algorithms in Theory and Practice: Evolution
               Strategies, Evolutionary Programming, Genetic Algorithms",
  author    = "Back, Thomas",
  abstract  = "This book presents a unified view of evolutionary algorithms:
               the exciting new probabilistic search tools inspired by
               biological models that have immense potential as practical
               problem-solvers in a wide variety of settings, academic,
               commercial, and industrial. In this work, the author compares
               the three most prominent representatives of evolutionary
               algorithms: genetic algorithms, evolution strategies, and
               evolutionary programming. The algorithms are presented within a
               unified framework, thereby clarifying the similarities and
               differences of these methods. The author also presents new
               results regarding the role of mutation and selection in genetic
               algorithms, showing how mutation seems to be much more important
               for the performance of genetic algorithms than usually assumed.
               The interaction of selection and mutation, and the impact of the
               binary code are further topics of interest. Some of the
               theoretical results are also confirmed by performing an
               experiment in meta-evolution on a parallel computer. The
               meta-algorithm used in this experiment combines components from
               evolution strategies and genetic algorithms to yield a hybrid
               capable of handling mixed integer optimization problems. As a
               detailed description of the algorithms, with practical
               guidelines for usage and implementation, this work will interest
               a wide range of researchers in computer science and engineering
               disciplines, as well as graduate students in these fields.",
  publisher = "Oxford University Press",
  month     =  "11~" # jan,
  year      =  1996,
  keywords  = "Mendeley Import (Jan 17)/BayesOpt",
  language  = "en"
}

@ARTICLE{Tran2015-lq,
  title         = "The Variational Gaussian Process",
  author        = "Tran, Dustin and Ranganath, Rajesh and Blei, David M",
  abstract      = "Variational inference is a powerful tool for approximate
                   inference, and it has been recently applied for
                   representation learning with deep generative models. We
                   develop the variational Gaussian process (VGP), a Bayesian
                   nonparametric variational family, which adapts its shape to
                   match complex posterior distributions. The VGP generates
                   approximate posterior samples by generating latent inputs
                   and warping them through random non-linear mappings; the
                   distribution over random mappings is learned during
                   inference, enabling the transformed outputs to adapt to
                   varying complexity. We prove a universal approximation
                   theorem for the VGP, demonstrating its representative power
                   for learning any model. For inference we present a
                   variational objective inspired by auto-encoders and perform
                   black box inference over a wide class of models. The VGP
                   achieves new state-of-the-art results for unsupervised
                   learning, inferring models such as the deep latent Gaussian
                   model and the recently proposed DRAW.",
  month         =  "20~" # nov,
  year          =  2015,
  keywords      = "NotRead",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1511.06499"
}

@ARTICLE{Theis2015-tx,
  title         = "A note on the evaluation of generative models",
  author        = "Theis, Lucas and van den Oord, A{\"a}ron and Bethge,
                   Matthias",
  abstract      = "Probabilistic generative models can be used for compression,
                   denoising, inpainting, texture synthesis, semi-supervised
                   learning, unsupervised feature learning, and other tasks.
                   Given this wide range of applications, it is not surprising
                   that a lot of heterogeneity exists in the way these models
                   are formulated, trained, and evaluated. As a consequence,
                   direct comparison between models is often difficult. This
                   article reviews mostly known but often underappreciated
                   properties relating to the evaluation and interpretation of
                   generative models with a focus on image models. In
                   particular, we show that three of the currently most
                   commonly used criteria---average log-likelihood, Parzen
                   window estimates, and visual fidelity of samples---are
                   largely independent of each other when the data is
                   high-dimensional. Good performance with respect to one
                   criterion therefore need not imply good performance with
                   respect to the other criteria. Our results show that
                   extrapolation from one criterion to another is not warranted
                   and generative models need to be evaluated directly with
                   respect to the application(s) they were intended for. In
                   addition, we provide examples demonstrating that Parzen
                   window estimates should generally be avoided.",
  month         =  "5~" # nov,
  year          =  2015,
  keywords      = "Mendeley Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1511.01844"
}

@ARTICLE{Bakry2015-td,
  title         = "Digging Deep into the layers of {CNNs}: In Search of How
                   {CNNs} Achieve View Invariance",
  author        = "Bakry, Amr and Elhoseiny, Mohamed and El-Gaaly, Tarek and
                   Elgammal, Ahmed",
  abstract      = "This paper is focused on studying the view-manifold
                   structure in the feature spaces implied by the different
                   layers of Convolutional Neural Networks (CNN). There are
                   several questions that this paper aims to answer: Does the
                   learned CNN representation achieve viewpoint invariance? How
                   does it achieve viewpoint invariance? Is it achieved by
                   collapsing the view manifolds, or separating them while
                   preserving them? At which layer is view invariance achieved?
                   How can the structure of the view manifold at each layer of
                   a deep convolutional neural network be quantified
                   experimentally? How does fine-tuning of a pre-trained CNN on
                   a multi-view dataset affect the representation at each layer
                   of the network? In order to answer these questions we
                   propose a methodology to quantify the deformation and
                   degeneracy of view manifolds in CNN layers. We apply this
                   methodology and report interesting results in this paper
                   that answer the aforementioned questions.",
  month         =  "9~" # aug,
  year          =  2015,
  keywords      = "
                   NotRead;assurance\_predictability;interpretability;trust\_informal\_treatment;assurance\_implicit;Mendeley
                   Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1508.01983"
}

@ARTICLE{Bashivan2015-fc,
  title         = "Learning Representations from {EEG} with Deep
                   {Recurrent-Convolutional} Neural Networks",
  author        = "Bashivan, Pouya and Rish, Irina and Yeasin, Mohammed and
                   Codella, Noel",
  abstract      = "One of the challenges in modeling cognitive events from
                   electroencephalogram (EEG) data is finding representations
                   that are invariant to inter- and intra-subject differences,
                   as well as to inherent noise associated with such data.
                   Herein, we propose a novel approach for learning such
                   representations from multi-channel EEG time-series, and
                   demonstrate its advantages in the context of mental load
                   classification task. First, we transform EEG activities into
                   a sequence of topology-preserving multi-spectral images, as
                   opposed to standard EEG analysis techniques that ignore such
                   spatial information. Next, we train a deep
                   recurrent-convolutional network inspired by state-of-the-art
                   video classification to learn robust representations from
                   the sequence of images. The proposed approach is designed to
                   preserve the spatial, spectral, and temporal structure of
                   EEG which leads to finding features that are less sensitive
                   to variations and distortions within each dimension.
                   Empirical evaluation on the cognitive load classification
                   task demonstrated significant improvements in classification
                   accuracy over current state-of-the-art approaches in this
                   field.",
  month         =  "19~" # nov,
  year          =  2015,
  keywords      = "
                   NotRead;trust\_informal\_treatment;assurance\_implicit;Mendeley
                   Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1511.06448"
}

@ARTICLE{Bengio2013-uv,
  title       = "Representation learning: a review and new perspectives",
  author      = "Bengio, Yoshua and Courville, Aaron and Vincent, Pascal",
  affiliation = "Department of Computer Science and Operations Research,
                 University of Montreal, Montreal, Quebec H3C 3J7, Canada.",
  abstract    = "The success of machine learning algorithms generally depends
                 on data representation, and we hypothesize that this is
                 because different representations can entangle and hide more
                 or less the different explanatory factors of variation behind
                 the data. Although specific domain knowledge can be used to
                 help design representations, learning with generic priors can
                 also be used, and the quest for AI is motivating the design of
                 more powerful representation-learning algorithms implementing
                 such priors. This paper reviews recent work in the area of
                 unsupervised feature learning and deep learning, covering
                 advances in probabilistic models, autoencoders, manifold
                 learning, and deep networks. This motivates longer term
                 unanswered questions about the appropriate objectives for
                 learning good representations, for computing representations
                 (i.e., inference), and the geometrical connections between
                 representation learning, density estimation, and manifold
                 learning.",
  journal     = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume      =  35,
  number      =  8,
  pages       = "1798--1828",
  month       =  aug,
  year        =  2013,
  keywords    = "
                 trust\_informal\_treatment;assurance\_implicit;vis\_dr;Mendeley
                 Import (Jan 17)/Assurances",
  language    = "en"
}

@INPROCEEDINGS{Weng_Wong2014-tj,
  title      = "Correct High-level Robot Behavior in Environments with
                Unexpected Events",
  booktitle  = "Robotics: Science and Systems {X}",
  author     = "Weng Wong, Kai and Ehlers, R{\"u}diger and Kress-Gazit, Hadas",
  publisher  = "Robotics: Science and Systems Foundation",
  month      =  "12~" # jul,
  year       =  2014,
  keywords   = "trust\_informal\_treatment;assurance\_implicit;V\&V;Mendeley
                Import (Jan 17)/Assurances",
  conference = "Robotics: Science and Systems 2014"
}

@INPROCEEDINGS{Conner2007-uw,
  title           = "Valet parking without a valet",
  booktitle       = "2007 {IEEE/RSJ} International Conference on Intelligent
                     Robots and Systems",
  author          = "Conner, D C and Kress-Gazit, H and Choset, H and Rizzi, A
                     A and Pappas, G J",
  abstract        = "What would it be like if we could give our robot high
                     level commands and it would automatically execute them in
                     a verifiably correct fashion in dynamically changing
                     environments? This work demonstrates a method for
                     generating continuous feedback control inputs that satisfy
                     high-level specifications. Using a collection of
                     continuous local feedback control policies in concert with
                     a synthesized discrete automaton, this paper demonstrates
                     the approach on an Ackermann-steered vehicle that
                     satisfies the command ``drive around until you find an
                     empty parking space, then park.'' The system reacts to
                     changing environmental conditions using only local
                     information, while guaranteeing the correct high level
                     behavior. The local policies consider the vehicle body
                     shape as well as bounds on drive and steering velocities.
                     The discrete automaton that invokes the local policies
                     guarantees executions that satisfy the high-level
                     specification based only on information about the current
                     availability of the nearest parking space. This paper also
                     demonstrates coordination of two vehicles using the
                     approach.",
  publisher       = "IEEE",
  pages           = "572--577",
  month           =  oct,
  year            =  2007,
  keywords        = "continuous systems;feedback;intelligent robots;mobile
                     robots;service robots;steering systems;velocity
                     control;Ackermann-steered vehicle;continuous local
                     feedback control;discrete automaton;drive
                     velocity;high-level specification;parking
                     space;robot;steering velocity;valet parking;vehicle body
                     shape;Automata;Automatic control;Feedback
                     control;Intelligent robots;Orbital robotics;Robot
                     kinematics;Robotics and automation;Shape;Space
                     vehicles;Vehicle
                     driving;V\&V;trust\_informal\_treatment;assurance\_implicit;Mendeley
                     Import (Jan 17)/Assurances",
  conference      = "2007 IEEE/RSJ International Conference on Intelligent
                     Robots and Systems"
}

@INPROCEEDINGS{Israelsen2014-um,
  title     = "Generalized Laguerre Reduction of the Volterra Kernel for
               Practical Identification of Nonlinear Dynamic Systems",
  booktitle = "Fuels and Petrochemicals Division 2014 - Core Programming Area
               at the 2014 {AIChE} Spring Meeting and 10th Global Congress on
               Process Safety, vol. 1 (2014)",
  author    = "Israelsen, Brett W and Smith, Dale A",
  abstract  = "The Volterra series can be used to model a large subset of
               nonlinear, dynamic systems. A major drawback is the number of
               coefficients required model such systems. In order to reduce the
               number of required coefficients, Laguerre polynomials are used
               to estimate the Volterra kernels. Existing literature proposes
               algorithms for a fixed number of Volterra kernels, and Laguerre
               series. This paper presents a novel algorithm for generalized
               calculation of the finite order Volterra-Laguerre (VL) series
               for a MIMO system. An example addresses the utility of the
               algorithm in practical application.",
  pages     = "1--16",
  month     =  "3~" # oct,
  year      =  2014,
  address   = "New Orleans, LA",
  keywords  = "laguerre; model reduction; statistical learning; system
               identification; volterra;My Papers;Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Pina2008-gr,
  title     = "Identifying generalizable metric classes to evaluate human-robot
               teams",
  booktitle = "Proc. 3rd Ann. Conf. {Human-Robot} Interaction",
  author    = "Pina, P and Cummings, M L and Crandall, J W and Della Penna, M",
  abstract  = "ABSTRACT In this paper, we describe an effort to identify
               generalizable metric classes to evaluate human-robot teams. We
               describe conceptual models for supervisory control of a single
               and multiple robots. Based on these models, we identify and
               discuss the main metric classes that must be taken into
               consideration to understand team performance. Finally, we
               discuss a case study of a search and rescue mission to
               illustrate the use of these metric ...",
  publisher = "academia.edu",
  pages     = "13--20",
  year      =  2008,
  keywords  = "NotRead"
}

@ARTICLE{Konolige1985-vx,
  title     = "A Computational Theory of Belief Introspection",
  author    = "Konolige, K",
  abstract  = "... the finite base set is decidable. 3 Comparison to Related
               Work Our definition of an ideal introspective agent has many
               points of similarity with work by Halpern and Moses [2] and
               Moore [12]. In both these latter cases an underlying ...",
  journal   = "IJCAI",
  publisher = "pdfs.semanticscholar.org",
  year      =  1985,
  keywords  = "
               introspection;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Charif2013-vo,
  title     = "Dynamic service composition enabled by introspective agent
               coordination",
  author    = "Charif, Y and Sabouret, N",
  abstract  = "Abstract Service composition has received much interest from
               many research communities. The major research efforts published
               to date propose the use of service orchestration to model this
               problem. However, the designed orchestration approaches are
               static since they",
  journal   = "Auton. Agent. Multi. Agent. Syst.",
  publisher = "Springer",
  year      =  2013,
  keywords  = "
               introspection;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Yilmaz2007-ff,
  title     = "A strategy for improving dynamic composability: ontology-driven
               introspective agent architectures",
  author    = "Yilmaz, Levent",
  abstract  = "ABSTRACT Seamless composability of disparate simulations within
               systems of systems context is challenging. Large complex
               simulations must respond to changing technology, environments,
               and objectives. The problem exacerbates when dynamic
               extensibility and",
  journal   = "Journal of Systemics, Cybernetics and Informatics",
  publisher = "iiisci.org",
  volume    =  5,
  number    =  5,
  pages     = "1--9",
  year      =  2007,
  keywords  = "introspection;Mendeley Import (Jan 17)/Assurances/Self-Aware"
}

@ARTICLE{Israelsen2016-wf,
  title         = "Towards Adaptive Training of Agent-based Sparring Partners
                   for Fighter Pilots",
  author        = "Israelsen, Brett W and Ahmed, Nisar and Center, Kenneth and
                   Green, Roderick and Bennett, Jr, Winston",
  abstract      = "A key requirement for the current generation of artificial
                   decision-makers is that they should adapt well to changes in
                   unexpected situations. This paper addresses the situation in
                   which an AI for aerial dog fighting, with tunable parameters
                   that govern its behavior, must optimize behavior with
                   respect to an objective function that is evaluated and
                   learned through simulations. Bayesian optimization with a
                   Gaussian Process surrogate is used as the method for
                   investigating the objective function. One key benefit is
                   that during optimization, the Gaussian Process learns a
                   global estimate of the true objective function, with
                   predicted outcomes and a statistical measure of confidence
                   in areas that haven't been investigated yet. Having a model
                   of the objective function is important for being able to
                   understand possible outcomes in the decision space; for
                   example this is crucial for training and providing feedback
                   to human pilots. However, standard Bayesian optimization
                   does not perform consistently or provide an accurate
                   Gaussian Process surrogate function for highly volatile
                   objective functions. We treat these problems by introducing
                   a novel sampling technique called Hybrid Repeat/Multi-point
                   Sampling. This technique gives the AI ability to learn
                   optimum behaviors in a highly uncertain environment. More
                   importantly, it not only improves the reliability of the
                   optimization, but also creates a better model of the entire
                   objective surface. With this improved model the agent is
                   equipped to more accurately/efficiently predict performance
                   in unexplored scenarios.",
  month         =  "13~" # dec,
  year          =  2016,
  keywords      = "My Papers",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1612.04315"
}

@ARTICLE{Israelsen2016-ea,
  title         = "Hybrid {Repeat/Multi-point} Sampling for Highly Volatile
                   Objective Functions",
  author        = "Israelsen, Brett and Ahmed, Nisar",
  abstract      = "A key drawback of the current generation of artificial
                   decision-makers is that they do not adapt well to changes in
                   unexpected situations. This paper addresses the situation in
                   which an AI for aerial dog fighting, with tunable parameters
                   that govern its behavior, will optimize behavior with
                   respect to an objective function that must be evaluated and
                   learned through simulations. Once this objective function
                   has been modeled, the agent can then choose its desired
                   behavior in different situations. Bayesian optimization with
                   a Gaussian Process surrogate is used as the method for
                   investigating the objective function. One key benefit is
                   that during optimization the Gaussian Process learns a
                   global estimate of the true objective function, with
                   predicted outcomes and a statistical measure of confidence
                   in areas that haven't been investigated yet. However,
                   standard Bayesian optimization does not perform consistently
                   or provide an accurate Gaussian Process surrogate function
                   for highly volatile objective functions. We treat these
                   problems by introducing a novel sampling technique called
                   Hybrid Repeat/Multi-point Sampling. This technique gives the
                   AI ability to learn optimum behaviors in a highly uncertain
                   environment. More importantly, it not only improves the
                   reliability of the optimization, but also creates a better
                   model of the entire objective surface. With this improved
                   model the agent is equipped to better adapt behaviors.",
  month         =  "13~" # dec,
  year          =  2016,
  keywords      = "My Papers",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1612.03981"
}

@ARTICLE{Nguyen2014-ra,
  title         = "Deep Neural Networks are Easily Fooled: High Confidence
                   Predictions for Unrecognizable Images",
  author        = "Nguyen, Anh and Yosinski, Jason and Clune, Jeff",
  abstract      = "Deep neural networks (DNNs) have recently been achieving
                   state-of-the-art performance on a variety of
                   pattern-recognition tasks, most notably visual
                   classification problems. Given that DNNs are now able to
                   classify objects in images with near-human-level
                   performance, questions naturally arise as to what
                   differences remain between computer and human vision. A
                   recent study revealed that changing an image (e.g. of a
                   lion) in a way imperceptible to humans can cause a DNN to
                   label the image as something else entirely (e.g. mislabeling
                   a lion a library). Here we show a related result: it is easy
                   to produce images that are completely unrecognizable to
                   humans, but that state-of-the-art DNNs believe to be
                   recognizable objects with 99.99\% confidence (e.g. labeling
                   with certainty that white noise static is a lion).
                   Specifically, we take convolutional neural networks trained
                   to perform well on either the ImageNet or MNIST datasets and
                   then find images with evolutionary algorithms or gradient
                   ascent that DNNs label with high confidence as belonging to
                   each dataset class. It is possible to produce images totally
                   unrecognizable to human eyes that DNNs believe with near
                   certainty are familiar objects, which we call ``fooling
                   images'' (more generally, fooling examples). Our results
                   shed light on interesting differences between human vision
                   and current DNNs, and raise questions about the generality
                   of DNN computer vision.",
  month         =  "5~" # dec,
  year          =  2014,
  keywords      = "Mendeley Import (Jan 17)/MLTheory/DeepLearning",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1412.1897"
}

@ARTICLE{LeCun2015-ok,
  title       = "Deep learning",
  author      = "LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey",
  affiliation = "1] Facebook AI Research, 770 Broadway, New York, New York
                 10003 USA. [2] New York University, 715 Broadway, New York,
                 New York 10003, USA. Department of Computer Science and
                 Operations Research Universit{\'e} de Montr{\'e}al, Pavillon
                 Andr{\'e}-Aisenstadt, PO Box 6128 Centre-Ville STN
                 Montr{\'e}al, Quebec H3C 3J7, Canada. 1] Google, 1600
                 Amphitheatre Parkway, Mountain View, California 94043, USA.
                 [2] Department of Computer Science, University of Toronto, 6
                 King's College Road, Toronto, Ontario M5S 3G4, Canada.",
  abstract    = "Deep learning allows computational models that are composed of
                 multiple processing layers to learn representations of data
                 with multiple levels of abstraction. These methods have
                 dramatically improved the state-of-the-art in speech
                 recognition, visual object recognition, object detection and
                 many other domains such as drug discovery and genomics. Deep
                 learning discovers intricate structure in large data sets by
                 using the backpropagation algorithm to indicate how a machine
                 should change its internal parameters that are used to compute
                 the representation in each layer from the representation in
                 the previous layer. Deep convolutional nets have brought about
                 breakthroughs in processing images, video, speech and audio,
                 whereas recurrent nets have shone light on sequential data
                 such as text and speech.",
  journal     = "Nature",
  volume      =  521,
  number      =  7553,
  pages       = "436--444",
  month       =  "28~" # may,
  year        =  2015,
  keywords    = "Mendeley Import (Jan 17)/WeeklyReading;Mendeley Import (Jan
                 17)/MLTheory/DeepLearning",
  language    = "en"
}

@ARTICLE{Park1991-ow,
  title    = "Universal Approximation Using {Radial-Basis-Function} Networks",
  author   = "Park, J and Sandberg, I W",
  abstract = "There have been several recent studies concerning feedforward
              networks and the problem of approximating arbitrary functionals
              of a finite number of real variables. Some of these studies deal
              with cases in which the hidden-layer nonlinearity is not a
              sigmoid. This was motivated by successful applications of
              feedforward networks with nonsigmoidal hidden-layer units. This
              paper reports on a related study of radial-basis-function (RBF)
              networks, and it is proved that RBF networks having one hidden
              layer are capable of universal approximation. Here the emphasis
              is on the case of typical RBF networks, and the results show that
              a certain class of RBF networks with the same smoothing factor in
              each kernel node is broad enough for universal approximation.",
  journal  = "Neural Comput.",
  volume   =  3,
  number   =  2,
  pages    = "246--257",
  year     =  1991,
  keywords = "Mendeley Import (Jan 17)/MLTheory/DeepLearning"
}

@ARTICLE{Dietterich1999-rm,
  title         = "Hierarchical Reinforcement Learning with the {MAXQ} Value
                   Function Decomposition",
  author        = "Dietterich, Thomas G",
  abstract      = "This paper presents the MAXQ approach to hierarchical
                   reinforcement learning based on decomposing the target
                   Markov decision process (MDP) into a hierarchy of smaller
                   MDPs and decomposing the value function of the target MDP
                   into an additive combination of the value functions of the
                   smaller MDPs. The paper defines the MAXQ hierarchy, proves
                   formal results on its representational power, and
                   establishes five conditions for the safe use of state
                   abstractions. The paper presents an online model-free
                   learning algorithm, MAXQ-Q, and proves that it converges wih
                   probability 1 to a kind of locally-optimal policy known as a
                   recursively optimal policy, even in the presence of the five
                   kinds of state abstraction. The paper evaluates the MAXQ
                   representation and MAXQ-Q through a series of experiments in
                   three domains and shows experimentally that MAXQ-Q (with
                   state abstractions) converges to a recursively optimal
                   policy much faster than flat Q learning. The fact that MAXQ
                   learns a representation of the value function has an
                   important benefit: it makes it possible to compute and
                   execute an improved, non-hierarchical policy via a procedure
                   similar to the policy improvement step of policy iteration.
                   The paper demonstrates the effectiveness of this
                   non-hierarchical execution experimentally. Finally, the
                   paper concludes with a comparison to related work and a
                   discussion of the design tradeoffs in hierarchical
                   reinforcement learning.",
  pages         = "227--303",
  month         =  "21~" # may,
  year          =  1999,
  keywords      = "TALAF;Mendeley Import (Jan 17)/AFRL\_STTR",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "cs/9905014"
}

@ARTICLE{Walter2015-tl,
  title    = "A Situationally Aware Voice-commandable Robotic Forklift Working
              Alongside People in Unstructured Outdoor Environments",
  author   = "Walter, Matthew R and Antone, Matthew and Chuangsuwanich, Ekapol
              and Correa, Andrew and Davis, Randall and Fletcher, Luke and
              Frazzoli, Emilio and Friedman, Yuli and Glass, James and How,
              Jonathan P and Jeon, Jeong Hwan and Karaman, Sertac and Luders,
              Brandon and Roy, Nicholas and Tellex, Stefanie and Teller, Seth",
  abstract = "One long-standing challenge in robotics is the realization of
              mobile autonomous robots able to operate safely in human
              workplaces, and be accepted by the human occupants. We describe
              the development of a multiton robotic forklift intended to
              operate alongside people and vehicles, handling palletized
              materials within existing, active outdoor storage facilities. The
              system has four novel characteristics. The first is a multimodal
              interface that allows users to efficiently convey task-level
              commands to the robot using a combination of pen-based gestures
              and natural language speech. These tasks include the
              manipulation, transport, and placement of palletized cargo within
              dynamic, human-occupied warehouses. The second is the robot's
              ability to learn the visual identity of an object from a single
              user-provided example and use the learned model to reliably and
              persistently detect objects despite significant spatial and
              temporal excursions. The third is a reliance on local sensing
              that allows the robot to handle variable palletized cargo and
              navigate within dynamic, minimally prepared environments without
              a global positioning system. The fourth concerns the robot's
              operation in close proximity to people, including its human
              supervisor, pedestrians who may cross or block its path, moving
              vehicles, and forklift operators who may climb inside the robot
              and operate it manually. This is made possible by interaction
              mechanisms that facilitate safe, effective operation around
              people. This paper provides a comprehensive description of the
              system's architecture and implementation, indicating how
              real-world operational requirements motivated key design choices.
              We offer qualitative and quantitative analyses of the robot
              operating in real settings and discuss the lessons learned from
              our effort.",
  journal  = "J. Field Robotics",
  volume   =  32,
  number   =  4,
  pages    = "590--628",
  month    =  "1~" # jun,
  year     =  2015,
  keywords = "Mendeley Import (Jan 17)/Human-RobotCollaboration",
  language = "en"
}

@ARTICLE{DeDonato2015-rp,
  title    = "Human-in-the-loop Control of a Humanoid Robot for Disaster
              Response: A Report from the {DARPA} Robotics Challenge Trials",
  author   = "DeDonato, Mathew and Dimitrov, Velin and Du, Ruixiang and
              Giovacchini, Ryan and Knoedler, Kevin and Long, Xianchao and
              Polido, Felipe and Gennert, Michael A and Pad{\i}r, Ta{\c
              s}k{\i}n and Feng, Siyuan and Moriguchi, Hirotaka and Whitman,
              Eric and Xinjilefu, X and Atkeson, Christopher G",
  abstract = "The DARPA Robotics Challenge (DRC) requires teams to integrate
              mobility, manipulation, and perception to accomplish several
              disaster-response tasks. We describe our hardware choices and
              software architecture, which enable human-in-the-loop control of
              a 28 degree-of-freedom Atlas humanoid robot over a limited
              bandwidth link. We discuss our methods, results, and lessons
              learned for the DRC Trials tasks. The effectiveness of our system
              architecture was demonstrated as the WPI-CMU DRC Team scored 11
              out of a possible 32 points, ranked seventh (out of 16) at the
              DRC Trials, and was selected as a finalist for the DRC Finals.",
  journal  = "J. Field Robotics",
  volume   =  32,
  number   =  2,
  pages    = "275--292",
  month    =  "1~" # mar,
  year     =  2015,
  keywords = "Mendeley Import (Jan 17)/Human-RobotCollaboration",
  language = "en"
}

@INPROCEEDINGS{Ahmed2010-mb,
  title     = "Variational Bayesian data fusion of multi-class discrete
               observations with applications to cooperative human-robot
               estimation",
  booktitle = "2010 {IEEE} International Conference on Robotics and Automation",
  author    = "Ahmed, N and Campbell, M",
  abstract  = "A new method is presented for fusing conventional continuous
               sensor observations with discrete multi-categorical
               state-dependent information, which can be furnished by humans in
               many cooperative human-robot interaction problems. The hybrid
               likelihood function for mapping between continuous hidden states
               and categorical observations are specified via softmax models.
               Although softmax models avoid discretization of continuous
               states, they are challenging to implement for real-time data
               fusion since they are not analytically integrable. An
               approximation based on variational Bayesian (VB) methods is
               presented here to obtain fast closed-form Gaussian solutions to
               the desired posteriors in cases where the hidden continuous
               states have Gaussian pdfs. A joint human-robot target
               localization example illustrates the properties and utility of
               the VB hybrid fusion strategy, which also applies more generally
               to inference in hybrid Bayesian networks and mixture models.",
  publisher = "IEEE",
  pages     = "186--191",
  month     =  may,
  year      =  2010,
  keywords  = "Bayes methods;Gaussian processes;human-robot interaction;sensor
               fusion;variational techniques;closed-form Gaussian
               solution;continuous sensor observations;cooperative human-robot
               estimation;cooperative human-robot interaction problem;discrete
               multicategorical state-dependent information;hybrid Bayesian
               network;hybrid likelihood function;joint human-robot target
               localization;mixture model;multiclass discrete
               observations;softmax model;variational Bayesian data
               fusion;Bayesian methods;Humanoid robots;Humans;Orbital
               robotics;Position measurement;Robot kinematics;Robot sensing
               systems;Robotics and automation;State estimation;USA
               Councils;Mendeley Import (Jan 17)/Human-RobotCollaboration"
}

@ARTICLE{Sklar2013-le,
  title    = "A Case for Argumentation to Enable {Human-Robot} Collaboration",
  author   = "Sklar, Elizabeth and Azhar, Mq and Flyr, Todd and Parsons, Simon",
  journal  = "Workshop on Argumentation for Multiagent Systems (ArgMAS)",
  pages    = "1--17",
  year     =  2013,
  keywords = "Mendeley Import (Jan 17)/Human-RobotCollaboration"
}

@ARTICLE{Murphy2008-ty,
  title     = "Cooperative use of unmanned sea surface and micro aerial
               vehicles at Hurricane Wilma",
  author    = "Murphy, Robin R and Steimle, Eric and Griffin, Chandler and
               Cullins, Charlie and Hall, Mike and Pratt, Kevin",
  abstract  = "On Oct. 24, 2005, Hurricane Wilma, a category 5 storm, made
               landfall at Cape Romano, Florida. Three days later, the Center
               for Robot-Assisted Search and Rescue at the University of South
               Florida deployed an iSENYS helicopter and a prototype unmanned
               water surface vehicle, AEOS-1, to survey damage in parts of
               Marco Island, 14 km from landfall. The effort was the first
               known use of unmanned sea surface vehicles (USVs) for emergency
               response and established their suitability for the recovery
               phase of disaster management by detecting damage to seawalls and
               piers, locating submerged debris (moorings and handrails), and
               determining safe lanes for sea navigation. It provides a
               preliminary domain theory of postdisaster port and littoral
               inspection with unmanned vehicles for use by the human--robot
               interaction community. It was also the first known demonstration
               of the strongly heterogeneous USV--micro aerial vehicle (MAV)
               team for any domain. The effort identified cooperative UAV--USV
               strategies and open issues for autonomous operations near
               structures. The effort showed that the MAV provided a
               much-needed external view for situation awareness and provided
               spotting for areas to be inspected. Concepts of operations for
               USV damage inspection and USV--MAV cooperation emerged,
               including a formula for computing the human--robot ratio: Nh =
               (2 $\times$ Nv) + 1, where Nh is the number of humans and Nv is
               the number of vehicles. The outstanding research issues span
               three areas: challenges for USVs operating near littoral
               structures, general issues for USV--MAV cooperation, and new
               applications. It is expected that the lessons learned will be
               transferrable to defense and homeland safety and security
               applications, such as port security, and other phases of
               emergency response, including rescue. \copyright{} 2008 Wiley
               Periodicals, Inc.",
  journal   = "J. Field Robotics",
  publisher = "Wiley Subscription Services, Inc., A Wiley Company",
  volume    =  25,
  number    =  3,
  pages     = "164--180",
  month     =  "1~" # mar,
  year      =  2008,
  keywords  = "Mendeley Import (Jan 17)/Human-RobotCollaboration",
  language  = "en"
}

@ARTICLE{Kaupp2010-xf,
  title    = "Human--robot communication for collaborative decision making ---
              A probabilistic approach",
  author   = "Kaupp, Tobias and Makarenko, Alexei and Durrant-Whyte, Hugh",
  abstract = "Humans and robots need to exchange information if the objective
              is to achieve a task collaboratively. Two questions are
              considered in this paper: what and when to communicate. To answer
              these questions, we developed a human-robot communication
              framework which makes use of common probabilistic robotics
              representations. The data stored in the representation determines
              what to communicate, and probabilistic inference mechanisms
              determine when to communicate. One application domain of the
              framework is collaborative human-robot decision making: robots
              use decision theory to select actions based on perceptual
              information gathered from their sensors and human operators. In
              this paper, operators are regarded as remotely located, valuable
              information sources which need to be managed carefully. Robots
              decide when to query operators using Value-Of-Information theory,
              i.e.??humans are only queried if the expected benefit of their
              observation exceeds the cost of obtaining it. This can be seen as
              a mechanism for adjustable autonomy whereby adjustments are
              triggered at run-time based on the uncertainty in the robots'
              beliefs related to their task. This semi-autonomous system is
              demonstrated using a navigation task and evaluated by a user
              study. Participants navigated a robot in simulation using the
              proposed system and via classical teleoperation. Results show
              that our system has a number of advantages over teleoperation
              with respect to performance, operator workload, usability, and
              the users' perception of the robot. We also show that despite
              these advantages, teleoperation may still be a preferable driving
              mode depending on the mission priorities. Crown Copyright ??
              2010.",
  journal  = "Rob. Auton. Syst.",
  volume   =  58,
  number   =  5,
  pages    = "444--456",
  month    =  may,
  year     =  2010,
  keywords = "Adjustable autonomy; Collaborative control; Human-robot
              communication; Information fusion; Semi-autonomous
              systems;Mendeley Import (Jan
              17)/Human-RobotCollaboration;Mendeley Import (Jan
              17)/WeeklyReading",
  language = "en"
}

@PHDTHESIS{Ahmed2012-yn,
  title    = "Probabilistic modeling and estimation with human inputs in
              semi-autonomous systems",
  author   = "Ahmed, Nisar Razzi",
  abstract = "This thesis addresses three important issues that arise in the
              analysis and design of joint human-robot teams. Each issue deals
              with a different aspect of the following question: how to best
              combine human and robot capabilities to accomplish some set of
              tasks? The first issue addressed here is that of predicting human
              supervisory control performance in large-scale networked teams of
              robots. It is shown that models based on individual operator
              characteristics such as working memory capacity can be used to
              probabilistically predict human supervisory control metrics under
              different operating conditions via linear regression, Bayesian
              network, and Gaussian process models. The second issue addressed
              here is that of modeling human supervisors of multi-robot teams
              as discrete strategic decision makers. A probabilistic
              discriminative modeling approach is presented here, and novel
              fully Bayesian learning techniques are presented and validated
              for identifying appropriate discriminative model parameters and
              model structures from experimental data. The third issue
              addressed here is that of combining useful information from human
              observations with information obtained from traditional robot
              sensors. A novel recursive Bayesian estimation framework is
              presented for fusing imprecise soft categorical human
              observations with robot sensor data via Gaussian and Gaussian
              mixture approximations. The proposed data fusion approach is
              validated in hardware with a real human-robot team on a
              cooperative multi-target search experiment.",
  year     =  2012,
  school   = "Cornell University",
  keywords = "0537:Engineering; 0554:Civil engineering; 0771:Robotics; Applied
              sciences; Civil engineering; Control systems; Engineering; Human
              inputs; Human-robot interactions; Machine learning; Probabilistic
              estimation; Probabilistic modeling; Robotics; Semi-autonomous
              systems;Mendeley Import (Jan 17)/Human-RobotCollaboration"
}

@INPROCEEDINGS{Kaupp2005-pk,
  title     = "Human sensor model for range observations",
  booktitle = "{IJCAI} Workshop Reasoning with Uncertainty in Robotics",
  author    = "Kaupp, Tobias and Makarenko, Alexei and Ramos, Fabio and
               Durrant-Whyte, Hugh",
  year      =  2005,
  keywords  = "Mendeley Import (Jan 17)/Human-RobotCollaboration"
}

@ARTICLE{Kubelka2015-fv,
  title    = "Robust Data Fusion of Multimodal Sensory Information for Mobile
              Robots",
  author   = "Kubelka, Vladim{\'\i}r and Oswald, Lorenz and Pomerleau, Fran{\c
              c}ois and Colas, Francis and Svoboda, Tom{\'a}{\v s} and
              Reinstein, Michal",
  abstract = "Urban search and rescue (USAR) missions for mobile robots require
              reliable state estimation systems resilient to conditions given
              by the dynamically changing environment. We design and evaluate a
              data fusion system for localization of a mobile skid-steer robot
              intended for USAR missions. We exploit a rich sensor suite
              including both proprioceptive (inertial measurement unit and
              tracks odometry) and exteroceptive sensors (omnidirectional
              camera and rotating laser rangefinder). To cope with the
              specificities of each sensing modality (such as significantly
              differing sampling frequencies), we introduce a novel fusion
              scheme based on an extended Kalman filter for six degree of
              freedom orientation and position estimation. We demonstrate the
              performance on field tests of more than 4.4 km driven under
              standard USAR conditions. Part of our datasets include ground
              truth positioning, indoor with a Vicon motion capture system and
              outdoor with a Leica theodolite tracker. The overall median
              accuracy of localization---achieved by combining all four
              modalities---was 1.2\% and 1.4\% of the total distance traveled
              for indoor and outdoor environments, respectively. To identify
              the true limits of the proposed data fusion, we propose and
              employ a novel experimental evaluation procedure based on failure
              case scenarios. In this way, we address the common issues such as
              slippage, reduced camera field of view, and limited laser
              rangefinder range, together with moving obstacles spoiling the
              metric map. We believe such a characterization of the failure
              cases is a first step toward identifying the behavior of state
              estimation under such conditions. We release all our datasets to
              the robotics community for possible benchmarking.",
  journal  = "J. Field Robotics",
  volume   =  32,
  number   =  4,
  pages    = "447--473",
  month    =  "1~" # jun,
  year     =  2015,
  keywords = "Mendeley Import (Jan 17)/Human-RobotCollaboration",
  language = "en"
}

@ARTICLE{Geber1975-zy,
  title     = "Congenital malformations of the central nervous system produced
               by narcotic analgesics in the hamster",
  author    = "Geber, W F and Schramm, L C",
  abstract  = "Maternal dose--fetal teratogenic response data were obtained for
               a variety of narcotic and related compounds by single
               subcutaneous injections of the drugs into pregnant hamsters
               during the critical periods of central nervous system
               organogenesis. The number of abnormal fetuses from females
               injected with diacetylmorphine (heroin), thebaine, phenazocine,
               pentazocine, propoxyphene, and methadone increased as the
               maternal dose of the compounds was increased. By contrast,
               morphine, hydromorphone, and meperidine produced an increase in
               the number (per cent) of fetal anomalies only up to a certain
               maternal dose level. Further increases in maternal dose levels
               did not produce additional fetal anomalies. Comparative studies
               of single and multiple maternal doses indicated that
               diacetylmorphine (heroin) and methadone produced a four- to
               sixfold increase in fetal anomalies with repetitive doses
               whereas the percentage of malformed fetuses remained the same
               with hydromorphone (Dilaudid). The narcotic antagonists
               nalorphine, naloxone, levallophan, and cyclazocine blocked the
               teratogenic effects of both single and multiple doses of the
               narcotics.",
  journal   = "Am. J. Obstet. Gynecol.",
  publisher = "Citeseer",
  volume    =  123,
  number    =  7,
  pages     = "705--713",
  month     =  "1~" # dec,
  year      =  1975,
  keywords  = "introduction to graphical;Mendeley Import (Jan
               17)/GraphicalModels",
  language  = "en"
}

@BOOK{Barber2012-ia,
  title     = "Bayesian Reasoning and Machine Learning",
  author    = "Barber, David",
  abstract  = "Machine learning methods extract value from vast data sets
               quickly and with modest resources. They are established tools in
               a wide range of industrial applications, including search
               engines, DNA sequencing, stock market analysis, and robot
               locomotion, and their use is spreading rapidly. People who know
               the methods have their choice of rewarding jobs. This hands-on
               text opens these opportunities to computer science students with
               modest mathematical backgrounds. It is designed for final-year
               undergraduates and master's students with limited background in
               linear algebra and calculus. Comprehensive and coherent, it
               develops everything from basic reasoning to advanced techniques
               within the framework of graphical models. Students learn more
               than a menu of techniques, they develop analytical and
               problem-solving skills that equip them for the real world.
               Numerous examples and exercises, both computer based and
               theoretical, are included in every chapter. Resources for
               students and instructors, including a MATLAB toolbox, are
               available online.",
  publisher = "Cambridge University Press",
  pages     = "646",
  month     =  "2~" # feb,
  year      =  2012,
  keywords  = "computational; information theoretic learning with statistics;
               learning; statistics \& optimisation; theory \&
               algorithms;classification;interpretability;time
               series;Textbook;Mendeley Import (Jan 17)/TextBooks;Mendeley
               Import (Jan 17)/GraphicalModels;Mendeley Import (Jan
               17)/Assurances",
  language  = "en"
}

@ARTICLE{Ahmed2008-lu,
  title     = "Bayesian Networks and Decision Graphs",
  author    = "Ahmed, S E",
  journal   = "Technometrics",
  publisher = "Springer",
  volume    =  50,
  number    =  1,
  pages     = "97--97",
  series    = "Information science and statistics",
  year      =  2008,
  address   = "New York",
  keywords  = "Textbook;Mendeley Import (Jan 17)/TextBooks;Mendeley Import (Jan
               17)/GraphicalModels"
}

@INPROCEEDINGS{Ahmed2015-pr,
  title     = "Bayesian hidden Markov models for {UAV-enabled} target
               localization on road networks with soft-hard data",
  booktitle = "{SPIE} Defense + Security",
  author    = "Ahmed, Nisar and Casbeer, David and Cao, Yongcan and Kingston,
               Derek",
  publisher = "International Society for Optics and Photonics",
  pages     = "94640Q--94640Q--15",
  month     =  "20~" # may,
  year      =  2015,
  keywords  = "Networks; Roads; Unmanned aerial vehicles; Unattended ground
               sensors; Sensors; Simulations; Matrices;Mendeley Import (Jan
               17)/GraphicalModels"
}

@ARTICLE{Weiss2002-yo,
  title       = "Motion illusions as optimal percepts",
  author      = "Weiss, Yair and Simoncelli, Eero P and Adelson, Edward H",
  affiliation = "School of Computer Science and Engineering, Hebrew University
                 of Jerusalem, Givat Ram Campus, Jerusalem 91904, Israel.
                 yweiss@cs.huji.ac.il",
  abstract    = "The pattern of local image velocities on the retina encodes
                 important environmental information. Although humans are
                 generally able to extract this information, they can easily be
                 deceived into seeing incorrect velocities. We show that these
                 'illusions' arise naturally in a system that attempts to
                 estimate local image velocity. We formulated a model of visual
                 motion perception using standard estimation theory, under the
                 assumptions that (i) there is noise in the initial
                 measurements and (ii) slower motions are more likely to occur
                 than faster ones. We found that specific instantiation of such
                 a velocity estimator can account for a wide variety of
                 psychophysical phenomena.",
  journal     = "Nat. Neurosci.",
  volume      =  5,
  number      =  6,
  pages       = "598--604",
  month       =  jun,
  year        =  2002,
  keywords    = "Mendeley Import (Jan 17)/GraphicalModels",
  language    = "en"
}

@BOOK{Bishop1995-tc,
  title     = "Neural Networks for Pattern Recognition",
  author    = "Bishop, Christopher M",
  abstract  = "This book provides the first comprehensive treatment of
               feed-forward neural networks from the perspective of statistical
               pattern recognition. After introducing the basic concepts of
               pattern recognition, the book describes techniques for modelling
               probability density functions, and discusses the properties and
               relative merits of the multi-layer perceptron and radial basis
               function network models. It also motivates the use of various
               forms of error functions, and reviews the principal algorithms
               for error function minimization. As well as providing a detailed
               discussion of learning and generalization in neural networks,
               the book also covers the important topics of data processing,
               feature extraction, and prior knowledge. The book concludes with
               an extensive treatment of Bayesian techniques and their
               applications to neural networks.",
  publisher = "Clarendon Press",
  volume    =  92,
  pages     = "482",
  month     =  "23~" # nov,
  year      =  1995,
  keywords  = "Important;Textbook;Mendeley Import (Jan 17)/TextBooks;Mendeley
               Import (Jan 17)/GraphicalModels",
  language  = "en"
}

@INPROCEEDINGS{Boyle2004-ib,
  title     = "Dependent gaussian processes",
  booktitle = "Advances in neural information processing systems",
  author    = "Boyle, Phillip and Frean, Marcus",
  abstract  = "Gaussian processes are usually parameterised in terms of their
               covari- ance functions. However, this makes it difficult to deal
               with multiple outputs, because ensuring that the covariance
               matrix is positive definite is problematic. An alternative
               formulation is to treat Gaussian processes as white noise
               sources convolved with smoothing kernels, and to param- eterise
               the kernel instead. Using this, we extend Gaussian processes to
               handle multiple, coupled outputs.",
  publisher = "MIT Press",
  pages     = "217--224",
  year      =  2004,
  keywords  = "Folder - Spring2016;Mendeley Import (Jan 17)/ReadingGroups"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Julian2012-mv,
  title     = "Distributed robotic sensor networks: An information-theoretic
               approach",
  author    = "Julian, Brian J and Angermann, Michael and Schwager, Mac and
               Rus, Daniela",
  abstract  = "In this paper we present an information-theoretic approach to
               distributively control multiple robots equipped with sensors to
               infer the state of an environment. The robots iteratively
               estimate the environment state using a sequential Bayesian
               filter, while continuously moving along the gradient of mutual
               information to maximize the informativeness of the observations
               provided by their sensors. The gradient-based controller is
               proven to be convergent between observations and, in its most
               general form, locally optimal. However, the computational
               complexity of the general form is shown to be intractable, and
               thus non-parametric methods are incorporated to allow the
               controller to scale with respect to the number of robots. For
               decentralized operation, both the sequential Bayesian filter and
               the gradient-based controller use a novel consensus-based
               algorithm to approximate the robots’ joint measurement
               probabilities, even when the network diameter, the maximum
               in/out degree, and the number of robots are unknow...",
  journal   = "Int. J. Rob. Res.",
  publisher = "SAGE PublicationsSage UK: London, England",
  volume    =  31,
  number    =  10,
  pages     = "1134--1154",
  month     =  "6~" # aug,
  year      =  2012,
  keywords  = "Folder - Fall2015;Mendeley Import (Jan 17)/ReadingGroups",
  language  = "en"
}

@ARTICLE{Wit1975-jl,
  title    = "Electrophysiology and pharmacology of cardiac arrhythmias. {IX}.
              Cardiac electrophysiologic effects of beta adrenergic receptro
              stimulation and blockade. Part {C}",
  author   = "Wit, A L and Hoffman, B F and Rosen, M R",
  abstract = "The recently introduced continuous Skip-gram model is an
              efficient method for learning high-quality distributed vector
              representations that capture a large num- ber of precise
              syntactic and semantic word relationships. In this paper we
              present several extensions that improve both the quality of the
              vectors and the training speed. By subsampling of the frequent
              words we obtain significant speedup and also learn more regular
              word representations. We also describe a simple alterna- tive to
              the hierarchical softmax called negative sampling. An inherent
              limitation of word representations is their indifference to word
              order and their inability to represent idiomatic phrases. For
              example, the meanings of ``Canada'' and ``Air'' cannot be easily
              combined to obtain ``Air Canada''. Motivated by this example,we
              present a simplemethod for finding phrases in text, and show that
              learning good vector representations for millions of phrases is
              possible.",
  journal  = "Am. Heart J.",
  volume   =  90,
  number   =  6,
  pages    = "795--803",
  month    =  dec,
  year     =  1975,
  keywords = "Folder - Fall2015;Mendeley Import (Jan 17)/ReadingGroups",
  language = "en"
}

@ARTICLE{Arslan2007-ds,
  title     = "Autonomous {Vehicle-Target} Assignment: A {Game-Theoretical}
               Formulation",
  author    = "Arslan, G{\"u}rdal and Marden, Jason R and Shamma, Jeff S",
  abstract  = "Weconsider an autonomous vehicle-target assignment problem where
               a group ofvehicles are expected to optimally assign themselves
               to a setof targets. We introduce a game-theoretical formulation
               of the problemin which the vehicles are viewed as
               self-interested decision makers.Thus, we seek the optimization
               of a global utility functionthrough autonomous vehicles that are
               capable of making individually rationaldecisions to optimize
               their own utility functions. The first importantaspect of the
               problem is to choose the utility functionsof the vehicles in
               such a way that the objectivesof the vehicles are localized to
               each vehicle yet alignedwith a global utility function. The
               second important aspect ofthe problem is to equip the vehicles
               with an appropriatenegotiation mechanism by which each vehicle
               pursues the optimization ofits own utility function. We present
               several design procedures andaccompanying caveats for vehicle
               utility design. We present two newnegotiation mechanisms,
               namely, generalized regret monitoring with fading memory
               andinertia and selective spatial adaptive play, and provide
               accompanying proofsof their convergence. Finally, we present
               simulations that illustrate howvehicle negotiations can
               consistently lead to near-optimal assignments provided thatthe
               utilities of the vehicles are designed appropriately. 2007
               American Society of Mechanical Engineers",
  journal   = "J. Dyn. Syst. Meas. Control",
  publisher = "American Society of Mechanical Engineers",
  volume    =  129,
  number    =  5,
  pages     = "584--596",
  month     =  "1~" # sep,
  year      =  2007,
  keywords  = "Vehicles; Public utilities; Negotiation;Folder -
               Spring2016;Mendeley Import (Jan 17)/ReadingGroups",
  language  = "en"
}

@ARTICLE{Hemakumara2013-jk,
  title    = "Learning {UAV} Stability and Control Derivatives Using Gaussian
              Processes",
  author   = "Hemakumara, P and Sukkarieh, S",
  abstract = "The stability and control derivatives of an unmanned aerial
              vehicle (UAV) map the platform's control inputs to its dynamic
              response. The modeling is labor intensive and requires coarse
              approximations. Similarly, models constructed through flight
              tests are only applicable to a narrow flight envelope, and
              classical system identification approaches require prior
              knowledge of the model structure, which, in some instances, may
              only be partially known. The goal of this study is to tackle
              these problems by introducing a new system identification method
              based on the dependent Gaussian processes. This allows
              high-fidelity nonlinear flight dynamic models to be constructed
              through experimental data. The proposed algorithm captures the
              cross coupling between input parameters and learns the system
              stability and control derivatives. In addition, it captures any
              dependences embodied in the outputs. This paper provides both the
              theoretical underpinnings and practical application of this
              approach. The theory was tested in simulation on a highly coupled
              oblique wing aircraft and was demonstrated on a delta-wing UAV
              platform using real flight data. The results are compared against
              an alternative parameteric model and show improvements in
              identifying the coupling between flight modes, the ability to
              provide uncertainty estimates and robustness, and applicability
              to a broader flight envelope.",
  journal  = "IEEE Trans. Rob.",
  volume   =  29,
  number   =  4,
  pages    = "813--824",
  month    =  aug,
  year     =  2013,
  keywords = "Gaussian processes; system identification; unmanned aerial
              vehicles (UAVs); aircraft control; autonomous aerial vehicles;
              nonlinear control systems; stability; Gaussian process; UAV
              control derivative; UAV stability; coarse approximation;
              delta-wing UAV platform; dynamic response; high-fidelity
              nonlinear flight dynamic model; oblique wing aircraft; unmanned
              aerial vehicle;Folder - Spring2016;Mendeley Import (Jan
              17)/ReadingGroups"
}

@ARTICLE{Choi2009-vk,
  title    = "{Consensus-Based} Decentralized Auctions for Robust Task
              Allocation",
  author   = "Choi, H L and Brunet, L and How, J P",
  abstract = "This paper addresses task allocation to coordinate a fleet of
              autonomous vehicles by presenting two decentralized algorithms:
              the consensus-based auction algorithm (CBAA) and its
              generalization to the multi-assignment problem, i.e., the
              consensus-based bundle algorithm (CBBA). These algorithms utilize
              a market-based decision strategy as the mechanism for
              decentralized task selection and use a consensus routine based on
              local communication as the conflict resolution mechanism to
              achieve agreement on the winning bid values. Under reasonable
              assumptions on the scoring scheme, both of the proposed
              algorithms are proven to guarantee convergence to a conflict-free
              assignment, and it is shown that the converged solutions exhibit
              provable worst-case performance. It is also demonstrated that
              CBAA and CBBA produce conflict-free feasible solutions that are
              robust to both inconsistencies in the situational awareness
              across the fleet and variations in the communication network
              topology. Numerical experiments confirm superior convergence
              properties and performance when compared with existing
              auction-based task-allocation algorithms.",
  journal  = "IEEE Trans. Rob.",
  volume   =  25,
  number   =  4,
  pages    = "912--926",
  month    =  aug,
  year     =  2009,
  keywords = "Distributed robot systems; Networked robots; Task allocation for
              multiple mobile robots; decentralised control; distributed
              control; mobile robots; autonomous vehicles; consensus-based
              bundle algorithm; decentralized auctions; market-based decision
              strategy; multiple mobile robots; robust task allocation;Folder -
              Fall2015;Mendeley Import (Jan 17)/ReadingGroups"
}

@INCOLLECTION{Goodfellow2014-xa,
  title     = "Generative Adversarial Nets",
  booktitle = "Advances in Neural Information Processing Systems 27",
  author    = "Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu,
               Bing and Warde-Farley, David and Ozair, Sherjil and Courville,
               Aaron and Bengio, Yoshua",
  editor    = "Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N D and
               Weinberger, K Q",
  abstract  = "We propose a new framework for estimating generative models via
               an adversar- ial process; in which we simultaneously train two
               models: a generative model G that captures the data
               distribution; and a discriminative model D that estimates the
               probability that a sample came from the training data rather
               thanG. The train- ing procedure for G is to maximize the
               probability of D making a mistake. This framework corresponds to
               a minimax two-player game. In the space of arbitrary functions G
               and D; a unique solution exists; with G recovering the training
               data distribution andD equal to 1 2 everywhere. In the case
               where G andD are defined by multilayer perceptrons; the entire
               system can be trained with backpropagation. There is no need for
               any Markov chains or unrolled approximate inference net- works
               during either training or generation of samples. Experiments
               demonstrate the potential of the framework through qualitative
               and quantitative evaluation of the generated samples. 1",
  publisher = "Curran Associates, Inc.",
  pages     = "2672--2680",
  year      =  2014,
  keywords  = "Folder - Spring2016;Mendeley Import (Jan 17)/ReadingGroups"
}

@INCOLLECTION{Duvenaud2011-ao,
  title     = "Additive Gaussian Processes",
  booktitle = "Advances in Neural Information Processing Systems 24",
  author    = "Duvenaud, David K and Nickisch, Hannes and Rasmussen, Carl E",
  editor    = "Shawe-Taylor, J and Zemel, R S and Bartlett, P L and Pereira, F
               and Weinberger, K Q",
  abstract  = "We introduce a Gaussian process model of functions which are
               additive. An additive function is one which decomposes into a
               sum of low-dimensional functions, each depending on only a
               subset of the input variables. Additive GPs generalize both
               Generalized Additive Models, and the standard GP models which
               use squared-exponential kernels. Hyperparameter learning in this
               model can be seen as Bayesian Hierarchical Kernel Learning
               (HKL). We introduce an expressive but tractable parameterization
               of the kernel function, which allows efficient evaluation of all
               input interaction terms, whose number is exponential in the
               input dimension. The additional structure discoverable by this
               model results in increased interpretability, as well as
               state-of-the-art predictive power in regression tasks.",
  publisher = "Curran Associates, Inc.",
  pages     = "226--234",
  year      =  2011,
  keywords  = "Learning/Statistics \& Optimisation;Folder - Spring2016;Mendeley
               Import (Jan 17)/ReadingGroups"
}

@ARTICLE{Gong2008-qa,
  title    = "Spectral Algorithm for Pseudospectral Methods in Optimal Control",
  author   = "Gong, Qi and Fahroo, Fariba and Ross, I Michael",
  journal  = "J. Guid. Control Dyn.",
  volume   =  31,
  number   =  3,
  pages    = "460--471",
  year     =  2008,
  keywords = "Folder - Spring2016;Mendeley Import (Jan 17)/ReadingGroups",
  language = "en"
}

@ARTICLE{Hargraves1987-yn,
  title    = "Direct trajectory optimization using nonlinear programming and
              collocation",
  author   = "Hargraves, C R and Paris, S W",
  abstract = "An algorithm for the direct numerical solution of an optimal
              control problem is given. The method employs cubic polynomials to
              represent state variables, linearly interpolates control
              variables, and uses collocation to satisfy the differential
              equations. This representation transforms the optimal control
              problem to a mathematical programming problem which is solved by
              sequential quadratic programming. The method is easy to program
              for a very general trajectory optimization problem and is shown
              to be very efficient for several sample problems. Results are
              compared with solutions obtained with other methods.",
  journal  = "J. Guid. Control Dyn.",
  volume   =  10,
  number   =  4,
  pages    = "338--342",
  year     =  1987,
  keywords = "Folder - Fall2015;Mendeley Import (Jan 17)/ReadingGroups",
  language = "en"
}

@ARTICLE{Roy2011-qt,
  title         = "Finding Approximate {POMDP} solutions Through Belief
                   Compression",
  author        = "Roy, N and Gordon, G and Thrun, S",
  abstract      = "Standard value function approaches to finding policies for
                   Partially Observable Markov Decision Processes (POMDPs) are
                   generally considered to be intractable for large models. The
                   intractability of these algorithms is to a large extent a
                   consequence of computing an exact, optimal policy over the
                   entire belief space. However, in real-world POMDP problems,
                   computing the optimal policy for the full belief space is
                   often unnecessary for good control even for problems with
                   complicated policy classes. The beliefs experienced by the
                   controller often lie near a structured, low-dimensional
                   subspace embedded in the high-dimensional belief space.
                   Finding a good approximation to the optimal value function
                   for only this subspace can be much easier than computing the
                   full value function. We introduce a new method for solving
                   large-scale POMDPs by reducing the dimensionality of the
                   belief space. We use Exponential family Principal Components
                   Analysis (Collins, Dasgupta and Schapire, 2002) to represent
                   sparse, high-dimensional belief spaces using small sets of
                   learned features of the belief state. We then plan only in
                   terms of the low-dimensional belief features. By planning in
                   this low-dimensional space, we can find policies for POMDP
                   models that are orders of magnitude larger than models that
                   can be handled by conventional techniques. We demonstrate
                   the use of this algorithm on a synthetic problem and on
                   mobile robot navigation tasks.",
  pages         = "1--40",
  month         =  "30~" # jun,
  year          =  2011,
  keywords      = "Folder - Fall2015;Mendeley Import (Jan 17)/ReadingGroups",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1107.0053"
}

@ARTICLE{Van_den_Kroonenberg2008-wa,
  title    = "Measuring the Wind Vector Using the Autonomous Mini Aerial
              Vehicle {M} 2 {AV}",
  author   = "van den Kroonenberg, Aline and Martin, Tim and Buschmann, Marco
              and Bange, Jens and V{\"o}rsmann, Peter",
  abstract = "Robust and optimal trajectories are computed for a dynamic
              soaring unmanned aerial vehicle. Dynamic soaring is technique for
              extracting energy from atmospheric wind gra- dients that large
              seabirds such as the albatross use as their primary means of
              propulsive power. The stochastic nature of the soaring problem is
              explicitly included in the opti- mization formulation by using an
              uncertainty quantification technique known as stochastic
              collocation. This non-intrusive method can generate accurate
              estimates of solution statis- tics with significantly fewer
              samples than a Monte Carlo based approach. An analysis of
              variance is first performed to determine which environmental
              parameters, aircraft proper- ties, and initial conditions
              contribute most to final energy and altitude variance. Robust
              dynamic soaring trajectories are then computed by using
              stochastic collocation over care- fully chosen nested sparse
              grids. I.",
  journal  = "J. Atmos. Ocean. Technol.",
  volume   =  25,
  number   =  11,
  pages    = "1969--1982",
  month    =  nov,
  year     =  2008,
  keywords = "Folder - Fall2015;Mendeley Import (Jan 17)/ReadingGroups"
}

@ARTICLE{Lawrance2011-sz,
  title    = "Autonomous Exploration of a Wind Field with a Gliding Aircraft",
  author   = "Lawrance, Nicholas R J and Sukkarieh, Salah",
  abstract = "Soaring is the process of exploiting favorable wind conditions to
              extend flight duration. This paper presents an approach for
              simultaneously mapping and using a wind field for soaring with an
              unpowered aircraft. Previous research by the authors and others
              has addressed soaring inknownwindfields. However, an adequate
              estimate of the wind field is required in order to generate
              energy-gain paths. Conversely, the exploration required to
              generate a useful map estimate requires energy. This work aims to
              address these problems simultaneously by attempting to maintain
              and improve a model-free wind map based on observations collected
              during the flight and to use the currently available map to
              generate energy-gain paths. Wind estimation is performed using
              Gaussian process regression.Apath planner generatesandselects
              paths basedonenergy efficiencyandfield exploration.Themethodis
              tested in simulation with wind fields consisting of single and
              multiple stationary thermal bubbles. The use of soaring flight
              with consistent improvement in map quality and accuracy is
              demonstrated in a number of scenarios. Nomenclature",
  journal  = "J. Guid. Control Dyn.",
  volume   =  34,
  number   =  3,
  pages    = "719--733",
  year     =  2011,
  keywords = "Folder - Fall2015;Mendeley Import (Jan 17)/WeeklyReading;Mendeley
              Import (Jan 17)/ReadingGroups"
}

@ARTICLE{Bar-Shalom2009-pg,
  title    = "The probabilistic data association filter",
  author   = "Bar-Shalom, Yaakov and Daum, Fred and Huang, Jim",
  journal  = "IEEE Control Syst. Mag.",
  volume   =  29,
  number   =  6,
  pages    = "82--100",
  month    =  dec,
  year     =  2009,
  keywords = "Folder - Fall2015;Mendeley Import (Jan 17)/ReadingGroups"
}

@ARTICLE{Schoenberg2012-fs,
  title     = "Posterior representation with a multi-modal likelihood using the
               gaussian sum filter for localization in a known map",
  author    = "Schoenberg, Jonathan R and Campbell, Mark and Miller, Isaac",
  abstract  = "A Gaussian sum filter (GSF) with component extended Kalman
               filters (EKF) is proposed as an approach to localizing an
               autonomous vehicle in an urban environment with limited GPS
               availability. The GSF uses vehicle-relative vision-based
               measurements of known map features coupled with inertial
               navigation solutions to accomplish localization in the absence
               of GPS. The vision-based measurements have multimodal
               measurement likelihood functions that are well represented as
               weighted sums of Gaussian densities. The GSF is used because of
               its ability to represent the posterior distribution of the
               vehicle pose with better efficiency (fewer terms, less
               computational complexity) than a corresponding bootstrap
               particle filter with various numbers of particles because of the
               interaction with measurement hypothesis tests. The
               expectation-maximization algorithm is used off line to determine
               the representational efficiency of the particle filter in terms
               of an effective number of Gaussian densities. In comparison, the
               GSF, which uses an iterative condensation procedure after each
               iteration of the filter to maintain real-time capabilities, is
               shown through a series of in-depth empirical studies to more
               accurately maintain a representation of the posterior
               distribution than the particle filter using 37 min of recorded
               data from Cornell University's autonomous vehicle driven in an
               urban environment, including a 32 min GPS blackout. \copyright{}
               2012 Wiley Periodicals, Inc.",
  journal   = "J. Field Robotics",
  publisher = "Wiley Subscription Services, Inc., A Wiley Company",
  volume    =  29,
  number    =  2,
  pages     = "240--257",
  month     =  "1~" # mar,
  year      =  2012,
  keywords  = "Folder - Fall2015;Mendeley Import (Jan 17)/ReadingGroups",
  language  = "en"
}

@INPROCEEDINGS{Klein2007-tr,
  title     = "Parallel Tracking and Mapping for Small {AR} Workspaces",
  booktitle = "2007 6th {IEEE} and {ACM} International Symposium on Mixed and
               Augmented Reality",
  author    = "Klein, G and Murray, D",
  abstract  = "This paper presents a method of estimating camera pose in an
               unknown scene. While this has previously been attempted by
               adapting SLAM algorithms developed for robotic exploration, we
               propose a system specifically designed to track a hand-held
               camera in a small AR workspace. We propose to split tracking and
               mapping into two separate tasks, processed in parallel threads
               on a dual-core computer: one thread deals with the task of
               robustly tracking erratic hand-held motion, while the other
               produces a 3D map of point features from previously observed
               video frames. This allows the use of computationally expensive
               batch optimisation techniques not usually associated with
               real-time operation: The result is a system that produces
               detailed maps with thousands of landmarks which can be tracked
               at frame-rate, with an accuracy and robustness rivalling that of
               state-of-the-art model-based systems.",
  publisher = "IEEE",
  pages     = "225--234",
  month     =  nov,
  year      =  2007,
  keywords  = "SLAM (robots);augmented reality;robot vision;SLAM
               algorithms;augmented reality;batch optimisation
               techniques;hand-held camera;parallel mapping;parallel
               tracking;robotic exploration;Algorithm design and
               analysis;Cameras;Concurrent computing;Handheld
               computers;Layout;Robot vision systems;Robustness;Simultaneous
               localization and mapping;Tracking;Yarn;Folder -
               Fall2015;NotRead;Mendeley Import (Jan 17)/ReadingGroups"
}

@INPROCEEDINGS{Pineau2003-wi,
  title     = "Point-based value iteration: An anytime algorithm for {POMDPs}",
  booktitle = "{IJCAI}",
  author    = "Pineau, Joelle and Gordon, Geoff and Thrun, Sebastian and
               {Others}",
  volume    =  3,
  pages     = "1025--1032",
  year      =  2003,
  keywords  = "Folder - Spring2016;Mendeley Import (Jan 17)/ReadingGroups"
}

@INPROCEEDINGS{Hemachandra2014-nk,
  title     = "Learning {Spatially-Semantic} Representations from Natural
               Language Descriptions and Scene Classification",
  booktitle = "Proceedings of the {IEEE} International Conference on Robotics
               and Automation ({ICRA})",
  author    = "Hemachandra, Sachithra and Walter, Matthew R and Tellex,
               Stefanie and Teller, Seth",
  publisher = "IEEE",
  pages     = "2623--2630",
  year      =  2014,
  keywords  = "Folder - Fall2015;Mendeley Import (Jan 17)/ReadingGroups"
}

@ARTICLE{Berniker2015-ih,
  title       = "Deep networks for motor control functions",
  author      = "Berniker, Max and Kording, Konrad P",
  affiliation = "Department of Mechanical and Industrial Engineering,
                 University of Illinois at Chicago Chicago, IL, USA ;
                 Department of Physical Medicine and Rehabilitation,
                 Northwestern University Chicago, IL, USA. Department of
                 Physical Medicine and Rehabilitation, Northwestern University
                 Chicago, IL, USA.",
  abstract    = "The motor system generates time-varying commands to move our
                 limbs and body. Conventional descriptions of motor control and
                 learning rely on dynamical representations of our body's state
                 (forward and inverse models), and control policies that must
                 be integrated forward to generate feedforward time-varying
                 commands; thus these are representations across space, but not
                 time. Here we examine a new approach that directly represents
                 both time-varying commands and the resulting state
                 trajectories with a function; a representation across space
                 and time. Since the output of this function includes time, it
                 necessarily requires more parameters than a typical dynamical
                 model. To avoid the problems of local minima these extra
                 parameters introduce, we exploit recent advances in machine
                 learning to build our function using a stacked autoencoder, or
                 deep network. With initial and target states as inputs, this
                 deep network can be trained to output an accurate temporal
                 profile of the optimal command and state trajectory for a
                 point-to-point reach of a non-linear limb model, even when
                 influenced by varying force fields. In a manner that mirrors
                 motor babble, the network can also teach itself to learn
                 through trial and error. Lastly, we demonstrate how this
                 network can learn to optimize a cost objective. This
                 functional approach to motor control is a sharp departure from
                 the standard dynamical approach, and may offer new insights
                 into the neural implementation of motor control.",
  journal     = "Front. Comput. Neurosci.",
  volume      =  9,
  number      = "March",
  pages       = "32",
  month       =  "19~" # mar,
  year        =  2015,
  keywords    = "arm reaches; deep learning; motor control; motor learning;
                 neural networks; optimal control;Folder - Spring2016;Mendeley
                 Import (Jan 17)/ReadingGroups",
  language    = "en"
}

@ARTICLE{Kurach2015-eo,
  title         = "Neural {Random-Access} Machines",
  author        = "Kurach, Karol and Andrychowicz, Marcin and Sutskever, Ilya",
  abstract      = "In this paper, we propose and investigate a new neural
                   network architecture called Neural Random Access Machine. It
                   can manipulate and dereference pointers to an external
                   variable-size random-access memory. The model is trained
                   from pure input-output examples using backpropagation. We
                   evaluate the new model on a number of simple algorithmic
                   tasks whose solutions require pointer manipulation and
                   dereferencing. Our results show that the proposed model can
                   learn to solve algorithmic tasks of such type and is capable
                   of operating on simple data structures like linked-lists and
                   binary trees. For easier tasks, the learned solutions
                   generalize to sequences of arbitrary length. Moreover,
                   memory access during inference can be done in a constant
                   time under some assumptions.",
  pages         = "17",
  month         =  "19~" # nov,
  year          =  2015,
  keywords      = "Computer Science - Learning;Folder - Spring2016;Mendeley
                   Import (Jan 17)/ReadingGroups",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1511.06392"
}

@ARTICLE{Chung2014-ld,
  title     = "Learning to soar: Resource-constrained exploration in
               reinforcement learning",
  author    = "Chung, Jen Jen and Lawrance, Nicholas R J and Sukkarieh, Salah",
  abstract  = "This paper examines temporal difference reinforcement learning
               with adaptive and directed exploration for resource-limited
               missions. The scenario considered is that of an unpowered aerial
               glider learning to perform energy-gaining flight trajectories in
               a thermal updraft. The presented algorithm,
               eGP-SARSA($\lambda$), uses a Gaussian process regression model
               to estimate the value function in a reinforcement learning
               framework. The Gaussian process also provides a variance on
               these estimates that is used to measure the contribution of
               future observations to the Gaussian process value function model
               in terms of information gain. To avoid myopic exploration we
               developed a resource-weighted objective function that combines
               an estimate of the future information gain using an action
               rollout with the estimated value function to generate directed
               explorative action sequences. A number of modifications and
               computational speed-ups to the algorithm are presented along
               with a standard GP-SARSA($\lambda$) implementation with
               $\epsilon$-greedy ...",
  journal   = "Int. J. Rob. Res.",
  publisher = "SAGE PublicationsSage UK: London, England",
  volume    =  34,
  number    =  2,
  pages     = "158--172",
  month     =  "16~" # dec,
  year      =  2014,
  keywords  = "aerial robotics; autonomous soaring; exploration; informative
               planning; reinforcement learning;Folder - Fall2015;Mendeley
               Import (Jan 17)/ReadingGroups",
  language  = "en"
}

@ARTICLE{Rong2014-so,
  title         = "word2vec Parameter Learning Explained",
  author        = "Rong, Xin",
  abstract      = "The word2vec model and application by Mikolov et al. have
                   attracted a great amount of attention in recent two years.
                   The vector representations of words learned by word2vec
                   models have been shown to carry semantic meanings and are
                   useful in various NLP tasks. As an increasing number of
                   researchers would like to experiment with word2vec or
                   similar techniques, I notice that there lacks a material
                   that comprehensively explains the parameter learning process
                   of word embedding models in details, thus preventing
                   researchers that are non-experts in neural networks from
                   understanding the working mechanism of such models. This
                   note provides detailed derivations and explanations of the
                   parameter update equations of the word2vec models, including
                   the original continuous bag-of-word (CBOW) and skip-gram
                   (SG) models, as well as advanced optimization techniques,
                   including hierarchical softmax and negative sampling.
                   Intuitive interpretations of the gradient equations are also
                   provided alongside mathematical derivations. In the
                   appendix, a review on the basics of neuron networks and
                   backpropagation is provided. I also created an interactive
                   demo, wevi, to facilitate the intuitive understanding of the
                   model.",
  pages         = "1--19",
  month         =  "11~" # nov,
  year          =  2014,
  keywords      = "Folder - Fall2015;Mendeley Import (Jan 17)/ReadingGroups",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1411.2738"
}

@INPROCEEDINGS{Hardy2014-kf,
  title     = "Multiple-step prediction using a two stage Gaussian Process
               model",
  booktitle = "2014 American Control Conference",
  author    = "Hardy, J and Havlak, F and Campbell, M",
  abstract  = "A two stage probabilistic prediction model is presented that
               uses nonparametric Gaussian Process (GP) regression to model
               continuous complex actions combined with a parametric model for
               known system dynamics. This two stage model is applied to the
               case of anticipating driver behavior and vehicle motion. The
               cross covariances between the initial state distribution and the
               control action distributions given by the GP regression model
               are computed analytically, allowing for a closed form evaluation
               of the joint distribution over the initial state and the GP
               outputs. Computing these cross covariances is necessary to
               capture important state dependent behavior in the GP data such
               as lane keeping for road vehicles. The proposed prediction model
               is evaluated using driving data collected from three human
               subjects navigating a standard four-way intersection in a
               driving simulation.",
  publisher = "IEEE",
  pages     = "3443--3449",
  month     =  jun,
  year      =  2014,
  keywords  = "Estimation; Modeling and simulation; Statistical learning;
               Gaussian processes; regression analysis; road traffic; GP
               regression model; closed form evaluation; cross covariances;
               driver behavior; driving simulation; four-way intersection;
               joint distribution; multiple-step prediction; nonparametric
               Gaussian process regression; parametric model; two stage
               Gaussian process model; two stage probabilistic prediction
               model; vehicle motion; Computational modeling; Data models;
               Mathematical model; Prediction algorithms; Predictive models;
               Vehicle dynamics; Vehicles;Folder - Fall2015;Mendeley Import
               (Jan 17)/ReadingGroups"
}

@ARTICLE{Piech2015-cr,
  title         = "Deep Knowledge Tracing",
  author        = "Piech, Chris and Spencer, Jonathan and Huang, Jonathan and
                   Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas and
                   Sohl-Dickstein, Jascha",
  abstract      = "Knowledge tracing---where a machine models the knowledge of
                   a student as they interact with coursework---is a well
                   established problem in computer supported education. Though
                   effectively modeling student knowledge would have high
                   educational impact, the task has many inherent challenges.
                   In this paper we explore the utility of using Recurrent
                   Neural Networks (RNNs) to model student learning. The RNN
                   family of models have important advantages over previous
                   methods in that they do not require the explicit encoding of
                   human domain knowledge, and can capture more complex
                   representations of student knowledge. Using neural networks
                   results in substantial improvements in prediction
                   performance on a range of knowledge tracing datasets.
                   Moreover the learned model can be used for intelligent
                   curriculum design and allows straightforward interpretation
                   and discovery of structure in student tasks. These results
                   suggest a promising new line of research for knowledge
                   tracing and an exemplary application task for RNNs.",
  pages         = "1--13",
  month         =  "19~" # jun,
  year          =  2015,
  keywords      = "Computer Science - Learning;Folder - Spring2016;Mendeley
                   Import (Jan 17)/ReadingGroups",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1506.05908"
}

@ARTICLE{Kaelbling1998-me,
  title    = "Planning and acting in partially observable stochastic domains",
  author   = "Kaelbling, Leslie Pack and Littman, Michael L and Cassandra,
              Anthony R",
  abstract = "In this paper, we bring techniques from operations research to
              bear on the problem of choosing optimal actions in partially
              observable stochastic domains. We begin by introducing the theory
              of Markov decision processes (mdps) and partially observable MDPs
              (pomdps). We then outline a novel algorithm for solving pomdps
              off line and show how, in some cases, a finite-memory controller
              can be extracted from the solution to a POMDP. We conclude with a
              discussion of how our approach relates to previous work, the
              complexity of finding exact solutions to pomdps, and of some
              possibilities for finding approximate solutions.",
  journal  = "Artif. Intell.",
  volume   =  101,
  number   = "1-2",
  pages    = "99--134",
  month    =  may,
  year     =  1998,
  keywords = "dblp;Folder - Fall2015;Mendeley Import (Jan 17)/ReadingGroups"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Kurniawati2008-te,
  title       = "{SARSOP}: Efficient {Point-Based} {POMDP} Planning by
                 Approximating Optimally Reachable Belief Spaces",
  booktitle   = "Robotics: Science and Systems",
  author      = "Kurniawati, Hanna and Hsu, David and Lee, Wee Sun",
  abstract    = "Motion planning in uncertain and dynamic environments is an
                 essential capability for autonomous robots. Partially
                 observable Markov decision processes (POMDPs) provide a
                 principled mathematical framework for solving such problems,
                 but they are often avoided in robotics due to high
                 computational complexity. Our goal is to create practical
                 POMDP algorithms and software for common robotic tasks. To
                 this end, we have developed a new point-based POMDP algorithm
                 that exploits the notion of optimally reachable belief spaces
                 to improve computational efﬁciency. In simulation, we
                 successfully applied the algorithm to a set of common robotic
                 tasks, including instances of coastal navigation, grasping,
                 mobile robot exploration, and target tracking, all modeled as
                 POMDPs with a large number of states. In most of the instances
                 studied, our algorithm substantially outperformed one of the
                 fastest existing point-based algorithms. A software package
                 implementing our algorithm will soon be released at
                 http://motion.comp.nus.edu.sg/ projects/pomdp/pomdp.html.",
  publisher   = "Zurich, Switzerland",
  volume      =  2008,
  pages       = "w/o page numbers",
  institution = "Zurich, Switzerland",
  year        =  2008,
  keywords    = "Folder - Spring2016;Mendeley Import (Jan 17)/ReadingGroups"
}

@ARTICLE{Kollar2008-go,
  title     = "Trajectory Optimization using Reinforcement Learning for Map
               Exploration",
  author    = "Kollar, Thomas and Roy, Nicholas",
  abstract  = "Automatically building maps from sensor data is a necessary and
               fundamental skill for mobile robots; as a result, considerable
               research attention has focused on the technical challenges
               inherent in the mapping problem. While statistical inference
               techniques have led to computationally efficient mapping
               algorithms, the next major challenge in robotic mapping is to
               automate the data collection process. In this paper, we address
               the problem of how a robot should plan to explore an unknown
               environment and collect data in order to maximize the accuracy
               of the resulting map. We formulate exploration as a constrained
               optimization problem and use reinforcement learning to find
               trajectories that lead to accurate maps. We demonstrate this
               process in simulation and show that the learned policy not only
               results in improved map building, but that the learned policy
               also transfers successfully to a real robot exploring on MIT
               campus.",
  journal   = "Int. J. Rob. Res.",
  publisher = "Sage PublicationsSage UK: London, England",
  volume    =  27,
  number    =  2,
  pages     = "175--196",
  month     =  "1~" # feb,
  year      =  2008,
  keywords  = "reinforcement learning; trajectory
               optimiza-;NotRead;reinforcement learning;Mendeley Import (Jan
               17)/ReinforcementLearning",
  language  = "en"
}

@BOOK{Deisenroth2010-ue,
  title     = "Efficient Reinforcement Learning using Gaussian Processes",
  author    = "Deisenroth, Marc",
  abstract  = "This book examines Gaussian processes in both model-based
               reinforcement learning (RL) and inference in nonlinear dynamic
               systems. First, we introduce PILCO, a fully Bayesian approach
               for efficient RL in continuous-valued state and action spaces
               when no expert knowledge is available. PILCO takes model
               uncertainties consistently into account during long-term
               planning to reduce model bias. Second, we propose principled
               algorithms for robust filtering and smoothing in GP dynamic
               systems.",
  publisher = "KIT Scientific Publishing",
  volume    =  9,
  year      =  2010,
  keywords  = "computational; information theoretic learning with statistics;
               learning; statistics \& optimisation; theory \&
               algorithms;NotRead;Mendeley Import (Jan
               17)/ReinforcementLearning"
}

@ARTICLE{Gosavi2009-mj,
  title    = "Reinforcement Learning: A Tutorial Survey and Recent Advances",
  author   = "Gosavi, Abhijit",
  abstract = "The purpose of this tutorial is to provide an introduction to
              reinforcement learning (RL) at a level easily understood by
              students and researchers in a wide range of disciplines. The
              intent is not to present a\textbackslashnrigorous mathematical
              discussion that requires a great deal of effort on the part of
              the reader, but rather to present a conceptual framework that
              might serve as an introduction to a more rigorous study of RL.
              The fundamental principles and techniques used to solve RL
              problems are presented. The most popular RL algorithms are
              presented. Section 1 presents an overview of RL and provides a
              simple example to develop intuition of the underlying dynamic
              programming mechanism. In Section 2 the parts of a reinforcement
              learning problem are discussed. These include the environment,
              reinforcement function, and value function. Section 3 gives a
              description of the most widely used reinforcement learning
              algorithms. These include TD($\lambda$) and both the residual and
              direct forms of value iteration, Q-learning, and advantage
              learning. In Section 4 some of the ancillary issues in RL are
              briefly discussed, such as choosing an exploration strategy and
              an appropriate discount factor. The conclusion is given in
              Section 5. Finally, Section 6 is a glossary of commonly used
              terms followed by references in Section 7 and a bibliography of
              RL applications in Section 8. The tutorial structure is such that
              each section builds on the information provided in previous
              sections.\textbackslashnIt is assumed that the reader has some
              knowledge of learning algorithms that rely on gradient descent
              (such as the backpropagation of errors algorithm).",
  journal  = "INFORMS J. Comput.",
  volume   =  21,
  number   =  2,
  pages    = "178--192",
  month    =  may,
  year     =  2009,
  keywords = "NotRead;Mendeley Import (Jan 17)/ReinforcementLearning"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Glorennec2000-bq,
  title     = "Reinforcement learning: An overview",
  author    = "Glorennec, P Y",
  abstract  = "Abstract Reinforcement learning relies on the association
               between a goal and a scalar signal, interpreted as reward or
               punishment. The objective is not to reproduce some reference
               signal, but to progessively find, by trial and error, the policy
               maximizing the",
  journal   = "European Symposium on Intelligent Techniques (ESIT …",
  publisher = "Citeseer",
  number    = "September",
  pages     = "14--15",
  year      =  2000,
  keywords  = "NotRead;Mendeley Import (Jan 17)/ReinforcementLearning",
  language  = "en\_US"
}

@ARTICLE{Kober2013-vt,
  title    = "Reinforcement learning in robotics: A survey",
  author   = "Kober, J and Bagnell, J A and Peters, J",
  abstract = "Reinforcement learning offers to robotics a framework and set of
              tools for the design of sophisticated and hard-to-engineer
              behaviors. Conversely, the challenges of robotic problems provide
              both inspiration, impact, and validation for developments in
              reinforcement learning. The relationship between disciplines has
              sufficient promise to be likened to that between physics and
              mathematics. In this article, we attempt to strengthen the links
              between the two research communities by providing a survey of
              work in reinforcement learning for behavior generation in robots.
              We highlight both key challenges in robot reinforcement learning
              as well as notable successes. We discuss how contributions tamed
              the complexity of the domain and study the role of algorithms,
              representations, and prior knowledge in achieving these
              successes. As a result, a particular focus of our paper lies on
              the choice between model-based and model-free as well as between
              value-function-based and policy-search methods. By analyzing a
              simple problem in some detail we demonstrate how reinforcement
              learning approaches may be profitably applied, and we note
              throughout open questions and the tremendous potential for future
              research.",
  journal  = "Int. J. Rob. Res.",
  volume   =  32,
  number   =  11,
  pages    = "1238--1274",
  month    =  "1~" # sep,
  year     =  2013,
  keywords = "learning control; reinforcement learning; robot;
              survey;NotRead;reinforcement learning;Mendeley Import (Jan
              17)/ReinforcementLearning",
  language = "en"
}

@INCOLLECTION{Ruvolo2009-rl,
  title     = "Optimization on a Budget: A Reinforcement Learning Approach",
  booktitle = "Advances in Neural Information Processing Systems 21",
  author    = "Ruvolo, Paul L and Fasel, Ian and Movellan, Javier R",
  editor    = "Koller, D and Schuurmans, D and Bengio, Y and Bottou, L",
  publisher = "Curran Associates, Inc.",
  pages     = "1385--1392",
  year      =  2009,
  keywords  = "NotRead;Mendeley Import (Jan 17)/ReinforcementLearning"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Gosavi2014-ua,
  title     = "{Simulation-Based} Optimization: Parametric Optimization
               Techniques and Reinforcement Learning",
  author    = "Gosavi, Abhijit",
  abstract  = "Simulation-Based Optimization: Parametric Optimization
               Techniques and Reinforcement Learning introduce the evolving
               area of static and dynamic simulation-based optimization.
               Covered in detail are model-free optimization techniques --
               especially designed for those discrete-event, stochastic systems
               which can be simulated but whose analytical models are difficult
               to find in closed mathematical forms.Key features of this
               revised and improved Second Edition include:· Extensive
               coverage, via step-by-step recipes, of powerful new algorithms
               for static simulation optimization, including simultaneous
               perturbation, backtracking adaptive search and nested
               partitions, in addition to traditional methods, such as response
               surfaces, Nelder-Mead search and meta-heuristics (simulated
               annealing, tabu search, and genetic algorithms)· Detailed
               coverage of the Bellman equation framework for Markov Decision
               Processes (MDPs), along with dynamic programming (value and
               policy iteration) for discounted, average, and total reward
               performance metrics· An in-depth consideration of dynamic
               simulation optimization via temporal differences and
               Reinforcement Learning: Q-Learning, SARSA, and R-SMART
               algorithms, and policy search, via API, Q-P-Learning,
               actor-critics, and learning automata· A special examination of
               neural-network-based function approximation for Reinforcement
               Learning, semi-Markov decision processes (SMDPs), finite-horizon
               problems, two time scales, case studies for industrial tasks,
               computer codes (placed online) and convergence proofs, via
               Banach fixed point theory and Ordinary Differential
               EquationsThemed around three areas in separate sets of chapters
               -- Static Simulation Optimization, Reinforcement Learning and
               Convergence Analysis -- this book is written for researchers and
               students in the fields of engineering (industrial, systems,
               electrical and computer), operations research, computer science
               and applied mathematics.",
  publisher = "Springer",
  volume    =  55,
  pages     = "508",
  series    = "Operations Research/Computer Science Interfaces Series",
  month     =  "30~" # oct,
  year      =  2014,
  address   = "Boston, MA",
  keywords  = "NotRead;Textbook;Mendeley Import (Jan 17)/TextBooks;Mendeley
               Import (Jan 17)/ReinforcementLearning",
  language  = "en"
}

@ARTICLE{Ormoneit2002-zi,
  title     = "{Kernel-Based} Reinforcement Learning",
  author    = "Ormoneit, Dirk and Sen, {\'S}aunak",
  abstract  = "We present a kernel-based approach to reinforcement learning
               that overcomes the stability problems of temporal-difference
               learning in continuous state-spaces. First, our algorithm
               converges to a unique solution of an approximate Bellman's
               equation regardless of its initialization values. Second, the
               method is consistent in the sense that the resulting policy
               converges asymptotically to the optimal policy. Parametric value
               function estimates such as neural networks do not possess this
               property. Our kernel-based approach also allows us to show that
               the limiting distribution of the value function estimate is a
               Gaussian process. This information is useful in studying the
               bias-variance tradeoff in reinforcement learning. We find that
               all reinforcement learning approaches to estimating the value
               function, parametric or non-parametric, are subject to a bias.
               This bias is typically larger in reinforcement learning than in
               a comparable regression problem.",
  journal   = "Mach. Learn.",
  publisher = "Kluwer Academic Publishers",
  volume    =  49,
  number    = "2-3",
  pages     = "161--178",
  month     =  "1~" # nov,
  year      =  2002,
  keywords  = "Kernel smoothing; Kernel-based learning; Lazy learning; Local
               averaging; Markov decision process; Reinforcement
               learning;Artificial Intelligence (incl. Robotics);Computer
               Science- general;NotRead;reinforcement learning;Mendeley Import
               (Jan 17)/ReinforcementLearning",
  language  = "en"
}

@INPROCEEDINGS{Engel2005-vp,
  title     = "Reinforcement Learning with Gaussian Processes",
  booktitle = "Proceedings of the 22Nd International Conference on Machine
               Learning",
  author    = "Engel, Yaakov and Mannor, Shie and Meir, Ron",
  abstract  = "Gaussian Process Temporal Difference (GPTD) learning offers a
               Bayesian solution to the policy evaluation problem of
               reinforcement learning. In this paper we extend the GPTD
               framework by addressing two pressing issues, which were not
               adequately treated in the original GPTD paper (Engel et al.,
               2003). The first is the issue of stochasticity in the state
               transitions, and the second is concerned with action selection
               and policy improvement. We present a new generative model for
               the value function, deduced from its relation with the
               discounted return. We derive a corresponding on-line algorithm
               for learning the posterior moments of the value Gaussian
               process. We also present a SARSA based extension of GPTD, termed
               GPSARSA, that allows the selection of actions and the gradual
               improvement of policies without requiring a world-model.",
  publisher = "ACM",
  pages     = "201--208",
  series    = "ICML '05",
  year      =  2005,
  address   = "New York, NY, USA",
  keywords  = "NotRead;Mendeley Import (Jan 17)/ReinforcementLearning"
}

@INPROCEEDINGS{Chevalier2013-qq,
  title      = "Fast Computation of the {Multi-Points} Expected Improvement
                with Applications in Batch Selection",
  booktitle  = "Learning and Intelligent Optimization",
  author     = "Chevalier, Cl{\'e}ment and Ginsbourger, David",
  editor     = "Nicosia, Giuseppe and Pardalos, Panos",
  abstract   = "The Multi-points Expected Improvement criterion (or
                \textbackslash(q\textbackslash)-EI) has recently been studied
                in batch-sequential Bayesian Optimization. This paper deals
                with a new way of computing \textbackslash(q\textbackslash)-EI,
                without using Monte-Carlo simulations, through a closed-form
                formula. The latter allows a very fast computation of
                \textbackslash(q\textbackslash)-EI for reasonably low values of
                \textbackslash(q\textbackslash) (typically, less than 10). New
                parallel kriging-based optimization strategies, tested on
                different toy examples, show promising results.",
  publisher  = "Springer Berlin Heidelberg",
  volume     = "7997 LNCS",
  pages      = "59--69",
  series     = "Lecture Notes in Computer Science",
  month      =  "7~" # jan,
  year       =  2013,
  keywords   = "Computer experiments; Expected improvement; Kriging; Parallel
                optimization;Acquisition/InfillFxns;BayesOpt;TALAF;batch
                selection;qEI;Mendeley Import (Jan 17)/BayesOpt;Mendeley Import
                (Jan 17)/BayesOpt/Acquisition/InfillFxns",
  language   = "en",
  conference = "International Conference on Learning and Intelligent
                Optimization"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Dick2013-mi,
  title    = "High-dimensional integration: The quasi-Monte Carlo way",
  author   = "Dick, Josef and Kuo, Frances Y and Sloan, Ian H",
  abstract = "This paper is a contemporary review of QMC (‘quasi-Monte Carlo’)
              methods, that is, equal-weight rules for the approximate
              evaluation of high-dimensional integrals over the unit cube [0,
              1]s,where s may be large, or even infinite. Af- ter a general
              introduction, the paper surveys recent developments in lattice
              methods, digital nets, and related themes. Among those recent
              developments are methods of construction of both lattices and
              digital nets, to yield QMC rules that have a prescribed rate of
              convergence for sufficiently smooth func- tions, and ideally also
              guaranteed slow growth (or no growth) of the worst-case error as
              s increases. A crucial role is played by parameters called
              ‘weights’, since a careful use of the weight parameters is needed
              to ensure that the worst-case errors in an appropriately weighted
              function space are bounded, or grow only slowly, as the dimension
              s increases. Important tools for the analysis are weighted
              function spaces, reproducing kernel Hilbert spaces, and
              discrepancy, all of which are discussed with an appropriate level
              of detail.",
  journal  = "Acta Numer.",
  volume   =  22,
  number   = "April 2013",
  pages    = "133--288",
  month    =  "2~" # may,
  year     =  2013,
  keywords = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;Mendeley Import (Jan
              17)/BayesOpt;Mendeley Import (Jan 17)/OptimizingHyperparameters",
  language = "en"
}

@BOOK{Ng2013-al,
  title     = "Learning and Intelligent Optimization",
  author    = "Ng, Amos H C and Dudas, Catarina and Bostr{\"o}m, Henrik and
               Deb, Kalyanmoy",
  editor    = "Nicosia, Giuseppe and Pardalos, Panos",
  abstract  = "This paper introduces a novel methodology for the optimization,
               analysis and decision support in production systems engineering.
               The methodology is based on the innovization procedure,
               originally introduced to unveil new and innovative design
               principles in engineering design problems. The innovization
               procedure stretches beyond an optimization task and attempts to
               discover new design/operational rules/principles relating to
               decision variables and objectives, so that a deeper
               understanding of the underlying problem can be obtained. By
               integrating the concept of innovization with simulation and data
               mining techniques, a new set of powerful tools can be developed
               for general systems analysis. The uniqueness of the approach
               introduced in this paper lies in that decision rules extracted
               from the multi-objective optimization using data mining are used
               to modify the original optimization. Hence, faster convergence
               to the desired solution of the decision-maker can be achieved.
               In other words, faster convergence and deeper knowledge of the
               relationships between the key decision variables and objectives
               can be obtained by interleaving the multi-objective optimization
               and data mining process. In this paper, such an interleaved
               approach is illustrated through a set of experiments carried out
               on a simulation model developed for a real-world production
               system analysis problem. \copyright{} 2013 Springer-Verlag.",
  publisher = "Springer Berlin Heidelberg",
  volume    =  7997,
  pages     = "1--18",
  series    = "Lecture Notes in Computer Science",
  year      =  2013,
  address   = "Berlin, Heidelberg",
  keywords  = "Data mining; Innovization; Multi-objective optimization;
               Production system simulation;BayesOpt;TALAF;Mendeley Import (Jan
               17)/BayesOpt"
}

@BOOK{Shewchuk1994-og,
  title     = "An Introduction to the Conjugate Gradient Method Without the
               Agonizing Pain",
  author    = "Shewchuk, Jonathan Richard",
  abstract  = "The Conjugate Gradient Method is the most prominent iterative
               method for solving sparse systems of linear equations.
               Unfortunately, many textbook treatments of the topic are written
               with neither illustrations nor intuition, and their victims can
               be found to this day babbling senselessly in the corners of
               dusty libraries. For this reason, a deep, geometric
               understanding of the method has been reserved for the elite
               brilliant few who have painstakingly decoded the mumblings of
               their forebears. Nevertheless, the Conjugate Gradient Method is
               a composite of simple, elegant ideas that almost anyone can
               understand. Of course, a reader as intelligent as yourself will
               learn them almost effortlessly. The idea of quadratic forms is
               introduced and used to derive the methods of Steepest Descent,
               Conjugate Directions, and Conjugate Gradients. Eigenvectors are
               explained and used to examine the convergence of the Jacobi
               Method, Steepest Descent, and Conjugate Gradients. Other topics
               include preconditioning and the nonlinear Conjugate Gradient
               Method. I have taken pains to make this article easy to read.
               Sixty-six illustrations are provided. Dense prose is avoided.
               Concepts are explained in several differentways. Most equations
               are coupled with an intuitive interpretation.",
  publisher = "Carnegie-Mellon University. Department of Computer Science",
  volume    =  49,
  pages     = "64",
  year      =  1994,
  keywords  = "1; 2; 5; agonizing pain; conjugate gradient method; convergence
               analysis; eigen do; eigenvalues; i try; jacobi iterations;
               preconditioning; thinking with
               eigenvectors;BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@ARTICLE{MacKay1992-sp,
  title    = "{Information-Based} Objective Functions for Active Data Selection",
  author   = "MacKay, D J C",
  abstract = "Learning can be made more efficient if we can actively select
              particularly salient data points. Within a Bayesian learning
              framework, objective functions are discussed that measure the
              expected informativeness of candidate measurements. Three
              alternative specifications of what we want to gain information
              about lead to three different criteria for data selection. All
              these criteria depend on the assumption that the hypothesis space
              is correct, which may prove to be their main weakness.",
  journal  = "Neural Comput.",
  volume   =  4,
  number   =  4,
  pages    = "590--604",
  month    =  jul,
  year     =  1992,
  keywords = "
              Acquisition/InfillFxns;BayesOpt;TALAF;trust\_informal\_treatment;assurance\_implicit;Mendeley
              Import (Jan 17)/BayesOpt;Mendeley Import (Jan
              17)/BayesOpt/Acquisition/InfillFxns;Mendeley Import (Jan
              17)/Assurances"
}

@ARTICLE{Williams1998-kr,
  title    = "Bayesian classification with Gaussian processes",
  author   = "Williams, C K I and Barber, D",
  abstract = "We consider the problem of assigning an input vector to one of m
              classes by predicting P(c|x) for c=1,...,m. For a two-class
              problem, the probability of class one given x is estimated by
              $\sigma$(y(x)), where $\sigma$(y)=1/(1+e-y). A Gaussian process
              prior is placed on y(x), and is combined with the training data
              to obtain predictions for new x points. We provide a Bayesian
              treatment, integrating over uncertainty in y and in the
              parameters that control the Gaussian process prior the necessary
              integration over y is carried out using Laplace's approximation.
              The method is generalized to multiclass problems (m>2) using the
              softmax function. We demonstrate the effectiveness of the method
              on a number of datasets",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  20,
  number   =  12,
  pages    = "1342--1351",
  month    =  dec,
  year     =  1998,
  keywords = "Bayes methods;Gaussian processes;Markov processes;Monte Carlo
              methods;optimisation;pattern classification;probability;Bayesian
              classification;Gaussian processes;Laplace approximation;Markov
              chain;Monte Carlo method;input vector;multiclass
              problems;optimisation;parameter
              uncertainty;probability;softmax;two-class problem;Bayesian
              methods;Computer Society;Gaussian noise;Gaussian
              processes;Logistics;Monte Carlo methods;Process control;Training
              data;Uncertain systems;Uncertainty;Bayes
              methods;BayesOpt;Important;Markov
              processes;monte\_carlo;NotRead;TALAF;Uncertainty;optimisation;pattern
              classification;probability;GPs;Mendeley Import (Jan 17)/BayesOpt"
}

@ARTICLE{Serofino2014-ro,
  title    = "Optimizing Without Derivatives: What Does the No Free Lunch
              Theorem Actually Say?",
  author   = "Serofino, Loris",
  journal  = "Not. Am. Math. Soc.",
  volume   =  61,
  number   =  07,
  pages    = "750",
  month    =  "1~" # aug,
  year     =  2014,
  keywords = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt",
  language = "en"
}

@INPROCEEDINGS{Snoek2012-tt,
  title     = "Practical Bayesian Optimization of Machine Learning Algorithms",
  booktitle = "Adv. Neural Inf. Process. Syst. 25",
  author    = "Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P",
  abstract  = "The use of machine learning algorithms frequently involves
               careful tuning of learning parameters and model hyperparameters.
               Unfortunately, this tuning is often a
               \{\textbackslashtextquoteleft\}\{\textbackslashtextquoteleft\}black
               art\{\textbackslashtextquoteright\}\{\textbackslashtextquoteright\}
               requiring expert experience, rules of thumb, or sometimes
               brute-force search. There is therefore great appeal for
               automatic approaches that can optimize the performance of any
               given learning algorithm to the problem at hand. In this work,
               we consider this problem through the framework of Bayesian
               optimization, in which a learning
               algorithm\{\textbackslashtextquoteright\}s generalization
               performance is modeled as a sample from a Gaussian process (GP).
               We show that certain choices for the nature of the GP, such as
               the type of kernel and the treatment of its hyperparameters, can
               play a crucial role in obtaining a good optimizer that can
               achieve expert-level performance. We describe new algorithms
               that take into account the variable cost (duration) of learning
               algorithm experiments and that can leverage the presence of
               multiple cores for parallel experimentation. We show that these
               proposed algorithms improve on previous automatic procedures and
               can reach or surpass human expert-level optimization for many
               algorithms including latent Dirichlet allocation, structured
               SVMs and convolutional neural networks.",
  pages     = "2951--2959",
  year      =  2012,
  keywords  = "bayesian optimization; deep learning; gaussian
               process;BayesOpt;Important;TALAF;Mendeley Import (Jan
               17)/BayesOpt"
}

@MISC{DBSCAN_wiki2016-cr,
  title    = "{DBSCAN}",
  author   = "{DBSCAN\_wiki}",
  abstract = "Density-based spatial clustering of applications with noise
              (DBSCAN) is a data clustering algorithm proposed by Martin Ester,
              Hans-Peter Kriegel, J\{{\"o}\}rg Sander and Xiaowei Xu in 1996.
              It is a density-based clustering algorithm: given a set of points
              in some space, it groups together points that are closely packed
              together (points with many nearby neighbors), marking as outliers
              points that lie alone in low-density regions (whose nearest
              neighbors are too far away). DBSCAN is one of the most common
              clustering algorithms and also most cited in scientific
              literature. In 2014, the algorithm was awarded the test of time
              award (an award given to algorithms which have received
              substantial attention in theory and practice) at the leading data
              mining conference, KDD.",
  month    =  jan,
  year     =  2016,
  keywords = "Acquisition/InfillFxns; BayesOpt;
              TALAF;Acquisition/InfillFxns;BayesOpt;TALAF;Mendeley Import (Jan
              17);Mendeley Import (Jan 17)/BayesOpt;Mendeley Import (Jan
              17)/BayesOpt/Acquisition/InfillFxns"
}

@ARTICLE{MacKay1999-ws,
  title    = "Comparison of Approximate Methods for Handling Hyperparameters",
  author   = "MacKay, David J C",
  abstract = "I examine two approximate methods for computational
              implementation of Bayesian hierarchical models, that is, models
              that include unknown hyperparameters such as regularization
              constants and noise levels. In the evidence framework, the model
              parameters are integrated over, and the resulting evidence is
              maximized over the hyperparameters. The optimized hyperparameters
              are used to define a gaussian approximation to the posterior
              distribution. In the alternative MAP method, the true posterior
              probability is found by integrating over the hyperparameters. The
              true posterior is then maximized over the model parameters, and a
              gaussian approximation is made. The similarities of the two
              approaches and their relative merits are discussed, and
              comparisons are made with the ideal hierarchical Bayesian
              solution. In moderately ill-posed problems, integration over
              hyperparameters yields a probability distribution with a skew
              peak, which causes signifi-cant biases to arise in the MAP
              method. In contrast, the evidence framework is shown to introduce
              negligible predictive error under straightforward conditions.
              General lessons are drawn concerning inference in many
              dimensions.",
  journal  = "Neural Comput.",
  volume   =  11,
  number   =  5,
  pages    = "1035--1068",
  year     =  1999,
  keywords = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;Mendeley Import (Jan
              17)/BayesOpt;Mendeley Import (Jan 17)/OptimizingHyperparameters"
}

@ARTICLE{Finkel2003-mt,
  title    = "{DIRECT} optimization algorithm user guide",
  author   = "Finkel, Daniel E",
  abstract = "O objetivo deste guia {\'e} breve para introduzir o leitor para o
              algoritmo de otimiza{\c c}{\~a}o DIRECT, descrever o tipo de
              problemas que resolve, como usar o programa que acompanha MATLAB
              direct.m, e fornecer uma sinopse de como ele procura o m{\'\i}nio
              global . Um exemplo de DIRECT sendo usado em um problema de teste
              {\'e} fornecida, e os motiviation para o algoritmo {\'e}
              tamb{\'e}m discutida. O ap{\^e}ndice fornece f{\'o}rmulas e dados
              para o 7 fun{\c c}{\~o}es de teste que foram utilizadas em [3].",
  journal  = "Center for Research in Scientific Computation, North Carolina
              State University",
  volume   =  2,
  pages    = "1--14",
  year     =  2003,
  keywords = "BayesOpt;DIRECT;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@MISC{noauthor_undated-pu,
  title        = "optimization - Hyperparameter estimation in Gaussian process
                  - Cross Validated",
  howpublished = "\url{http://stats.stackexchange.com/questions/30069/hyperparameter-estimation-in-gaussian-process}",
  keywords     = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;Mendeley Import
                  (Jan 17)/BayesOpt;Mendeley Import (Jan
                  17)/OptimizingHyperparameters"
}

@ARTICLE{Cawley2010-ey,
  title     = "On Over-fitting in Model Selection and Subsequent Selection Bias
               in Performance Evaluation",
  author    = "Cawley, Gavin C and Talbot, Nicola L C",
  abstract  = "Model selection strategies for machine learning algorithms
               typically involve\textbackslashnthe numerical optimisation of an
               appropriate model selection criterion, often\textbackslashnbased
               on an estimator of generalisation performance, such as k-fold
               \textbackslashncross-validation. The error of such an estimator
               can be broken down into bias \textbackslashnand variance
               components. While unbiasedness is often cited as a beneficial
               \textbackslashnquality of a model selection criterion, we
               demonstrate that a low variance is \textbackslashnat least as
               important, as a non-negligible variance introduces the potential
               \textbackslashnfor over-fitting in model selection as well as in
               training the model. While \textbackslashnthis observation is in
               hindsight perhaps rather obvious, the degradation in
               \textbackslashnperformance due to over-fitting the model
               selection criterion can be \textbackslashnsurprisingly large, an
               observation that appears to have received little
               \textbackslashnattention in the machine learning literature to
               date. In this paper, we show \textbackslashnthat the effects of
               this form of over-fitting are often of comparable
               \textbackslashnmagnitude to differences in performance between
               learning algorithms, and thus \textbackslashncannot be ignored
               in empirical evaluation. Furthermore, we show that some
               \textbackslashncommon performance evaluation practices are
               susceptible to a form of selection \textbackslashnbias as a
               result of this form of over-fitting and hence are unreliable. We
               \textbackslashndiscuss methods to avoid over-fitting in model
               selection and subsequent \textbackslashnselection bias in
               performance evaluation, which we hope will be incorporated
               \textbackslashninto best practice. While this study concentrates
               on cross-validation based \textbackslashnmodel selection, the
               findings are quite general and apply to any model
               \textbackslashnselection practice involving the optimisation of
               a model selection criterion \textbackslashnevaluated over a
               finite sample of data, including maximisation of the Bayesian
               \textbackslashnevidence and optimisation of performance bounds.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  11,
  pages     = "2079--2107",
  month     =  "1~" # mar,
  year      =  2010,
  keywords  = "bias-variance trade-off; model selection; over-; performance
               evaluation; selection
               bias;BayesOpt;GPs;Important;OptimizingHyperparameters;TALAF;Mendeley
               Import (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters"
}

@INPROCEEDINGS{Swersky2013-ej,
  title    = "Multi-task bayesian optimization",
  author   = "Swersky, Kevin and Snoek, Jasper and Adams, Ryan P",
  pages    = "2004--2012",
  year     =  2013,
  keywords = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@ARTICLE{Prechelt1998-ue,
  title       = "Automatic early stopping using cross validation: quantifying
                 the criteria",
  author      = "Prechelt, Lutz",
  affiliation = "Fakult{\"a}t f{\"u}r Informatik, Universit{\"a}t Karlsruhe,
                 Karlsruhe, Germany",
  abstract    = "Cross validation can be used to detect when overfitting starts
                 during supervised training of a neural network; training is
                 then stopped before convergence to avoid the overfitting
                 ('early stopping'). The exact criterion used for cross
                 validation based early stopping, however, is chosen in an
                 ad-hoc fashion by most researchers or training is stopped
                 interactively. To aid a more well-founded selection of the
                 stopping criterion, 14 different automatic stopping criteria
                 from three classes were evaluated empirically for their
                 efficiency and effectiveness in 12 different classification
                 and approximation tasks using multi-layer perceptrons with
                 RPROP training. The experiments show that, on average, slower
                 stopping criteria allow for small improvements in
                 generalization (in the order of 4\%), but cost about a factor
                 of 4 longer in training time.",
  journal     = "Neural Netw.",
  volume      =  11,
  number      =  4,
  pages       = "761--767",
  month       =  jun,
  year        =  1998,
  keywords    = "Cross validation; Early stopping; Empirical study;
                 Generalization; Overfitting; Supervised
                 learning;BayesOpt;GPs;OptimizingHyperparameters;TALAF;Mendeley
                 Import (Jan 17)/BayesOpt;Mendeley Import (Jan
                 17)/OptimizingHyperparameters",
  language    = "en"
}

@ARTICLE{Reunanen2003-hn,
  title     = "Overfitting in making comparisons between variable selection
               methods",
  author    = "Reunanen, Juha",
  abstract  = "This paper addresses a common methodological flaw in the
               comparison of variable selection meth-ods. A practical approach
               to guide the search or the selection process is to compute
               cross-validation performance estimates of the different variable
               subsets. Used with computationally intensive search algorithms,
               these estimates may overfit and yield biased predictions.
               Therefore, they cannot be used reliably to compare two selection
               methods, as is shown by the empirical results of this paper.
               In-stead, like in other instances of the model selection
               problem, independent test sets should be used for determining
               the final performance. The claims made in the literature about
               the superiority of more exhaustive search algorithms over
               simpler ones are also revisited, and some of them infirmed.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  3,
  pages     = "1371--1382",
  month     =  "1~" # mar,
  year      =  2003,
  keywords  = "Algorithm comparison; Cross-validation; Overfitting; Variable
               selection; k nearest
               neighbors;BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;Mendeley
               Import (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters"
}

@ARTICLE{Sobester2005-ms,
  title     = "On the Design of Optimization Strategies Based on Global
               Response Surface Approximation Models",
  author    = "S{\'o}bester, Andr{\'a}s and Leary, Stephen J and Keane, Andy J",
  abstract  = "Striking the correct balance between global exploration of
               search spaces and local exploitation of promising basins of
               attraction is one of the principal concerns in the design of
               global optimization algorithms. This is true in the case of
               techniques based on global response surface approximation models
               as well. After constructing such a model using some initial
               database of designs it is far from obvious how to select further
               points to examine so that the appropriate mix of exploration and
               exploitation is achieved. In this paper we propose a selection
               criterion based on the expected improvement measure, which
               allows relatively precise control of the scope of the search. We
               investigate its behavior through a set of artificial test
               functions and two structural optimization problems. We also look
               at another aspect of setting up search heuristics of this type:
               the choice of the size of the database that the initial
               approximation is built upon.",
  journal   = "J. Global Optimiz.",
  publisher = "Kluwer Academic Publishers",
  volume    =  33,
  number    =  1,
  pages     = "31--59",
  month     =  "1~" # sep,
  year      =  2005,
  keywords  = "Expected improvement; Gaussian kernels; Radial basis
               functions;Acquisition/InfillFxns;BayesOpt;Computer Science-
               general;TALAF;Mendeley Import (Jan 17)/BayesOpt;Mendeley Import
               (Jan 17)/BayesOpt/Acquisition/InfillFxns;Mendeley Import (Jan
               17)",
  language  = "en"
}

@ARTICLE{Zhou2011-eo,
  title    = "A Simple Approach to Emulation for Computer Models With
              Qualitative and Quantitative Factors",
  author   = "Zhou, Qiang and Qian, Peter Z G and Zhou, Shiyu",
  abstract = "We propose a flexible yet computationally efficient approach for
              building Gaussian process models for computer experiments with
              both qualitative and quantitative factors. This approach uses the
              hypersphere parameterization to model the correlations of the
              qualitative factors, thus avoiding the need of directly solving
              optimization problems with positive definite constraints. The
              effectiveness of the proposed method is successfully illustrated
              by several examples.",
  journal  = "Technometrics",
  volume   =  53,
  number   =  3,
  pages    = "266--273",
  year     =  2011,
  keywords = "computer experiment; hypersphere decomposition;
              kriging;BayesOpt;Important;TALAF;Mendeley Import (Jan
              17)/BayesOpt"
}

@ARTICLE{Ma2015-kx,
  title     = "Active Pointillistic Pattern Search",
  author    = "Ma, Y and Sutherland, D J and Garnett, R and Schneider, J G",
  abstract  = "Abstract We introduce the problem of active pointillistic
               pattern search (APPS), which seeks to discover regions of a
               domain exhibiting desired behavior with limited observations.
               Unusually, the patterns we consider are defined by large-scale
               properties of an underlying",
  journal   = "AISTATS",
  publisher = "jmlr.org",
  pages     = "672--680",
  year      =  2015,
  keywords  = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@ARTICLE{Kushner1964-le,
  title    = "A new method of locating the maximal point of an arbitrary
              multipeak curve in the presence of noise",
  author   = "Kushner, Harold J",
  journal  = "J. Basic Eng.",
  volume   =  86,
  number   =  1,
  pages    = "97--106",
  year     =  1964,
  keywords = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@ARTICLE{Wang2013-lg,
  title     = "Adaptive Hamiltonian and Riemann manifold Monte Carlo samplers",
  author    = "Wang, Z and Mohamed, S and De Freitas, N",
  abstract  = "Abstract In this paper we address the widelyexperienced
               difficulty in tuning Monte Carlo sampler based on simulating
               Hamiltonian dynamics. We develop an algorithm that allows for
               the adaptation of Hamiltonian and Riemann manifold Hamiltonian
               Monte Carlo samplers using Bayesian optimization that allows for
               infinite adaptation of the parameters of these samplers. We show
               that the resulting samplers are ergodic, and that the use of our
               ...",
  journal   = "International Conference on Machine",
  publisher = "jmlr.org",
  volume    =  28,
  pages     = "10",
  year      =  2013,
  keywords  = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@ARTICLE{Picheny2013-uu,
  title     = "A benchmark of kriging-based infill criteria for noisy
               optimization",
  author    = "Picheny, Victor and Wagner, Tobias and Ginsbourger, David",
  abstract  = "Responses of many real-world problems can only be evaluated
               perturbed by noise. In order to make an efficient optimization
               of these problems possible, intelligent optimization strategies
               successfully coping with noisy evaluations are required. In this
               article, a comprehensive review of existing kriging-based
               methods for the optimization of noisy functions is provided. In
               summary, ten methods for choosing the sequential samples are
               described using a unified formalism. They are compared on
               analytical benchmark problems, whereby the usual assumption of
               homoscedastic Gaussian noise made in the underlying models is
               meet. Different problem configurations (noise level, maximum
               number of observations, initial number of observations) and
               setups (covariance functions, budget, initial sample size) are
               considered. It is found that the choices of the initial sample
               size and the covariance function are not critical. The choice of
               the method, however, can result in significant differences in
               the performance. In particular, the three most intuitive
               criteria are found as poor alternatives. Although no criterion
               is found consistently more efficient than the others, two
               specialized methods appear more robust on average.",
  journal   = "Struct. Multidiscip. Optim.",
  publisher = "Springer Berlin Heidelberg",
  volume    =  48,
  number    =  3,
  pages     = "607--626",
  month     =  "21~" # apr,
  year      =  2013,
  keywords  = "EGO; Metamodeling;
               Noise;Acquisition/InfillFxns;BayesOpt;TALAF;Mendeley Import (Jan
               17)/BayesOpt;Mendeley Import (Jan
               17)/BayesOpt/Acquisition/InfillFxns",
  language  = "en"
}

@INPROCEEDINGS{Martinez-Cantin2007-zu,
  title     = "Active Policy Learning for Robot Planning and Exploration under
               Uncertainty",
  booktitle = "Robotics: Science and Systems",
  author    = "Martinez-Cantin, Ruben and de Freitas, Nando and Doucet, Arnaud
               and Castellanos, Jos{\'e} A",
  pages     = "321--328",
  year      =  2007,
  keywords  = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@ARTICLE{Hennig2011-wa,
  title         = "Entropy Search for {Information-Efficient} Global
                   Optimization",
  author        = "Hennig, Philipp and Schuler, Christian J",
  abstract      = "Contemporary global optimization algorithms are based on
                   local measures of utility, rather than a probability measure
                   over location and value of the optimum. They thus attempt to
                   collect low function values, not to learn about the optimum.
                   The reason for the absence of probabilistic global
                   optimizers is that the corresponding inference problem is
                   intractable in several ways. This paper develops desiderata
                   for probabilistic optimization algorithms, then presents a
                   concrete algorithm which addresses each of the computational
                   intractabilities with a sequence of approximations and
                   explicitly adresses the decision problem of maximizing
                   information gain from each evaluation.",
  pages         = "1809--1837",
  month         =  "6~" # dec,
  year          =  2011,
  keywords      = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt;Mendeley
                   Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1112.1217"
}

@ARTICLE{Kuindersma2013-oq,
  title     = "Variable risk control via stochastic optimization",
  author    = "Kuindersma, Scott R and Grupen, Roderic A and Barto, Andrew G",
  abstract  = "We present new global and local policy search algorithms
               suitable for problems with policy-dependent cost variance (or
               risk), a property present in many robot control tasks. These
               algorithms exploit new techniques in non-parametric
               heteroscedastic regression to directly model the
               policy-dependent distribution of cost. For local search, the
               learned cost model can be used as a critic for performing
               risk-sensitive gradient descent. Alternatively,
               decision-theoretic criteria can be applied to globally select
               policies to balance exploration and exploitation in a principled
               way, or to perform greedy minimization with respect to various
               risk-sensitive criteria. This separation of learning and policy
               selection permits variable risk control, where risk-sensitivity
               can be flexibly adjusted and appropriate policies can be
               selected at runtime without relearning. We describe experiments
               in dynamic stabilization and manipulation with a mobile
               manipulator that demonstrate learning of flexible,
               risk-sensitive policies in...",
  journal   = "Int. J. Rob. Res.",
  publisher = "SAGE PublicationsSage UK: London, England",
  volume    =  32,
  number    =  7,
  pages     = "806--825",
  month     =  "1~" # jul,
  year      =  2013,
  keywords  = "bayesian optimization; dynamic mobile manipulation; policy
               search; risk-sensitive; robot
               learning;BayesOpt;Important;NotRead;TALAF;Mendeley Import (Jan
               17)/BayesOpt",
  language  = "en"
}

@ARTICLE{Brochu2010-tj,
  title         = "A Tutorial on Bayesian Optimization of Expensive Cost
                   Functions, with Application to Active User Modeling and
                   Hierarchical Reinforcement Learning",
  author        = "Brochu, Eric and Cora, Vlad M and de Freitas, Nando",
  abstract      = "We present a tutorial on Bayesian optimization, a method of
                   finding the maximum of expensive cost functions. Bayesian
                   optimization employs the Bayesian technique of setting a
                   prior over the objective function and combining it with
                   evidence to get a posterior function. This permits a
                   utility-based selection of the next observation to make on
                   the objective function, which must take into account both
                   exploration (sampling from areas of high uncertainty) and
                   exploitation (sampling areas likely to offer improvement
                   over the current best observation). We also present two
                   detailed extensions of Bayesian optimization, with
                   experiments---active user modelling with preferences, and
                   hierarchical reinforcement learning---and a discussion of
                   the pros and cons of Bayesian optimization based on our
                   experiences.",
  month         =  "12~" # dec,
  year          =  2010,
  keywords      = "BayesOpt;Important;TALAF;Mendeley Import (Jan 17)/BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1012.2599"
}

@ARTICLE{Bergstra2012-tg,
  title     = "Random Search for Hyper-parameter Optimization",
  author    = "Bergstra, James and Bengio, Yoshua",
  abstract  = "Grid search and manual search are the most widely used
               strategies for hyper-parameter optimiza- tion. This paper shows
               empirically and theoretically that randomly chosen trials are
               more efficient for hyper-parameter optimization than trials on a
               grid. Empirical evidence comes from a compar- ison with a large
               previous study that used grid search and manual search to
               configure neural net- works and deep belief networks. Compared
               with neural networks configured by a pure grid search, we find
               that random search over the same domain is able to find models
               that are as good or better within a small fraction of the
               computation time. Granting random search the same computational
               budget, random search finds better models by effectively
               searching a larger, less promising con- figuration space.
               Compared with deep belief networks configured by a thoughtful
               combination of manual search and grid search, purely random
               search over the same 32-dimensional configuration space found
               statistically equal performance on four of seven data sets, and
               superior performance on one of seven. A Gaussian process
               analysis of the function from hyper-parameters to validation set
               performance reveals that for most data sets only a few of the
               hyper-parameters really matter, but that different
               hyper-parameters are important on different data sets. This
               phenomenon makes grid search a poor choice for configuring
               algorithms for new data sets. Our analysis casts some light on
               why recent ``High Throughput''methods achieve surprising
               success---they appear to search through a large number of
               hyper-parameters because most hyper-parameters do not matter
               much. We anticipate that growing interest in large hierarchical
               models will place an increasing burden on techniques for
               hyper-parameter optimization; this work shows that randomsearch
               is a natural base- line against which to judge progress in the
               development of adaptive (sequential) hyper-parameter
               optimization algorithms.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  13,
  number    =  1,
  pages     = "281--305",
  month     =  feb,
  year      =  2012,
  keywords  = "deep learning; global optimization; model selection; neural
               networks; response surface; response surface
               modeling;BayesOpt;GPs;Important;OptimizingHyperparameters;TALAF;Mendeley
               Import (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters"
}

@MISC{noauthor_undated-uo,
  title        = "machine learning - Marginalization of {GP} regression
                  hyperparameters with Laplace approximation - Cross Validated",
  howpublished = "\url{http://stats.stackexchange.com/questions/173216/marginalization-of-gp-regression-hyperparameters-with-laplace-approximation}",
  keywords     = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;Mendeley Import
                  (Jan 17)/BayesOpt;Mendeley Import (Jan
                  17)/OptimizingHyperparameters"
}

@ARTICLE{Perrin2009-oo,
  title       = "Sex differences in the growth of white matter during
                 adolescence",
  author      = "Perrin, J S and Leonard, G and Perron, M and Pike, G B and
                 Pitiot, A and Richer, L and Veillette, S and Pausova, Z and
                 Paus, T",
  affiliation = "Brain and Body Centre, University of Nottingham, UK.",
  abstract    = "The purpose of this study was to examine sex differences in
                 the maturation of white matter during adolescence (12 to 18
                 years of age). We measured lobular volumes of white matter and
                 white-matter ``density'' throughout the brain using
                 T1-weighted images, and estimated the myelination index using
                 magnetisation-transfer ratio (MTR). In male adolescents, we
                 observed age-related increases in white-matter lobular volumes
                 accompanied by decreases in the lobular values of white-matter
                 MTR. White-matter density in the putative cortico-spinal tract
                 (pCST) decreased with age. In female adolescents, on the other
                 hand, we found only small age-related increase in white-matter
                 volumes and no age-related changes in white-matter MTR, with
                 the exception of the frontal lobe where MTR increased.
                 White-matter density in the pCST also increased with age.
                 These results suggest that sex-specific mechanisms may
                 underlie the growth of white matter during adolescence. We
                 speculate that these mechanisms involve primarily age-related
                 increases in axonal calibre in males and increased myelination
                 in females.",
  journal     = "Neuroimage",
  volume      =  45,
  number      =  4,
  pages       = "1055--1066",
  month       =  "1~" # may,
  year        =  2009,
  keywords    = "BayesOpt;NotRead;TALAF;Mendeley Import (Jan 17)/BayesOpt",
  language    = "en"
}

@ARTICLE{Boyle2007-oo,
  title     = "Gaussian processes for regression and optimisation",
  author    = "Boyle, Phillip",
  abstract  = "Gaussian processes have proved to be useful and powerful
               constructs for the purposes of regression. The classical method
               proceeds by parameterising a covariance function, and then
               infers the parameters given the training data. In this thesis,
               the classical approach is augmented by interpreting Gaussian
               processes as the outputs of linear filters excited by white
               noise. This enables a straightforward definition of dependent
               Gaussian processes as the outputs of a multiple output linear
               filter excited by multiple noise sources. We show how dependent
               Gaussian processes defined in this way can also be used for the
               purposes of system identification. Onewell known
               problemwithGaussian process regression is that the compu-
               tational complexity scales poorlywith the amount of training
               data.We review one approximate solution that alleviates this
               problem, namely reduced rank Gaussian processes. We then show
               how the reduced rank approximation can be applied to allow for
               the efficient computation of dependent Gaussian pro- cesses. We
               then examine the application ofGaussian processes to the
               solution of other machine learning problems. To do so, we review
               methods for the parameter- isation of full covariance matrices.
               Furthermore, we discuss how improve- ments can be made by
               marginalising over alternative models, and introduce methods to
               perform these computations efficiently. In particular, we intro-
               duce sequential annealed importance sampling as a method for
               calculating model evidence in an on-line fashion as newdata
               arrives. Gaussian process regression can also be applied to
               optimisation. An algo- rithm is described that uses model
               comparison between multiple models to find the optimumof a
               function while taking as fewsamples as possible. This algorithm
               shows impressive performance on the standard control problem of
               double pole balancing. Finally, we describe how Gaussian
               processes can be used to efficiently estimate gradients of noisy
               functions, and numerically estimate integrals.",
  publisher = "Victoria University of Wellington",
  pages     = "190",
  year      =  2007,
  keywords  = "BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;Mendeley
               Import (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters",
  language  = "en\_NZ"
}

@PHDTHESIS{Huang2005-ps,
  title    = "Experimental planning and sequential kriging optimization using
              variable fidelity data",
  author   = "Huang, Deng",
  abstract = "Engineers in many industries routinely need to improve the
              product or process designs using data from the field, lab
              experiments, and computer experiments. Historically, designers
              have performed a calibration exercise to ``fix'' the lab system
              or computer model and then used an analysis method or
              optimization procedure that ignores the fact that systematic
              differences between products in the field and other environments
              necessarily exist. A new line of research is not based on the
              assumption that calibration is perfect and seeks to develop
              experimental planning and optimization schemes using data form
              multiple experimental sources. We use the term ``fidelity'' to
              refer to the extent to which a surrogate experimental system can
              reproduce results of the system of interest. For experimental
              planning, we present perhaps the first optimal designs for
              variable fidelity experimentation, using an extension of the
              Expected Integrated Mean Squared Error (EIMSE) criterion, where
              the Generalized Least Squares (GLS) method was used to generate
              the predictions. Numerical tests are used to compare the method
              performance with alternatives and to investigate the robustness
              to incorporated assumptions. The method is applied to automotive
              engine valve heat treatment process design in which real world
              data were mixed with data from two types of computer simulations.
              Sequential Kriging Optimization (SKO) is a method developed in
              recent years for solving expensive black-box problems in areas
              such as large-scale circuit board design and manufacturing
              process improvement. We propose an extension of the SKO method,
              named Multiple Fidelity Sequential Kriging Optimization (MFSKO),
              where surrogate systems are exploited to reduce the total
              evaluation cost. As a pre-step to MFSKO, we extended SKO to
              address stochastic black-box systems. In the empirical studies
              using numerical test functions, SKO compared favorably with
              alternatives in terms of consistency in finding global optima and
              efficiency as measured by number of evaluations. Also, in the
              presence of noise, the new expected improvement function for
              infill sample selection appears to achieve the desired balance
              between the need for global and local searches. In the proposed
              MFSKO method, data on all experimental systems are integrated to
              build a kriging meta-model that provides a global prediction of
              the system of interest and a measure of prediction uncertainty.
              The location and fidelity level of the next evaluation are
              selected by maximizing an augmented expected improvement
              function, which is connected with the evaluation costs. The
              proposed method was applied to test functions from the literature
              and metal-forming process design problems via Finite Element
              simulations. The method manifests sensible search patterns,
              robust performance, and appreciable reduction in total evaluation
              cost as compared to the original method.",
  year     =  2005,
  keywords = "0546:Industrial engineering; 0796:Operations research; Applied
              sciences; Engine valves; Industrial engineering; Kriging;
              Multiple fidelity; Operations
              research;Acquisition/InfillFxns;BayesOpt;TALAF;Mendeley Import
              (Jan 17)/BayesOpt;Mendeley Import (Jan
              17)/BayesOpt/Acquisition/InfillFxns"
}

@MISC{noauthor_undated-gj,
  title        = "Experiences with bayesian hyperparameter optimization?",
  abstract     = "I was checking the paper, [Practical Bayesian Optimization of
                  Machine Learning](http://arxiv.org/pdf/1206.2944) and i was
                  wondering if anyone here...",
  howpublished = "\url{https://www.reddit.com/r/MachineLearning/comments/2m1cad/experiences_with_bayesian_hyperparameter/}",
  keywords     = "BayesOpt;NotRead;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@INPROCEEDINGS{Bonilla2007-jw,
  title     = "Multi-task Gaussian process prediction",
  booktitle = "Advances in neural information processing systems",
  author    = "Bonilla, Edwin V and Chai, Kian M and Williams, Christopher",
  abstract  = "In this paper we investigate multi-task learning in the context
               of Gaussian Processes (GP). We propose a model that learns a
               shared covariance function on input-dependent features and a
               ``free-form'' covariance matrix over tasks. This allows for good
               flexibility when modelling inter-task dependencies while
               avoiding the need for large amounts of data for training. We
               show that under the assumption of noise-free observations and a
               block design, predictions for a given task only depend on its
               target values and therefore a cancellation of inter-task
               transfer occurs. We evaluate the benefits of our model on two
               practical applications: a compiler performance prediction
               problem and an exam score prediction task. Additionally, we make
               use of GP approximations and properties of our model in order to
               provide scalability to large data sets.",
  volume    =  20,
  pages     = "153--160",
  year      =  2007,
  keywords  = "learning; statistics \&
               optimisation;BayesOpt;NotRead;TALAF;Mendeley Import (Jan
               17)/BayesOpt"
}

@TECHREPORT{Seeger2005-dc,
  title    = "Semiparametric latent factor models",
  author   = "Seeger, Matthias and Teh, Yee-Whye and Jordan, Michael",
  abstract = "(x n , y n",
  year     =  2005,
  keywords = "BayesOpt;NotRead;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@ARTICLE{Schliep2012-en,
  title         = "Multilevel latent Gaussian process model for mixed discrete
                   and continuous multivariate response data",
  author        = "Schliep, Erin M and Hoeting, Jennifer A",
  abstract      = "We propose a Bayesian model for mixed ordinal and continuous
                   multivariate data to evaluate a latent spatial Gaussian
                   process. Our proposed model can be used in many contexts
                   where mixed continuous and discrete multivariate responses
                   are observed in an effort to quantify an unobservable
                   continuous measurement. In our example, the latent, or
                   unobservable measurement is wetland condition. While
                   predicted values of the latent wetland condition variable
                   produced by the model at each location do not hold any
                   intrinsic value, the relative magnitudes of the wetland
                   condition values are of interest. In addition, by including
                   point-referenced covariates in the model, we are able to
                   make predictions at new locations for both the latent random
                   variable and the multivariate response. Lastly, the model
                   produces ranks of the multivariate responses in relation to
                   the unobserved latent random field. This is an important
                   result as it allows us to determine which response variables
                   are most closely correlated with the latent variable. Our
                   approach offers an alternative to traditional indices based
                   on best professional judgment that are frequently used in
                   ecology. We apply our model to assess wetland condition in
                   the North Platte and Rio Grande River Basins in Colorado.
                   The model facilitates a comparison of wetland condition at
                   multiple locations and ranks the importance of in-field
                   measurements.",
  month         =  "18~" # may,
  year          =  2012,
  keywords      = "BayesOpt;Important;TALAF;Mendeley Import (Jan 17)/BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "1205.4163"
}

@INPROCEEDINGS{Tamrat2004-be,
  title      = "The X-31: A {Post-Stall} Technology ({PST}) Fighter
                {Close-In-Combat} Results Assessment, And A Look At New {CIC}
                Performance Evaluation Metrics",
  booktitle  = "{AIAA} Atmospheric Flight Mechanics Conference and Exhibit",
  author     = "Tamrat, Befecadu",
  abstract   = "Key historical fighter performance metrics were reviewed, and
                their shortcomings and strengths were discussed. Flight time
                histories of a typical air-to-air combat engagement of the X-31
                against its F-18 adversary were assessed, and conclusions were
                drawn. Enough parameters were shown to help the reader make
                his/her own assessment as well. Given sufficient roll rate
                capability about flight path, the control of rate of descent
                was identified as key to X-31 superior close-in-combat
                performance. It was shown that open-loop turn reversal
                maneuvers, based on point-mass simulation, could be used to
                compare potential CIC out come of fighters. Based on this, the
                Dog-Fight-Metric (DFM) was formulated. DFM is a simple
                empirical metric that could be used to evaluate/ design CIC
                fighters. It emphasizes low wing-loading and high drag at
                moderate to high angles of attack. The use of this metric
                suggested that equal DFM could predict equal close- in-combat
                outcome. For example, the X-31 and the F-18 both flying at DFM
                25, had corresponding angles of attack of 43 and 35 degrees,
                respectively. It is concluded that dissimilar fighters could be
                made to have similar CIC outcome (equal DFM) when flown at
                different angles of attack. Conversely, two similar fighters
                could be configured as adversaries via angle of attack
                limiting.",
  publisher  = "American Institute of Aeronautics and Astronautics",
  pages      = "1--21",
  month      =  "16~" # aug,
  year       =  2004,
  address    = "Reston, Virigina",
  keywords   = "TALAF;Mendeley Import (Jan 17)/BayesOpt",
  language   = "en",
  conference = "AIAA Atmospheric Flight Mechanics Conference and Exhibit"
}

@PHDTHESIS{Pietilainen2010-ab,
  title    = "Approximations for Integration over the Hyperparameters in
              Gaussian Processes",
  author   = "Pietil{\"a}inen, Ville",
  abstract = "This thesis examines three numerical approximations for the
              analytically intractable in- tegral over the posterior
              distribution of the hyperparameters in Gaussian processes. The
              properties of the approximations are studied, and their
              performance is compared to each other and to a method using a
              point-estimate. Traditionally the integral over the posterior of
              the hyperparameters is computed using Markov chain Monte Carlo
              (MCMC) -methods. However, MCMC methods suffer from a heavy
              computational burden of Gaussian processes, because the
              complexity of Gaussian process models grows with the amount of
              the data used. An alternative approach has been to use only a
              point estimate for the hyperparameters instead of integrating
              over their posterior distribution. This is a computationally
              attractive approach, but it ignores the uncertainty related to
              the hyperparameters. The approximations discussed in this thesis
              attempt to take the uncertainty in the hyper- parameters into
              consideration better than does a point estimate method, and to be
              compu- tationally lighter than MCMC methods. The results
              demonstrate that the integration over the hyperparameters is
              beneficial in particular conditions. In addition, it is shown
              that a point estimate method yields equally accurate results with
              the integration methods in other situations. The amount of the
              data and the use of the models determine the need for the
              integration methods and the determining conditions are discussed
              in this work",
  year     =  2010,
  keywords = "BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;Mendeley
              Import (Jan 17)/BayesOpt;Mendeley Import (Jan
              17)/OptimizingHyperparameters"
}

@MISC{John_undated-qj,
  title    = "Making a singular matrix non-singular",
  author   = "{John}",
  abstract = "Someone asked me on Twitter Is there a trick to make an singular
              (non-invertible) matrix invertible? The only response I could
              think of in less than 140",
  keywords = "Acquisition/InfillFxns;BayesOpt;TALAF;Mendeley Import (Jan
              17)/BayesOpt;Mendeley Import (Jan
              17)/BayesOpt/Acquisition/InfillFxns"
}

@ARTICLE{Qi2004-ot,
  title     = "Predictive automatic relevance determination by expectation
               propagation",
  author    = "Qi, Yuan (alan) and Minka, Thomas P and Picard, Rosalind W and
               Ghahramani, Zoubin",
  abstract  = "In many real-world classification problems the input contains a
               large \textbackslashr\textbackslashnnumber of potentially
               irrelevant features. This paper proposes a new Bayesian
               framework for determining the relevance of input features. This
               \textbackslashr\textbackslashnapproach extends one of
               the\textbackslashr\textbackslashnmost successful Bayesian
               methods for feature selection and sparse
               \textbackslashr\textbackslashnlearning, known as Automatic
               Relevance Determination (ARD). ARD finds
               \textbackslashr\textbackslashnthe relevance of features by
               optimizing the model marginal likelihood,
               also\textbackslashr\textbackslashn known as the evidence. We
               show that this can lead to overfitting. To
               \textbackslashr\textbackslashnaddress this problem, we propose
               Predictive ARD based on estimating
               \textbackslashr\textbackslashnthe predictive performance of the
               classifier. While the actual leave-one-out
               \textbackslashr\textbackslashnpredictive performance is
               generally very costly to compute, the
               expectation\textbackslashr\textbackslashn propagation (EP)
               algorithm proposed by Minka provides an estimate of
               \textbackslashr\textbackslashnthis predictive performance as a
               side-effect of its iterations. We exploit
               this\textbackslashr\textbackslashn in our algorithm to do
               feature selection, and to select data points in a
               \textbackslashr\textbackslashnsparse Bayesian kernel classifier.
               Moreover, we provide two other
               \textbackslashr\textbackslashnimprovements to previous
               algorithms, by replacing Laplace's
               \textbackslashr\textbackslashnapproximation with the generally
               more accurate EP, and by incorporating
               \textbackslashr\textbackslashnthe fast optimization algorithm
               proposed by Faul and Tipping. Our
               \textbackslashr\textbackslashnexperiments show that our method
               based on the EP estimate of predictive
               \textbackslashr\textbackslashnperformance is more accurate on
               test data than relevance determination
               by\textbackslashr\textbackslashnoptimizing the evidence.",
  journal   = "Twenty-first international conference on Machine learning - ICML
               '04",
  publisher = "ACM",
  pages     = "85",
  series    = "ICML '04",
  year      =  2004,
  address   = "New York, NY, USA",
  keywords  = "BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;Mendeley
               Import (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters"
}

@INCOLLECTION{Swiler2014-ao,
  title     = "Surrogate Models for Mixed {Discrete-Continuous} Variables",
  booktitle = "Constraint Programming and Decision Making",
  author    = "Swiler, Laura P and Hough, Patricia D and Qian, Peter and Xu, Xu
               and Storlie, Curtis and Lee, Herbert",
  editor    = "Ceberio, Martine and Kreinovich, Vladik",
  abstract  = "Large-scale computational models have become common tools for
               analyzing complex man-made systems. However, when coupled with
               optimization or uncertainty quantification methods in order to
               conduct extensive model exploration and analysis, the
               computational expense quickly becomes intractable. Furthermore,
               these models may have both continuous and discrete parameters.
               One common approach to mitigating the computational expense is
               the use of response surface approximations. While well developed
               for models with continuous parameters, they are still new and
               largely untested for models with both continuous and discrete
               parameters. In this work, we describe and investigate the
               performance of three types of response surfaces developed for
               mixed-variable models: Adaptive Component Selection and
               Shrinkage Operator, Treed Gaussian Process, and Gaussian Process
               with Special Correlation Functions. We focus our efforts on test
               problems with a small number of parameters of interest, a
               characteristic of many physics-based engineering models. We
               present the results of our studies and offer some insights
               regarding the performance of each response surface approximation
               method.",
  publisher = "Springer International Publishing",
  pages     = "181--202",
  series    = "Studies in Computational Intelligence",
  year      =  2014,
  keywords  = "BayesOpt;NotRead;TALAF;Mendeley Import (Jan 17)/BayesOpt",
  language  = "en"
}

@ARTICLE{Hoffman2014-dk,
  title     = "Modular mechanisms for Bayesian optimization",
  author    = "Hoffman, M W and Shahriari, B",
  abstract  = "Abstract The design of methods for Bayesian optimization
               involves a great number of choices that are often implicit in
               the overall algorithm design. In this work we argue for a
               modular approach to Bayesian optimization and present a Python
               implementation, pybo, that allows us to easily vary these
               choices. In particular this includes selection of the
               acquisition function, kernel, and hyperpriors as well as
               less-discussed components such ...",
  journal   = "NIPS Workshop on Bayesian Optimization",
  publisher = "Citeseer",
  pages     = "1--5",
  year      =  2014,
  keywords  = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@INPROCEEDINGS{Laumanns2002-na,
  title      = "Bayesian Optimization Algorithms for Multi-objective
                Optimization",
  booktitle  = "Parallel Problem Solving from Nature --- {PPSN} {VII}",
  author     = "Laumanns, Marco and Ocenasek, Jiri",
  editor     = "Guerv{\'o}s, Juan Juli{\'a}n Merelo and Adamidis, Panagiotis
                and Beyer, Hans-Georg and Schwefel, Hans-Paul and
                Fern{\'a}ndez-Villaca{\~n}as, Jos{\'e}-Luis",
  abstract   = "In recent years, several researchers have concentrated on using
                probabilistic models in evolutionary algorithms. These
                Estimation Distribution Algorithms (EDA) incorporate methods
                for automated learning of correlations between variables of the
                encoded solutions. The process of sampling new individuals from
                a probabilistic model respects these mutual dependencies such
                that disruption of important building blocks is avoided, in
                comparison with classical recombination operators. The goal of
                this paper is to investigate the usefulness of this concept in
                multi-objective optimization, where the aim is to approximate
                the set of Pareto-optimal solutions. We integrate the model
                building and sampling techniques of a special EDA called
                Bayesian Optimization Algorithm, based on binary decision
                trees, into an evolutionary multi-objective optimizer using a
                special selection scheme. The behavior of the resulting
                Bayesian Multi-objective Optimization Algorithm (BMOA) is
                empirically investigated on the multi-objective knapsack
                problem.",
  publisher  = "Springer Berlin Heidelberg",
  pages      = "298--307",
  series     = "Lecture Notes in Computer Science",
  month      =  "7~" # sep,
  year       =  2002,
  keywords   = "BayesOpt;NotRead;TALAF;Mendeley Import (Jan 17)/BayesOpt",
  language   = "en",
  conference = "International Conference on Parallel Problem Solving from
                Nature"
}

@INPROCEEDINGS{Macdonald2009-yw,
  title     = "Comparison of sampling techniques on the performance of
               {Monte-Carlo} based sensitivity analysis",
  booktitle = "Eleventh International {IBPSA} Conference",
  author    = "Macdonald, Iain A",
  abstract  = "Sensitivity analysis is a key part of a comprehensive energy
               simulation study. Monte-Carlo techniques have been successfully
               applied to many simulation tools. Several sampling techniques
               have been proposed in the literature; however to date there has
               been no comparison of their performance for typical building
               simulation applications. This paper examines the performance of
               simple random, stratified and Latin Hypercube sampling when
               applied to a typical building simulation problem. An integrated
               natural ventilation problem was selected as it has an
               inexpensive calculation time thus allowing multiple sensitivity
               analyses to be undertaken, while being realistic as wind and
               temperature effects are both modeled. The research shows that
               compared to simple random sampling: LHS and stratified sampling
               produce results that are not significantly different (at a 5\%
               level) with increased robustness (less variance in the mean
               prediction). However, it should not be inferred from this that
               fewer simulation runs are required for LHS and stratified
               sampling. Given the results presented here and in previous work
               it would indicate that for practical purposes Monte-Carlo
               uncertainty analysis in typical building simulation applications
               should use about 100 runs and simple random sampling.
               INTRODUCTION The field of sensitivity analysis is becoming",
  pages     = "992--999",
  year      =  2009,
  keywords  = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@MISC{Rasmussen2006-rs,
  title    = "{GPML} Toolbox",
  author   = "Rasmussen, Carl Edward and Williams, Christopher K I and {ebrary
              Inc.}",
  abstract = "The GPML toolbox provides a wide range of functionality for
              Gaussian process (GP) inference and prediction. GPs are specified
              by mean and covariance functions; we offer a library of simple
              mean and covariance functions and mechanisms to compose more
              complex ones. Several likelihood functions are supported
              including Gaussian and heavy-tailed for regression as well as
              others suitable for classification. Finally, a range of inference
              methods is provided, including exact and variational inference,
              Expectation Propagation, and Laplace's method dealing with
              non-Gaussian likelihoods and FITC for dealing with large
              regression tasks.",
  journal  = "Adaptive computation and machine learning",
  volume   =  11,
  pages    = "1 online resource xviii, 248 p.",
  month    =  dec,
  year     =  2006,
  keywords = "Gaussian processes Data processing.; Machine learning
              Mathematical models.;BayesOpt;Important;TALAF;Folder -
              NIPS2015;GPs;Machine learning;Variatinal
              Inference;Textbook;Mendeley Import (Jan 17)/BayesOpt;Mendeley
              Import (Jan 17);Mendeley Import (Jan 17)/TextBooks;Mendeley
              Import (Jan 17)/WeeklyReading;Mendeley Import (Jan 17)/GPs"
}

@ARTICLE{Garg2010-xm,
  title    = "Occam's razor",
  author   = "Garg, A",
  journal  = "A.Word.A.Day",
  pages    = "294--300",
  year     =  2010,
  keywords = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@PHDTHESIS{Gelbart2015-lg,
  title    = "Constrained Bayesian Optimization and Applications",
  author   = "Gelbart, Michael Adam",
  abstract = "Bayesian optimization is an approach for globally optimizing
              black-box functions that are expensive to evaluate, non-convex,
              and possibly noisy. Recently, Bayesian optimization has been used
              with great effectiveness for applications like tuning the
              hyperparameters of machine learning algorithms and automatic A/B
              testing for websites. This thesis considers Bayesian optimization
              in the presence of black-box constraints. Prior work on
              constrained Bayesian optimization consists of a variety of
              methods that can be used with some efficacy in specific contexts.
              Here, by forming a connection with multi-task Bayesian
              optimization, we formulate a more general class of constrained
              Bayesian optimization problems that we call Bayesian optimization
              with decoupled constraints. In this general framework, the
              objective and constraint functions are divided into tasks that
              can be evaluated independently of each other, and resources with
              which these tasks can be performed. We then present two methods
              for solving problems in this general class. The first method, an
              extension to a constrained variant of expected improvement, is
              fast and straightforward to implement but performs poorly in some
              circumstances and is not sufficiently flexible to address all
              varieties of decoupled problems. The second method, Predictive
              Entropy Search with Constraints (PESC), is highly effective and
              sufficiently flexible to address all problems in the general
              class of decoupled problems without any ad hoc modifications. The
              two weaknesses of PESC are its implementation difficulty and slow
              execution time. We address these issues by, respectively,
              providing a publicly available implementation within the popular
              Bayesian optimization software Spearmint, and developing an
              extension to PESC that achieves greater speed without significant
              performance losses. We demonstrate the effectiveness of these
              methods on real-world machine learning meta-optimization
              problems.",
  year     =  2015,
  keywords = "Acquisition/InfillFxns;BayesOpt;TALAF;Mendeley Import (Jan
              17)/BayesOpt;Mendeley Import (Jan
              17)/BayesOpt/Acquisition/InfillFxns;Mendeley Import (Jan 17)",
  language = "en"
}

@INPROCEEDINGS{Hutter2011-yp,
  title      = "Sequential {Model-Based} Optimization for General Algorithm
                Configuration",
  booktitle  = "Learning and Intelligent Optimization",
  author     = "Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin",
  editor     = "Coello Coello, Carlos A",
  abstract   = "State-of-the-art algorithms for hard computational problems
                often expose many parameters that can be modified to improve
                empirical performance. However, manually exploring the
                resulting combinatorial space of parameter settings is tedious
                and tends to lead to unsatisfactory outcomes. Recently,
                automated approaches for solving this algorithm configuration
                problem have led to substantial improvements in the state of
                the art for solving various problems. One promising approach
                constructs explicit regression models to describe the
                dependence of target algorithm performance on parameter
                settings; however, this approach has so far been limited to the
                optimization of few numerical algorithm parameters on single
                instances. In this paper, we extend this paradigm for the first
                time to general algorithm configuration problems, allowing many
                categorical parameters and optimization for sets of instances.
                We experimentally validate our new algorithm configuration
                procedure by optimizing a local search and a tree search solver
                for the propositional satisfiability problem (SAT), as well as
                the commercial mixed integer programming (MIP) solver CPLEX. In
                these experiments, our procedure yielded state-of-the-art
                performance, and in many cases outperformed the previous best
                configuration approach.",
  publisher  = "Springer Berlin Heidelberg",
  volume     = "6683 LNCS",
  pages      = "507--523",
  series     = "Lecture Notes in Computer Science",
  month      =  "17~" # jan,
  year       =  2011,
  keywords   = "BayesOpt;Important;NotRead;TALAF;Mendeley Import (Jan
                17)/BayesOpt;Mendeley Import (Jan 17)",
  language   = "en",
  conference = "International Conference on Learning and Intelligent
                Optimization"
}

@ARTICLE{Shahriari2016-xh,
  title    = "Taking the Human Out of the Loop: A Review of Bayesian
              Optimization",
  author   = "Shahriari, B and Swersky, K and Wang, Z and Adams, R P and de
              Freitas, N",
  abstract = "Big Data applications are typically associated with systems
              involving large numbers of users, massive complex software
              systems, and large-scale heterogeneous computing and storage
              architectures. The construction of such systems involves many
              distributed design choices. The end products (e.g.,
              recommendation systems, medical analysis tools, real-time game
              engines, speech recognizers) thus involve many tunable
              configuration parameters. These parameters are often specified
              and hard-coded into the software by various developers or teams.
              If optimized jointly, these parameters can result in significant
              improvements. Bayesian optimization is a powerful tool for the
              joint optimization of design choices that is gaining great
              popularity in recent years. It promises greater automation so as
              to increase both product quality and human productivity. This
              review paper introduces Bayesian optimization, highlights some of
              its methodological aspects, and showcases a wide range of
              applications.",
  journal  = "Proc. IEEE",
  volume   =  104,
  number   =  1,
  pages    = "148--175",
  month    =  jan,
  year     =  2016,
  keywords = "Bayes methods;Big Data;optimisation;storage allocation;Bayesian
              optimization;Big data application;human productivity;large-scale
              heterogeneous computing;massive complex software system;product
              quality;storage architecture;Bayes methods;Big data;Decision
              making;Design of experiments;Genomes;Linear
              programming;Optimization;Statistical analysis;Decision
              making;decision making;design of experiments;genomic
              medicine;optimization;response surface methodology;statistical
              learning;Acquisition/InfillFxns;BayesOpt;Important;NotRead;TALAF;Mendeley
              Import (Jan 17)/BayesOpt;Mendeley Import (Jan
              17)/BayesOpt/Acquisition/InfillFxns"
}

@MISC{Cook2010-zq,
  title        = "Don't invert that matrix",
  booktitle    = "Johndcook.com",
  author       = "Cook, John C",
  abstract     = "There is hardly ever a good reason to invert a matrix. What
                  do you do if you need to solve Ax = b where A is an n x n
                  matrix? Isn't the solution A-1 b? Yes,",
  year         =  2010,
  howpublished = "\url{http://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/}",
  keywords     = "Acquisition/InfillFxns;BayesOpt;TALAF;Mendeley Import (Jan
                  17)/BayesOpt;Mendeley Import (Jan
                  17)/BayesOpt/Acquisition/InfillFxns"
}

@PHDTHESIS{Lizotte2008-ou,
  title     = "Practical Bayesian Optimization",
  author    = "Lizotte, Daniel James",
  abstract  = "Global optimization of non-convex functions over real vector
               spaces is a problem of widespread theoretical and practical
               interest............ Examples: AIBO Gait parameters Stereo
               Camera Parameters",
  publisher = "University of Alberta",
  year      =  2008,
  address   = "Edmonton, Alta., Canada",
  keywords  = "Bayes; Kernels; Learning; Optimization; Parameter optimization;
               local
               search;Acquisition/InfillFxns;BayesOpt;Important;TALAF;Mendeley
               Import (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/BayesOpt/Acquisition/InfillFxns"
}

@INCOLLECTION{Ginsbourger2010-wo,
  title     = "Kriging Is {Well-Suited} to Parallelize Optimization",
  booktitle = "Computational Intelligence in Expensive Optimization Problems",
  author    = "Ginsbourger, David and Le Riche, Rodolphe and Carraro, Laurent",
  editor    = "Tenne, Yoel and Goh, Chi-Keong",
  abstract  = "The optimization of expensive-to-evaluate functions generally
               relies on metamodel-based exploration strategies. Many
               deterministic global optimization algorithms used in the field
               of computer experiments are based on Kriging (Gaussian process
               regression). Starting with a spatial predictor including a
               measure of uncertainty, they proceed by iteratively choosing the
               point maximizing a criterion which is a compromise between
               predicted performance and uncertainty. Distributing the
               evaluation of such numerically expensive objective functions on
               many processors is an appealing idea. Here we investigate a
               multi-points optimization criterion, the multipoints expected
               improvement (\textbackslash(q-\{\textbackslashmathbb
               E\}I\textbackslash)), aimed at choosing several points at the
               same time. An analytical expression of the
               \textbackslash(q-\{\textbackslashmathbb E\}I\textbackslash) is
               given when q = 2, and a consistent statistical estimate is given
               for the general case. We then propose two classes of heuristic
               strategies meant to approximately optimize the
               \textbackslash(q-\{\textbackslashmathbb E\}I\textbackslash), and
               apply them to the classical Branin-Hoo test-case function. It is
               finally demonstrated within the covered example that the latter
               strategies perform as good as the best Latin Hypercubes and
               Uniform Designs ever found by simulation (2000 designs drawn at
               random for every q $\in$ [1,10]).",
  publisher = "Springer Berlin Heidelberg",
  pages     = "131--162",
  series    = "Adaptation Learning and Optimization",
  year      =  2010,
  keywords  = "Appl.Mathematics/Computational Methods of Enginee; Applications
               of Mathematics; Artificial Intelligence (incl.
               Robotics);Acquisition/InfillFxns;BayesOpt;TALAF;qEI;Mendeley
               Import (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/BayesOpt/Acquisition/InfillFxns;Mendeley Import (Jan 17)",
  language  = "en"
}

@ARTICLE{Goldberg2000-zp,
  title    = "Bayesian Optimization Algorithm, Population Sizing, and Time to
              Convergence Martin Pelikan, David. E. Goldberg, and Erick
              {Cantu-Paz}",
  author   = "Goldberg, David E and Pelikan, Martin",
  journal  = "Population",
  number   =  2000001,
  pages    = "275--282",
  year     =  2000,
  keywords = "BayesOpt; NotRead; TALAF;BayesOpt;NotRead;TALAF;Mendeley Import
              (Jan 17);Mendeley Import (Jan 17)/BayesOpt"
}

@ARTICLE{Hensman2013-ai,
  title         = "Gaussian Processes for Big Data",
  author        = "Hensman, James and Fusi, Nicolo and Lawrence, Neil D",
  abstract      = "We introduce stochastic variational inference for Gaussian
                   process models. This enables the application of Gaussian
                   process (GP) models to data sets containing millions of data
                   points. We show how GPs can be vari- ationally decomposed to
                   depend on a set of globally relevant inducing variables
                   which factorize the model in the necessary manner to perform
                   variational inference. Our ap- proach is readily extended to
                   models with non-Gaussian likelihoods and latent variable
                   models based around Gaussian processes. We demonstrate the
                   approach on a simple toy problem and two real world data
                   sets.",
  pages         = "282--290",
  month         =  "26~" # sep,
  year          =  2013,
  keywords      = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1309.6835"
}

@ARTICLE{Brochu2010-bp,
  title         = "Portfolio Allocation for Bayesian Optimization",
  author        = "Brochu, Eric and Hoffman, Matthew W and de Freitas, Nando",
  abstract      = "Bayesian optimization with Gaussian processes has become an
                   increasingly popular tool in the machine learning community.
                   It is efficient and can be used when very little is known
                   about the objective function, making it popular in expensive
                   black-box optimization scenarios. It uses Bayesian methods
                   to sample the objective efficiently using an acquisition
                   function which incorporates the model's estimate of the
                   objective and the uncertainty at any given point. However,
                   there are several different parameterized acquisition
                   functions in the literature, and it is often unclear which
                   one to use. Instead of using a single acquisition function,
                   we adopt a portfolio of acquisition functions governed by an
                   online multi-armed bandit strategy. We propose several
                   portfolio strategies, the best of which we call GP-Hedge,
                   and show that this method outperforms the best individual
                   acquisition function. We also provide a theoretical bound on
                   the algorithm's performance.",
  pages         = "327--336",
  month         =  "28~" # sep,
  year          =  2010,
  keywords      = "BayesOpt; TALAF;BayesOpt;TALAF;Mendeley Import (Jan
                   17);Mendeley Import (Jan 17)/BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1009.5419"
}

@INPROCEEDINGS{Ponweiser2008-zf,
  title     = "Clustered multiple generalized expected improvement: A novel
               infill sampling criterion for surrogate models",
  booktitle = "2008 {IEEE} Congress on Evolutionary Computation ({IEEE} World
               Congress on Computational Intelligence)",
  author    = "Ponweiser, W and Wagner, T and Vincze, M",
  abstract  = "Surrogate model-based optimization is a well-known technique for
               optimizing expensive black-box functions. By applying this
               function approximation, the number of real problem evaluations
               can be reduced because the optimization is performed on the
               model. In this case two contradictory targets have to be
               achieved: increasing global model accuracy and exploiting
               potentially optimal areas. The key to these targets is the
               criterion for selecting the next point, which is then evaluated
               on the expensive black-box function - the dasiainfill sampling
               criterionpsila. Therefore, a novel approach - the dasiaClustered
               Multiple Generalized Expected Improvementpsila (CMGEI) - is
               introduced and motivated by an empirical study. Furthermore,
               experiments benchmarking its performance compared to the state
               of the art are presented.",
  pages     = "3515--3522",
  month     =  jun,
  year      =  2008,
  keywords  = "function approximation;optimisation;clustered multiple
               generalized expected improvement;expensive black-box
               functions;function approximation;infill sampling
               criterion;surrogate model-based optimization;Accuracy;Distance
               measurement;Function approximation;Mathematical
               model;Optimization;Simulated
               annealing;Uncertainty;Acquisition/InfillFxns;BayesOpt;Mathematical
               model;neural\_networks;Optimization
               methods;TALAF;optimisation;Mendeley Import (Jan
               17)/BayesOpt;Mendeley Import (Jan
               17)/BayesOpt/Acquisition/InfillFxns"
}

@ARTICLE{Jones1993-ua,
  title     = "Lipschitzian optimization without the Lipschitz constant",
  author    = "Jones, D R and Perttunen, C D and Stuckman, B E",
  abstract  = "We present a new algorithm for finding the global minimum of a
               multivariate function subject to simple bounds. The algorithm is
               a modification of the standard Lipschitzian approach that
               eliminates the need to specify a Lipschitz constant. This is
               done by carrying out simultaneous searches using all possible
               constants from zero to infinity. On nine standard test
               functions, the new algorithm converges in fewer function
               evaluations than most competing methods.The motivation for the
               new algorithm stems from a different way of looking at the
               Lipschitz constant. In particular, the Lipschitz constant is
               viewed as a weighting parameter that indicates how much emphasis
               to place on global versus local search. In standard Lipschitzian
               methods, this constant is usually large because it must equal or
               exceed the maximum rate of change of the objective function. As
               a result, these methods place a high emphasis on global search
               and exhibit slow convergence. In contrast, the new algorithm
               carries out simultaneous searches using all possible constants,
               and therefore operates at both the global and local level. Once
               the global part of the algorithm finds the basin of convergence
               of the optimum, the local part of the algorithm quickly and
               automatically exploits it. This accounts for the fast
               convergence of the new algorithm on the test functions.",
  journal   = "J. Optim. Theory Appl.",
  publisher = "Kluwer Academic Publishers-Plenum Publishers",
  volume    =  79,
  number    =  1,
  pages     = "157--181",
  month     =  "1~" # oct,
  year      =  1993,
  keywords  = "1staff research scientist; covering; development center; general
               motors research and; global optimization; lipschitzian
               optimization; space; space partitioning;
               warren;BayesOpt;DIRECT;Important;TALAF;Mendeley Import (Jan
               17)/BayesOpt",
  language  = "en"
}

@ARTICLE{Alvarez2011-wg,
  title     = "Computationally Efficient Convolved Multiple Output Gaussian
               Processes",
  author    = "{\'A}lvarez, Mauricio A and Lawrence, Neil D",
  abstract  = "Recently there has been an increasing interest in regression
               methods that deal with multiple outputs. This has been motivated
               partly by frameworks like multitask learning, multisensor
               networks or structured output data. From a Gaussian processes
               perspective, the problem reduces to specifying an appropriate
               covariance function that, whilst being positive semi-definite,
               captures the dependencies between all the data points and across
               all the outputs. One approach to account for non-trivial
               correlations between outputs employs convolution processes.
               Under a latent function interpretation of the convolution
               transform we establish dependencies between output variables.
               The main drawbacks of this approach are the associated
               computational and storage demands. In this paper we address
               these issues. We present different efficient approximations for
               dependent output Gaussian processes constructed through the
               convolution formalism. We exploit the conditional independencies
               present naturally in the model. This leads to a form of the
               covariance similar in spirit to the so called PITC and FITC
               approximations for a single output. We show experimental results
               with synthetic and real data, in particular, we show results in
               school exams score prediction, pollution prediction and gene
               expression data. \copyright{} 2011 Mauricio A. {\'A}lvarez and
               Neil D. Lawrence.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  12,
  pages     = "1459--1500",
  month     =  jul,
  year      =  2011,
  keywords  = "convolution processes; efficient approximations; gaussian
               processes; ing; multitask learn-; multivariate processes;
               structured outputs;BayesOpt;TALAF;Mendeley Import (Jan
               17)/BayesOpt"
}

@MISC{noauthor_undated-cw,
  title        = "machine learning - How can you detect if a Gaussian process
                  is over-fitting? - Cross Validated",
  howpublished = "\url{http://stats.stackexchange.com/questions/47174/how-can-you-detect-if-a-gaussian-process-is-over-fitting}",
  keywords     = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;Mendeley Import
                  (Jan 17)/BayesOpt;Mendeley Import (Jan
                  17)/OptimizingHyperparameters"
}

@ARTICLE{Wang2014-sv,
  title         = "Theoretical Analysis of Bayesian Optimisation with Unknown
                   Gaussian Process {Hyper-Parameters}",
  author        = "Wang, Ziyu and de Freitas, Nando",
  abstract      = "Bayesian optimisation has gained great popularity as a tool
                   for optimising the parameters of machine learning algorithms
                   and models. Somewhat ironically, setting up the
                   hyper-parameters of Bayesian optimisation methods is
                   notoriously hard. While reasonable practical solutions have
                   been advanced, they can often fail to find the best optima.
                   Surprisingly, there is little theoretical analysis of this
                   crucial problem in the literature. To address this, we
                   derive a cumulative regret bound for Bayesian optimisation
                   with Gaussian processes and unknown kernel hyper-parameters
                   in the stochastic setting. The bound, which applies to the
                   expected improvement acquisition function and sub-Gaussian
                   observation noise, provides us with guidelines on how to
                   design hyper-parameter estimation methods. A simple
                   simulation demonstrates the importance of following these
                   guidelines.",
  pages         = "1--16",
  month         =  "30~" # jun,
  year          =  2014,
  keywords      = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1406.7758"
}

@ARTICLE{Mahendran2011-tx,
  title         = "Bayesian Optimization for Adaptive {MCMC}",
  author        = "Mahendran, Nimalan and Wang, Ziyu and Hamze, Firas and de
                   Freitas, Nando",
  abstract      = "This paper proposes a new randomized strategy for adaptive
                   MCMC using Bayesian optimization. This approach applies to
                   non-differentiable objective functions and trades off
                   exploration and exploitation to reduce the number of
                   potentially costly objective function evaluations. We
                   demonstrate the strategy in the complex setting of sampling
                   from constrained, discrete and densely connected
                   probabilistic graphical models where, for each variation of
                   the problem, one needs to adjust the parameters of the
                   proposal mechanism automatically to ensure efficient mixing
                   of the Markov chains.",
  pages         = "152",
  month         =  "29~" # oct,
  year          =  2011,
  keywords      = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt;Mendeley
                   Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "stat.CO",
  eprint        = "1110.6497"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Bergstra2013-je,
  title     = "Making a Science of Model Search: Hyperparameter Optimization in
               Hundreds of Dimensions for Vision Architectures",
  booktitle = "Proceedings of The 30th International Conference on Machine
               Learning",
  author    = "Bergstra, James and Yamins, Daniel and Cox, David",
  abstract  = "Many computer vision algorithms depend on configuration settings
               that are typically hand-tuned in the course of evaluating the
               algorithm for a particular data set. While such parameter tuning
               is often presented as being incidental to the algorithm,
               correctly setting these parameter choices is frequently critical
               to realizing a method’s full potential. Compounding matters,
               these parameters often must be re-tuned when the algorithm is
               applied to a new problem domain, and the tuning process itself
               often depends on personal experience and intuition in ways that
               are hard to quantify or describe. Since the performance of a
               given technique depends on both the fundamental quality of the
               algorithm and the details of its tuning, it is sometimes
               difficult to know whether a given technique is genuinely better,
               or simply better tuned. In this work, we propose a meta-modeling
               approach to support automated hyperparameter optimization, with
               the goal of providing practical tools that replace hand-tuning
               with a reproducible and unbiased optimization process. Our
               approach is to expose the underlying expression graph of how a
               performance metric (e.g. classification accuracy on validation
               examples) is computed from hyperparameters that govern not only
               how individual processing steps are applied, but even which
               processing steps are included. A hyperparameter optimization
               algorithm transforms this graph into a program for optimizing
               that performance metric. Our approach yields state of the art
               results on three disparate computer vision problems: a
               face-matching verification task (LFW), a face identification
               task (PubFig83) and an object recognition task (CIFAR-10), using
               a single broad class of feed-forward vision architectures.",
  pages     = "115--123",
  year      =  2013,
  keywords  = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;Mendeley Import
               (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters"
}

@INCOLLECTION{Bergstra2011-mp,
  title     = "Algorithms for {Hyper-Parameter} Optimization",
  booktitle = "Advances in Neural Information Processing Systems 24",
  author    = "Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and
               K{\'e}gl, Bal{\'a}zs",
  editor    = "Shawe-Taylor, J and Zemel, R S and Bartlett, P L and Pereira, F
               and Weinberger, K Q",
  abstract  = "Several recent advances to the state of the art in image
               classification benchmarks have come from better configurations
               of existing techniques rather than novel ap- proaches to feature
               learning. Traditionally, hyper-parameter optimization has been
               the job of humans because they can be very efficient in regimes
               where only a few trials are possible. Presently, computer
               clusters and GPU processors make it pos- sible to run more
               trials and we show that algorithmic approaches can find better
               results. We present hyper-parameter optimization results on
               tasks of training neu- ral networks and deep belief networks
               (DBNs). We optimize hyper-parameters using random search and two
               new greedy sequential methods based on the ex- pected
               improvement criterion. Random search has been shown to be
               sufficiently efficient for learning neural networks for several
               datasets, but we show it is unreli- able for training DBNs. The
               sequential algorithms are applied to the most difficult DBN
               learning problems from [1] and find significantly better results
               than the best previously reported. This work contributes novel
               techniques for making response surface models P(y|x) in which
               many elements of hyper-parameter assignment (x) are known to be
               irrelevant given particular values of other elements.",
  publisher = "Curran Associates, Inc.",
  pages     = "2546--2554",
  year      =  2011,
  keywords  = "BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;Mendeley
               Import (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters"
}

@ARTICLE{Gopalan2013-vt,
  title         = "Thompson Sampling for Complex Bandit Problems",
  author        = "Gopalan, Aditya and Mannor, Shie and Mansour, Yishay",
  editor        = "Sammut, Claude and Webb, Geoffrey I",
  abstract      = "We consider stochastic multi-armed bandit problems with
                   complex actions over a set of basic arms, where the decision
                   maker plays a complex action rather than a basic arm in each
                   round. The reward of the complex action is some function of
                   the basic arms' rewards, and the feedback observed may not
                   necessarily be the reward per-arm. For instance, when the
                   complex actions are subsets of the arms, we may only observe
                   the maximum reward over the chosen subset. Thus, feedback
                   across complex actions may be coupled due to the nature of
                   the reward function. We prove a frequentist regret bound for
                   Thompson sampling in a very general setting involving
                   parameter, action and observation spaces and a likelihood
                   function over them. The bound holds for discretely-supported
                   priors over the parameter space and without additional
                   structural properties such as closed-form posteriors,
                   conjugate prior structure or independence across arms. The
                   regret bound scales logarithmically with time but, more
                   importantly, with an improved constant that non-trivially
                   captures the coupling across complex actions due to the
                   structure of the rewards. As applications, we derive
                   improved regret bounds for classes of complex bandit
                   problems involving selecting subsets of arms, including the
                   first nontrivial regret bounds for nonlinear MAX reward
                   feedback from subsets.",
  pages         = "1--9",
  month         =  "3~" # nov,
  year          =  2013,
  keywords      = "bandit problem;Acquisition/InfillFxns;BayesOpt;Machine
                   learning;NotRead;TALAF;Mendeley Import (Jan
                   17)/BayesOpt;Mendeley Import (Jan
                   17)/BayesOpt/Acquisition/InfillFxns",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1311.0466"
}

@ARTICLE{Qian2008-ip,
  title    = "Gaussian Process Models for Computer Experiments With Qualitative
              and Quantitative Factors",
  author   = "Qian, Peter Z G and Wu, Huaiqing and Wu, C F Jeff",
  abstract = "Modeling experiments with qualitative and quantitative factors is
              an important issue in computer modeling. We propose a framework
              for building Gaussian process models that incorporate both types
              of factors. The key to the development of these new models is an
              approach for constructing correlation functions with qualitative
              and quantitative factors. An iterative estimation procedure is
              developed for the proposed models. Modern optimization techniques
              are used in the estimation to ensure the validity of the
              constructed correlation functions. The proposed method is
              illustrated with an example involving a known function and a real
              example for modeling the thermal distribution of a data center.",
  journal  = "Technometrics",
  volume   =  50,
  number   =  3,
  pages    = "383--396",
  year     =  2008,
  keywords = "cokriging; design of experiments; kriging; multivariate gaussian
              processes; semi-definite programming;BayesOpt;TALAF;Mendeley
              Import (Jan 17)/BayesOpt"
}

@MISC{Jones1998-tz,
  title    = "10.1023/A:1008306431147",
  author   = "Jones, Donald R and Schonlau, Matthias and Welch, William J",
  abstract = "Inmany engineering optimization problems, the number of function
              evaluations is severely limited by time or cost. These problems
              pose a special challenge to the field of global optimization,
              since existing methods often require more function evaluations
              than can be comfortably afforded. One way to address this
              challenge is to fit response surfaces to data collected by
              evaluating the objective and constraint functions at a few
              points. These surfaces can then be used for visualization,
              tradeoff analysis, and optimization. In this paper, we introduce
              the reader to a response surface methodology that is especially
              good at modeling the nonlinear, multimodal functions that often
              occur in engineering. We then show how these approximating
              functions can be used to construct an efficient global
              optimization algorithm with a credible stopping rule. The key to
              using response surfaces for global optimization lies in balancing
              the need to exploit the approximating surface (by sampling where
              it is minimized) with the need to improve the approximation (by
              sampling where prediction error may be high). Striking this
              balance requires solving certain auxiliary problems which have
              previously been considered intractable, but we show how these
              computational obstacles can be overcome.",
  journal  = "Journal of Global Optimization",
  volume   =  13,
  number   =  4,
  pages    = "455--492",
  year     =  1998,
  keywords = "bayesian global optimization; kriging; process; random function;
              response surface; stochastic;
              visualization;Acquisition/InfillFxns;BayesOpt;TALAF;Mendeley
              Import (Jan 17)/BayesOpt;Mendeley Import (Jan
              17)/BayesOpt/Acquisition/InfillFxns"
}

@INPROCEEDINGS{Contal2013-ig,
  title      = "Parallel Gaussian Process Optimization with Upper Confidence
                Bound and Pure Exploration",
  booktitle  = "Machine Learning and Knowledge Discovery in Databases",
  author     = "Contal, Emile and Buffoni, David and Robicquet, Alexandre and
                Vayatis, Nicolas",
  editor     = "Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried
                and {\v Z}elezn{\'y}, Filip",
  abstract   = "In this paper, we consider the challenge of maximizing an
                unknown function f for which evaluations are noisy and are
                acquired with high cost. An iterative procedure uses the
                previous measures to actively select the next estimation of f
                which is predicted to be the most useful. We focus on the case
                where the function can be evaluated in parallel with batches of
                fixed size and analyze the benefit compared to the purely
                sequential procedure in terms of cumulative regret. We
                introduce the Gaussian Process Upper Confidence Bound and Pure
                Exploration algorithm (GP-UCB-PE) which combines the UCB
                strategy and Pure Exploration in the same batch of evaluations
                along the parallel iterations. We prove theoretical upper
                bounds on the regret with batches of size K for this procedure
                which show the improvement of the order of
                \textbackslash(\textbackslashsqrt\{K\}\textbackslash) for fixed
                iteration cost over purely sequential versions. Moreover, the
                multiplicative constants involved have the property of being
                dimension-free. We also confirm empirically the efficiency of
                GP-UCB-PE on real and synthetic problems compared to
                state-of-the-art competitors.",
  publisher  = "Springer Berlin Heidelberg",
  volume     = "8188 LNAI",
  pages      = "225--240",
  series     = "Lecture Notes in Computer Science",
  month      =  "23~" # sep,
  year       =  2013,
  keywords   = "BayesOpt;TALAF;batch selection;Mendeley Import (Jan
                17)/BayesOpt;Mendeley Import (Jan
                17)/BayesOpt/Acquisition/InfillFxns",
  language   = "en",
  conference = "Joint European Conference on Machine Learning and Knowledge
                Discovery in Databases"
}

@ARTICLE{Shahriari2014-pu,
  title         = "An Entropy Search Portfolio for Bayesian Optimization",
  author        = "Shahriari, Bobak and Wang, Ziyu and Hoffman, Matthew W and
                   Bouchard-C{\^o}t{\'e}, Alexandre and de Freitas, Nando",
  abstract      = "Bayesian optimization is a sample-efficient method for
                   black-box global optimization. How- ever, the performance of
                   a Bayesian optimization method very much depends on its
                   exploration strategy, i.e. the choice of acquisition
                   function, and it is not clear a priori which choice will
                   result in superior performance. While portfolio methods
                   provide an effective, principled way of combining a
                   collection of acquisition functions, they are often based on
                   measures of past performance which can be misleading. To
                   address this issue, we introduce the Entropy Search
                   Portfolio (ESP): a novel approach to portfolio construction
                   which is motivated by information theoretic considerations.
                   We show that ESP outperforms existing portfolio methods on
                   several real and synthetic problems, including
                   geostatistical datasets and simulated control tasks. We not
                   only show that ESP is able to offer performance as good as
                   the best, but unknown, acquisition function, but
                   surprisingly it often gives better performance. Finally,
                   over a wide range of conditions we find that ESP is robust
                   to the inclusion of poor acquisition functions.",
  pages         = "10",
  month         =  "18~" # jun,
  year          =  2014,
  keywords      = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1406.4625"
}

@ARTICLE{Ebisu1975-qe,
  title    = "Isolation and purification of Flavobacterium
              alpha-1,3-glucanase-hydrolyzing, insoluble, sticky glucan of
              Streptococcus mutans",
  author   = "Ebisu, S and Kato, K and Kotani, S and Misaki, A",
  abstract = "Studies were made on the physical and chemical properties of
              polysaccharides synthesized by cell-free extracts of
              Streptococcus mutans, Streptococcus sanguis, and Streptococcus
              sp. and their susceptibilities to dextranases. Among the
              polysaccharides examined, insoluble glucans were rather resistant
              to available dextranase preparations, and the insoluble, sticky
              glucan produced by S. mutans OMZ 176, which could be important in
              formation of dental plaques, was the most resistant. By
              enrichment culture of soil specimens, using OMZ 176 glucans as
              the sole carbon source, an organism was isolated that produced
              colonies surrounded by a clear lytic zone on opaque agar plates
              containing the OMZ 176 glucan. The organism was identified as a
              strain of Flavobacterium and named the Ek-14 bacterium. EK-14
              bacterium was grown in Trypticase soy broth, and an enzyme
              capable of hydrolyzing the OMZ 176 glucan was concentrated from
              the culture supernatant and purified by negative adsorption on a
              diethylaminoethyl-cellulose (DE-32) column and gradient elution
              chromatography with a carboxymethyl-cellulose (CM-32) column. The
              enzyme was a basic protein with an isoelectric point of pH 8.5
              and molecular weight of 65,000. Its optimum pH was 6.3 and its
              optimal temperature was 42 C. The purified enzyme released 11\%
              of the total glucose residues of the OMZ 176 glucan as reducing
              sugars and solubilized about half of the substrate glucan. The
              products were found to be isomaltose, nigerose, and nigerotriose,
              with some oligosaccharides. The purified enzyme split the
              alpha-1,3-glucan endolytically and was inactive toward glucans
              containing alpha-1,6, alpha-1,4, beta-1,3, beta-1,4, and/or
              beta-1,6 bonds as the main linkages.",
  journal  = "J. Bacteriol.",
  volume   =  124,
  number   =  3,
  pages    = "1489--1501",
  month    =  dec,
  year     =  1975,
  keywords = "classification; cross-validation bootstrap; prediction
              rule;BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;Mendeley
              Import (Jan 17)/BayesOpt;Mendeley Import (Jan
              17)/OptimizingHyperparameters",
  language = "en"
}

@INPROCEEDINGS{Domingos1998-xw,
  title     = "A {Process-Oriented} Heuristic for Model Selection",
  booktitle = "{ICML}",
  author    = "Domingos, Pedro M",
  abstract  = "Current methods to avoid overfitting are either data-oriented
               (using separate data for validation) or representation-oriented
               (penalizing complexity in the model). This paper proposes
               process-oriented evaluation, where a model's expected
               generalization error is computed as a function of the search
               process that led to it. The paper develops the necessary
               theoretical framework, and applies it to one type of learning:
               rule induction. A process-oriented version of the CN2 rule
               learner is empirically compared with the default CN2. The
               process-oriented version is more accurate in a large majority of
               the datasets, with high significance, and also produces simpler
               models. Experiments in artificial domains suggest that
               processoriented evaluation is particularly useful in
               high-dimensional domains. 1 INTRODUCTION Overfitting avoidance
               is often considered the central problem of machine learning
               (e.g., (Cheeseman \& Oldford, 1994)). If a learner is
               sufficiently powerful, it must guard against selec...",
  pages     = "127--135",
  year      =  1998,
  keywords  = "
               imported;BayesOpt;GPs;NotRead;OptimizingHyperparameters;TALAF;Mendeley
               Import (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters"
}

@INPROCEEDINGS{Feurer2014-xr,
  title     = "Using meta-learning to initialize Bayesian optimization of
               hyperparameters",
  booktitle = "{CEUR} Workshop Proceedings",
  author    = "Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank",
  abstract  = "Model selection and hyperparameter optimization is cru-cial in
               applying machine learning to a novel dataset. Recently, a
               sub-community of machine learning has focused on solving this
               prob-lem with Sequential Model-based Bayesian Optimization
               (SMBO), demonstrating substantial successes in many
               applications. However, for expensive algorithms the
               computational overhead of hyperpa-rameter optimization can still
               be prohibitive. In this paper we ex-plore the possibility of
               speeding up SMBO by transferring knowl-edge from previous
               optimization runs on similar datasets; specifi-cally, we propose
               to initialize SMBO with a small number of config-urations
               suggested by a metalearning procedure. The resulting simple
               MI-SMBO technique can be trivially applied to any SMBO method,
               allowing us to perform experiments on two quite different SMBO
               methods with complementary strengths applied to optimize two
               ma-chine learning frameworks on 57 classification datasets. We
               find that our initialization procedure mildly improves the state
               of the art in low-dimensional hyperparameter optimization and
               substantially im-proves the state of the art in the more complex
               problem of combined model selection and hyperparameter
               optimization.",
  volume    =  1201,
  pages     = "3--10",
  year      =  2014,
  keywords  = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;Mendeley Import
               (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters"
}

@ARTICLE{Rodriguez2010-ss,
  title       = "Sensitivity analysis of kappa-fold cross validation in
                 prediction error estimation",
  author      = "Rodr{\'\i}guez, Juan Diego and P{\'e}rez, Aritz and Lozano,
                 Jose Antonio",
  affiliation = "University of the Basque Country, San Seabstian, Spain.
                 juandiego.rodriguez@ehu.es",
  abstract    = "In the machine learning field, the performance of a classifier
                 is usually measured in terms of prediction error. In most
                 real-world problems, the error cannot be exactly calculated
                 and it must be estimated. Therefore, it is important to choose
                 an appropriate estimator of the error. This paper analyzes the
                 statistical properties, bias and variance, of the kappa-fold
                 cross-validation classification error estimator (kappa-cv).
                 Our main contribution is a novel theoretical decomposition of
                 the variance of the kappa-cv considering its sources of
                 variance: sensitivity to changes in the training set and
                 sensitivity to changes in the folds. The paper also compares
                 the bias and variance of the estimator for different values of
                 kappa. The experimental study has been performed in artificial
                 domains because they allow the exact computation of the
                 implied quantities and we can rigorously specify the
                 conditions of experimentation. The experimentation has been
                 performed for two classifiers (naive Bayes and nearest
                 neighbor), different numbers of folds, sample sizes, and
                 training sets coming from assorted probability distributions.
                 We conclude by including some practical recommendation on the
                 use of kappa-fold cross validation.",
  journal     = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume      =  32,
  number      =  3,
  pages       = "569--575",
  month       =  mar,
  year        =  2010,
  keywords    = "Bayes methods;BayesOpt;GPs;Machine
                 learning;NotRead;OptimizingHyperparameters;STATISTICAL
                 ANALYSIS;TALAF;estimation theory;learning (artificial
                 intelligence);pattern classification;probability;sensitivity
                 analysis;Mendeley Import (Jan 17)/BayesOpt;Mendeley Import
                 (Jan 17)/OptimizingHyperparameters;Mendeley Import (Jan 17)",
  language    = "en"
}

@INPROCEEDINGS{Ester1996-xn,
  title     = "A density-based algorithm for discovering clusters in large
               spatial databases with noise",
  booktitle = "Proceedings of the Second International Conference on Knowledge
               Discovery and Data Mining ({KDD-96)}, Portland, Oregon, {USA}",
  author    = "Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and
               Xu, Xiaowei",
  abstract  = "Clustering algorithms are attractive for the task of class
               identification in spatial databases. However, the application to
               large spatial databases rises the following requirements for
               clustering algorithms: minimal requirements of domain knowledge
               to determine the input parameters, discovery of clusters with
               arbitrary shape and good efficiency on large databases. The
               well-known clustering algorithms offer no solution to the
               combination of these requirements. In this paper, we present the
               new clustering algorithm DBSCAN relying on a density-based
               notion of clusters which is designed to discover clusters of
               arbitrary shape. DBSCAN requires only one input parameter and
               supports the user in determining an appropriate value for it. We
               performed an experimental evaluation of the effectiveness and
               efficiency of DBSCAN using synthetic data and real data of the
               SEQUOIA 2000 benchmark. The results of our experiments
               demonstrate that (1) DBSCAN is significantly more effective in
               discovering clusters of arbitrary shape than the well-known
               algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a
               factor of more than 100 in terms of efficiency.",
  publisher = "AAAI Press",
  pages     = "226--231",
  year      =  1996,
  keywords  = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@INCOLLECTION{Golovin2010-sr,
  title     = "{Near-Optimal} Bayesian Active Learning with Noisy Observations",
  booktitle = "Advances in Neural Information Processing Systems 23",
  author    = "Golovin, Daniel and Krause, Andreas and Ray, Debajyoti",
  editor    = "Lafferty, J D and Williams, C K I and Shawe-Taylor, J and Zemel,
               R S and Culotta, A",
  abstract  = "We tackle the fundamental problem of Bayesian active learning
               with noise, where we need to adaptively select from a number of
               expensive tests in order to identify an unknown hypothesis
               sampled from a known prior distribution. In the case of
               noise-free observations, a greedy algorithm called generalized
               binary search (GBS) is known to perform near-optimally. We show
               that if the observations are noisy, perhaps surprisingly, GBS
               can perform very poorly. We develop EC2, a novel, greedy active
               learning algorithm and prove that it is competitive with the
               optimal policy, thus obtaining the first competitiveness
               guarantees for Bayesian active learning with noisy observations.
               Our bounds rely on a recently discovered diminishing returns
               property called adaptive submodularity, generalizing the
               classical notion of submodular set functions to adaptive
               policies. Our results hold even if the tests have non--uniform
               cost and their noise is correlated. We also propose EffECXtive,
               a particularly fast approximation of EC2, and evaluate it on a
               Bayesian experimental design problem involving human subjects,
               intended to tease apart competing economic theories of how
               people make decisions under uncertainty.",
  publisher = "Curran Associates, Inc.",
  pages     = "766--774",
  year      =  2010,
  keywords  = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@INCOLLECTION{Rao2006-vr,
  title     = "On the Dangers of {Cross-Validation}. An Experimental Evaluation",
  booktitle = "Proceedings of the 2008 {SIAM} International Conference on Data
               Mining",
  author    = "Rao, R Bharat and Fung, Glenn and Rosales, Romer",
  abstract  = "Abstract Cross validation allows models to be tested using the
               full training set by means of repeated resampling; thus,
               maximizing the total number of points used for testing and
               potentially, helping to protect against overfitting.
               Improvements in computational power, recent reductions in the
               (computational) cost of classification algorithms, and the
               development of closed-form solutions (for performing cross
               validation in certain classes of learning algorithms) makes it
               possible to test thousand or millions of variants of learning
               models on the data. Thus, it is now possible to calculate cross
               validation performance on a much larger number of tuned models
               than would have been possible otherwise. However, we empirically
               show how under such large number of models the risk for
               overfitting increases and the performance estimated by cross
               validation is no longer an effective estimate of generalization;
               hence, this paper provides an empirical reminder of the dangers
               of cross validation. We use a closed-form solution that makes
               this evaluation possible for the cross validation problem of
               interest. In addition, through extensive experiments we expose
               and discuss the effects of the overuse/misuse of cross
               validation in various aspects, including model selection,
               feature selection, and data dimensionality. This is illustrated
               on synthetic, benchmark, and real-world data sets.",
  publisher = "SIAM",
  pages     = "588--596",
  year      =  2006,
  keywords  = "BayesOpt;GPs;OptimizingHyperparameters;TALAF;Mendeley Import
               (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters"
}

@ARTICLE{Snelson2004-kh,
  title     = "Warped gaussian processes",
  author    = "Snelson, E and Rasmussen, C E and {others}",
  abstract  = "Abstract We generalise the Gaussian process (GP) framework for
               regression by learning a nonlinear transformation of the GP
               outputs. This allows for non-Gaussian processes and non-
               Gaussian noise. The learning algorithm chooses a nonlinear
               transformation such that",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "books.google.com",
  volume    =  16,
  pages     = "337--344",
  year      =  2004,
  keywords  = "Computational; Information-Theoretic Learning with;
               Learning/Statistics \& Optimisation; Theory \&
               Algorithms;BayesOpt;Important;NotRead;TALAF;Mendeley Import (Jan
               17)/BayesOpt"
}

@INPROCEEDINGS{Gardner2014-qw,
  title     = "Bayesian Optimization with Inequality Constraints",
  booktitle = "Proceedings of The 31st International Conference on Machine
               Learning",
  author    = "Gardner, Jacob and Kusner, Matt and {Zhixiang} and Weinberger,
               Kilian and Cunningham, John",
  abstract  = "Bayesian optimization is a powerful framework for minimizing
               expensive objective functions while using very few function
               evaluations. It has been successfully applied to a variety of
               problems, including hyperparameter tuning and experimental
               design. However, this framework has not been extended to the
               inequality-constrained optimization setting, particularly the
               setting in which evaluating feasibility is just as expensive as
               evaluating the objective. Here we present constrained Bayesian
               optimization, which places a prior distribution on both the
               objective and the constraint functions. We evaluate our method
               on simulated and real data, demonstrating that constrained
               Bayesian optimization can quickly find optimal and feasible
               points, even when small feasible regions cause standard methods
               to fail.",
  volume    =  32,
  pages     = "937--945",
  year      =  2014,
  keywords  = "BayesOpt;TALAF;Mendeley Import (Jan 17)/BayesOpt;Mendeley Import
               (Jan 17)"
}

@INPROCEEDINGS{Wang2013-yv,
  title     = "Bayesian Optimization in High Dimensions via Random Embeddings",
  booktitle = "Proceedings of the {Twenty-Third} International Joint Conference
               on Artificial Intelligence",
  author    = "Wang, Ziyu and Zoghi, Masrour and Hutter, Frank and Matheson,
               David and De Freitas, Nando",
  abstract  = "Bayesian optimization techniques have been successfully applied
               to robotics, planning, sensor placement, recommendation,
               advertising, intelligent user interfaces and automatic algorithm
               configuration. Despite these successes, the approach is
               restricted to problems of moderate dimension, and several
               workshops on Bayesian optimization have identified its scaling
               to high-dimensions as one of the holy grails of the field. In
               this paper, we introduce a novel random embedding idea to attack
               this problem. The resulting Random EMbedding Bayesian
               Optimization (REMBO) algorithm is very simple, has important
               invariance properties, and applies to domains with both
               categorical and continuous variables. We present a thorough
               theoretical analysis of REMBO, including regret bounds that only
               depend on the problem's intrinsic dimensionality. Empirical
               results confirm that REMBO can effectively solve problems with
               billions of dimensions, provided the intrinsic dimensionality is
               low. They also show that REMBO achieves state-of-the-art
               performance in optimizing the 47 discrete parameters of a
               popular mixed integer linear programming solver.",
  publisher = "AAAI Press",
  pages     = "1778--1784",
  series    = "IJCAI '13",
  year      =  2013,
  address   = "Beijing, China",
  keywords  = "BayesOpt;Important;TALAF;Mendeley Import (Jan 17)/BayesOpt"
}

@ARTICLE{Cawley2007-he,
  title     = "Preventing {Over-Fitting} During Model Selection via Bayesian
               Regularisation of the {Hyper-Parameters}",
  author    = "Cawley, Gavin C and Talbot, Nicola L C",
  abstract  = "While the model parameters of a kernel machine are typically
               given by the solution of a convex optimisation problem, with a
               single global optimum, the selection of good values for the
               regularisation and kernel parameters is much less
               straightforward. Fortunately the leave-one-out cross-validation
               procedure can be performed or a least approximated very
               efficiently in closed form for a wide variety of kernel learning
               methods, providing a convenient means for model selection.
               Leave-one-out cross-validation based estimates of performance,
               however, generally exhibit a relatively high variance and are
               therefore prone to over-fitting. In this paper, we investigate
               the novel use of Bayesian regularisation at the second level of
               inference, adding a regularisation term to the model selection
               criterion corresponding to a prior over the hyper-parameter
               values, where the additional regularisation parameters are
               integrated out analytically. Results obtained on a suite of
               thirteen real-world and synthetic benchmark data sets clearly
               demonstrate the benefit of this approach.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  8,
  pages     = "841--861",
  month     =  may,
  year      =  2007,
  keywords  = "bayesian regularisation; kernel methods; model
               selection;BayesOpt;GPs;OptimizingHyperparameters;TALAF;Mendeley
               Import (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters"
}

@ARTICLE{Girosi1995-td,
  title    = "Regularization Theory and Neural Networks Architectures",
  author   = "Girosi, Federico and Jones, Michael and Poggio, Tomaso",
  abstract = "We had previously shown that regularization principles lead to
              approximation schemes that are equivalent to networks with one
              layer of hidden units, called regularization networks. In
              particular, standard smoothness functionals lead to a subclass of
              regularization networks, the well known radial basis functions
              approximation schemes. This paper shows that regularization
              networks encompass a much broader range of approximation schemes,
              including many of the popular general additive models and some of
              the neural networks. In particular, we introduce new classes of
              smoothness functionals that lead to different classes of basis
              functions. Additive splines as well as some tensor product
              splines can be obtained from appropriate classes of smoothness
              functionals. Furthermore, the same generalization that extends
              radial basis functions (RBF) to hyper basis functions (HBF) also
              leads from additive models to ridge approximation models,
              containing as special cases Breiman's hinge functions, some forms
              of projection pursuit regression, and several types of neural
              networks. We propose to use the term generalized regularization
              networks for this broad class of approximation schemes that
              follow from an extension of regularization. In the probabilistic
              interpretation of regularization, the different classes of basis
              functions correspond to different classes of prior probabilities
              on the approximating function spaces, and therefore to different
              types of smoothness assumptions. In summary, different multilayer
              networks with one hidden layer, which we collectively call
              generalized regularization networks, correspond to different
              classes of priors and associated smoothness functionals in a
              classical regularization principle. Three broad classes are (1)
              radial basis functions that can be generalized to hyper basis
              functions, (2) some tensor product splines, and (3) additive
              splines that can be generalized to schemes of the type of ridge
              approximation, hinge functions, and several perceptron-like
              neural networks with one hidden layer.",
  journal  = "Neural Comput.",
  volume   =  7,
  number   =  2,
  pages    = "219--269",
  month    =  mar,
  year     =  1995,
  keywords = "Mendeley Import (Jan 17)/MLTheory"
}

@INCOLLECTION{Bertero1986-vf,
  title     = "Regularization methods for linear inverse problems",
  booktitle = "Inverse Problems",
  author    = "Bertero, M",
  editor    = "Talenti, Giorgio",
  publisher = "Springer Berlin Heidelberg",
  pages     = "52--112",
  series    = "Lecture Notes in Mathematics",
  year      =  1986,
  keywords  = "NotRead;Textbook;Mendeley Import (Jan 17)/TextBooks;Mendeley
               Import (Jan 17)/MLTheory",
  language  = "en"
}

@ARTICLE{Tarantola2005-jt,
  title     = "Inverse Problem Theory - A . Tarantola - Siam 2004",
  author    = "Tarantola, Albert",
  publisher = "Society for Industrial and Applied Mathematics",
  number    = "July",
  pages     = "2--3",
  year      =  2005,
  address   = "Philadelphia, PA",
  keywords  = "NotRead;Textbook;Mendeley Import (Jan 17)/TextBooks;Mendeley
               Import (Jan 17)/MLTheory"
}

@BOOK{Vapnik2000-bm,
  title     = "The Nature of Statistical Learning Theory",
  author    = "Vapnik, Vladimir Naoumovitch",
  abstract  = "The aim of this book is to discuss the fundamental ideas which
               lie behind the statistical theory of learning and
               generalization. It considers learning as a general problem of
               function estimation based on empirical data. Omitting proofs and
               technical details, the author concentrates on discussing the
               main results of learning theory and their connections to
               fundamental problems in statistics. These include: * the setting
               of learning problems based on the model of minimizing the risk
               functional from empirical data * a comprehensive analysis of the
               empirical risk minimization principle including necessary and
               sufficient conditions for its consistency * non-asymptotic
               bounds for the risk achieved using the empirical risk
               minimization principle * principles for controlling the
               generalization ability of learning machines using small sample
               sizes based on these bounds * the Support Vector methods that
               control the generalization ability when estimating function
               using small sample size. The second edition of the book contains
               three new chapters devoted to further development of the
               learning theory and SVM techniques. These include: * the theory
               of direct method of learning based on solving multidimensional
               integral equations for density, conditional probability, and
               conditional density estimation * a new inductive principle of
               learning. Written in a readable and concise style, the book is
               intended for statisticians, mathematicians, physicists, and
               computer scientists. Vladimir N. Vapnik is Technology Leader
               AT\&T Labs-Research and Professor of London University. He is
               one of the founders of statistical learning theory, and the
               author of seven books published in English, Russian, German, and
               Chinese. Written for: Researchers",
  publisher = "Springer New York",
  volume    =  8,
  pages     = "i--xix 1--314",
  year      =  2000,
  address   = "New York, NY",
  keywords  = "Computational learning theory.; Reasoning.; Various-Artists;
               book; juergen; learning; learning-theory svm; statistical
               learning theory; statistical-learning-theory; svm; svm/ support
               vector machines/ learning theory; svms;NotRead;Textbook;Mendeley
               Import (Jan 17)/TextBooks;Mendeley Import (Jan 17)/MLTheory"
}

@MISC{noauthor_undated-td,
  title    = "References for Machine Learning Theory",
  keywords = "Mendeley Import (Jan 17)/MLTheory"
}

@ARTICLE{Cohen2014-nc,
  title    = "Using Gambling to Entice {Low-Income} Families to Save",
  author   = "Cohen, Patricia",
  abstract = "A growing number of credit unions and nonprofit groups are using
              lotteries to encourage low-income families to save.",
  journal  = "The New York Times",
  month    =  "30~" # aug,
  year     =  2014,
  keywords = "Mendeley Import (Jan 17)/Discounting"
}

@ARTICLE{Watkins1992-ah,
  title    = "Q-learning",
  author   = "Watkins, Christopher J C H and Dayan, Peter",
  abstract = "Q-learning (Watkins, 1989) is a simple way for agents to learn
              how to act optimally in controlled Markovian domains. It amounts
              to an incremental method for dynamic programming which imposes
              limited computational demands. It works by successively improving
              its evaluations of the quality of particular actions at
              particular states. This paper presents and proves in detail a
              convergence theorem forQ-learning based on that outlined in
              Watkins (1989). We show thatQ-learning converges to the optimum
              action-values with probability 1 so long as all actions are
              repeatedly sampled in all states and the action-values are
              represented discretely. We also sketch extensions to the cases of
              non-discounted, but absorbing, Markov environments, and where
              manyQ values can be changed each iteration, rather than just one.",
  journal  = "Mach. Learn.",
  volume   =  8,
  number   = "3-4",
  pages    = "279--292",
  month    =  may,
  year     =  1992,
  keywords = "Artificial Intelligence (incl. Robotics);Q-learning;reinforcement
              learning;Mendeley Import (Jan 17)/Discounting",
  language = "en"
}

@ARTICLE{Yen2006-ru,
  title    = "Agents with Shared Mental Models for",
  author   = "Yen, John and Fan, Xiaocong and Sun, Shuang and Hanratty, Timothy
              and Dumer, John",
  journal  = "Decis. Support Syst.",
  volume   =  41,
  number   =  3,
  pages    = "1--31",
  year     =  2006,
  keywords = "agent teamwork; homeland security; information overload; team
              decision-making;Mendeley Import (Jan 17)/Discounting"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bartels2015-gq,
  title     = "To Know and to Care: How Awareness and Valuation of the Future
               Jointly Shape Consumer Spending",
  author    = "Bartels, Daniel M and Urminsky, Oleg",
  abstract  = "Reducing spending in the present requires the combination of
               being both motivated to provide for one’s future self (valuing
               the future) and actively considering long-term implications of
               one’s choices (awareness of the future). Feeling more connected
               to the future self---thinking that the important psychological
               properties that define your current self are preserved in the
               person you will be in the future---helps motivate consumers to
               make far-sighted choices by changing their valuation of future
               outcomes (e.g., discount factors). However, this change only
               reduces spending when opportunity costs are considered.
               Correspondingly, cues that highlight opportunity costs reduce
               spending primarily when people discount the future less or are
               more connected to their future selves. Implications for the
               efficacy of behavioral interventions and for research on time
               discounting are discussed.",
  journal   = "J. Consum. Res.",
  publisher = "The Oxford University Press",
  volume    =  41,
  number    =  6,
  pages     = "1469--1485",
  month     =  "1~" # apr,
  year      =  2015,
  keywords  = "Mendeley Import (Jan 17)/Discounting",
  language  = "en"
}

@INCOLLECTION{Cooper2013-qy,
  title     = "On Computable Numbers, with an Application to the
               Entscheidungsproblem -- A Correction",
  booktitle = "Alan Turing: His Work and Impact",
  author    = "Cooper, S Barry and Leeuwen, Jan Van",
  abstract  = "Turing, A. M. ``On Computable Numbers with an Application to the
               Entscheidungsproblem.'' ,",
  publisher = "Elsevier",
  volume    =  58,
  pages     = "13--115",
  year      =  2013,
  keywords  = "Folder - HistoricalCSPapers;Mendeley Import (Jan 17)/Other"
}

@ARTICLE{Hosseini2014-pr,
  title         = "Online Distributed Optimization on Dynamic Networks",
  author        = "Hosseini, Saghar and Chapman, Airlie and Mesbahi, Mehran",
  abstract      = "This paper presents a distributed optimization scheme over a
                   network of agents in the presence of cost uncertainties and
                   over switching communication topologies. Inspired by recent
                   advances in distributed convex optimization, we propose a
                   distributed algorithm based on a dual sub-gradient
                   averaging. The objective of this algorithm is to minimize a
                   cost function cooperatively. Furthermore, the algorithm
                   changes the weights on the communication links in the
                   network to adapt to varying reliability of neighboring
                   agents. A convergence rate analysis as a function of the
                   underlying network topology is then presented, followed by
                   simulation results for representative classes of sensor
                   networks.",
  pages         = "1--23",
  month         =  "22~" # dec,
  year          =  2014,
  keywords      = "Computer Science - Learning;Folder -
                   CSCI5454-Algorithms;NotRead;Mendeley Import (Jan 17)/Other",
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1412.7215"
}

@INPROCEEDINGS{Osting2013-yi,
  title     = "Enhanced statistical rankings via targeted data collection",
  booktitle = "Proceedings of The 30th International Conference on Machine
               Learning",
  author    = "Osting, Braxton and Brune, Christoph and Osher, Stanley",
  abstract  = "Given a graph where vertices represent alternatives and pairwise
               comparison data, \textbackslash(y\_\{ij\}\textbackslash), is
               given on the edges, the statistical ranking problem is to find a
               potential function, defined on the vertices, such that the
               gradient of the potential function agrees with pairwise
               comparisons. We study the dependence of the statistical ranking
               problem on the available pairwise data, i.e., pairs (i,j) for
               which the pairwise comparison data
               \textbackslash(y\_\{ij\}\textbackslash) is known, and propose a
               framework to identify data which, when augmented with the
               current dataset, maximally increases the Fisher information of
               the ranking. Under certain assumptions, the data collection
               problem decouples, reducing to a problem of finding an edge set
               on the graph (with a fixed number of edges) such that the second
               eigenvalue of the graph Laplacian is maximal. This reduction of
               the data collection problem to a spectral graph-theoretic
               question is one of the primary contributions of this work. As an
               application, we study the Yahoo! Movie user rating dataset and
               demonstrate that the addition of a small number of well-chosen
               pairwise comparisons can significantly increase the Fisher
               informativeness of the ranking.",
  volume    =  28,
  pages     = "489--497",
  year      =  2013,
  keywords  = "Folder - CSCI5454-Algorithms;NotRead;Mendeley Import (Jan
               17)/Other"
}

@ARTICLE{Kim2010-ox,
  title    = "Bisection Algorithm of Increasing Algebraic Connectivity by
              Adding an Edge",
  author   = "Kim, Y",
  abstract = "For a given graph (or network) G, consider another graph G' by
              adding or deleting an edge e to or from G. We propose a
              computationally efficient algorithm of finding e such that the
              second smallest eigenvalue (algebraic connectivity, ??2(G')) of
              G' is maximized or minimized. Theoretically, the proposed
              algorithm runs in O(4mnlog(d/??)), where n is the number of nodes
              in G, m is the number of disconnected edges in G, d is the
              difference between ??3(G) and ??2(G), and ?? > 0 is a
              sufficiently small constant. However, extensive simulations show
              that the practical computational complexity of the proposed
              algorithm, O(5.7mn), is nearly comparable to that of a simple
              greedy-type heuristic, O(2mn). This algorithm can also be easily
              modified for finding e which affects ??2(G) the least.",
  journal  = "IEEE Trans. Automat. Contr.",
  volume   =  55,
  number   =  1,
  pages    = "170--174",
  month    =  jan,
  year     =  2010,
  keywords = "Africa; Algebraic connectivity; Computational complexity;
              Computational modeling; Control theory; Eigenvalues and
              eigenfunctions; Energy consumption; Folder - CSCI5454-Algorithms;
              Important; Laplace equations; Laplacian matrix; Mechatronics;
              NotRead; Robust stability; bisection algorithm; graph theory;
              greedy type heuristic; practical computational
              complexity;Algebraic connectivity;Computational
              complexity;Computational modeling;Eigenvalues and
              eigenfunctions;Folder - CSCI5454-Algorithms;Important;Laplace
              equations;NotRead;graph theory;Mendeley Import (Jan 17);Mendeley
              Import (Jan 17)/Other"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{noauthor_undated-xo,
  title    = "Five Surprises from My Computer Science Academic Job Search",
  abstract = "Next in series: The job talk is a performance I’ve just about
              settled into a rhythm at Princeton --- classes started two weeks
              ago --- and next year’s academic job search cycle is already
              underway! Ind...",
  keywords = "Folder - AcademicJobs;Mendeley Import (Jan 17)/Other"
}

@ARTICLE{Mosk-Aoyama2008-ta,
  title    = "Maximum algebraic connectivity augmentation is {NP-hard}",
  author   = "Mosk-Aoyama, Damon",
  abstract = "The algebraic connectivity of a graph, which is the
              second-smallest eigenvalue of the Laplacian of the graph, is a
              measure of connectivity. We show that the problem of adding a
              specified number of edges to an input graph to maximize the
              algebraic connectivity of the augmented graph is NP-hard.",
  journal  = "Oper. Res. Lett.",
  volume   =  36,
  number   =  6,
  pages    = "677--679",
  month    =  nov,
  year     =  2008,
  keywords = "Algebraic connectivity; Computational complexity;Algebraic
              connectivity;Computational complexity;Folder -
              CSCI5454-Algorithms;Important;Mendeley Import (Jan
              17)/Other;Mendeley Import (Jan 17)"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Duffy2014-rt,
  title    = "You do not need to work 80 hours a week to succeed in academia",
  author   = "Duffy, Meghan",
  abstract = "There is a persistent myth (some might even call it a zombie
              idea) that getting tenure in academia requires working 80 hours a
              week. There’s even a joke along the lines of ``The great thing
              about ac...",
  month    =  feb,
  year     =  2014,
  keywords = "Folder - AcademicJobs;Mendeley Import (Jan 17)/Other"
}

@ARTICLE{Bertrand2013-rz,
  title    = "Distributed computation of the Fiedler vector with application to
              topology inference in ad hoc networks",
  author   = "Bertrand, Alexander and Moonen, Marc",
  abstract = "The Fiedler vector of a graph is the eigenvector corresponding to
              the smallest non-trivial eigenvalue of the graph's Laplacian
              matrix. The entries of the Fiedler vector are known to provide a
              powerful heuristic for topology inference, e.g., to identify
              densely connected node clusters, to search for bottleneck links
              in the information dissemination, or to increase the overall
              connectivity of the network. In this paper, we consider ad hoc
              networks where the nodes can process and exchange data in a
              synchronous fashion, and we propose a distributed algorithm for
              in-network estimation of the Fiedler vector and the algebraic
              connectivity of the corresponding network graph. The algorithm is
              fully scalable with respect to the network size in terms of
              per-node computational complexity and data transmission.
              Simulation results demonstrate the performance of the algorithm.",
  journal  = "Signal Processing",
  volume   =  93,
  number   =  5,
  pages    = "1106--1117",
  year     =  2013,
  keywords = "Distributed algorithms; Fiedler vector; Spectral graph theory;
              Wireless sensor networks;Distributed algorithms;Fiedler
              vector;Folder - CSCI5454-Algorithms;NotRead;Spectral graph
              theory;Mendeley Import (Jan 17)/Other"
}

@ARTICLE{Shannon1997-oy,
  title    = "The mathematical theory of communication. 1963",
  author   = "Shannon, C E",
  abstract = "The recent development of various methods of modulation such as
              PCM and PPM which exchange bandwidth for signal-to-noise ratio
              has intensified the interest in a general theory of
              communication. A basis for such a theory is contained in the
              important papers of Nyquist and Hartley on this subject. In the
              present paper we will extend the theory to include a number of
              new factors, in particular the effect of noise in the channel,
              and the savings possible due to the statistical structure of the
              original message and due to the nature of the final destination
              of the information. The fundamental problem of communication is
              that of reproducing at one point either exactly or approximately
              a message selected at another point. Frequently the messages have
              meaning; that is they refer to or are correlated according to
              some system with certain physical or conceptual entities. These
              semantic aspects of communication are irrelevant to the
              engineering problem. The significant aspect is that the actual
              message is one selected from a set of possible messages. The
              system must be designed to operate for each possible selection,
              not just the one which will actually be chosen since this is
              unknown at the time of design. If the number of messages in the
              set is finite then this number or any monotonic function of this
              number can be regarded as a measure of the information produced
              when one message is chosen from the set, all choices being
              equally likely. As was pointed out by Hartley the most natural
              choice is the logarithmic function. Although this definition must
              be generalized considerably when we consider the influence of the
              statistics of the message and when we have a continuous range of
              messages, we will in all cases use an essentially logarithmic
              measure.",
  journal  = "MD Comput.",
  volume   =  14,
  number   =  4,
  pages    = "306--317",
  month    =  jul,
  year     =  1997,
  keywords = "Folder - HistoricalCSPapers;Information theory;Mendeley Import
              (Jan 17)/Other",
  language = "en"
}

@MISC{noauthor_undated-pz,
  title        = "theorems - Define proof environment in thmtools - {TeX} -
                  {LaTeX} Stack Exchange",
  howpublished = "\url{http://tex.stackexchange.com/questions/154883/define-proof-environment-in-thmtools}",
  keywords     = "Mendeley Import (Jan 17)/Other"
}

@ARTICLE{Kolokolnikov2014-xg,
  title         = "Maximizing algebraic connectivity for certain families of
                   graphs",
  author        = "Kolokolnikov, Theodore",
  abstract      = "We investigate the bounds on algebraic connectivity of
                   graphs subject to constraints on the number of edges,
                   vertices, and topology. We show that the algebraic
                   connectivity for any tree on $n$ vertices and with maximum
                   degree $d$ is bounded above by $2(d-2)
                   \frac\{1\}\{n\}+O(\frac\{\ln n\}\{n^\{2\}\}) .$ We then
                   investigate upper bounds on algebraic connectivity for cubic
                   graphs. We show that algebraic connectivity of a cubic graph
                   of girth $g$ is bounded above by
                   $3-2^\{3/2\}\cos(\pi/\lfloor g/2\rfloor) ,$ which is an
                   improvement over the bound found by Nilli [A. Nilli,
                   Electron. J. Combin., 11(9), 2004]. Finally, we propose
                   several conjectures and open questions.",
  pages         = "122--140",
  month         =  "17~" # dec,
  year          =  2014,
  keywords      = "Algebraic connectivity; Cubic graphs; Folder -
                   CSCI5454-Algorithms; NotRead; Optimal networks;
                   Trees;Algebraic connectivity;Folder -
                   CSCI5454-Algorithms;NotRead;Mendeley Import (Jan
                   17);Mendeley Import (Jan 17)/Other",
  archivePrefix = "arXiv",
  primaryClass  = "cs.DM",
  eprint        = "1412.6147"
}

@ARTICLE{Di_Lorenzo2014-ws,
  title    = "Distributed Estimation and Control of Algebraic Connectivity Over
              Random Graphs",
  author   = "Di Lorenzo, Paolo and Barbarossa, Sergio",
  abstract = "In this paper we propose a distributed algorithm for the
              estimation and control of the connectivity of ad-hoc networks in
              the presence of a random topology. First, given a generic random
              graph, we introduce a novel stochastic power iteration method
              that allows each node to estimate and track the algebraic
              connectivity of the underlying expected graph. Using results from
              stochastic approximation theory, we prove that the proposed
              method converges almost surely (a.s.) to the desired value of
              connectivity even in the presence of imperfect communication
              scenarios. The estimation strategy is then used as a basic tool
              to adapt the power transmitted by each node of a wireless
              network, in order to maximize the network connectivity in the
              presence of realistic Medium Access Control (MAC) protocols or
              simply to drive the connectivity toward a desired target value.
              Numerical results corroborate our theoretical findings, thus
              illustrating the main features of the algorithm and its
              robustness to fluctuations of the network graph due to the
              presence of random link failures.",
  journal  = "IEEE Trans. Signal Process.",
  volume   =  62,
  number   =  21,
  pages    = "5615--5628",
  month    =  nov,
  year     =  2014,
  keywords = "Algebraic connectivity; Fiedler vector; distributed computation;
              random graph; spectral graph theory; stochastic approximation;
              stochastic power iteration; topology control;Algebraic
              connectivity;Distributed algorithms;Eigenvalues and
              eigenfunctions;Fiedler vector;Folder -
              CSCI5454-Algorithms;Laplace equations;NotRead;Spectral graph
              theory;Vectors;estimation theory;graph theory;Mendeley Import
              (Jan 17)/Other;Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Ghosh2006-yl,
  title     = "Growing Well-connected Graphs",
  booktitle = "Proceedings of the 45th {IEEE} Conference on Decision and
               Control",
  author    = "Ghosh, A and Boyd, S",
  abstract  = "The algebraic connectivity of a graph is the second smallest
               eigenvalue of the graph Laplacian, and is a measure of how
               well-connected the graph is. We study the problem of adding
               edges (from a set of candidate edges) to a graph so as to
               maximize its algebraic connectivity. This is a difficult
               combinatorial optimization, so we seek a heuristic for
               approximately solving the problem. The standard convex
               relaxation of the problem can be expressed as a semidefinite
               program (SDP); for modest sized problems, this yields a cheaply
               computable upper bound on the optimal value, as well as a
               heuristic for choosing the edges to be added. We describe a new
               greedy heuristic for the problem. The heuristic is based on the
               Fiedler vector, and therefore can be applied to very large
               graphs",
  publisher = "IEEE",
  pages     = "6605--6611",
  month     =  dec,
  year      =  2006,
  keywords  = "Folder - CSCI5454-Algorithms; Important; eigenvalues and
               eigenfunctions; graph theory; optimisation; Fiedler vector;
               algebraic connectivity; combinatorial optimization; convex
               relaxation; eigenvalue; graph Laplacian; greedy heuristic;
               semidefinite program; well-connected graphs; Control systems;
               Information systems; Joining processes; Laboratories; Laplace
               equations; Robust stability; USA Councils; Upper bound;
               Vectors;Folder - CSCI5454-Algorithms;Important;Mendeley Import
               (Jan 17);Mendeley Import (Jan 17)/Other"
}

@INPROCEEDINGS{Bhuiyan2012-ux,
  title     = "{GUISE}: Uniform Sampling of Graphlets for Large Graph Analysis",
  booktitle = "2012 {IEEE} 12th International Conference on Data Mining",
  author    = "Bhuiyan, M A and Rahman, M and Rahman, M and Hasan, M Al",
  abstract  = "Graphlet frequency distribution (GFD) has recently become
               popular for characterizing large networks. However, the
               computation of GFD for a network requires the exact count of
               embedded graphlets in that network, which is a computationally
               expensive task. As a result, it is practically infeasible to
               compute the GFD for even a moderately large network. In this
               paper, we propose GUISE, which uses a Markov Chain Monte Carlo
               (MCMC) sampling method for constructing the approximate GFD of a
               large network. Our experiments on networks with millions of
               nodes show that GUISE obtains the GFD within few minutes,
               whereas the exhaustive counting based approach takes several
               days.",
  pages     = "91--100",
  month     =  dec,
  year      =  2012,
  keywords  = "Markov processes;Monte Carlo methods;graph theory;sampling
               methods;GFD;GUISE;MCMC;Markov chain Monte Carlo sampling
               method;computationally expensive task;embedded
               graphlets;graphlet frequency distribution;large graph
               analysis;uniform graphlet sampling;Biological information
               theory;Context;Markov processes;Monte Carlo methods;Probability
               distribution;Radiation detectors;Vectors;Folder -
               CSCI5454-Algorithms;Important;Markov
               processes;monte\_carlo;NotRead;Vectors;graph theory;Mendeley
               Import (Jan 17)/Other"
}

@ARTICLE{Titman2008-ct,
  title       = "A general goodness-of-fit test for Markov and hidden Markov
                 models",
  author      = "Titman, Andrew C and Sharples, Linda D",
  affiliation = "Medical Research Council Biostatistics Unit, Cambridge, U.K.
                 andrew.titman@mrc-bsu.cam.ac.uk",
  abstract    = "Markov models are a convenient and useful method of estimating
                 transition rates between levels of a categorical response
                 variable, such as a disease stage, which changes over time. In
                 medical applications the response variable is typically
                 observed at irregular intervals. A Pearson-type
                 goodness-of-fit test for such models was proposed by
                 Aguirre-Hernandez and Farewell (Statist. Med. 2002;
                 21:1899-1911), but this test is not applicable in the common
                 situation where the process includes an absorbing state, such
                 as death, for which the time of entry is known precisely nor
                 when the data include censored state observations. This paper
                 presents a modification to the Pearson-type test to allow for
                 these cases. An extension of the method, to allow for the
                 class of hidden Markov models where the response variable is
                 subject to misclassification error, is given. The method is
                 applied to data on cardiac allograft vasculopathy in
                 post-heart-transplant patients.",
  journal     = "Stat. Med.",
  volume      =  27,
  number      =  12,
  pages       = "2177--2195",
  month       =  "30~" # may,
  year        =  2008,
  keywords    = "goodness-of-fit; hidden markov model; markov model;Folder -
                 HMMGoodnessOfFit;NotRead;goodness-of-fit;hidden Markov
                 model;trust\_informal\_treatment;assurance\_implicit;Mendeley
                 Import (Jan 17)/Assurances;Mendeley Import (Jan
                 17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language    = "en"
}

@ARTICLE{Laskey1995-jp,
  title    = "Sensitivity analysis for probability assessments in
              Bayesiannetworks",
  author   = "Laskey, K B",
  journal  = "IEEE Trans. Syst. Man Cybern.",
  volume   =  25,
  number   =  6,
  pages    = "901--909",
  year     =  1995,
  keywords = "NotRead;trust\_informal\_treatment;assurance\_implicit;Mendeley
              Import (Jan 17)/Assurances;Mendeley Import (Jan
              17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@ARTICLE{Sinharay2006-yc,
  title    = "Model Diagnostics for Bayesian Networks",
  author   = "Sinharay, S",
  abstract = "Bayesian networks are frequently used in educational assessments
              primarily for learning about students' knowledge and skills.
              There is a lack of works on assessing fit of Bayesian networks.
              This article employs the posterior predictive model checking
              method, a popular Bayesian model checking tool, to assess fit of
              simple Bayesian networks. A number of aspects of model fit, those
              of usual interest to practitioners, are assessed using various
              diagnostic tools. This article suggests a direct data display for
              assessing overall fit, suggests several diagnostics for assessing
              item fit, suggests a graphical approach to examine if the model
              can explain the association among the items, and suggests a
              version of the Mantel-Haenszel statistic for assessing
              differential item functioning. Limited simulation studies and a
              real data application demonstrate the effectiveness of the
              suggested model diagnostics. [PUBLICATION ABSTRACT]",
  journal  = "J. Educ. Behav. Stat.",
  volume   =  31,
  number   =  1,
  pages    = "1--33",
  month    =  "1~" # jan,
  year     =  2006,
  keywords = "NotRead;trust\_informal\_treatment;assurance\_implicit;Mendeley
              Import (Jan 17)/Assurances;Mendeley Import (Jan
              17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@MISC{Titman2012-zw,
  title    = "Assessing Goodness-of-fit in Hidden Markov Models",
  author   = "Titman, Andrew",
  year     =  2012,
  keywords = "NotRead;trust\_informal\_treatment;assurance\_implicit;Mendeley
              Import (Jan 17)/Assurances;Mendeley Import (Jan
              17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@INPROCEEDINGS{De_Villiers2014-ch,
  title     = "A {URREF} interpretation of Bayesian network information fusion",
  booktitle = "17th International Conference on Information Fusion ({FUSION})",
  author    = "de Villiers, J P and Pavlin, G and Costa, P and Laskey, K and
               Jousselme, A L",
  abstract  = "In order for the uncertainty representation and reasoning
               evaluation framework (URREF) ontology for the evaluation of
               information fusion systems to have maximum value, it must be
               generally applicable irrespective of the application,
               uncertainty representation, reasoning scheme or data format.
               Since the URREF ontology is still an evolving framework, it is
               the focus of ongoing refinement through the efforts of the
               Evaluation of Techniques for Uncertainty Representation Working
               Group (ETURWG). Recent efforts by the authors to apply the URREF
               definitions to the evaluation of Bayesian network (BN) fusion
               systems have identified a need to translate and map the
               terminology of the URREF to the Bayesian network paradigm. The
               BN-to-URREF mapping is addressed in this paper within the
               context of the atomic decision procedure (ADP) and the latest
               view of the URREF ontology. The atomic decision procedure
               describes the information fusion process from input to output
               and consists of information sources (ADP-1), the interpretation
               and processing of this information and the qualification of the
               uncertainty it contains (ADP-2), the fusion of and reasoning
               with this uncertain information (ADP-3) and finally the decision
               scheme and output information (ADP-4). The URREF evaluation of
               the BN information fusion process allows for evaluation
               according to (1) representation criteria which relate to ADP-2
               and evaluate modeling and model parameterization, (2) reasoning
               criteria which relate to ADP-3 and evaluate the reasoning
               scheme, which in the case of a BN entails the computation of
               marginal probability densities over hypothesis variables and (3)
               data criteria which relate to ADP-1 and evaluate the sources and
               the information generated by said sources, together with the
               qualification and representation of the uncertainty inherent to
               the infor",
  pages     = "1--8",
  month     =  jul,
  year      =  2014,
  keywords  = "belief networks;inference mechanisms;ontologies (artificial
               intelligence);sensor fusion;ADP;ADP-1 source;ADP-2 source;BN
               fusion systems;BN-to-URREF mapping;Bayesian network information
               fusion;ETURWG;Evaluation of Techniques for Uncertainty
               Representation Working Group;URREF interpretation;URREF
               ontology;atomic decision procedure;data criteria;decision
               criteria;information fusion process;information fusion systems
               evaluation;model parameterization;representation
               criteria;uncertainty representation and reasoning evaluation
               framework;Bayes methods;Cognition;Data
               models;Joints;Ontologies;Probability
               distribution;Uncertainty;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Laskey2015-gz,
  title    = "Uncertainty representation , quantification and evaluation for
              data and information fusion",
  author   = "Laskey, K and De Villiers, J P and Blasch, Erik and Jousselme, A
              L and Pavlin, G and de Waal, A and Costa, P",
  abstract = "Mathematical and uncertainty modelling is an important component
              of data fusion (the fusion of unprocessed sensor data) and
              information fusion (the fusion of processed or interpreted data).
              If uncertainties in the modelling process are not or are
              incorrectly accounted for, fusion processes may provide under- or
              overconfident results, or in some cases incorrect results. These
              are often owing to incorrect or invalid simplifying assumptions
              during the modelling process. The authors investigate the sources
              of uncertainty in the modelling process. In particular, four
              processes of abstraction are identified where uncertainty may
              enter the modelling process. These are isolation abstraction
              (where uncertainty is introduced by isolating a portion of the
              real world to be modelled), datum uncertainty (where uncertainty
              is introduced by representing real world information by a
              mathematical quantity), data generation abstraction (where
              uncertainty is introduced through a mathematical representation
              of the mapping between a real-world process and an observable
              datum), and process abstraction (where uncertainty is introduced
              through a mathematical representation of real world entities and
              processes). The uncertainties associated with these abstraction
              processes are characterised according to the uncertainty
              representation and reasoning evaluation framework (URREF)
              ontology. A Bayesian network information fusion use case that
              models the rhino poaching problem is utilised to demonstrate the
              taxonomies introduced in this paper.",
  journal  = "International Conference on Information Fusion",
  pages    = "50--57",
  month    =  jul,
  year     =  2015,
  keywords = "URREF
              ontology;trust\_informal\_treatment;assurance\_implicit;Mendeley
              Import (Jan 17)/Assurances"
}

@ARTICLE{MacKay_Altman2004-fl,
  title       = "Assessing the goodness-of-fit of hidden Markov models",
  author      = "MacKay Altman, Rachel",
  affiliation = "Department of Statistics, University of British Columbia,
                 333-6356 Agricultural Road, Vancouver, British Columbia,
                 Canada V6T 1Z2. rachel@stat.ubc.ca",
  abstract    = "In this article, we propose a graphical technique for
                 assessing the goodness-of-fit of a stationary hidden Markov
                 model (HMM). We show that plots of the estimated distribution
                 against the empirical distribution detect lack of fit with
                 high probability for large sample sizes. By considering plots
                 of the univariate and multidimensional distributions, we are
                 able to examine the fit of both the assumed marginal
                 distribution and the correlation structure of the observed
                 data. We provide general conditions for the convergence of the
                 empirical distribution to the true distribution, and
                 demonstrate that these conditions hold for a wide variety of
                 time-series models. Thus, our method allows us to compare not
                 only the fit of different HMMs, but also that of other models
                 as well. We illustrate our technique using a multiple
                 sclerosis data set.",
  journal     = "Biometrics",
  volume      =  60,
  number      =  2,
  pages       = "444--450",
  month       =  jun,
  year        =  2004,
  keywords    = "Goodness-of-fit; Hidden Markov model; Model selection;
                 Multiple sclerosis; Probability plot; Stationary time
                 series;Folder - HMMGoodnessOfFit;Model
                 selection;goodness-of-fit;hidden Markov
                 model;assurance\_implicit;trust\_informal\_treatment;Mendeley
                 Import (Jan 17)/Assurances;Mendeley Import (Jan
                 17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language    = "en"
}

@ARTICLE{Dannemann2008-ch,
  title     = "Likelihood Ratio Testing for Hidden Markov Models Under
               Non-standard Conditions",
  author    = "Dannemann, J{\"o}rn and Holzmann, Hajo",
  abstract  = "Abstract. In practical applications, when testing parametric
               restrictions for hidden Markov models (HMMs), one frequently
               encounters non-standard situations such as testing for zero
               entries in the transition matrix, one-sided tests for the
               parameters of the transition matrix or for the components of the
               stationary distribution of the underlying Markov chain, or
               testing boundary restrictions on the parameters of the
               state-dependent distributions. In this paper, we briefly discuss
               how the relevant asymptotic distribution theory for the
               likelihood ratio test (LRT) when the true parameter is on the
               boundary extends from the independent and identically
               distributed situation to HMMs. Then we concentrate on discussing
               a number of relevant examples. The finite-sample performance of
               the LRT in such situations is investigated in a simulation
               study. An application to series of epileptic seizure counts
               concludes the paper.",
  journal   = "Scand. Stat. Theory Appl.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  35,
  number    =  2,
  pages     = "309--321",
  month     =  "1~" # jun,
  year      =  2008,
  keywords  = "Bimodality; Boundary; Hidden Markov model; Likelihood ratio
               test; Marginal distribution; Maximum-likelihood estimation;
               Overdispersion;Folder -
               HMMGoodnessOfFit;NotRead;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances;Mendeley Import (Jan
               17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language  = "en"
}

@ARTICLE{Titman2010-qx,
  title       = "Model diagnostics for multi-state models",
  author      = "Titman, Andrew C and Sharples, Linda D",
  affiliation = "Department of Mathematics and Statistics, Lancaster
                 University, UK. a.titman@lancaster.ac.uk",
  abstract    = "Multi-state models are a popular method of describing medical
                 processes that can be represented as discrete states or
                 stages. They have particular use when the data are
                 panel-observed, meaning they consist of discrete snapshots of
                 disease status at irregular time points which may be unique to
                 each patient. However, due to the difficulty of inference in
                 more complicated cases, strong assumptions such as the Markov
                 property, patient homogeneity and time homogeneity are
                 applied. It is important that the validity of these
                 assumptions is tested. A review of methods for diagnosing
                 model fit for panel-observed continuous-time Markov and
                 misclassification-type hidden Markov models is given, with
                 illustrative application to a dataset on cardiac allograft
                 vasculopathy progression in post-heart transplant patients.",
  journal     = "Stat. Methods Med. Res.",
  volume      =  19,
  number      =  6,
  pages       = "621--651",
  month       =  dec,
  year        =  2010,
  keywords    = "Folder -
                 HMMGoodnessOfFit;trust\_informal\_treatment;assurance\_implicit;Mendeley
                 Import (Jan 17)/Assurances;Mendeley Import (Jan
                 17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language    = "en"
}

@ARTICLE{Shenoy1989-tw,
  title    = "A valuation-based language for expert systems",
  author   = "Shenoy, Prakash P",
  abstract = "A new language based on valuations is proposed as an alternative
              to rule-based languages for constructing knowledge-based systems.
              Valuation-based languages are superior to rule-based languages
              for maintaining consistency in the knowledge base, for caching
              inferences, for managing uncertainty, and for nonmonotonic
              reasoning. An abstract description of a valuation-based language
              is given. Two specific instances of valuation-based languages are
              described. The first is designed to represent categorical
              knowledge. The ability of such a language to maintain consistency
              and cache inferences is demonstrated with an example. The second
              is an evidential language---a valuation-based language in which
              valuations are belief functions. The ability of evidential
              languages to perform nonmonotonic reasoning and manage
              uncertainty is demonstrated with an example.",
  journal  = "Int. J. Approx. Reason.",
  volume   =  3,
  number   =  5,
  pages    = "383--411",
  year     =  1989,
  keywords = "caching inferences; consistency in knowledge bases; evidential
              systems; knowledge-based system; management of uncertainty;
              nonmonotonic reasoning; rule-based language; rule-based system;
              truth maintenance systems; valuation system; valuation-based
              language;Mendeley Import (Jan 17)/Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Johnson2004-mv,
  title    = "A Bayesian $\chi$ 2 test for goodness-of-fit",
  author   = "Johnson, Valen E",
  abstract = "This article describes an extension of classical $\chi$2
              goodness-of-fit tests to Bayesian model assessment. The
              extension, which essentially involves evaluating Pearson’s
              goodness-of-fit statistic at a parameter value drawn from its
              posterior distribution, has the important property that it is
              asymptotically distributed as a $\chi$2 random variable on K−1
              degrees of freedom, independently of the dimension of the
              underlying parameter vector. By examining the posterior
              distribution of this statistic, global goodness-of-fit
              diagnostics are obtained. Advantages of these diagnostics include
              ease of interpretation, computational convenience and favorable
              power properties. The proposed diagnostics can be used to assess
              the adequacy of a broad class of Bayesian models, essentially
              requiring only a finite-dimensional parameter vector and
              conditionally independent observations.",
  journal  = "Ann. Stat.",
  volume   =  32,
  number   =  6,
  pages    = "2361--2384",
  month    =  dec,
  year     =  2004,
  keywords = "Bayes factor; Bayesian model assessment; Discrepancy functions;
              Intrinsic Bayes factor; Pearson's chi-squared statistic;
              Posterior-predictive diagnostics;
              p-value;trust\_informal\_treatment;assurance\_implicit;Mendeley
              Import (Jan 17)/Assurances",
  language = "en"
}

@INPROCEEDINGS{Costa2012-fa,
  title     = "Towards unbiased evaluation of uncertainty reasoning: The
               {URREF} ontology",
  booktitle = "2012 15th International Conference on Information Fusion",
  author    = "Costa, P C G and Laskey, K B and Blasch, E and Jousselme, A L",
  abstract  = "Current advances in technology, sensor collection, data storage,
               and data distribution have afforded more complex, distributed,
               and operational information fusion systems (IFSs). IFSs
               notionally consist of low-level (data collection, registration,
               and association in time and space) and high-level information
               fusion (user coordination, situational awareness, and mission
               control), which require a common ontology for effective
               communication and data processing. In this paper, we describe
               the ontology reference model developed as part of the
               uncertainty representation and reasoning evaluation framework
               (URREF). The URREF ontology is intended to provide guidance for
               defining the actual concepts and criteria that together comprise
               the comprehensive uncertainty evaluation framework being
               developed by the Evaluation of Technologies for Uncertainty
               Representation Working Group (ETURWG).",
  pages     = "2301--2308",
  month     =  jul,
  year      =  2012,
  keywords  = "component; e; evaluation; i; information fusion; knowledge
               representation; level 1 fusion; measures of effectiveness;
               ontology; performance; report updates; situation and threat;
               uncertainty reasoning; ontologies (artificial intelligence);
               sensor fusion; Evaluation of Technologies for Uncertainty
               Representation Working Group ETURWG; IFS; URREF ontology;
               comprehensive uncertainty evaluation framework; data collection;
               data distribution; data processing; data storage; high-level
               information fusion; information fusion systems; mission control;
               ontology reference model; reasoning evaluation framework;
               registration; sensor collection; situational awareness; unbiased
               evaluation; uncertainty representation; user coordination;
               Accuracy; Cognition; Measurement; Ontologies; Quality of
               service; Standards; Uncertainty; Performance
               Evaluation;Measurement;Ontologies;URREF
               ontology;Uncertainty;information fusion;ontologies (artificial
               intelligence);sensor
               fusion;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Yuan2012-tb,
  title       = "Goodness-of-fit diagnostics for Bayesian hierarchical models",
  author      = "Yuan, Ying and Johnson, Valen E",
  affiliation = "Department of Biostatistics, The University of Texas MD
                 Anderson Cancer Center, Houston, Texas 77030, USA.",
  abstract    = "This article proposes methodology for assessing goodness of
                 fit in Bayesian hierarchical models. The methodology is based
                 on comparing values of pivotal discrepancy measures (PDMs),
                 computed using parameter values drawn from the posterior
                 distribution, to known reference distributions. Because the
                 resulting diagnostics can be calculated from standard output
                 of Markov chain Monte Carlo algorithms, their computational
                 costs are minimal. Several simulation studies are provided,
                 each of which suggests that diagnostics based on PDMs have
                 higher statistical power than comparable posterior-predictive
                 diagnostic checks in detecting model departures. The proposed
                 methodology is illustrated in a clinical application; an
                 application to discrete data is described in supplementary
                 material.",
  journal     = "Biometrics",
  volume      =  68,
  number      =  1,
  pages       = "156--164",
  month       =  mar,
  year        =  2012,
  keywords    = "Discrepancy measures; Markov chain Monte Carlo; Model
                 checking; Model criticism; Model hierarchy;
                 Posterior-predictive
                 density;NotRead;trust\_informal\_treatment;assurance\_implicit;Mendeley
                 Import (Jan 17)/Assurances",
  language    = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Spiegelhalter2002-ia,
  title     = "Bayesian measures of model complexity and fit",
  author    = "Spiegelhalter, David J and Best, Nicola G and Carlin, Bradley P
               and Van Der Linde, Angelika",
  abstract  = "Summary. We consider the problem of comparing complex
               hierarchical models in which the number of parameters is not
               clearly defined. Using an information theoretic argument we
               derive a measure pD for the effective number of parameters in a
               model as the difference between the posterior mean of the
               deviance and the deviance at the posterior means of the
               parameters of interest. In general pD approximately corresponds
               to the trace of the product of Fisher's information and the
               posterior covariance, which in normal models is the trace of the
               ‘hat’ matrix projecting observations onto fitted values. Its
               properties in exponential families are explored. The posterior
               mean deviance is suggested as a Bayesian measure of fit or
               adequacy, and the contributions of individual observations to
               the fit and complexity can give rise to a diagnostic plot of
               deviance residuals against leverages. Adding pD to the posterior
               mean deviance gives a deviance information criterion for
               comparing models, which is related to other information criteria
               and has an approximate decision theoretic justification. The
               procedure is illustrated in some examples, and comparisons are
               drawn with alternative Bayesian and classical proposals.
               Throughout it is emphasized that the quantities required are
               trivial to compute in a Markov chain Monte Carlo analysis.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Blackwell Publishers",
  volume    =  64,
  number    =  4,
  pages     = "583--639",
  month     =  "1~" # oct,
  year      =  2002,
  keywords  = "Bayesian model comparison; Decision theory; Deviance information
               criterion; Effective number of parameters; Hierarchical models;
               Information theory; Leverage; Markov chain Monte Carlo methods;
               Model dimension;Decision theory;Information
               theory;NotRead;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@ARTICLE{Yanjie_Li2011-jj,
  title    = "Partially Observable Markov Decision Processes and Performance
              Sensitivity Analysis",
  author   = "{Yanjie Li} and {Baoqun Yin} and {Hongsheng Xi}",
  abstract = "The sensitivity-based optimization of Markov systems has become
              an increasingly important area. From the perspective of
              performance sensitivity analysis, policy-iteration algorithms and
              gradient estimation methods can be directly obtained for Markov
              decision processes (MDPs). In this correspondence, the
              sensitivity-based optimization is extended to average reward
              partially observable MDPs (POMDPs). We derive the
              performance-difference and performance-derivative formulas of
              POMDPs. On the basis of the performance-derivative formula, we
              present a new method to estimate the performance gradients. From
              the performance-difference formula, we obtain a sufficient
              optimality condition without the discounted reward formulation.
              We also propose a policy-iteration algorithm to obtain a nearly
              optimal finite-state-controller policy.",
  journal  = "IEEE Trans. Syst. Man Cybern. B Cybern.",
  volume   =  38,
  number   =  6,
  pages    = "1645--1651",
  month    =  dec,
  year     =  2011,
  keywords = "Markov processes;estimation theory;optimisation;sensitivity
              analysis;meh..;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Vo2008-za,
  title     = "Random finite sets in multi-object filtering",
  author    = "Vo, B T",
  abstract  = "Abstract THE multi-object filtering problem is a logical and
               fundamental generalization of the ubiquitous single-object
               vector filtering problem. Multi-object filtering essentially
               concerns the joint detection and estimation of the unknown and
               time-varying number of objects present, and the dynamic state of
               each of these objects, given a sequence of observation sets.
               This problem is intrinsically challenging because, given an
               observation set, there is ...",
  journal   = "IEEE Trans. Aerosp. Electron. Syst.",
  publisher = "Citeseer",
  number    = "October",
  pages     = "254",
  year      =  2008,
  keywords  = "Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Ahmed2014-pt,
  title       = "Statistical modelling of networked human-automation
                 performance using working memory capacity",
  author      = "Ahmed, Nisar and de Visser, Ewart and Shaw, Tyler and
                 Mohamed-Ameen, Amira and Campbell, Mark and Parasuraman, Raja",
  affiliation = "a Autonomous Systems Laboratory, Department of Mechanical and
                 Aerospace Engineering , Cornell University , 155 Rhodes Hall,
                 Ithaca , NY 14853 , USA.",
  abstract    = "This study examines the challenging problem of modelling the
                 interaction between individual attentional limitations and
                 decision-making performance in networked human-automation
                 system tasks. Analysis of real experimental data from a task
                 involving networked supervision of multiple unmanned aerial
                 vehicles by human participants shows that both task load and
                 network message quality affect performance, but that these
                 effects are modulated by individual differences in working
                 memory (WM) capacity. These insights were used to assess three
                 statistical approaches for modelling and making predictions
                 with real experimental networked supervisory performance data:
                 classical linear regression, non-parametric Gaussian processes
                 and probabilistic Bayesian networks. It is shown that each of
                 these approaches can help designers of networked
                 human-automated systems cope with various uncertainties in
                 order to accommodate future users by linking expected
                 operating conditions and performance from real experimental
                 data to observable cognitive traits like WM capacity.
                 Practitioner Summary: Working memory (WM) capacity helps
                 account for inter-individual variability in operator
                 performance in networked unmanned aerial vehicle supervisory
                 tasks. This is useful for reliable performance prediction near
                 experimental conditions via linear models; robust statistical
                 prediction beyond experimental conditions via Gaussian process
                 models and probabilistic inference about unknown task
                 conditions/WM capacities via Bayesian network models.",
  journal     = "Ergonomics",
  volume      =  57,
  number      =  3,
  pages       = "295--318",
  year        =  2014,
  keywords    = "automation;Mendeley Import (Jan 17)/WeeklyReading",
  language    = "en"
}

@ARTICLE{Bahl1974-za,
  title    = "284 {IEEE} {TRANSACTTONS} {ON} {INFORMATION} {THEORY}, {MARCH}
              1974",
  author   = "Bahl, L R and Cocke, J and Jelinek, F and Raviv, J",
  year     =  1974,
  keywords = "Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Khaleghi2013-io,
  title    = "Multisensor data fusion: A review of the state-of-the-art",
  author   = "Khaleghi, Bahador and Khamis, Alaa and Karray, Fakhreddine O and
              Razavi, Saiedeh N",
  abstract = "There has been an ever-increasing interest in multi-disciplinary
              research on multisensor data fusion technology, driven by its
              versatility and diverse areas of application. Therefore, there
              seems to be a real need for an analytical review of recent
              developments in the data fusion domain. This paper proposes a
              comprehensive review of the data fusion state of the art,
              exploring its conceptualizations, benefits, and challenging
              aspects, as well as existing methodologies. In addition, several
              future directions of research in the data fusion community are
              highlighted and described.",
  journal  = "Inf. Fusion",
  volume   =  14,
  number   =  1,
  pages    = "28--44",
  year     =  2013,
  keywords = "multisensor data fusion; Taxonomy; Fusion methodologies;Mendeley
              Import (Jan 17)/WeeklyReading"
}

@BOOK{Bourgault2005-fj,
  title     = "Decentralized Control in a Bayesian World",
  author    = "Bourgault, Fr{\'e}d{\'e}ric",
  publisher = "University of Sydney",
  year      =  2005,
  keywords  = "Mendeley Import (Jan 17)/WeeklyReading",
  language  = "en"
}

@INPROCEEDINGS{Wang2014-cv,
  title     = "Using humans as sensors: An estimation-theoretic perspective",
  booktitle = "{IPSN-14} Proceedings of the 13th International Symposium on
               Information Processing in Sensor Networks",
  author    = "Wang, D and Amin, M T and Li, S and Abdelzaher, T and Kaplan, L
               and Gu, S and Pan, C and Liu, H and Aggarwal, C C and Ganti, R
               and Wang, X and Mohapatra, P and Szymanski, B and Le, H",
  abstract  = "The explosive growth in social network content suggests that the
               largest ``sensor network'' yet might be human. Extending the
               participatory sensing model, this paper explores the prospect of
               utilizing social networks as sensor networks, which gives rise
               to an interesting reliable sensing problem. In this problem,
               individuals are represented by sensors (data sources) who
               occasionally make observations about the physical world. These
               observations may be true or false, and hence are viewed as
               binary claims. The reliable sensing problem is to determine the
               correctness of reported observations. From a networked sensing
               standpoint, what makes this sensing problem formulation
               different is that, in the case of human participants, not only
               is the reliability of sources usually unknown but also the
               original data provenance may be uncertain. Individuals may
               report observations made by others as their own. The
               contribution of this paper lies in developing a model that
               considers the impact of such information sharing on the
               analytical foundations of reliable sensing, and embed it into a
               tool called Apollo that uses Twitter as a ``sensor network'' for
               observing events in the physical world. Evaluation, using
               Twitter-based case-studies, shows good correspondence between
               observations deemed correct by Apollo and ground truth.",
  publisher = "IEEE Press",
  pages     = "35--46",
  month     =  apr,
  year      =  2014,
  keywords  = "data reliability; expectation maximization; humans as sensors;
               maximum likelihood estimation; social sensing; uncertain data
               provenance; Internet; estimation theory; sensors; social
               networking (online); Apollo; Twitter-based case-studies;
               estimation-theoretic perspective; humans; information sharing;
               largest sensor network; networked sensing standpoint;
               participatory sensing model; reliable sensing problem; sensing
               problem formulation; social network content; Computer network
               reliability; Reliability; Silicon; Twitter;Mendeley Import (Jan
               17)/WeeklyReading"
}

@INPROCEEDINGS{Bousquet2003-xk,
  title     = "Introduction to Statistical Learning Theory",
  booktitle = "Advanced Lectures on Machine Learning",
  author    = "Bousquet, Olivier and Boucheron, St{\'e}phane and Lugosi,
               G{\'a}bor",
  publisher = "Springer",
  volume    =  3176,
  pages     = "169--207",
  year      =  2003,
  keywords  = "kernel; learning theory;Mendeley Import (Jan 17)/WeeklyReading"
}

@INPROCEEDINGS{Amin2014-rz,
  title     = "{Crowd-Sensing} with Polarized Sources",
  booktitle = "2014 {IEEE} International Conference on Distributed Computing in
               Sensor Systems",
  author    = "Amin, M T A and Abdelzaher, T and Wang, D and Szymanski, B",
  abstract  = "The paper presents a new model for crowd-sensing applications,
               where humans are used as the sensing sources to report
               information regarding the physical world. In contrast to
               previous work on the topic, we consider a model where the
               sources in question are polarized. Such might be the case, for
               example, in political disputes and in situations involving
               different communities with largely dissimilar beliefs that color
               their interpretation and reporting of physical world events.
               Reconstructing accurate ground truth is more complicated when
               sources are polarized. The paper describes an algorithm that
               significantly improves the quality of reconstruction results in
               the presence of polarized sources. For evaluation, we recorded
               human observations from Twitter for four months during a recent
               Egyptian uprising against the former president. We then used our
               algorithm to reconstruct a version of events and compared it to
               other versions produced by state of the art algorithms. Our
               analysis of the data set shows the presence of two clearly
               defined camps in the social network that tend of propagate
               largely disjoint sets of claims (which is indicative of
               polarization), as well as third population whose claims overlap
               subsets of the former two. Experiments show that, in the
               presence of polarization, our reconstruction tends to align more
               closely with ground truth in the physical world than the
               existing algorithms.",
  publisher = "IEEE",
  pages     = "67--74",
  month     =  may,
  year      =  2014,
  keywords  = "Community Polarization; Crowd-sensing; Fact-finders; Social
               Networks; information retrieval; social networking (online);
               Egyptian uprising; Twitter; crowd-sensing applications; ground
               truth reconstruction; information reporting; polarized sources;
               social network; Algorithm design and analysis; Estimation;
               Reliability; Sensors; Vectors;Mendeley Import (Jan
               17)/WeeklyReading"
}

@ARTICLE{Shahriari2015-jc,
  title         = "Unbounded Bayesian Optimization via Regularization",
  author        = "Shahriari, Bobak and Bouchard-C{\^o}t{\'e}, Alexandre and de
                   Freitas, Nando",
  abstract      = "Bayesian optimization has recently emerged as a popular and
                   efficient tool for global optimization and hyperparameter
                   tuning. Currently, the established Bayesian optimization
                   practice requires a user-defined bounding box which is
                   assumed to contain the optimizer. However, when little is
                   known about the probed objective function, it can be
                   difficult to prescribe such bounds. In this work we modify
                   the standard Bayesian optimization framework in a principled
                   way to allow automatic resizing of the search space. We
                   introduce two alternative methods and compare them on two
                   common synthetic benchmarking test functions as well as the
                   tasks of tuning the stochastic gradient descent optimizer of
                   a multi-layered perceptron and a convolutional neural
                   network on MNIST.",
  pages         = "9",
  month         =  "14~" # aug,
  year          =  2015,
  keywords      = "Folder - NIPS2015;Mendeley Import (Jan 17)/WeeklyReading",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1508.03666"
}

@PHDTHESIS{Lindsey2014-ld,
  title    = "Probabilistic Models of Student Learning and Forgetting",
  author   = "Lindsey, Robert Victor",
  abstract = "This thesis uses statistical machine learning techniques to
              construct predictive models of human learning and to improve
              human learning by discovering optimal teaching methodologies. In
              Chapters 2 and 3, I present and evaluate models for predicting
              the changing memory strength of material being studied over time.
              The models combine a psychological theory of memory with Bayesian
              methods for inferring individual differences. In Chapter 4, I
              develop methods for delivering efficient, systematic,
              personalized review using the statistical models. Results are
              presented from three large semester-long experiments with middle
              school students which demonstrate how this ``big data'' approach
              to education yields substantial gains in the long-term retention
              of course material. In Chapter 5, I focus on optimizing various
              aspects of instruction for populations of students. This involves
              a novel experimental paradigm which combines Bayesian
              nonparametric modeling techniques and probabilistic generative
              models of student performance. In Chapters 6 and 7, I present
              supporting laboratory behavioral studies and theoretical
              analyses. These include an examination of the relationship
              between study format and the testing effect, and a parsimonious
              theoretical account of long-term recency effects.",
  year     =  2014,
  school   = "University of Colorado, Boulder",
  keywords = "Mendeley Import (Jan 17)/WeeklyReading"
}

@INPROCEEDINGS{Blasch2002-qt,
  title     = "{JDL} level 5 fusion model: user refinement issues and
               applications in group tracking",
  booktitle = "{AeroSense} 2002",
  author    = "Blasch, Erik P and Plano, Susan",
  abstract  = "The 1999 Joint Director of Labs (JDL) revised model incorporates
               five levels for fusion methodologies including level 0 for
               preprocessing, level 1 for object refinement, level 2 for
               situation refinement, level 3 for threat refinement, and level 4
               for process refinement. The model was developed to define the
               fusion process. However, the model is only for automatic
               processing of a machine and does not account for human
               processing. Typically, a fusion architecture supports a user and
               thus, we propose a Level 5 User refinement to delineate the
               human from the machine in the process refinement. Typical human
               in the loop models do not deal with a machine fusion process,
               but only present the information to the human on a display. We
               seek to address issues for designing a fusion system which
               supports a user: trust, workload, attention and situation
               awareness. In this paper, we overview the need for a Level 5,
               the issues concerning the human for realizable fusion
               architectures, and examples where the human is instrumental in
               the fusion process such as group tracking.",
  publisher = "International Society for Optics and Photonics",
  volume    =  4729,
  pages     = "270--279",
  month     =  "31~" # jul,
  year      =  2002,
  keywords  = "Group Tracking; HCI; HMI; Identity; Information Fusion;
               attention; situation awareness; trust; workload;Mendeley Import
               (Jan 17)/WeeklyReading"
}

@ARTICLE{Kaupp2007-nc,
  title     = "Shared environment representation for a human-robot team
               performing information fusion",
  author    = "Kaupp, Tobias and Douillard, Bertrand and Ramos, Fabio and
               Makarenko, Alexei and Upcroft, Ben",
  abstract  = "This paper addresses the problem of building a shared
               environment representation by a human-robot team. Rich
               environment models are required in real applications for both
               autonomous operation of robots and to support human
               decision-making. Two probabilistic models are used to describe
               outdoor environment features such as trees: geometric (position
               in the world) and visual. The visual representation is used to
               improve data association and to classify features. Both models
               are able to incorporate observations from robotic platforms and
               human operators. Physically, humans and robots form a
               heterogeneous sensor network. In our experiments, the
               human-robot team consists of an unmanned air vehicle, a ground
               vehicle, and two human operators. They are deployed for an
               information gathering task and perform information fusion
               cooperatively. All aspects of the system including the fusion
               algorithms are fully decentralized. Experimental results are
               presented in form of the acquired multi-attribute feature map,
               information exchange patterns demonstrating human-robot
               information fusion, and quantitative model evaluation. Learned
               lessons from deploying the system in the field are also
               presented. \copyright{} 2007 Wiley Periodicals, Inc.",
  journal   = "J. Field Robotics",
  publisher = "Wiley Subscription Services, Inc., A Wiley Company",
  volume    =  24,
  number    = "11-12",
  pages     = "911--942",
  month     =  "1~" # nov,
  year      =  2007,
  keywords  = "Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Lember2014-pa,
  title     = "Bridging Viterbi and Posterior Decoding: A Generalized Risk
               Approach to Hidden Path Inference Based on Hidden Markov Models",
  author    = "Lember, J{\"u}ri and Koloydenko, Alexey A",
  abstract  = "Motivated by the unceasing interest in hidden Markov models
               (HMMs), this paper re-examines hidden path inference in these
               models, using primarily a risk-based framework. While the most
               common maximum a posteriori (MAP), or Viterbi, path estimator
               and the minimum error, or Posterior Decoder (PD) have long been
               around, other path estimators, or decoders, have been either
               only hinted at or applied more recently and in dedicated
               applications generally unfamiliar to the statistical learning
               community. Over a decade ago, however, a family of
               algorithmically defined decoders aiming to hybridize the two
               standard ones was proposed elsewhere. The present paper gives a
               careful analysis of this hybridization approach, identifies
               several problems and issues with it and other previously
               proposed approaches, and proposes practical resolutions of
               those. Furthermore, simple modifications of the classical
               criteria for hidden path recognition are shown to lead to a new
               class of decoders. Dynamic programming algorithms to compute
               these decoders in the usual forward-backward manner are
               presented. A particularly interesting subclass of such
               estimators can be also viewed as hybrids of the MAP and PD
               estimators. Similar to previously proposed MAP-PD hybrids, the
               new class is parameterized by a small number of tunable
               parameters. Unlike their algorithmic predecessors, the new
               risk-based decoders are more clearly interpretable, and, most
               importantly, work ``out-of-the box'' in practice, which is
               demonstrated on some real bioinformatics tasks and data. Some
               further generalizations and applications are discussed in the
               conclusion.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  15,
  number    =  1,
  pages     = "1--58",
  month     =  jan,
  year      =  2014,
  keywords  = "admissible path; decoder; hmm; hybrid; imum error;
               interpolation; map sequence; min-; optimal accuracy; posterior
               decoding; power transform; risk; segmental classification;
               symbol; symbol-by-; viterbi algorithm; minimum error;
               symbol-by-symbol;Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Maclaurin2015-at,
  title         = "Early Stopping is Nonparametric Variational Inference",
  author        = "Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P",
  abstract      = "We show that unconverged stochastic gradient descent can be
                   interpreted as a procedure that samples from a nonparametric
                   variational approximate posterior distribution. This
                   distribution is implicitly defined as the transformation of
                   an initial distribution by a sequence of optimization
                   updates. By tracking the change in entropy over this
                   sequence of transformations during optimization, we form a
                   scalable, unbiased estimate of the variational lower bound
                   on the log marginal likelihood. We can use this bound to
                   optimize hyperparameters instead of using cross-validation.
                   This Bayesian interpretation of SGD suggests improved,
                   overfitting-resistant optimization procedures, and gives a
                   theoretical foundation for popular tricks such as early
                   stopping and ensembling. We investigate the properties of
                   this marginal likelihood estimator on neural network models.",
  pages         = "8",
  month         =  "6~" # apr,
  year          =  2015,
  keywords      = "Folder - NIPS2015;Variatinal Inference;Mendeley Import (Jan
                   17)/WeeklyReading",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1504.01344"
}

@INPROCEEDINGS{Pravia2008-sd,
  title     = "Generation of a fundamental data set for hard/soft information
               fusion",
  booktitle = "2008 11th International Conference on Information Fusion",
  author    = "Pravia, M A and Prasanth, R K and Arambel, P O and Sidner, C and
               Chong, C Y",
  abstract  = "We report here on our effort to investigate the types of
               hard/soft information that can be realistically collected in an
               urban operational environment and to generate a data set that
               can be used for the development of hard/soft data fusion
               algorithms. Specifically, we discuss: 1) sources of ldquohard
               informationldquo (i.e. information from physics-based sources)
               and ldquosoft informationrdquo (i.e. information from
               human-based sources), 2) conceptual examples of hard/soft
               fusion, 3) data collection requirements and strategy, and 4)
               conceptual requirements for future algorithms with reference
               points in ongoing work. Specific techniques or frameworks for
               automated hard/soft fusion are beyond the scope of the effort
               being reported here. The presentation focuses on describing the
               context and requirements for hard/soft fusion at Joint Directors
               of Laboratories (JDL) Levels 1 and 2, and providing information
               about initial steps to generate a first-of-its-kind hard/soft
               data set that will serve as a foundation and a
               verification/validation resource for future research.",
  publisher = "IEEE",
  pages     = "1--8",
  month     =  jun,
  year      =  2008,
  keywords  = "HUMINT; Human-based information; information fusion; low-level
               fusion; physics-based information; sensor fusion; data fusion;
               fundamental data set;Mendeley Import (Jan 17)/WeeklyReading"
}

@INCOLLECTION{Fong2005-lf,
  title     = "The {Peer-to-Peer} {Human-Robot} Interaction Project",
  booktitle = "Space 2005",
  author    = "Fong, Terrence and Nourbakhsh, Illah and Kunz, Clayton and
               Fluckiger, Lorenzo and Schreiner, John and Ambrose, Robert and
               Burridge, Robert and Simmons, Reid and Hiatt, Laura and Schultz,
               Alan and Trafton, J Gregory and Bugajska, Magda and Scholtz,
               Jean",
  volume    =  2005,
  pages     = "1--11",
  year      =  2005,
  keywords  = "Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Kaupp2008-vb,
  title     = "Probabilistic human-robot information fusion",
  author    = "Kaupp, Tobias and {Others}",
  publisher = "University of Sydney.",
  year      =  2008,
  keywords  = "Mendeley Import (Jan 17)/WeeklyReading"
}

@INPROCEEDINGS{Kaupp2005-wt,
  title     = "Adaptive human sensor model in sensor networks",
  booktitle = "2005 7th International Conference on Information Fusion",
  author    = "Kaupp, T and Makarenko, A and Ramos, F and Upcroft, B and
               Williams, S and Durrant-Whyte, H",
  abstract  = "This paper presents the design of a probabilistic model of human
               perception as an integral part of a decentralized data fusion
               system. The system consists of a team of human operators and
               robotic platforms, together forming a heterogeneous sensor
               network. Human operators are regarded as information sources
               submitting raw observations. The observations are converted into
               a probabilistic representation suitable for fusion with the
               system's belief. The conversion is performed by a human sensor
               model (HSM). The initial HSM is built offline based on an
               average of multiple human subjects conducting a calibration
               experiment. Since individual human operators may vary in their
               performance, an online adaptation of the HSM is required. The
               network estimate is used for adaptation because the true feature
               state is unknown at runtime. Results of an outdoor calibration
               experiment using range and bearing observations are presented.
               Simulations show the feasibility of efficient online adaptation.",
  publisher = "IEEE",
  volume    =  1,
  pages     = "8 pp.--",
  month     =  jul,
  year      =  2005,
  keywords  = "Data fusion; Human-network interaction; Sensor model adaptation;
               Sensor network; User modeling; calibration; distributed sensors;
               humanoid robots; multivariable systems; probability; sensor
               fusion; visual perception; HSM; adaptive human sensor model;
               decentralized data fusion system; heterogenous sensor network;
               human perception; information source; online adaptation; outdoor
               calibration; probabilistic model; robotic platform; Adaptation
               model; Humans; Intelligent networks; Robot sensing systems;
               Robotics and automation; Sensor phenomena and characterization;
               State estimation; Tin;Mendeley Import (Jan 17)/WeeklyReading"
}

@MISC{Mackay1997-vb,
  title    = "Ensemble learning for hidden Markov models",
  author   = "Mackay, D J C",
  abstract = "The standard method for training Hidden Markov Models optimizes a
              point estimate of the model parameters. this estimate, which can
              be viewed as the maximum of a posterior probability density over
              the model parameters, may be susceptible to overfitting, and
              contains no indication of parameter uncertainty. Also, this
              maximum may be unrepresentative of the posterior probability
              distribution. In this paper we study a method in which we
              optimize an ensemble which approximates the entire posterior
              probability distribution. This ensemble learning algorithm
              requires the same resources as the traidition Baum-Welch
              algorithm.",
  journal  = "Technical report, Cavendish Laboratory, University of Cambridge",
  year     =  1997,
  keywords = "Mendeley Import (Jan 17)/WeeklyReading"
}

@INPROCEEDINGS{Lewis2009-ew,
  title     = "Using humans as sensors in robotic search",
  booktitle = "2009 12th International Conference on Information Fusion",
  author    = "Lewis, M and Wang, H and Velagapudi, P and Scerri, P and Sycara,
               K",
  abstract  = "The human role in sophisticated information gathering systems is
               usually conceived to be that of the consumer. Human sensory and
               perceptual capabilities, however, outstrip our abilities to
               process information by a substantial amount. The use of human
               operators as ldquoperceptual sensorsrdquo is standard practice
               for both UAVs and ground robotics where humans are called upon
               to ldquoprocessrdquo camera video to find targets and assist in
               navigation. In this paper we illustrate the human role as sensor
               referencing results of an earlier experiment investigating human
               performance of operator, navigator, or ldquoperceptual
               sensorrdquo tasks for teams of 4, 8, and 12 simulated pioneer
               P3AT robots. The experiment shows humans to be resource limited
               for the navigation/control task as the number of robots
               increases while the perceptual sensor function was less
               affected. We discuss the implications of using humans in a
               ldquoperceptual sensorrdquo role for information gathering from
               robotic teams and some of the difficulties including shifts in
               context and difficulties in developing situation awareness that
               are likely to arise.",
  publisher = "IEEE",
  pages     = "1249--1256",
  month     =  jul,
  year      =  2009,
  keywords  = "Human roles; Human-robot interaction; Search and rescue; human
               factors; information retrieval systems; natural language
               processing; human operators; information gathering systems;
               navigator; perceptual sensors; robotic search; Automatic
               control; Cameras; Communication system control; Feeds; Humans;
               Layout; Mobile robots; Navigation; Robot sensing systems; Robot
               vision systems;Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{An2011-ft,
  title       = "[Effect of oral rinse with Turkish gall on the clinical
                 periodontal parameters and halitosis]",
  author      = "An, Yue Bang and He, Lu and Meng, Huan Xin and Liu, Ting Ting
                 and Yan, Feng Hua",
  affiliation = "Department of Periodontology, Peking University School and
                 Hospital of Stomatology, Beijing 100081, China.",
  abstract    = "OBJECTIVE: To assess the effect of traditional Chinese
                 medicine, oral rinse of Turkish gall (extracts from Turkish
                 gall) on periodontal and halitosis parameters. METHODS: This
                 single-blinded randomized controlled trial recruited 70
                 patients with gingivitis or mild to moderate chronic
                 periodontitis. All the patients received the same professional
                 oral hygiene instruction and using toothbrush and
                 fluoride-only paste with same brand and size dispatched by the
                 investigator from baseline to the end. The tested group by
                 oral rinse of Turkish gall on the base of conventional oral
                 hygiene, and control group with conventional oral hygiene
                 only, were assigned randomly at baseline. Then periodontal
                 parameters including Plaque index (PlI), Bleeding index (BI),
                 Probing depth (PD) and Staining index (SI), halitosis index
                 including volatile sulphur compounds (VSCs) by Halimeter,
                 organoleptic score (OS), area of tongue coating (Ta),
                 thickness of tongue coating (Tt), were measured at baseline
                 and two-weeks after. RESULTS: After two weeks, all the
                 periodontal parameters except for SI were significantly
                 improved within each group (P<0.001). As compared to those in
                 control group, the patients in tested group only exhibited
                 significant improvement of PlI after two-week-use of oral
                 rinse of Turkish gall (P<0.05). Though values of OS and Tt in
                 tested group, OS only in control group, decreased
                 significantly (P<0.05) respectively after two weeks, there
                 were no statistical difference on the improvement of halitosis
                 parameters. CONCLUSION: For untreated patients with gingivitis
                 and periodontitis, oral rinse of Turkish gall displayed
                 significant inhibition of dental plaque, while no predominated
                 effect on halitosis when compared to correct conventional
                 plaque control methods.",
  journal     = "Beijing Da Xue Xue Bao",
  volume      =  43,
  number      =  1,
  pages       = "22--25",
  month       =  "18~" # feb,
  year        =  2011,
  keywords    = "Automatic control; Biomedical monitoring; Computerized
                 monitoring; Condition monitoring; Department of Defense; DoD;
                 Intelligent vehicles; Mobile robots; Navigation; Remotely
                 operated vehicles; Surveillance; Target recognition; aerospace
                 computing; artificial intelligence; automated target
                 recognition; autonomous vehicles; battlefield surveillance;
                 complex machinery; computerised instrumentation; control;
                 emerging technology; guidance; identification; knowledge based
                 methods; knowledge based systems; medical diagnosis; military
                 computing; military systems; multisensor data fusion;
                 nonlinearities; pattern recognition; process models; sensor
                 fusion; smart buildings; statistical estimation;Mendeley
                 Import (Jan 17)/WeeklyReading",
  language    = "zh"
}

@ARTICLE{Bruemmer2005-eu,
  title    = "Shared understanding for collaborative control",
  author   = "Bruemmer, D J and Few, D A and Boring, R L and Marble, J L and
              Walton, M C and Nielsen, C W",
  abstract = "This paper presents results from three experiments in which human
              operators were teamed with a mixed-initiative robot control
              system to accomplish various indoor search and exploration tasks.
              By assessing human workload and error together with overall
              performance, these experiments provide an objective means to
              contrast different modes of robot autonomy and to evaluate both
              the usability of the interface and the effectiveness of
              autonomous robot behavior. The first experiment compares the
              performance achieved when the robot takes initiative to support
              human driving with the opposite case when the human takes
              initiative to support autonomous robot driving. The utility of
              robot autonomy is shown through achievement of better performance
              when the robot is in the driver's seat. The second experiment
              introduces a virtual three-dimensional (3-D) map representation
              that supports collaborative understanding of the task and
              environment. When used in place of video, the 3-D map reduced
              operator workload and navigational error. By lowering bandwidth
              requirements, use of the virtual 3-D interface enables
              long-range, nonline-of-sight communication. Results from the
              third experiment extend the findings of experiment 1 by showing
              that collaborative control can increase performance and reduce
              error even when the complexity of the environment is increased
              and workload is distributed amongst multiple operators.",
  journal  = "IEEE Transactions on Systems, Man, and Cybernetics - Part A:
              Systems and Humans",
  volume   =  35,
  number   =  4,
  pages    = "494--504",
  month    =  jul,
  year     =  2005,
  keywords = "Dynamic autonomy; Human-robot interaction (HRI); Mixed
              initiative; Shared control; intelligent robots; man-machine
              systems; mobile robots; user interfaces; autonomous robot
              behavior; autonomous robot driving; collaborative control;
              mixed-initiative robot control system; shared understanding;
              virtual 3D interface; Bandwidth; Cognitive robotics;
              Collaboration; Collaborative work; Communication system control;
              Humans; Navigation; Robot control; Usability; human--robot
              interaction (HRI);Mendeley Import (Jan 17)/WeeklyReading"
}

@INPROCEEDINGS{Taguchi2009-ea,
  title     = "Identification of Probability weighted multiple {ARX} models and
               its application to behavior analysis",
  booktitle = "Proceedings of the 48h {IEEE} Conference on Decision and Control
               ({CDC}) held jointly with 2009 28th Chinese Control Conference",
  author    = "Taguchi, S and Suzuki, T and Hayakawa, S and Inagaki, S",
  abstract  = "This paper proposes a probability weighted ARX (PrARX) model
               wherein the multiple ARX models are composed by the
               probabilistic weighting functions. As the probabilistic
               weighting function, a `softmax' function is introduced. Then,
               the parameter estimation problem for the proposed model is
               formulated as a single optimization problem. Furthermore, the
               identified PrARX model can be easily transformed to the
               corresponding PWARX model with complete partitions between
               regions. Finally, the proposed model is applied to the modeling
               of the driving behavior, and the usefulness of the model is
               verified and discussed.",
  publisher = "IEEE",
  pages     = "3952--3957",
  month     =  dec,
  year      =  2009,
  keywords  = "autoregressive processes;optimisation;probability;PrARX
               model;behavior analysis;multiple ARX model;optimization
               problem;parameter estimation problem;probabilistic weighting
               function;softmax function;Control system synthesis;Decision
               making;Entropy;Humans;Mathematical model;Motion
               control;Parameter estimation;Partitioning algorithms;Stochastic
               processes;System identification;Mendeley Import (Jan
               17)/WeeklyReading"
}

@INPROCEEDINGS{Lambert2003-xf,
  title     = "Grand challenges of information fusion",
  booktitle = "Proceedings of the Sixth International Conference on Information
               Fusion",
  author    = "Lambert, Dale A",
  volume    =  1,
  pages     = "213--220",
  year      =  2003,
  keywords  = "data fusion; data fusion system; information fusion; interface
               data fusion; jdl model; sensor fusion; situation awareness;
               system extends well beyond; the focal data fusion; $\lambda$ jdl
               model;Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Heikkila2012-zw,
  title     = "Affordance-based indirect task communication for astronaut-robot
               cooperation",
  author    = "Heikkil{\"a}, Seppo S and Halme, Aarne and Schiele, Andr{\'e}",
  abstract  = "The problem with human--robot task communication is that robots
               cannot understand complex human speech, whereas humans cannot
               efficiently use the fixed task request utterances required by
               robots. However, future planetary exploration missions will
               require astronauts on extravehicular activities to communicate
               task requests to robot assistants by using speech- and
               gesture-type user interfaces that can be easily embedded in
               their space suits. The solution proposed in this paper is
               indirect task communication based on the humanlike ability to
               utilize object--action relationships in task communication. A
               conventional task communication method, in which the astronaut
               needs to communicate all the task parameters explicitly, is
               compared with communication methods where affordances, i.e.,
               action possibilities, are used to complete the task
               communication. This comparison is done with three user
               experiments: one performed with a fully autonomous centauroid
               robot in a geological exploration work context and two with a
               simulated robot in a lander assembly work context. The
               experiments indicate that affordance-based indirect task
               communication methods can be used to decrease both the human
               workload and the task communication times in a planetary
               exploration type of work context, and that combined direct and
               indirect task communication methods seem to be preferable from a
               human point of view. \copyright{} 2012 Wiley Periodicals, Inc.",
  journal   = "J. Field Robotics",
  publisher = "Wiley Subscription Services, Inc., A Wiley Company",
  volume    =  29,
  number    =  4,
  pages     = "576--600",
  month     =  "1~" # jul,
  year      =  2012,
  keywords  = "Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Sellner2006-co,
  title    = "Coordinated Multiagent Teams and Sliding Autonomy for
              {Large-Scale} Assembly",
  author   = "Sellner, B and Heger, F W and Hiatt, L M and Simmons, R and
              Singh, S",
  abstract = "Recent research in human-robot interaction has investigated the
              concept of Sliding, or Adjustable, Autonomy, a mode of operation
              bridging the gap between explicit teleoperation and complete
              robot autonomy. This work has largely been in single-agent
              domains-involving only one human and one robot-and has not
              examined the issues that arise in multiagent domains. We discuss
              the issues involved in adapting Sliding Autonomy concepts to
              coordinated multiagent teams. In our approach, remote human
              operators have the ability to join, or leave, the team at will to
              assist the autonomous agents with their tasks (or aspects of
              their tasks) while not disrupting the team's coordination. Agents
              model their own and the human operator's performance on subtasks
              to enable them to determine when to request help from the
              operator. To validate our approach, we present the results of two
              experiments. The first evaluates the human/multirobot team's
              performance under four different collaboration strategies
              including complete teleoperation, pure autonomy, and two distinct
              versions of Sliding Autonomy. The second experiment compares a
              variety of user interface configurations to investigate how
              quickly a human operator can attain situational awareness when
              asked to help. The results of these studies support our belief
              that by incorporating a remote human operator into multiagent
              teams, the team as a whole becomes more robust and efficient",
  journal  = "Proc. IEEE",
  volume   =  94,
  number   =  7,
  pages    = "1425--1444",
  month    =  jul,
  year     =  2006,
  keywords = "Architectures; Assembly; Autonomy; Experiments; Human-robot
              interaction; Multiagent systems; Sliding autonomy; Teleoperation;
              User modeling; man-machine systems; mobile robots; multi-robot
              systems; telerobotics; autonomous robots; multiagent team
              coordination; robot team; Autonomous agents; Computer science;
              Human robot interaction; Large-scale systems; Orbital robotics;
              Robot kinematics; Robot sensing systems; Robotic assembly;
              Robustness; Solar energy; human--robot interaction;Mendeley
              Import (Jan 17)/WeeklyReading"
}

@INPROCEEDINGS{Jones2009-cs,
  title     = "Incorporating the human analyst into the data fusion process by
               modeling situation awareness using fuzzy cognitive maps",
  booktitle = "2009 12th International Conference on Information Fusion",
  author    = "Jones, R E T and Connors, E S and Endsley, M R",
  abstract  = "Current data fusion models lack the capability of fully
               supporting the cognitive processes of the human analyst. The
               data fusion community has expressed a need to better incorporate
               users within their models. The purpose of this paper is to
               describe how to use fuzzy logic to develop a data fusion model
               that supports situation awareness (SA). Developing this model
               based on the formal representation of the analyst provided by
               the goal-directed task analysis (GDTA) methodology advances
               current data fusion models because it provides valuable insight
               on how to effectively support human cognition within the data
               fusion process.",
  publisher = "IEEE",
  pages     = "1265--1271",
  month     =  jul,
  year      =  2009,
  keywords  = "1; Fuzzy Logic; Fuzzy sets; High level fusion; Situation
               assessment; and; and human cognitive processes; and projection;
               are not fully supported; comprehension; fuzzy cognitive maps;
               high level fusion; level 2; level 3 sa processes; situation
               awareness; inference mechanisms; sensor fusion; data fusion
               process; goal-directed task analysis; human analyst; human
               cognition; Automation; Cognition; Data analysis; Data
               engineering; Face; Humans; Information analysis; Surface
               acoustic waves; human cognitive processes;Mendeley Import (Jan
               17)/WeeklyReading"
}

@ARTICLE{Kall2005-ml,
  title       = "An {HMM} posterior decoder for sequence feature prediction
                 that includes homology information",
  author      = "K{\"a}ll, Lukas and Krogh, Anders and Sonnhammer, Erik L L",
  affiliation = "Center for Genomics and Bioinformatics, Karolinska Institutet
                 SE-17 177 Stockholm, Sweden.",
  abstract    = "MOTIVATION: When predicting sequence features like
                 transmembrane topology, signal peptides, coil-coil structures,
                 protein secondary structure or genes, extra support can be
                 gained from homologs. RESULTS: We present here a general
                 hidden Markov model (HMM) decoding algorithm that combines
                 probabilities for sequence features of homologs by considering
                 the average of the posterior label probability of each
                 position in a global sequence alignment. The algorithm is an
                 extension of the previously described 'optimal accuracy'
                 decoder, allowing homology information to be used. It was
                 benchmarked using an HMM for transmembrane topology and signal
                 peptide prediction, Phobius. We found that the performance was
                 substantially increased when incorporating information from
                 homologs. AVAILABILITY: A prediction server for transmembrane
                 topology and signal peptides that uses the algorithm is
                 available at http://phobius.cgb.ki.se/poly.html. An
                 implementation of the algorithm is available on request from
                 the authors.",
  journal     = "Bioinformatics",
  volume      = "21 Suppl 1",
  number      = "SUPPL. 1",
  pages       = "i251--7",
  month       =  jun,
  year        =  2005,
  keywords    = "Mendeley Import (Jan 17)/WeeklyReading",
  language    = "en"
}

@INPROCEEDINGS{Khaleghi2010-zm,
  title     = "Random finite set theoretic based soft/hard data fusion with
               application for target tracking",
  booktitle = "2010 {IEEE} Conference on Multisensor Fusion and Integration",
  author    = "Khaleghi, B and Khamis, A and Karray, F",
  abstract  = "This paper reports on ongoing research on development of data
               fusion systems capable of processing soft as well as hard data.
               Such fusion systems are distinguished from the conventional
               systems where input data are assumed to be provided by typically
               well-characterized electronic sensor systems. The incorporation
               of soft human-generated data into fusion process is an emerging
               trend in fusion community majorly motivated by asymmetric
               warfare situations where observational opportunities for
               traditional hard sensors is restricted. Random finite set theory
               is a mathematical framework with powerful representational and
               computational abilities making it a promising approach to
               address several fundamental challenges in soft/hard fusion
               systems. In this paper the first prototype soft/hard fusion
               system based on random finite set theory is described.
               Experimental results obtained using the developed system prove
               the plausibility as well as efficiency of a random finite set
               theoretic approach to fusion of soft/hard data.",
  publisher = "IEEE",
  pages     = "50--55",
  year      =  2010,
  keywords  = "random processes;sensor fusion;set theory;target
               tracking;asymmetric warfare;electronic sensor systems;hard
               sensors;random finite set theory;soft human-generated
               data;soft-hard data fusion system;target tracking;Data
               models;Humans;Observers;Robot sensing systems;Target
               tracking;Mendeley Import (Jan 17)/WeeklyReading"
}

@INPROCEEDINGS{Bourgault2008-nq,
  title     = "Scalable Bayesian human-robot cooperation in mobile sensor
               networks",
  booktitle = "2008 {IEEE/RSJ} International Conference on Intelligent Robots
               and Systems",
  author    = "Bourgault, F and Chokshi, A and Wang, J and Shah, D and
               Schoenberg, J and Iyer, R and Cedano, F and Campbell, M",
  abstract  = "In this paper, scalable collaborative human-robot systems for
               information gathering applications are approached as a
               decentralized Bayesian sensor network problem. Human-computer
               augmented nodes and autonomous mobile sensor platforms are
               collaborating on a peer-to-peer basis by sharing information via
               wireless communication network. For each node, a computer
               (onboard the platform or carried by the human) implements both a
               decentralized Bayesian data fusion algorithm and a decentralized
               Bayesian control negotiation algorithm. The individual node
               controllers iteratively negotiate anonymously with each other in
               the information space to find cooperative search plans based on
               both observed and predicted information that explicitly consider
               the platforms (humans and robots) motion models, their sensors
               detection functions, as well as the target arbitrary motion
               model. The results of a collaborative multi-target search
               experiment conducted with a team of four autonomous mobile
               sensor platforms and five humans carrying small portable
               computers with wireless communication are presented to
               demonstrate the efficiency of the approach.",
  publisher = "IEEE",
  pages     = "2342--2349",
  year      =  2008,
  keywords  = "Bayes methods;human computer interaction;mobile
               radio;robots;sensor fusion;wireless sensor networks;autonomous
               mobile sensor platforms;collaborative multitarget search
               experiment;cooperative search plans;decentralized Bayesian
               control negotiation algorithm;decentralized Bayesian data fusion
               algorithm;decentralized Bayesian sensor network
               problem;human-computer augmented nodes;individual node
               controllers;information gathering;mobile sensor
               networks;scalable Bayesian human-robot cooperation;scalable
               collaborative human-robot systems;sensors detection
               functions;target arbitrary motion model;wireless
               communication;wireless communication network;Bayesian
               methods;Mathematical model;Mobile communication;Peer to peer
               computing;Probability density function;Robot sensing
               systems;Robots;Mendeley Import (Jan 17)/WeeklyReading"
}

@INPROCEEDINGS{Raghavan2007-we,
  title     = "When will feature feedback help? quantifying the complexity of
               classification problems",
  booktitle = "{IJCAI} Workshop on Human in the Loop Computing",
  author    = "Raghavan, Hema and Madani, Omid and Jones, Rosie",
  year      =  2007,
  keywords  = "Mendeley Import (Jan 17)/WeeklyReading"
}

@PHDTHESIS{Makarenko2004-rj,
  title     = "A Decentralised Architecture for Active Sensor Networks",
  author    = "Makarenko, A A",
  publisher = "IEEE",
  year      =  2004,
  keywords  = "Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Carvalho2008-fm,
  title       = "Centroid estimation in discrete high-dimensional spaces with
                 applications in biology",
  author      = "Carvalho, Luis E and Lawrence, Charles E",
  affiliation = "Division of Applied Mathematics, Brown University, 182 George
                 Street, Providence, RI 02912, USA.",
  abstract    = "Maximum likelihood estimators and other direct
                 optimization-based estimators dominated statistical estimation
                 and prediction for decades. Yet, the principled foundations
                 supporting their dominance do not apply to the discrete
                 high-dimensional inference problems of the 21st century. As it
                 is well known, statistical decision theory shows that maximum
                 likelihood and related estimators use data only to identify
                 the single most probable solution. Accordingly, unless this
                 one solution so dominates the immense ensemble of all
                 solutions that its probability is near one, there is no
                 principled reason to expect such an estimator to be
                 representative of the posterior-weighted ensemble of
                 solutions, and thus represent inferences drawn from the data.
                 We employ statistical decision theory to find more
                 representative estimators, centroid estimators, in a general
                 high-dimensional discrete setting by using a family of loss
                 functions with penalties that increase with the number of
                 differences in components. We show that centroid estimates are
                 obtained by maximizing the marginal probabilities of the
                 solution components for unconstrained ensembles and for an
                 important class of problems, including sequence alignment and
                 the prediction of RNA secondary structure, whose ensembles
                 contain exclusivity constraints. Three genomics examples are
                 described that show that these estimators substantially
                 improve predictions of ground-truth reference sets.",
  journal     = "Proc. Natl. Acad. Sci. U. S. A.",
  volume      =  105,
  number      =  9,
  pages       = "3209--3214",
  month       =  "4~" # mar,
  year        =  2008,
  keywords    = "Mendeley Import (Jan 17)/WeeklyReading",
  language    = "en"
}

@BOOK{Mahler2007-vz,
  title     = "Statistical Multisource-multitarget Information Fusion",
  author    = "Mahler, Ronald P S",
  abstract  = "Information fusion is the process of gathering, filtering,
               correlating and integrating relevant information from various
               sources into one representational format. It is used by signal
               processing engineers and information operations specialists to
               help them make decisions involving tasks like sensor management,
               tracking, and system control. This comprehensive resource
               provides practitioners with an in-depth understanding of
               finite-set statistics (FISST) - a recently developed method that
               has been gaining much attention among professionals because it
               unifies information fusion, utilizing statistics that most
               engineers learn as undergraduates. The book helps professionals
               use FISST to create efficient information fusion systems that
               can be implemented to address real-world challenges in the
               field.",
  publisher = "Artech House",
  pages     = "856",
  month     =  "1~" # jan,
  year      =  2007,
  keywords  = "Mendeley Import (Jan 17)/WeeklyReading",
  language  = "en"
}

@ARTICLE{Makarenko2003-ss,
  title     = "Scalable human-robot interactions in active sensor networks",
  author    = "Makarenko, A A and Kaupp, T and Durrant-Whyte, H F",
  abstract  = "Decentralized sensor networks promise virtually unlimited
               scalability and can tolerate individual component failures. An
               experimental active sensor network that leverages
               environment-centric modes of human-robot interaction can keep up
               with a network's arbitrary growth. Spatially distributed sensors
               provide better coverage, faster response to dynamically changing
               environments, better survivability, and robustness to failure.
               Taking an extra step to a decentralized system provides further
               benefits of scalability, modularity, and performance. Our active
               sensor network is a collection of sensing platforms connected
               into a network. Some or all of the network components have
               actuators that we can control, making them, in this sense,
               active. A mobile robot with onboard sensors and a communication
               facility is an example of an active component. We investigate
               the scalability of an important aspect of an ASN: interaction
               with human operations.",
  journal   = "IEEE Pervasive Comput.",
  publisher = "IEEE",
  volume    =  2,
  number    =  4,
  pages     = "63--71",
  month     =  oct,
  year      =  2003,
  keywords  = "distributed sensors;man-machine systems;mobile
               robots;multi-robot systems;sensor fusion;active sensor
               networks;environment-centric modes;mobile robot;network
               components;scalable human-robot interactions;sensing
               platforms;spatial distributed sensors;Actuators;Communication
               system control;Human robot interaction;Intelligent
               networks;Mobile robots;Robot
               kinematics;Robustness;Scalability;Sensor fusion;Sensor
               systems;Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Smith2006-bj,
  title    = "Approaches to Multisensor Data Fusion in Target Tracking: A
              Survey",
  author   = "Smith, D and Singh, S",
  abstract = "The tracking of objects using distributed multiple sensors is an
              important field of work in the application areas of autonomous
              robotics, military applications, and mobile systems. In this
              survey, we review a number of computationally intelligent methods
              that are used for developing robust tracking schemes through
              sensor data fusion. The survey discusses the application of the
              various algorithms at different layers of the JDL model and
              highlights the weaknesses and strengths of the approaches in the
              context of different applications",
  journal  = "IEEE Trans. Knowl. Data Eng.",
  volume   =  18,
  number   =  12,
  pages    = "1696--1710",
  month    =  dec,
  year     =  2006,
  keywords = "Data fusion; Distributed sensors; Information fusion; Tracking;
              sensor fusion; target tracking; JDL model; distributed multiple
              sensors; multisensor data fusion; robust tracking; Competitive
              intelligence; Computational intelligence; Intelligent robots;
              Intelligent sensors; Military computing; Mobile robots; Robot
              sensing systems; Robustness; Sensor systems and applications;
              data fusion.;Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Krishnan2015-qb,
  title         = "Deep Kalman Filters",
  author        = "Krishnan, Rahul G and Shalit, Uri and Sontag, David",
  abstract      = "Kalman Filters are one of the most influential models of
                   time-varying phenomena. They admit an intuitive
                   probabilistic interpretation, have a simple functional form,
                   and enjoy widespread adoption in a variety of disciplines.
                   Motivated by recent variational methods for learning deep
                   generative models, we introduce a unified algorithm to
                   efficiently learn a broad spectrum of Kalman filters. Of
                   particular interest is the use of temporal generative models
                   for counterfactual inference. We investigate the efficacy of
                   such models for counterfactual inference, and to that end we
                   introduce the ``Healing MNIST'' dataset where long-term
                   structure, noise and actions are applied to sequences of
                   digits. We show the efficacy of our method for modeling this
                   dataset. We further show how our model can be used for
                   counterfactual inference for patients, based on electronic
                   health record data of 8,000 patients over 4.5 years.",
  number        =  2000,
  pages         = "1--7",
  month         =  "16~" # nov,
  year          =  2015,
  keywords      = "Folder - NIPS2015;Variatinal Inference;Mendeley Import (Jan
                   17)/WeeklyReading",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1511.05121"
}

@INPROCEEDINGS{Pavlin2007-yo,
  title     = "Causal Bayesian Networks for Robust and Efficient Fusion of
               Information Obtained from Sensors and Humans",
  booktitle = "2007 {IEEE} Instrumentation Measurement Technology Conference
               {IMTC} 2007",
  author    = "Pavlin, G and Maris, M and Groen, F",
  abstract  = "This paper discusses techniques for fusion in contemporary
               situation assessment applications. Such applications often
               require reasoning about phenomena that cannot be observed
               directly, but information about their effects (i.e. symptoms)
               can be accessed through the existing sensory and communication
               infrastructure. Reasoning about hidden phenomena requires
               interpretation of relevant observations. Observations can be of
               heterogeneous types and can originate from humans as well as
               various sensory systems. Interpretation in such settings can be
               very challenging, as there might exist complex dependences
               between different phenomena. In addition, we are often
               confronted with significant modeling and observation
               uncertainties. Particularly challenging is the fact that a large
               portion of such information often originates from humans.
               Consequently, it can be very difficult to obtain perception
               models that precisely describe the distributions of hidden
               phenomena and human reports. In this paper we show that Bayesian
               networks (BNs) are suitable for the development of fusion
               systems in such settings, because they can efficiently describe
               the monitoring domains. Moreover, BNs support construction of
               efficient and robust distributed fusion systems.",
  publisher = "IEEE",
  pages     = "1--6",
  month     =  may,
  year      =  2007,
  keywords  = "belief networks;sensor fusion;causal Bayesian
               networks;communication infrastructure;fusion systems
               development;information fusion;perception models;robust
               distributed fusion systems;Bayesian methods;Crisis
               management;Decision making;Electronic
               mail;Humans;Monitoring;Robustness;Sensor fusion;State
               estimation;Uncertainty;Bayesian networks;Information
               fusion;heterogeneous information sources;Mendeley Import (Jan
               17)/WeeklyReading"
}

@INPROCEEDINGS{Topp2006-ib,
  title     = "Topological Modelling for Human Augmented Mapping",
  booktitle = "2006 {IEEE/RSJ} International Conference on Intelligent Robots
               and Systems",
  author    = "Topp, E A and Christensen, H I",
  abstract  = "Service robots designed for domestic settings need to navigate
               in an environment that they have to share with their users.
               Thus, they have to be able to report their current state and
               whereabouts in a way that is comprehensible for the user. Pure
               metric maps do not usually correspond to the understanding of
               the environment a user would provide. Thus, the robotic map
               needs to be integrated with the human representation. This paper
               describes our framework of human augmented mapping that allows
               us to achieve this integration. We propose further a method to
               specify and represent regions that relate to a user's view on
               the environment. We assume an interactive setup for the
               specification of regions and show the applicability of our
               method in terms of distinctiveness for space segmentation and in
               terms of localisation purposes",
  publisher = "IEEE",
  pages     = "2257--2263",
  month     =  oct,
  year      =  2006,
  keywords  = "SLAM (robots);home automation;man-machine systems;mobile
               robots;path planning;service robots;topology;domestic
               settings;human augmented mapping;robotic map;service
               robots;space segmentation;topological modelling;Computer
               science;Content addressable storage;Humans;Intelligent
               robots;Navigation;Orbital robotics;Performance evaluation;Robot
               sensing systems;Service robots;Simultaneous localization and
               mapping;Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Zilberstein1996-ro,
  title    = "Intelligent information gathering using decision models",
  author   = "Zilberstein, Shlomo and Lesser, Victor",
  journal  = "Computer Science Department, University of Massachusetts, Boston,
              Massachusetts",
  year     =  1996,
  keywords = "Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Rabiner1986-eq,
  title    = "An introduction to hidden Markov models",
  author   = "Rabiner, Lawrence and Juang, Biing-Hwang",
  journal  = "ASSP Magazine, IEEE",
  volume   =  3,
  number   =  1,
  pages    = "4--16",
  year     =  1986,
  keywords = "Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Gartner2003-yn,
  title     = "A survey of kernels for structured data",
  author    = "G{\"a}rtner, Thomas",
  abstract  = "Kernel methods in general and support vector machines in
               particular have been successful in various learning tasks on
               data represented in a single table. Much 'real-world' data,
               however, is structured - it has no natural representation in a
               single table. Usually, to apply kernel methods to 'real-world'
               data, extensive pre-processing is performed to embed the data
               into areal vector space and thus in a single table. This survey
               describes several approaches of defining positive definite
               kernels on structured instances directly.",
  journal   = "ACM SIGKDD Explorations Newsletter",
  publisher = "ACM",
  volume    =  5,
  number    =  1,
  pages     = "49--58",
  month     =  "1~" # jul,
  year      =  2003,
  keywords  = "inductive logic programming; ing; kernel methods;
               multi-relational data min-; structured data; multi-relational
               data mining;Mendeley Import (Jan 17)/WeeklyReading"
}

@ARTICLE{Maddison2014-oy,
  title     = "A* sampling",
  author    = "Maddison, C J and Tarlow, D and Minka, T",
  abstract  = "Abstract The problem of drawing samples from a discrete
               distribution can be converted into a discrete optimization
               problem. In this work, we show how sampling from a continuous
               distribution can be converted into an optimization problem over
               continuous space. Central to",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "papers.nips.cc",
  pages     = "3086--3094",
  year      =  2014,
  keywords  = "Mendeley Import (Jan 17)/WeeklyReading"
}

@BOOK{Bar-Shalom2001-tg,
  title     = "Estimation with Applications to Tracking and Navigation: Theory
               Algorithms and Software",
  author    = "Bar-Shalom, Yaakov and Rong Li, X and Kirubarajan, Thiagalingam",
  abstract  = "Expert coverage of the design and implementation of state
               estimation algorithms for tracking and navigation Estimation
               with Applications to Tracking and Navigation treats the
               estimation of various quantities from inherently inaccurate
               remote observations. It explains state estimator design using a
               balanced combination of linear systems, probability, and
               statistics. The authors provide a review of the necessary
               background mathematical techniques and offer an overview of the
               basic concepts in estimation. They then provide detailed
               treatments of all the major issues in estimation with a focus on
               applying these techniques to real systems. Other features
               include: Problems that apply theoretical material to real-world
               applications In-depth coverage of the Interacting Multiple Model
               (IMM) estimator Companion DynaEst(TM) software for MATLAB(TM)
               implementation of Kalman filters and IMM estimators Design
               guidelines for tracking filters Suitable for graduate
               engineering students and engineers working in remote sensors and
               tracking, Estimation with Applications to Tracking and
               Navigation provides expert coverage of this important area.",
  publisher = "Wiley",
  volume    =  9,
  pages     = "584",
  month     =  "25~" # jun,
  year      =  2001,
  keywords  = "Textbook;Mendeley Import (Jan 17)/TextBooks",
  language  = "en"
}

@ARTICLE{Schueller1997-al,
  title    = "A state-of-the-art report on computational stochastic mechanics",
  author   = "Schu{\"e}ller, G I",
  abstract = "This state-of-the-art report assesses the current state of
              development of computational procedures as utilized in stochastic
              mechanics. The theoretical developments and aspects of practical
              applications are discussed in this report, which is structured in
              four sections. The first section is concerned with the latest
              developments in Monte Carlo simulation (MCS)-including parallel
              processing and various types of variance reduction techniques. In
              the second section, various possibilities for representing
              stochastic processes and random fields (including discrete and
              conditional representations) and wavelets are reviewed. The third
              section is concerned with various methods for prediction of the
              response of structural systems under stochastic excitation, e.g.
              by using the FEM method to solve the Fokker-Planck equation
              (FPE), path integral method, moment closure schemes, maximum
              entropy considerations, etc. This section also includes a brief
              subsection on computational aspects of stochastic stability. Tbe
              final section is concerned with the treatment of stochastic
              uncertainties of system properties, such as material and
              geometric variation, by applying stochastic finite element
              methods (SFEMs). All sections include statements on the current
              limitations and also of the future potential of the discussed
              procedures. \copyright{} 1997 Elsevier Science Ltd.",
  journal  = "Probab. Eng. Mech.",
  volume   =  12,
  number   =  4,
  pages    = "197--321",
  month    =  oct,
  year     =  1997,
  keywords = "Textbook;Mendeley Import (Jan 17)/TextBooks"
}

@ARTICLE{Tanaka1988-ic,
  title     = "Sensitivity analysis in principal component analysis:influence
               on the subspace spanned by principal components",
  author    = "Tanaka, Yukata",
  abstract  = "This tutorial is designed to give the reader an understanding of
               Principal Components Analysis (PCA). PCA is a useful statistical
               technique that has found application in fields such as face
               recognition and image compression, and is a common technique for
               finding patterns in data of high dimension. Before getting to a
               description of PCA, this tutorial first introduces mathematical
               concepts that will be used in PCA. It covers standard deviation,
               covariance, eigenvec- tors and eigenvalues. This background
               knowledge is meant to make the PCA section very straightforward,
               but can be skipped if the concepts are already familiar. There
               are examples all the way through this tutorial that are meant to
               illustrate the concepts being discussed. If further information
               is required, the mathematics textbook Elementary Linear Algebra
               5e by Howard Anton, Publisher JohnWiley \& Sons Inc, ISBN
               0-471-85223-6 is a good source of information regarding the
               mathematical back- ground. 1",
  journal   = "Communications in Statistics - Theory and Methods",
  publisher = "Wiley",
  volume    =  17,
  number    =  9,
  pages     = "3157--3175",
  month     =  "27~" # jan,
  year      =  1988,
  address   = "New York",
  keywords  = "Textbook;Mendeley Import (Jan 17)/TextBooks",
  language  = "English"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bubeck2015-va,
  title     = "Convex Optimization: Algorithms and Complexity",
  author    = "Bubeck, S{\'e}bastien",
  abstract  = "This monograph presents the main complexity theorems in convex
               optimization and their corresponding algorithms. Starting from
               the fundamental theory of black-box optimization, the material
               progresses towards recent advances in structural optimization
               and stochastic optimization. Our presentation of black-box
               optimization, strongly influenced by Nesterov’s seminal book and
               Nemirovski’s lecture notes, includes the analysis of cutting
               plane methods, as well as (accelerated) gradient descent
               schemes. We also pay special attention to non- Euclidean
               settings (relevant algorithms include Frank-Wolfe, mirror
               descent, and dual averaging) and discuss their relevance in
               machine learning. We provide a gentle introduction to structural
               optimization with FISTA (to optimize a sum of a smooth and a
               simple non-smooth term), saddle-point mirror prox (Nemirovski’s
               alternative to Nesterov’s smoothing), and a concise description
               of interior point methods. In stochastic optimization we discuss
               stochastic gradient descent, minibatches, random coordinate
               descent, and sublinear algorithms. We also briefly touch upon
               convex relaxation of combinatorial problems and the use of
               randomness to round solutions, as well as random walks based
               methods.",
  journal   = "Foundations and Trends\textregistered{} in Machine Learning",
  publisher = "Now Publishers",
  volume    =  8,
  number    = "3-4",
  pages     = "231--357",
  year      =  2015,
  keywords  = "Optimization;NotRead;Textbook;Mendeley Import (Jan 17)/TextBooks",
  language  = "en"
}

@ARTICLE{Lehman2014-ky,
  title    = "Mathematics for Computer Science. 2011",
  author   = "Lehman, Eric and Leighton, F T and Meyer, A R",
  journal  = "URL: http://courses. csail. mit. edu/6. 042/spring12/mcs. pdf
              [cited 2012-09-06]",
  pages    = "1--912",
  year     =  2014,
  keywords = "Important;Textbook;Mendeley Import (Jan 17)/TextBooks"
}

@BOOK{Cover2006-wt,
  title     = "Elements of Information Theory",
  author    = "Cover, Thomas M and Thomas, Joy A",
  abstract  = "The latest edition of this classic is updated with new problem
               sets and material The Second Edition of this fundamental
               textbook maintains the book's tradition of clear,
               thought-provoking instruction. Readers are provided once again
               with an instructive mix of mathematics, physics, statistics, and
               information theory. All the essential topics in information
               theory are covered in detail, including entropy, data
               compression, channel capacity, rate distortion, network
               information theory, and hypothesis testing. The authors provide
               readers with a solid understanding of the underlying theory and
               applications. Problem sets and a telegraphic summary at the end
               of each chapter further assist readers. The historical notes
               that follow each chapter recap the main points. The Second
               Edition features: * Chapters reorganized to improve teaching *
               200 new problems * New material on source coding, portfolio
               theory, and feedback capacity * Updated references Now current
               and enhanced, the Second Edition of Elements of Information
               Theory remains the ideal textbook for upper-level undergraduate
               and graduate courses in electrical engineering, statistics, and
               telecommunications. An Instructor's Manual presenting detailed
               solutions to all the problems in the book is available from the
               Wiley editorial department.",
  publisher = "Wiley",
  pages     = "1--748",
  edition   = "2nd ed",
  month     =  "18~" # jul,
  year      =  2006,
  address   = "Hoboken, N.J",
  keywords  = "Information theory;NotRead;Textbook;Mendeley Import (Jan
               17)/TextBooks",
  language  = "en"
}

@BOOK{Konig2014-ud,
  title     = "Eigenvalue Distribution of Compact Operators",
  author    = "K{\"o}nig, H",
  publisher = "Birkh{\"a}user Basel",
  volume    =  16,
  pages     = "262",
  series    = "Operator Theory: Advances and Applications",
  month     =  "11~" # apr,
  year      =  2014,
  address   = "Basel",
  keywords  = "Textbook;Mendeley Import (Jan 17)/TextBooks",
  language  = "en"
}

@ARTICLE{Hastie2009-ho,
  title    = "The Elements of Statistical Learning",
  author   = "Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome",
  abstract = "During the past decade there has been an explosion in computation
              and information technology. With it has come vast amounts of data
              in a variety of fields such as medicine, biology, finance, and
              marketing. The challenge of understanding these data has led to
              the development of new tools in the field of statistics, and
              spawned new areas such as data mining, machine learning, and
              bioinformatics. Many of these tools have common underpinnings but
              are often expressed with different terminology. This book
              describes the important ideas in these areas in a common
              conceptual framework. While the approach is statistical, the
              emphasis is on concepts rather than mathematics. Many examples
              are given, with a liberal use of color graphics. It should be a
              valuable resource for statisticians and anyone interested in data
              mining in science or industry. The book's coverage is broad, from
              supervised learning (prediction) to unsupervised learning. The
              many topics include neural networks, support vector machines,
              classification trees and boosting-the first comprehensive
              treatment of this topic in any book. Trevor Hastie, Robert
              Tibshirani, and Jerome Friedman are professors of statistics at
              Stanford University. They are prominent researchers in this area:
              Hastie and Tibshirani developed generalized additive models and
              wrote a popular book of that title. Hastie wrote much of the
              statistical modeling software in S-PLUS and invented principal
              curves and surfaces. Tibshirani proposed the Lasso and is
              co-author of the very successful An Introduction to the
              Bootstrap. Friedman is the co-inventor of many data-mining tools
              including CART, MARS, and projection pursuit. FROM THE REVIEWS:
              TECHNOMETRICS ``This is a vast and complex book. Generally, it
              concentrates on explaining why and how the methods work, rather
              than how to use them. Examples and especially the visualizations
              are principle features...As a source for the methods of
              statistical learning...it will probably be a long time before
              there is a competitor to this book.''",
  journal  = "Elements",
  volume   =  1,
  pages    = "337--387",
  year     =  2009,
  keywords = "Textbook;Mendeley Import (Jan 17)/TextBooks"
}

@BOOK{Olver2010-me,
  title     = "{NIST} Handbook of Mathematical Functions",
  author    = "Olver, F W J (editor)",
  publisher = "Cambridge University Press, Cambridge, London and New York",
  year      =  2010,
  keywords  = "TALAF;Textbook;Mendeley Import (Jan 17)/TextBooks"
}

@MISC{noauthor_undated-le,
  title        = "plt - File Exchange - {MATLAB} Central",
  abstract     = "Finding the nearest positive definite matrix",
  howpublished = "\url{http://www.mathworks.de/matlabcentral/fileexchange/4936-plt}",
  keywords     = "TALAF;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@INPROCEEDINGS{Oddi2012-jt,
  title           = "A distributed multi-path algorithm for wireless ad-hoc
                     networks based on Wardrop routing",
  booktitle       = "21st Mediterranean Conference on Control and Automation",
  author          = "Oddi, G and Pietrabissa, A",
  abstract        = "Reinforcement learning, one of the most active research
                     areas in artificial intelligence, is a computational
                     approach to learning whereby an agent tries to maximize
                     the total amount of reward it receives when interacting
                     with a complex, uncertain environment. In Reinforcement
                     Learning, Richard Sutton and Andrew Barto provide a clear
                     and simple account of the key ideas and algorithms of
                     reinforcement learning. Their discussion ranges from the
                     history of the field's intellectual foundations to the
                     most recent developments and applications. The only
                     necessary mathematical background is familiarity with
                     elementary concepts of probability. The book is divided
                     into three parts. Part I defines the reinforcement
                     learning problem in terms of Markov decision processes.
                     Part II provides basic solution methods: dynamic
                     programming, Monte Carlo methods, and temporal-difference
                     learning. Part III presents a unified view of the solution
                     methods and incorporates artificial neural networks,
                     eligibility traces, and planning; the two final chapters
                     present case studies and consider the future of
                     reinforcement learning.",
  publisher       = "IEEE",
  volume          =  3,
  pages           = "930--935",
  year            =  2012,
  keywords        = "Textbook;Mendeley Import (Jan 17)/TextBooks",
  conference      = "2013 21st Mediterranean Conference on Control \&
                     Automation (MED)"
}

@ARTICLE{Bubeck2012-xo,
  title     = "Regret Analysis of Stochastic and Nonstochastic Multi-armed
               Bandit Problems",
  author    = "Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicol{\`o}",
  abstract  = "Multi-armed bandit problems are the most basic examples of
               sequential decision problems with an exploration-exploitation
               trade-off. This is the balance between staying with the option
               that gave highest payoffs in the past and exploring new options
               that might give higher payoffs in the future. Although the study
               of bandit problems dates back to the Thirties,
               exploration-exploitation trade-offs arise in several modern
               applications, such as ad placement, website optimization, and
               packet routing. Mathematically, a multi-armed bandit is defined
               by the payoff process associated with each option. In this
               survey, we focus on two extreme cases in which the analysis of
               regret is particularly simple and elegant: i.i.d. payoffs and
               adversarial payoffs. Besides the basic setting of finitely many
               actions, we also analyze some of the most important variants and
               extensions, such as the contextual bandit model.",
  journal   = "Foundations and Trends\textregistered{} in Machine Learning",
  publisher = "Now Publishers",
  volume    =  5,
  number    =  1,
  pages     = "1--122",
  year      =  2012,
  keywords  = "Learning and statistical methods; Game theoretic learning;
               Online learning; Optimization; Reinforcement
               learning;NotRead;Textbook;Mendeley Import (Jan 17)/TextBooks",
  language  = "en"
}

@BOOK{Wasserman2013-bh,
  title     = "All of Statistics: A Concise Course in Statistical Inference",
  author    = "Wasserman, Larry",
  abstract  = "Taken literally, the title ``All of Statistics'' is an
               exaggeration. But in spirit, the title is apt, as the book does
               cover a much broader range of topics than a typical introductory
               book on mathematical statistics. This book is for people who
               want to learn probability and statistics quickly. It is suitable
               for graduate or advanced undergraduate students in computer
               science, mathematics, statistics, and related disciplines. The
               book includes modern topics like nonparametric curve estimation,
               bootstrapping, and clas sification, topics that are usually
               relegated to follow-up courses. The reader is presumed to know
               calculus and a little linear algebra. No previous knowledge of
               probability and statistics is required. Statistics, data mining,
               and machine learning are all concerned with collecting and
               analyzing data. For some time, statistics research was con
               ducted in statistics departments while data mining and machine
               learning re search was conducted in computer science
               departments. Statisticians thought that computer scientists were
               reinventing the wheel. Computer scientists thought that
               statistical theory didn't apply to their problems. Things are
               changing. Statisticians now recognize that computer scientists
               are making novel contributions while computer scientists now
               recognize the generality of statistical theory and methodology.
               Clever data mining algo rithms are more scalable than
               statisticians ever thought possible. Formal sta tistical theory
               is more pervasive than computer scientists had realized.",
  publisher = "Springer Science \& Business Media",
  month     =  "11~" # dec,
  year      =  2013,
  keywords  = "Textbook;Mendeley Import (Jan 17)/TextBooks",
  language  = "en"
}

@BOOK{Zwillinger2012-ue,
  title     = "Standard mathematical tables and formulae",
  author    = "Zwillinger, Daniel",
  publisher = "CRC press",
  year      =  2012,
  keywords  = "Textbook;Mendeley Import (Jan 17)/TextBooks"
}

@BOOK{Khalil2002-hc,
  title     = "Nonlinear Systems",
  author    = "Khalil, Hassan K",
  abstract  = "This book is written is such a way that the level of
               mathematical sophistication builds up from chapter to chapter.
               It has been reorganized into four parts: basic analysis,
               analysis of feedback systems, advanced analysis, and nonlinear
               feedback control. Updated content includes subjects which have
               proven useful in nonlinear control design in recent years---new
               in the 3rd edition are: expanded treatment of passivity and
               passivity-based control; integral control, high-gain feedback,
               recursive methods, optimal stabilizing control, control Lyapunov
               functions, and observers. For use as a self-study or reference
               guide by engineers and applied mathematicians.",
  publisher = "Prentice Hall",
  pages     = "767",
  edition   = "3rd ed",
  year      =  2002,
  address   = "Upper Saddle River, N.J",
  keywords  = "Textbook;Mendeley Import (Jan 17)/TextBooks",
  language  = "en"
}

@BOOK{MacKay2003-ty,
  title     = "Information Theory, Inference and Learning Algorithms",
  author    = "MacKay, David J C",
  abstract  = "Information theory and inference, often taught separately, are
               here united in one entertaining textbook. These topics lie at
               the heart of many exciting areas of contemporary science and
               engineering - communication, signal processing, data mining,
               machine learning, pattern recognition, computational
               neuroscience, bioinformatics, and cryptography. This textbook
               introduces theory in tandem with applications. Information
               theory is taught alongside practical communication systems, such
               as arithmetic coding for data compression and sparse-graph codes
               for error-correction. A toolbox of inference techniques,
               including message-passing algorithms, Monte Carlo methods, and
               variational approximations, are developed alongside applications
               of these tools to clustering, convolutional codes, independent
               component analysis, and neural networks. The final part of the
               book describes the state of the art in error-correcting codes,
               including low-density parity-check codes, turbo codes, and
               digital fountain codes -- the twenty-first century standards for
               satellite communications, disk drives, and data broadcast.
               Richly illustrated, filled with worked examples and over 400
               exercises, some with detailed solutions, David MacKay's
               groundbreaking book is ideal for self-learning and for
               undergraduate or graduate courses. Interludes on crosswords,
               evolution, and sex provide entertainment along the way. In sum,
               this is a textbook on information, communication, and coding for
               a new generation of students, and an unparalleled entry point
               into these subjects for professionals in areas as diverse as
               computational biology, financial engineering, and machine
               learning.",
  publisher = "Cambridge University Press",
  volume    =  100,
  pages     = "1--640",
  month     =  "25~" # sep,
  year      =  2003,
  keywords  = "NotRead;Textbook;Mendeley Import (Jan 17)/TextBooks;Mendeley
               Import (Jan 17)/CurrentStudy",
  language  = "en"
}

@BOOK{Dasgupta_undated-cn,
  title    = "Algorithms",
  author   = "Dasgupta, S and Papadimitriou, C H and Vazirani, U V",
  keywords = "Folder - CSCI5454-Algorithms;Textbook;Mendeley Import (Jan
              17);Mendeley Import (Jan 17)/TextBooks;Mendeley Import (Jan
              17)/Other"
}

@ARTICLE{OCallaghan2012-up,
  title    = "Gaussian process occupancy maps",
  author   = "O'Callaghan, S T and Ramos, F T",
  journal  = "Int. J. Rob. Res.",
  volume   =  31,
  number   =  1,
  pages    = "42--62",
  month    =  "1~" # jan,
  year     =  2012,
  keywords = "BayesOpt;GPs;TALAF;Mendeley Import (Jan 17)/BayesOpt;Mendeley
              Import (Jan 17)/GPs",
  language = "en"
}

@INPROCEEDINGS{Guestrin2005-rw,
  title     = "Near-optimal Sensor Placements in Gaussian Processes",
  booktitle = "Proceedings of the 22Nd International Conference on Machine
               Learning",
  author    = "Guestrin, Carlos and Krause, Andreas and Singh, Ajit Paul",
  publisher = "ACM",
  pages     = "265--272",
  series    = "ICML '05",
  year      =  2005,
  address   = "New York, NY, USA",
  keywords  = "BayesOpt;GPs;NotRead;TALAF;Mendeley Import (Jan
               17)/BayesOpt;Mendeley Import (Jan 17)/GPs"
}

@INPROCEEDINGS{Zagorecki2015-qy,
  title     = "An Approximation of Surprise Index as a Measure of Confidence",
  booktitle = "2015 {AAAI} Fall Symposium Series",
  author    = "Zagorecki, Adam and Kozniewski, Marcin and Druzdzel, Marek",
  abstract  = "Probabilistic graphical models, such as Bayesian networks, are
               intuitive and theoretically sound tools for modeling
               uncertainty. A major problem with applying Bayesian networks in
               practice is that it is hard to judge whether a model fits well a
               case that it is supposed to solve. One way of expressing a
               possible dissonance between a model and a case is the
               \{\textbackslashem surprise index\}, proposed by Habbema, which
               expresses the degree of surprise by the evidence given the
               model. While this measure reflects the intuition that the
               probability of a case should be judged in the context of a
               model, it is computationally intractable. In this paper, we
               propose an efficient way of approximating the surprise index.",
  month     =  "23~" # sep,
  year      =  2015,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;model\_check;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@ARTICLE{Lake2015-xn,
  title       = "Human-level concept learning through probabilistic program
                 induction",
  author      = "Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum,
                 Joshua B",
  affiliation = "Center for Data Science, New York University, 726 Broadway,
                 New York, NY 10003, USA. brenden@nyu.edu. Department of
                 Computer Science and Department of Statistics, University of
                 Toronto, 6 King's College Road, Toronto, ON M5S 3G4, Canada.
                 Department of Brain and Cognitive Sciences, Massachusetts
                 Institute of Technology, 77 Massachusetts Avenue, Cambridge,
                 MA 02139, USA.",
  abstract    = "People learning new concepts can often generalize successfully
                 from just a single example, yet machine learning algorithms
                 typically require tens or hundreds of examples to perform with
                 similar accuracy. People can also use learned concepts in
                 richer ways than conventional algorithms-for action,
                 imagination, and explanation. We present a computational model
                 that captures these human learning abilities for a large class
                 of simple visual concepts: handwritten characters from the
                 world's alphabets. The model represents concepts as simple
                 programs that best explain observed examples under a Bayesian
                 criterion. On a challenging one-shot classification task, the
                 model achieves human-level performance while outperforming
                 recent deep learning approaches. We also present several
                 ``visual Turing tests'' probing the model's creative
                 generalization abilities, which in many cases are
                 indistinguishable from human behavior.",
  journal     = "Science",
  volume      =  350,
  number      =  6266,
  pages       = "1332--1338",
  month       =  "11~" # dec,
  year        =  2015,
  keywords    = "Folder - Spring2016;NotRead;Mendeley Import (Jan
                 17)/MLTheory;Mendeley Import (Jan 17)/ReadingGroups",
  language    = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Hutchins2015-if,
  title     = "Representing Autonomous Systems’ {Self-Confidence} through
               Competency Boundaries",
  author    = "Hutchins, Andrew R and Cummings, M L and Draper, Mark and
               Hughes, Thomas",
  publisher = "SAGE Publications",
  volume    =  59,
  pages     = "279--283",
  year      =  2015,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;perf\_prediction;vis\_dr;Mendeley
               Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Laskey1991-mf,
  title     = "Conflict and Surprise: Heuristics for Model Revision",
  booktitle = "Proceedings of the Seventh Conference on Uncertainty in
               Artificial Intelligence",
  author    = "Laskey, Kathryn Blackmond",
  publisher = "Morgan Kaufmann Publishers Inc.",
  pages     = "197--204",
  series    = "UAI'91",
  year      =  1991,
  address   = "San Francisco, CA, USA",
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;model\_check;Mendeley
               Import (Jan 17)/Assurances"
}

@BOOK{Rasmussen2004-vo,
  title     = "Gaussian processes in machine learning",
  author    = "Rasmussen, Carl Edward",
  publisher = "Springer",
  pages     = "63--71",
  year      =  2004,
  keywords  = "BayesOpt;GPs;TALAF;Mendeley Import (Jan 17)/BayesOpt;Mendeley
               Import (Jan 17)/GPs"
}

@INCOLLECTION{Sweet2016-tz,
  title     = "Towards {Self-Confidence} in Autonomous Systems",
  booktitle = "{AIAA} Infotech @ Aerospace",
  author    = "Sweet, Nicholas and Ahmed, Nisar R and Kuter, Ugur and Miller,
               Christopher",
  pages     = "1651",
  year      =  2016,
  keywords  = "Mendeley Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Kaipa2015-hy,
  title     = "Toward Estimating Task Execution Confidence for Robotic
               {Bin-Picking} Applications",
  booktitle = "2015 {AAAI} Fall Symposium Series",
  author    = "Kaipa, Krishnanand N and Kankanhalli-Nagendra, Akshaya S and
               Gupta, Satyandra K",
  abstract  = "We present an approach geared toward estimating task execution
               confidence for robotic bin-picking applications. This requires
               estimating execution confidence for all constituent subtasks
               including part recognition and pose estimation, singulation,
               transport, and fine positioning. This paper is focussed on
               computing associated confidence parameters for the part
               recognition and pose estimation subtask. In particular, our
               approach allows a robot to evaluate how good the part
               recognition and pose estimation is, based on a
               confidence-measure, and thereby determine whether to proceed
               with the task execution (part singulation) or to request help
               from a human in order to resolve the associated failure. The
               value of a mean-square distance metric at a local minimum where
               the part matching solution is found is used as a surrogate for
               the confidence parameter. Experiments with a Baxter robot are
               used illustrate our approach.",
  month     =  "23~" # sep,
  year      =  2015,
  keywords  = "
               trust\_informal\_treatment;assurance\_explicit;classification;perf\_prediction;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@BOOK{Shawe-Taylor2004-jq,
  title     = "Kernel Methods for Pattern Analysis",
  author    = "Shawe-Taylor, John and Cristianini, Nello",
  abstract  = "This book fulfils two major roles: firstly it provides
               practitioners with a large toolkit of algorithms, kernels and
               solutions ready to be implemented, suitable for standard pattern
               discovery problems in field such as bioinformatics, text
               analysis, image analysis. Secondly it provides an easy
               introduction for students and researchers to the growing field
               of kernel-based pattern analysis, demonstrating with examples
               how to handcraft an algorithm or a kernel for a new specific
               application, and covering all the necessary conceptual and
               mathematical tools to do so.",
  publisher = "Cambridge University Press",
  volume    = "XXXIII",
  pages     = "81--87",
  month     =  "28~" # jun,
  year      =  2004,
  keywords  = "BayesOpt;GPs;TALAF;Textbook;Mendeley Import (Jan
               17)/TextBooks;Mendeley Import (Jan 17)/BayesOpt;Mendeley Import
               (Jan 17)/GPs",
  language  = "en"
}

@INPROCEEDINGS{Ng1997-kg,
  title     = "Preventing`` overfitting'' of cross-validation data",
  booktitle = "{ICML}",
  author    = "Ng, Andrew Y",
  abstract  = "Suppose that, for a learning task, we have to select one
               hypothesis out of a set of hypotheses (that may, for example,
               have been generated by multiple applications of a randomized
               learning algorithm). A common approach is to evaluate each
               hypothesis in the set on some previously unseen cross-validation
               data, and then to select the hypothesis that had the lowest
               cross-validation error. But when the cross-validation data is
               partially corrupted such as by noise, and if the set of
               hypotheses we are selecting from is large, then ``folklore''
               also warns about ``overfitting'' the crossvalidation data
               [Klockars and Sax, 1986, Tukey, 1949, Tukey, 1953]. In this
               paper, we explain how this ``overfitting'' really occurs, and
               show the surprising result that it can be overcome by selecting
               a hypothesis with a higher cross-validation error, over others
               with lower cross-validation errors. We give reasons for not
               selecting the hypothesis with the lowest cross-validation error,
               and propose a new algorithm, L...",
  publisher = "Morgan Kaufmann",
  volume    =  97,
  pages     = "245--253",
  year      =  1997,
  keywords  = "BayesOpt;GPs;Important;OptimizingHyperparameters;TALAF;Mendeley
               Import (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/OptimizingHyperparameters"
}

@MISC{Kaipa_undated-hw,
  title        = "Kaipa Webpage",
  author       = "Kaipa, Krishnanand N",
  howpublished = "\url{http://www.terpconnect.umd.edu/~kkrishna/research.html}",
  keywords     = "Mendeley Import (Jan 17)/Assurances"
}

@INCOLLECTION{Murray2010-rv,
  title     = "Slice sampling covariance hyperparameters of latent Gaussian
               models",
  booktitle = "Advances in Neural Information Processing Systems 23",
  author    = "Murray, Iain and Adams, Ryan P",
  editor    = "Lafferty, J D and Williams, C K I and Shawe-Taylor, J and Zemel,
               R S and Culotta, A",
  abstract  = "The Gaussian process (GP) is a popular way to specify
               dependencies between random variables in a probabilistic model.
               In the Bayesian framework the covariance structure can be
               specified using unknown hyperparameters. Integrating over these
               hyperparameters considers different possible explanations for
               the data when making predictions. This integration is often
               performed using Markov chain Monte Carlo (MCMC) sampling.
               However, with non-Gaussian observations standard hyperparameter
               sampling approaches require careful tuning and may converge
               slowly. In this paper we present a slice sampling approach that
               requires little tuning while mixing well in both strong- and
               weak-data regimes.",
  publisher = "Curran Associates, Inc.",
  volume    =  2,
  pages     = "1732--1740",
  year      =  2010,
  keywords  = "Acquisition/InfillFxns;BayesOpt;NotRead;TALAF;Mendeley Import
               (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/BayesOpt/Acquisition/InfillFxns"
}

@BOOK{Bishop2006-cl,
  title     = "Pattern Recognition and Machine Learning",
  author    = "Bishop, Christopher M",
  abstract  = "This is the first textbook on pattern recognition to present the
               Bayesian viewpoint. The book presents approximate inference
               algorithms that permit fast approximate answers in situations
               where exact answers are not feasible. It uses graphical models
               to describe probability distributions when no other books apply
               graphical models to machine learning. No previous knowledge of
               pattern recognition or machine learning concepts is assumed.
               Familiarity with multivariate calculus and basic linear algebra
               is required, and some experience in the use of probabilities
               would be helpful though not essential as the book includes a
               self-contained introduction to basic probability theory.",
  publisher = "Springer",
  volume    =  4,
  pages     = "738",
  series    = "Information science and statistics",
  month     =  "17~" # aug,
  year      =  2006,
  address   = "New York",
  keywords  = "BayesOpt;GPs;Important;Machine learning;TALAF;Textbook;Mendeley
               Import (Jan 17)/TextBooks;Mendeley Import (Jan
               17)/MLTheory;Mendeley Import (Jan 17)/BayesOpt;Mendeley Import
               (Jan 17)/GraphicalModels;Mendeley Import (Jan 17)/GPs",
  language  = "en"
}

@PHDTHESIS{Hutter2009-og,
  title    = "Automated configuration of algorithms for solving hard
              computational problems",
  author   = "Hutter, Frank",
  year     =  2009,
  school   = "University of British Columbia",
  keywords = "BayesOpt;GPs;NotRead;TALAF;Mendeley Import (Jan
              17)/BayesOpt;Mendeley Import (Jan 17)/GPs"
}

@ARTICLE{Sutton1999-cj,
  title    = "Between {MDPs} and {semi-MDPs}: A framework for temporal
              abstraction in reinforcement learning",
  author   = "Sutton, Richard S and Precup, Doina and Singh, Satinder",
  abstract = "Learning, planning, and representing knowledge at multiple levels
              of temporal abstraction are key, longstanding challenges for AI.
              In this paper we consider how these challenges can be addressed
              within the mathematical framework of reinforcement learning and
              Markov decision processes (MDPs). We extend the usual notion of
              action in this framework to include options---closed-loop
              policies for taking action over a period of time. Examples of
              options include picking up an object, going to lunch, and
              traveling to a distant city, as well as primitive actions such as
              muscle twitches and joint torques. Overall, we show that options
              enable temporally abstract knowledge and action to be included in
              the reinforcement learning framework in a natural and general
              way. In particular, we show that options may be used
              interchangeably with primitive actions in planning methods such
              as dynamic programming and in learning methods such as
              Q-learning. Formally, a set of options defined over an MDP
              constitutes a semi-Markov decision process (SMDP), and the theory
              of SMDPs provides the foundation for the theory of options.
              However, the most interesting issues concern the interplay
              between the underlying MDP and the SMDP and are thus beyond SMDP
              theory. We present results for three such cases: (1) we show that
              the results of planning with options can be used during execution
              to interrupt options and thereby perform even better than
              planned, (2) we introduce new intra-option methods that are able
              to learn about an option from fragments of its execution, and (3)
              we propose a notion of subgoal that can be used to improve the
              options themselves. All of these results have precursors in the
              existing literature; the contribution of this paper is to
              establish them in a simpler and more general setting with fewer
              changes to the existing reinforcement learning framework. In
              particular, we show that these results can be obtained without
              committing to (or ruling out) any particular approach to state
              abstraction, hierarchy, function approximation, or the
              macro-utility problem.",
  journal  = "Artif. Intell.",
  volume   =  112,
  number   = "1--2",
  pages    = "181--211",
  year     =  1999,
  keywords = "Temporal abstraction; Reinforcement learning; Markov decision
              processes; Options; Macros; Macroactions; Subgoals; Intra-option
              learning; Hierarchical planning; Semi-Markov decision
              processes;BayesOpt;NotRead;TALAF;reinforcement learning;Mendeley
              Import (Jan 17)/BayesOpt"
}

@ARTICLE{Rasmussen2006-lz,
  title    = "Advances in Gaussian processes",
  author   = "Rasmussen, Carl Edward",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   =  19,
  year     =  2006,
  keywords = "BayesOpt;GPs;TALAF;Mendeley Import (Jan 17)/BayesOpt;Mendeley
              Import (Jan 17)/GPs"
}

@ARTICLE{Giudici2000-sl,
  title       = "Likelihood-ratio tests for hidden Markov models",
  author      = "Giudici, P and Ryd{\'e}n, T and Vandekerkhove, P",
  affiliation = "Department of Economics and Quantitative Methods, University
                 of Pavia, Italy. giudici@unipv.it",
  abstract    = "We consider hidden Markov models as a versatile class of
                 models for weakly dependent random phenomena. The topic of the
                 present paper is likelihood-ratio testing for hidden Markov
                 models, and we show that, under appropriate conditions, the
                 standard asymptotic theory of likelihood-ratio tests is valid.
                 Such tests are crucial in the specification of multivariate
                 Gaussian hidden Markov models, which we use to illustrate the
                 applicability of our general results. Finally, the methodology
                 is illustrated by means of a real data set.",
  journal     = "Biometrics",
  volume      =  56,
  number      =  3,
  pages       = "742--747",
  month       =  sep,
  year        =  2000,
  keywords    = "Gaussian hidden Markov model; Likelihood-ratio test;Mendeley
                 Import (Jan
                 17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language    = "en"
}

@BOOK{Zucchini2009-ay,
  title     = "Hidden Markov Models for Time Series: An Introduction Using {R}",
  author    = "Zucchini, Walter and MacDonald, Iain L",
  abstract  = "Reveals How HMMs Can Be Used as General-Purpose Time Series
               Models Implements all methods in RHidden Markov Models for Time
               Series: An Introduction Using R applies hidden Markov models
               (HMMs) to a wide range of time series types, from
               continuous-valued, circular, and multivariate series to binary
               data, bounded and unbounded counts, and categorical
               observations. It also discusses how to employ the freely
               available computing environment R to carry out computations for
               parameter estimation, model selection and checking, decoding,
               and forecasting. Illustrates the methodology in actionAfter
               presenting the simple Poisson HMM, the book covers estimation,
               forecasting, decoding, prediction, model selection, and Bayesian
               inference. Through examples and applications, the authors
               describe how to extend and generalize the basic model so it can
               be applied in a rich variety of situations. They also provide R
               code for some of the examples, enabling the use of the codes in
               similar applications. Effectively interpret data using HMMs This
               book illustrates the wonderful flexibility of HMMs as
               general-purpose models for time series data. It provides a broad
               understanding of the models and their uses.",
  publisher = "CRC Press",
  pages     = "288",
  month     =  "28~" # apr,
  year      =  2009,
  keywords  = "Mendeley Import (Jan
               17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language  = "en"
}

@TECHREPORT{Afrl_undated-fx,
  title    = "Constructive Entity Behavior v2",
  author   = "{Afrl}",
  keywords = "AFRL;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR;Mendeley Import
              (Jan 17)/AFRL\_STTR/AFRL"
}

@TECHREPORT{Afrl_undated-ex,
  title    = "Interface Design Document for the Not So Grand Challenge ( {NSGC}
              ) Project",
  author   = "{Afrl}",
  keywords = "AFRL;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR;Mendeley Import
              (Jan 17)/AFRL\_STTR/AFRL"
}

@TECHREPORT{Afrl_undated-nd,
  title    = "{PETS} interface design document",
  author   = "{Afrl}",
  keywords = "AFRL;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR;Mendeley Import
              (Jan 17)/AFRL\_STTR/AFRL"
}

@TECHREPORT{Afrl_undated-jz,
  title    = "Group definitions and behaviors",
  author   = "{Afrl}",
  keywords = "AFRL;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR;Mendeley Import
              (Jan 17)/AFRL\_STTR/AFRL"
}
@INPROCEEDINGS{Lara-Cabrera2013-yn,
  title     = "A review of computational intelligence in {RTS} games",
  booktitle = "2013 {IEEE} Symposium on Foundations of Computational
               Intelligence ({FOCI})",
  author    = "Lara-Cabrera, R and Cotta, C and Fern{\'a}ndez-Leiva, A J",
  abstract  = "Real-time strategy games offer a wide variety of fundamental AI
               research challenges. Most of these challenges have applications
               outside the game domain. This paper provides a review on
               computational intelligence in real-time strategy games (RTS). It
               starts with challenges in real-time strategy games, then it
               reviews different tasks to overcome this challenges. Later, it
               describes the techniques used to solve this challenges and it
               makes a relationship between techniques and tasks. Finally, it
               presents a set of different frameworks used as test-beds for the
               techniques employed. This paper is intended to be a starting
               point for future researchers on this topic.",
  pages     = "114--121",
  month     =  apr,
  year      =  2013,
  keywords  = "Buildings; Computational intelligence; Evolutionary computation;
               Games; NotRead; Planning; RTS games; artificial intelligence;
               computational intelligence; computer games; game domain; real
               time strategy games; real-time strategy games; real-time
               systems; review;Evolutionary
               computation;GameAI;Games;NotRead;ai\_planning;TALAF;artificial
               intelligence;computational intelligence;computer games;real-time
               strategy games;real-time systems;Mendeley Import (Jan
               17)/AFRL\_STTR"
}

@ARTICLE{Nguyen2013-wv,
  title    = "Monte Carlo Tree Search for Collaboration Control of Ghosts in
              Ms. {Pac-Man}",
  author   = "Nguyen, K Q and Thawonmas, R",
  abstract = "In this paper, we present an application of Monte Carlo tree
              search (MCTS) to control ghosts in the game called Ms. Pac-Man.
              Our proposed ghost team consists of a ghost controlled by rules
              and three ghosts controlled individually by different MCTS. Given
              a limited time response, in order to increase the reliability of
              MCTS results, we introduce a mechanism for predicting Ms.
              Pac-Man's future movements and use this mechanism for simulating
              Ms. Pac-Man during Monte Carlo simulations. Our ghost team won
              the first Ms. Pac-Man Versus Ghost Team Competition at the 2011
              IEEE Congress on Evolutionary Computation (CEC). Its performances
              for a variety of design choices are also shown and discussed.",
  journal  = "IEEE Trans. Comput. Intell. AI Games",
  volume   =  5,
  number   =  1,
  pages    = "57--68",
  month    =  mar,
  year     =  2013,
  keywords = "2011 IEEE CEC; 2011 IEEE Congress on Evolutionary Computation;
              Collaboration; Computers; Games; Ghosts; MCTS results
              reliability; Monte Carlo; Monte Carlo methods; Monte Carlo
              simulations; Monte Carlo tree search; Monte Carlo tree search
              (MCTS); Ms. Pac-Man; Ms. Pac-Man Versus Ghost Team Competition;
              Pac-Man; Prediction algorithms; Reliability; Time factors;
              computer games; ghost collaboration control; groupware; movement
              prediction; tree searching; video
              game;Computers;GameAI;Games;monte\_carlo;Monte Carlo tree search
              (MCTS);TALAF;computer games;tree searching;Mendeley Import (Jan
              17)/AFRL\_STTR"
}

@INPROCEEDINGS{Ha2015-xe,
  title     = "A stochastic game-theoretic approach for analysis of multiple
               cooperative air combat",
  booktitle = "2015 American Control Conference ({ACC})",
  author    = "Ha, J S and Chae, H J and Choi, H L",
  abstract  = "This paper presents a game theory-based methodology to analyze
               multiple beyond-visual-range (BVR) air combat scenarios. The
               combat scenarios are formalized as a stochastic game consisting
               of a sequence of normal-form games with a continuous sub-game.
               The proposed game formulation improves the scalability by
               reducing the decision space while taking advantage of some
               underlying symmetry structures of the combat scenario. The
               equilibrium strategy and the value functions of the game are
               computed through some dynamic programming procedure; the impact
               of the aircraft's velocity and the cooperation scheme for the
               combat is analyzed based on the equilibrium strategies.",
  pages     = "3728--3733",
  month     =  jul,
  year      =  2015,
  keywords  = "Aircraft; BVR air combat scenarios; Games; Missiles; Nash
               equilibrium; NotRead; Resource management; Stochastic processes;
               aircraft velocity; autonomous aerial vehicles;
               beyond-visual-range air combat scenarios; continuous subgame;
               cooperation scheme; decision space; dynamic programming;
               equilibrium strategies; equilibrium strategy; game formulation;
               game theory-based methodology; game value functions; military
               aircraft; multiple cooperative air combat analysis; normal-form
               games; stochastic game-theoretic approach; stochastic games;
               symmetry structures; unmanned air combat system; velocity
               control;Aircraft;Games;Missiles;NotRead;Stochastic
               processes;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@MISC{Synnaeve2012-uc,
  title    = "A Bayesian Tactician",
  author   = "Synnaeve, Gabriel and Bessiere, Pierre",
  abstract = "We describe a generative Bayesian model of tactical attacks in
              strategy games, which can be used both to predict attacks and to
              take tactical decisions. This model is designed to easily
              integrate and merge information from other (probabilistic)
              estimations and heuristics. In particular, it handles uncertainty
              in enemy units' positions as well as their probable tech tree. We
              claim that learning, being it supervised or through
              reinforcement, adapts to skewed data sources. We evaluated our
              approach on StarCraft1: the parameters are learned on a new
              (freely available) dataset of game states, deterministically
              re-created from replays, and the whole model is evaluated for
              prediction in realistic conditions. It is also the tactical
              decision-making component of a competitive StarCraft AI.",
  journal  = "Computer Games Workshop at ECAI 2012",
  pages    = "114--125",
  year     =  2012,
  keywords = "Bayesian modeling; RTS games; game AI; machine learning;
              tactics;GameAI;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR;Mendeley
              Import (Jan 17)"
}

@ARTICLE{Ciavarelli1980-of,
  title     = "Operational performance measures for air combat: Development and
               application",
  author    = "Ciavarelli, A P and Williams, A M and {others}",
  abstract  = "Abstract The content of this paper summarizes four years of
               research designed to develop valid and reliable performance
               criteria for the Navy's Tactical Aircrew Combat Training System
               (TACTS). Performance measurement methods for assessing missile
               envelope recognition and air combat engagements have been
               developed and applied in an operational setting. TACTS measures
               used in performance assessment were selected on ...",
  journal   = "Proc. Hum. Fact. Ergon. Soc. Annu. Meet.",
  publisher = "pro.sagepub.com",
  volume    =  24,
  number    =  1,
  pages     = "560--564",
  year      =  1980,
  keywords  = "NotRead;NotRead;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR",
  language  = "en"
}

@TECHREPORT{Moore1979-kv,
  title       = "Air Combat Training: Good Stick Index Validation",
  author      = "Moore, Samuel B and Madison, Walker G and Sepp, George D and
                 Stracener, Jerrell T and Coward, Robert E",
  institution = "DTIC Document",
  year        =  1979,
  keywords    = "*FLIGHT TRAINING; AERIAL WARFARE; Automation; FLIGHT
                 MANEUVERS; FLIGHT SIMULATORS; GSI(GOOD STICK INDEX);
                 Humanities and History; MATHEMATICAL PREDICTION; Measurement;
                 PE62205F; PERFORMANCE TESTS; PERFORMANCE(HUMAN); PERSONNEL
                 SELECTION; PILOTS; PROFICIENCY; SCORING; SKILLS; STATISTICAL
                 ANALYSIS; STUDENTS; Statistics and Probability; TEST
                 CONSTRUCTION(PSYCHOLOGY); VALIDATION;
                 WUAFHRL11231216;Measurement;STATISTICAL
                 ANALYSIS;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR",
  language    = "en"
}

@INPROCEEDINGS{Luo2005-gw,
  title     = "Air Combat {Decision-Making} for Cooperative Multiple Target
               Attack Using Heuristic Adaptive Genetic Algorithm",
  booktitle = "2005 International Conference on Machine Learning and
               Cybernetics",
  author    = "Luo, De-Lin and Shen, Chun-Lin and Wang, Biao and Wu, Wen-Hai",
  abstract  = "The Decision-Making (DM) problem is investigated for Cooperative
               Multiple Target Attack in air combat. It is to search for a
               proper attack assignment of M friendly fighters, with multiple
               target attack capability, to N hostile fighters called targets
               to achieve an optimal missile-target attack effect. Thus,
               Missile-Target Assignment (MTA) is regarded as the main part of
               the DM problem and has to be solved firstly. Then, the DM
               solution is derived from the optimal MTA solution. To the MTA
               problem, a Heuristic Adaptive Genetic Algorithm (HAGA) is
               proposed to search for its optimal solution. The HAGA utilizes
               specific heuristic knowledge to improve the search capability of
               the Adaptive Genetic Algorithm (AGA). Simulation results show
               that the HAGA is effective and has much better performance than
               the AGA.",
  volume    =  1,
  pages     = "473--478",
  month     =  aug,
  year      =  2005,
  keywords  = "Aerospace engineering; Automation; Decision making; Delta
               modulation; Educational institutions; Genetic engineering;
               Heuristic algorithms; Missiles; Multiple target attack; Neural
               networks; cooperative; cooperative air combat; decision-making;
               genetic algorithm; genetic algorithms; heuristic; multiple
               target attack;Combat Optimization;Decision
               making;Missiles;neural\_networks;TALAF;genetic algorithm;genetic
               algorithms;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@BOOK{Shaw1985-db,
  title     = "Fighter Combat: Tactics and Maneuvering",
  author    = "Shaw, Robert L",
  abstract  = "This book provides a detailed discussion of one-on-one
               dog-fights and multi-fighter team work tactics. Full discussions
               of fighter aircraft and weapons systems performance are provided
               along with an explanation of radar intercept tactics and an
               analysis of the elements involved in the performance of fighter
               missions.",
  publisher = "Naval Institute Press",
  pages     = "428",
  year      =  1985,
  address   = "Annapolis, Md",
  keywords  = "Fighter plane combat;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR",
  language  = "en"
}

@ARTICLE{Chaslot2008-pn,
  title     = "{Monte-Carlo} Tree Search: A New Framework for Game {AI}",
  author    = "Chaslot, G and Bakkes, S and Szita, I and Spronck, P",
  abstract  = "Abstract Classic approaches to game AI require either a high
               quality of domain knowledge, or a long time to generate
               effective AI behaviour. These two characteristics hamper the
               goal of establishing challenging game AI. In this paper, we put
               forward Monte-Carlo Tree Search as a novel, unified framework to
               game AI. In the framework, randomized explorations of the search
               space are used to predict the most promising game actions. We
               will demonstrate ...",
  journal   = "AIIDE",
  publisher = "aaai.org",
  pages     = "216--217",
  year      =  2008,
  keywords  = "NotRead; demonstration papers;NotRead;TALAF;Mendeley Import (Jan
               17)/AFRL\_STTR"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Teach1981-nf,
  title     = "An Analysis of Physician Attitudes Regarding {Computer-Based}
               Clinical Consultation Systems",
  booktitle = "Use and Impact of Computers in Clinical Medicine",
  author    = "Teach, Randy L and Shortliffe, Edward H",
  editor    = "Anderson, James G and Jay, Stephen J",
  abstract  = "Physician attitudes regarding computer-based clinical decision
               aids and the effect of a 2-day tutorial on medical computing are
               studied. The results indicate that physicians are accepting of
               applications that enhance their patient management capabilities,
               but tend to oppose applications in which they perceive an
               infringement on their management role. Expectations about the
               effect of computing on current medical practices are found to be
               generally favorable, although considerable individual
               differences exist among subgroups. The study participants place
               substantial demands on the performance capabilities of
               acceptable consultations systems, and emphasize the need for
               humanlike interactive capabilities. The tutorial had no effect
               on attitudes regarding appropriate clinical uses of computers
               nor on expectations about the effect of the technology on
               medical practice. However, it did increase the participants’
               knowledge of medical computing and led to more informed demands
               on system performance. We discuss the implications of the study
               and offer suggestions for developing and implementing
               computer-based clinical decision aids.",
  publisher = "Springer New York",
  volume    =  558,
  pages     = "68--85",
  series    = "Computers and Medicine",
  year      =  1981,
  keywords  = "ConsultationSystems;TALAF;Mendeley Import (Jan
               17)/AFRL\_STTR;Mendeley Import (Jan
               17)/AFRL\_STTR/ConsultationSystems",
  language  = "en"
}

@INPROCEEDINGS{Bakkes2004-lk,
  title      = "{TEAM}: The {Team-Oriented} Evolutionary Adaptability Mechanism",
  booktitle  = "Entertainment Computing -- {ICEC} 2004",
  author     = "Bakkes, Sander and Spronck, Pieter and Postma, Eric",
  editor     = "Rauterberg, Matthias",
  abstract   = "Many commercial computer games allow a team of players to match
                their skills against another team, controlled by humans or by
                the computer. Most players prefer human opponents, since the
                artificial intelligence of a computer-controlled team is in
                general inferior. An adaptive mechanism for team-oriented
                artificial intelligence would allow computer-controlled
                opponents to adapt to human player behaviour, thereby providing
                a means of dealing with weaknesses in the game AI. Current
                commercial computer games lack challenging adaptive mechanisms.
                This paper proposes ``TEAM'', a novel team-oriented adaptive
                mechanism which is inspired by evolutionary algorithms. The
                performance of TEAM is evaluated in an experiment involving an
                actual commercial computer game (the Capture The Flag
                team-based game mode of the popular commercial computer game
                Quake III). The experimental results indicate that TEAM
                succeeds in endowing computer-controlled opponents with
                successful adaptive performance. We therefore conclude that
                TEAM can be successfully applied to generate challenging
                adaptive opponent behaviour in team-oriented commercial
                computer games.",
  publisher  = "Springer Berlin Heidelberg",
  pages      = "273--282",
  series     = "Lecture Notes in Computer Science",
  month      =  "1~" # sep,
  year       =  2004,
  keywords   = "Artificial Intelligence (incl. Robotics); Computer Appl. in
                Arts and Humanities; Computer Applications; Information Systems
                Applications (incl. Internet); Multimedia Information Systems;
                User Interfaces and Human Computer Interaction;Artificial
                Intelligence (incl. Robotics);GameAI;TALAF;Mendeley Import (Jan
                17)/AFRL\_STTR",
  language   = "en",
  conference = "International Conference on Entertainment Computing"
}

@TECHREPORT{Wooldridge1982-et,
  title       = "Air combat maneuvering performance measurement state space
                 analysis",
  author      = "Wooldridge, Lee and Kelly, Michael J and Obermayer, Richard W
                 and Vreuls, Donald and Nelson, William H",
  institution = "DTIC Document",
  year        =  1982,
  keywords    = "NotRead;NotRead;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@ARTICLE{Browne2012-lj,
  title    = "A Survey of Monte Carlo Tree Search Methods",
  author   = "Browne, C B and Powley, E and Whitehouse, D and Lucas, S M and
              Cowling, P I and Rohlfshagen, P and Tavener, S and Perez, D and
              Samothrakis, S and Colton, S",
  abstract = "Monte Carlo tree search (MCTS) is a recently proposed search
              method that combines the precision of tree search with the
              generality of random sampling. It has received considerable
              interest due to its spectacular success in the difficult problem
              of computer Go, but has also proved beneficial in a range of
              other domains. This paper is a survey of the literature to date,
              intended to provide a snapshot of the state of the art after the
              first five years of MCTS research. We outline the core
              algorithm's derivation, impart some structure on the many
              variations and enhancements that have been proposed, and
              summarize the results from the key game and nongame domains to
              which MCTS methods have been applied. A number of open research
              questions indicate that the field is ripe for future work.",
  journal  = "IEEE Trans. Comput. Intell. AI Games",
  volume   =  4,
  number   =  1,
  pages    = "1--43",
  month    =  mar,
  year     =  2012,
  keywords = "Artificial intelligence (AI); Computers; Decision theory; Game
              theory; Games; MCTS research; Markov processes; Monte Carlo
              methods; Monte Carlo tree search (MCTS); Monte carlo tree search
              methods; NotRead; artificial intelligence; bandit-based methods;
              computer Go; game search; key game; nongame domains; random
              sampling generality; tree searching; upper confidence bounds
              (UCB); upper confidence bounds for trees (UCT);Computers;Decision
              theory;Game theory;GameAI;Games;Markov
              processes;monte\_carlo;Monte Carlo tree search
              (MCTS);NotRead;TALAF;artificial intelligence;tree
              searching;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@TECHREPORT{L3_undated-ea,
  title    = "{WARFIGHTER} {READINESS} {SCIENCE} Contract No .:
              {FA8650-05-D-6502} Task Order 0013 : Operational Development and
              Validation of {Competency-Based} Methods and Tools for Enhancing
              Human Performance in Air and Space Warfighting Systems Prepared
              by : {AFRL-RH-WP-TR-20}",
  author   = "{L3}",
  keywords = "AFRL;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR;Mendeley Import
              (Jan 17)/AFRL\_STTR/AFRL"
}

@ARTICLE{Mulgund2001-sp,
  title    = "{Large-Scale} Air Combat Tactics Optimization Using Genetic
              Algorithms",
  author   = "Mulgund, Sandeep and Harper, Karen and Zacharias, Greg",
  journal  = "J. Guid. Control Dyn.",
  volume   =  24,
  number   =  1,
  pages    = "140--142",
  year     =  2001,
  keywords = "Combat Metrics;Combat Optimization;TALAF;Mendeley Import (Jan
              17)/AFRL\_STTR",
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Liaw2013-wp,
  title    = "{EVOLVING} A {TEAM} {IN} A {FIRST-PERSON} {SHOOTER} {GAME} {BY}
              {USING} A {GENETIC} {ALGORITHM}",
  author   = "Liaw, Chishyan and Wang, Wei-Hua and Tsai, Ching-Tsorng and Ko,
              Chao-Hui and Hao, Gorden",
  abstract = "Evolving game agents in a first-person shooter game is important
              to game developers and players. Choosing a proper set of
              parameters in a multiplayer game is not a straightforward process
              because consideration must be given to a large number of
              parameters, and therefore requires effort and thorough knowledge
              of the game. Thus, numerous artificial intelligence (AI)
              techniques are applied in the designing of game characters’
              behaviors. This study applied a genetic algorithm to evolve a
              team in the mode of One Flag CTF in Quake III Arena to behave
              intelligently. The source code of the team AI is modified, and
              the progress of the game is represented as a finite state
              machine. A fitness function is used to evaluate the effect of a
              team's tactics in certain circumstances during the game. The team
              as a whole evolves intelligently, and consequently, effective
              strategies are discovered and applied in various situations. The
              experimental results have demonstrated that the proposed
              evolution method is capable of evolving a team's behaviors and
              optimizing the commands in a shooter game. The evolution strategy
              enhances the original game AI and assists game designers in
              tuning the parameters more effectively. In addition, this
              adaptive capability increases the variety of a game and makes
              gameplay more interesting and challenging.",
  journal  = "Appl. Artif. Intell.",
  volume   =  27,
  number   =  3,
  pages    = "199--212",
  year     =  2013,
  keywords = "NotRead;GameAI;NotRead;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@TECHREPORT{McManus2003-og,
  title       = "A Parallel Distributed System for Aircraft Tactical Decision
                 Generation",
  author      = "McManus, John W",
  abstract    = "A research program investigating the use of artificial
                 intelligence (AI) techniques to aid in the development of a
                 Tactical Decision Generator (TDG) for Within Visual Range
                 (WVR) air combat engagements is discussed. The application of
                 AI programming and problem solving methods in the development
                 and implementation of a concurrent version of the Computerized
                 Logic For Air-to-Air Warfare Simulations (CLAWS) program, a
                 second generation TDG, is presented. Concurrent computing
                 environments and programming approaches are discussed and the
                 design and performance of a prototype concurrent TDG system
                 are presented.",
  publisher   = "NASA Langley Technical Report Server",
  institution = "NASA Langley Technical Report Server",
  year        =  2003,
  keywords    = "NotRead;NotRead;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@ARTICLE{Busemeyer1993-gd,
  title       = "Decision field theory: a dynamic-cognitive approach to
                 decision making in an uncertain environment",
  author      = "Busemeyer, J R and Townsend, J T",
  affiliation = "Psychological Sciences, Purdue University, West Lafayette,
                 Indiana 47907-1364.",
  abstract    = "Decision field theory provides for a mathematical foundation
                 leading to a dynamic, stochastic theory of decision behavior
                 in an uncertain environment. This theory is used to explain
                 (a) violations of stochastic dominance, (b) violations of
                 strong stochastic transitivity, (c) violations of independence
                 between alternatives, (d) serial position effects on
                 preference, (e) speed-accuracy trade-off effects in decision
                 making, (f) the inverse relation between choice probability
                 and decision time, (g) changes in the direction of preference
                 under time pressure, (h) slower decision times for avoidance
                 as compared with approach conflicts, and (i) preference
                 reversals between choice and selling price measures of
                 preference. The proposed theory is compared with 4 other
                 theories of decision making under uncertainty.",
  journal     = "Psychol. Rev.",
  volume      =  100,
  number      =  3,
  pages       = "432--459",
  month       =  jul,
  year        =  1993,
  keywords    = "AI benchmarks; AI-assisted game design; Artificial
                 intelligence; Computational modeling; Evolutionary
                 computation; Games; Important; NPC behavior learning;
                 Planning; Seminars; artificial intelligence; commercial games;
                 computational intelligence; computational narrative believable
                 agents; computer games; human computer interaction;
                 human-computer interaction; player modeling; player-game
                 interaction; procedural content generation; search and
                 planning;Computational modeling;Evolutionary
                 computation;GameAI;Games;ai\_planning;TALAF;artificial
                 intelligence;computational intelligence;computer games;player
                 modeling;Mendeley Import (Jan 17)/AFRL\_STTR",
  language    = "en"
}

@INPROCEEDINGS{Gonsalves2004-hk,
  title      = "Software Toolkit for Optimizing Mission Plans ({STOMP})",
  booktitle  = "{AIAA} 1st Intelligent Systems Technical Conference",
  author     = "Gonsalves, Paul and Burge, Janet",
  abstract   = "Recent military actions have demonstrated the need for
                addressing time-sensitive and time-critical targeting. The
                capabilities of precision guided munitions and the further
                development of strike warfare platforms and tactics portend a
                huge increase in effectiveness and lethality of air operations
                and achieving the vision of the US Air Force's Global Strike
                Task Force concept. To fully realize the benefits of these
                strike capable assets and to address the challenges inherent in
                time-sensitive targeting, decision support systems are needed
                to assist warfighters in optimal allocation and near real-time
                re-deployment of air assets, and to support predictive
                battlespace awareness. While meeting a specific operational
                need, additional benefits can accrue through the employment of
                such decision support systems for virtual and constructive
                simulation based training, experimentation, and Command and
                Control (C2) system evaluation and acquisition. Here, we detail
                a Software Toolkit for Optimizing Mission Plans (STOMP). STOMP
                integrates a genetic algorithm-based mechanism to rapidly
                generate, analyze, and visualize mission plans in tandem with
                software interoperability to provide the requisite interface
                and connectivity with Air Force C2 systems and synthetic
                battlespace environments. Copyright \copyright{} 2004 by
                Charles River Analytics, Inc.",
  publisher  = "American Institute of Aeronautics and Astronautics",
  volume     =  1,
  pages      = "391--399",
  month      =  "20~" # sep,
  year       =  2004,
  address    = "Reston, Virigina",
  keywords   = "Combat Optimization;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR",
  language   = "en",
  conference = "AIAA 1st Intelligent Systems Technical Conference"
}

@ARTICLE{Powlson2008-sj,
  title       = "When does nitrate become a risk for humans?",
  author      = "Powlson, David S and Addiscott, Tom M and Benjamin, Nigel and
                 Cassman, Ken G and de Kok, Theo M and van Grinsven, Hans and
                 L'Hirondel, Jean-Louis and Avery, Alex A and van Kessel, Chris",
  editor      = "Sch{\"o}lkopf, B and Platt, J C and Hoffman, T",
  affiliation = "Soil Science Dep, Rothamsted Research, Harpenden, Herts, UK.",
  abstract    = "Is nitrate harmful to humans? Are the current limits for
                 nitrate concentration in drinking water justified by science?
                 There is substantial disagreement among scientists over the
                 interpretation of evidence on the issue. There are two main
                 health issues: the linkage between nitrate and (i) infant
                 methaemoglobinaemia, also known as blue baby syndrome, and
                 (ii) cancers of the digestive tract. The evidence for nitrate
                 as a cause of these serious diseases remains controversial. On
                 one hand there is evidence that shows there is no clear
                 association between nitrate in drinking water and the two main
                 health issues with which it has been linked, and there is even
                 evidence emerging of a possible benefit of nitrate in
                 cardiovascular health. There is also evidence of nitrate
                 intake giving protection against infections such as
                 gastroenteritis. Some scientists suggest that there is
                 sufficient evidence for increasing the permitted concentration
                 of nitrate in drinking water without increasing risks to human
                 health. However, subgroups within a population may be more
                 susceptible than others to the adverse health effects of
                 nitrate. Moreover, individuals with increased rates of
                 endogenous formation of carcinogenic N-nitroso compounds are
                 likely to be susceptible to the development of cancers in the
                 digestive system. Given the lack of consensus, there is an
                 urgent need for a comprehensive, independent study to
                 determine whether the current nitrate limit for drinking water
                 is scientifically justified or whether it could safely be
                 raised.",
  journal     = "J. Environ. Qual.",
  publisher   = "MIT Press",
  volume      =  37,
  number      =  2,
  pages       = "291--295",
  month       =  mar,
  year        =  2008,
  keywords    = "NotRead; bayesian learning; dynamic difficulty adjustment;
                 match-making;NotRead;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR",
  language    = "en"
}

@MISC{Aikins1983-tw,
  title        = "Prototypical knowledge for expert systems - {ScienceDirect}",
  author       = "Aikins, Janice S",
  abstract     = "Knowledge of situations typically encountered in performing a
                  task is an important and useful source of information for
                  solving that task. This paper presents a system that uses a
                  representation of prototypical knowledge to guide computer
                  consultations, and to focus the application of production
                  rules used to represent inferential knowledge in the domain.
                  The explicit representation of control knowledge for each
                  prototypical situation is also emphasized. ?? 1983.",
  month        =  feb,
  year         =  1983,
  howpublished = "\url{http://www.sciencedirect.com/science/article/pii/0004370283900176}",
  note         = "Accessed: 2017-1-17",
  keywords     = "ConsultationSystems;TALAF;Mendeley Import (Jan
                  17)/AFRL\_STTR;Mendeley Import (Jan
                  17)/AFRL\_STTR/ConsultationSystems"
}

@ARTICLE{Togelius2016-oq,
  title    = "Why video games are essential for inventing artificial
              intelligence",
  author   = "Togelius, Julian",
  month    =  jan,
  year     =  2016,
  keywords = "GameAI;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@TECHREPORT{Thomas1989-ys,
  title       = "Performance Measurement Development for Air Combat",
  author      = "Thomas, Gary S and Miller, David C",
  institution = "DTIC Document",
  year        =  1989,
  keywords    = "NotRead;NotRead;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@ARTICLE{Munos2014-bk,
  title    = "From Bandits to {Monte-Carlo} Tree Search: The Optimistic
              Principle Applied to Optimization and Planning",
  author   = "Munos, Remi",
  abstract = "This work covers several aspects of the optimism in the face of
              uncertainty principle applied to large scale optimization
              problems under finite numerical budget. The initial motivation
              for the research reported here originated from the empirical
              success of the so-called Monte-Carlo Tree Search method
              popularized in computer-go and further extended to many other
              games as well as optimization and planning problems. Our
              objective is to contribute to the development of theoretical
              foundations of the field by characterizing the complexity of the
              underlying optimization problems and designing efficient
              algorithms with performance guarantees. The main idea presented
              here is that it is possible to decompose a complex decision
              making problem (such as an optimization problem in a large search
              space) into a sequence of elementary decisions, where each
              decision of the sequence is solved using a (stochastic)
              multi-armed bandit (simple mathematical model for decision making
              in stochastic environments). This so-called hierarchical bandit
              approach (where the reward observed by a bandit in the hierarchy
              is itself the return of another bandit at a deeper level)
              possesses the nice feature of starting the exploration by a
              quasi-uniform sampling of the space and then focusing
              progressively on the most promising area, at different scales,
              according to the evaluations observed so far, and eventually
              performing a local search around the global optima of the
              function. The performance of the method is assessed in terms of
              the optimality of the returned solution as a function of the
              number of function evaluations. Our main contribution to the
              field of function optimization is a class of hierarchical
              optimistic algorithms designed for general search spaces (such as
              metric spaces, trees, graphs, Euclidean spaces, ...) with
              different algorithmic instantiations depending on whether the
              evaluations are noisy or noiseless and whether some measure of
              the ''smoothness'' of the function is known or unknown. The
              performance of the algorithms depend on the local behavior of the
              function around its global optima expressed in terms of the
              quantity of near-optimal states measured with some metric. If
              this local smoothness of the function is known then one can
              design very efficient optimization algorithms (with convergence
              rate independent of the space dimension), and when it is not
              known, we can build adaptive techniques that can, in some cases,
              perform almost as well as when it is known.",
  journal  = "FNT in Machine Learning",
  volume   =  7,
  number   =  1,
  pages    = "1--129",
  year     =  2014,
  keywords = "Bandit theory; Important; Monte-Carlo Tree Search; NotRead;
              Optimism in the face of uncertainty; Upper Confidence
              Bounds;GameAI;TALAF;Important;NotRead;Mendeley Import (Jan
              17)/AFRL\_STTR;Mendeley Import (Jan 17)"
}

@BOOK{Johansson2010-jx,
  title    = "Evaluating the performance of {TEWA} systems",
  author   = "Johansson, Fredrik",
  abstract = "It is in military engagements the task of the air defense to
              protect valuable assets such as air bases from being destroyed by
              hostile aircrafts and missiles. In order to fulfill this mission,
              the ...",
  year     =  2010,
  keywords = "TALAF;Mendeley Import (Jan 17)/AFRL\_STTR",
  language = "eng"
}

@INPROCEEDINGS{Othman2012-fw,
  title     = "Simulation-based optimization of {StarCraft} tactical {AI}
               through evolutionary computation",
  booktitle = "2012 {IEEE} Conference on Computational Intelligence and Games
               ({CIG})",
  author    = "Othman, N and Decraene, J and Cai, W and Hu, N and Low, M Y H
               and Gouaillard, A",
  abstract  = "The development of competent AI for real-time strategy games
               such as StarCraft is made difficult by the myriad of strategic
               and tactical reasonings which must be performed concurrently. A
               significant portion of StarCraft gameplay is in managing
               tactical conflict with opposing forces. We present a modular
               framework for simulating AI vs. AI conflicts through an XML
               specification, whereby the behavioural and tactical components
               for each force can be varied. Evolutionary computation can be
               employed on aspects of the scenario to yield superior solutions.
               Through evolution, a StarCraft AI tournament bot achieved a
               success rate of 68\% against its original version. We also
               demonstrate the use of evolutionary computation to yield a
               tactical attack path to maximise enemy casualties. We believe
               that our framework can be used to perform automatic refinement
               on AI bots in StarCraft.",
  pages     = "394--401",
  year      =  2012,
  keywords  = "AI bots; Computational modeling; Evolutionary computation;
               Force; Games; NotRead; StarCraft gameplay; StarCraft tactical
               AI; XML; XML specification; artificial intelligence; computer
               games; inference mechanisms; real-time strategy games; real-time
               systems; simulation-based optimization; software agents;
               strategic reasoning; tactical conflict management; tactical
               reasoning;Computational modeling;Evolutionary
               computation;GameAI;Games;NotRead;TALAF;artificial
               intelligence;computer games;inference mechanisms;real-time
               strategy games;real-time systems;software agents;Mendeley Import
               (Jan 17)/AFRL\_STTR"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Vallim2013-jb,
  title    = "Online behavior change detection in computer games",
  author   = "Vallim, Rosane M M and Andrade Filho, Jos{\'e} A and de Mello,
              Rodrigo F and de Carvalho, Andr{\'e} C P L F",
  abstract = "Abstract Player Modelling has been receiving much attention from
              the game community in the recent years. The ability to build
              accurate models of player behavior can be useful in many aspects
              of a game. One important aspect is the tracking of a player’s
              behavior along time, informing every time a change is perceived.
              This way, the game Artificial Intelligence can adapt itself to
              better respond to this new behavior. In order to build models of
              player behavior, researchers frequently resort to Machine
              Learning techniques. Such methods work on previously recorded
              game metrics representing player’s interactions with the game
              environment. However, if the player changes styles over time, the
              constructed models get out of date. In order to address this
              drawback, this work proposes the use of and incremental learning
              technique to track a player’s behavior during his/her interaction
              with the game environment. Our approach attempts to automatically
              detect the moments in time when the player changes behavior. We
              apply a change detection technique from the area of Data Stream
              Mining that is based on incremental clustering and novelty
              detection. We also propose three modifications to the original
              technique, in order to formalize change detection, improve
              detection rate and reduce detection delay. Simulations were
              performed considering data produced by the Unreal Tournament
              game, showing the applicability of the method to online tracking
              of a player’s behavior and informing whenever behavior changes
              occur.",
  journal  = "Expert Syst. Appl.",
  volume   =  40,
  number   =  16,
  pages    = "6258--6265",
  month    =  "15~" # nov,
  year     =  2013,
  keywords = "Behavior change detection; Data; Data Stream Mining; Mining;
              NotRead; Online player modeling;
              Stream;GameAI;NotRead;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@INPROCEEDINGS{Mora2012-xl,
  title     = "Dealing with noisy fitness in the design of a {RTS} game bot",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture
               Notes in Artificial Intelligence and Lecture Notes in
               Bioinformatics)",
  author    = "Mora, Antonio M and Fern{\'a}ndez-Ares, Antonio and
               Merelo-Guerv{\'o}s, Juan Juli{\'a}n and Garc{\'\i}a-S{\'a}nchez,
               Pablo",
  abstract  = "This work describes an evolutionary algorithm (EA) for evolving
               the constants, weights and probabilities of a rule-based
               decision engine of a bot designed to play the Planet Wars game.
               The evaluation of the individuals is based on the result of some
               non-deterministic combats, whose outcome depends on random draws
               as well as the enemy action, and is thus noisy. This noisy
               fitness is addressed in the EA and then, its effects are deeply
               analysed in the experimental section. The conclusions shows that
               reducing randomness via repeated combats and re-evaluations
               reduces the effect of the noisy fitness, making then the EA an
               effective approach for solving the problem.",
  publisher = "Springer Berlin Heidelberg",
  volume    = "7248 LNCS",
  pages     = "234--244",
  series    = "Lecture Notes in Computer Science",
  month     =  apr,
  year      =  2012,
  keywords  = "Artificial Intelligence (incl. Robotics); Computation by
               Abstract Devices; Computer Communication Networks; Image
               Processing and Computer Vision; Math Applications in Computer
               Science; NotRead; Programming Techniques;Artificial Intelligence
               (incl. Robotics);GameAI;NotRead;TALAF;Mendeley Import (Jan
               17)/AFRL\_STTR",
  language  = "en"
}

@INPROCEEDINGS{Mulgund1998-ad,
  title     = "Air combat tactics optimization using stochastic genetic
               algorithms",
  booktitle = "Systems, Man, and Cybernetics, 1998. 1998 {IEEE} International
               Conference on",
  author    = "Mulgund, S and Harper, K and Krishnakumar, K and Zacharias, G",
  abstract  = "Describes the development of a software tool for optimizing
               large-scale air combat tactics using stochastic genetic
               algorithms. The tool integrates four key components: (1)
               autonomous blue/red player agents, with their individual
               aircraft and tactics; (2) an engagement simulator used to play
               out a tactical scenario; (3) performance metrics reflecting
               engagement outcome and tactical advantage; and (4) a GA
               ``engine'' for performance-based optimization of blue team
               tactics. The tool's capabilities are demonstrated through the
               optimization of blue team formation and intercept geometry in a
               series of tactical engagements. The tactics implementation uses
               a hierarchical concept that builds large formation tactics from
               small conventional fighting units, facilitating the design of
               tactics compatible with existing air combat principles",
  volume    =  4,
  pages     = "3136--3141 vol.4",
  month     =  oct,
  year      =  1998,
  keywords  = "Aircraft; Algorithm design and analysis; Genetic algorithms;
               Important; Measurement; Optimization methods; Rivers; Search
               methods; Software tools; Stochastic processes; USA Councils; air
               combat tactics optimization; autonomous blue/red player agents;
               engagement simulator; genetic algorithms; intercept geometry;
               military computing; performance metrics; stochastic genetic
               algorithms; tactical engagements; tactical
               scenario;Aircraft;Combat Metrics;Combat
               Optimization;Measurement;Optimization methods;Stochastic
               processes;TALAF;genetic algorithms;Mendeley Import (Jan
               17)/AFRL\_STTR"
}

@MISC{Paper2015-lw,
  title        = "{ResearchGate} - Share and discover research",
  author       = "Paper, Conference",
  abstract     = "Find over 100+ million publications, 11+ million researchers
                  and 1 million answers to research questions. ResearchGate is
                  a network dedicated to science and research. Connect,
                  collaborate and discover scientific publications, jobs and
                  conferences. All for free.",
  year         =  2015,
  howpublished = "\url{http://www.researchgate.net/profile/Frederik{_}Schadd/publication/221024452{_}Opponent{_}Modeling{_}}",
  note         = "Accessed: 2017-1-17",
  keywords     = "NotRead;GameAI;NotRead;TALAF;Mendeley Import (Jan
                  17)/AFRL\_STTR"
}

@ARTICLE{Kelly1988-wt,
  title    = "Performance Measurement during Simulated {Air-to-Air} Combat",
  author   = "Kelly, Michael J",
  abstract = "Measurement and assessment of operator performance of complex
              tasks with decisional and psychomotor components of which only
              ultimate outcome measures are well defined provide difficult
              methodological challenges. One such task is the practice of
              air-to-air combat skills in simulated combat environments such as
              the Air Force Air Combat Maneuvering Instrumentation (ACMI) or
              ground-based visual flight simulators. Measurement and assessment
              of pilot performance in these environments are especially
              challenging because of the special cognitive and psychomotor
              skills involved in the freeplay and gamesmanship of two or more
              competing pilots. A review of the existing approaches to
              automated aircrew performance measurement was performed. Although
              the most common measurement models, using positional advantage or
              disadvantage, provide an adequate performance metric, other
              measures, such as control manipulation and the management of
              kinetic and potential energy, must be added to provide a refined
              performance measurement algorithm.",
  journal  = "Human Factors: The Journal of the Human Factors and Ergonomics
              Society",
  volume   =  30,
  number   =  4,
  pages    = "495--506",
  year     =  1988,
  keywords = "Important; NotRead;Combat Metrics;TALAF;Mendeley Import (Jan
              17)/AFRL\_STTR"
}

@ARTICLE{Clough2002-as,
  title    = "Metrics, schmetrics! How the heck do you determine a {UAV's}
              autonomy anyway?",
  author   = "Clough, Bt",
  abstract = "The recently released DoD Unmanned Aerial Vehicles Roadmap
              discusses advancements in UAV autonomy in terms of autonomous
              control levels (ACL). The ACL concept was pioneered by
              researchers in the Air Force Research Laboratory's Air Vehicles
              Directorate who are charged with developing autonomous air
              vehicles. In the process of developing intelligent autonomous
              agents for UAV control systems we were constantly challenged to
              ``tell us how autonomous a UAV is, and how do you think it can be
              measured?'' Usually we hand-waved away the argument and hoped the
              questioner will go away since this is a very subjective, and
              complicated, subject, but within the last year we've been
              directed to develop national intelligent autonomous UAV control
              metrics - an IQ test for the flyborgs, if you will. The ACL chart
              is the result. We've done this via intense discussions with other
              government labs and industry, and this paper covers the agreed
              metrics (an extension of the OODA - observe, orient, decide, and
              act - loop) as well as the precursors, ``dead-ends'', and
              out-and-out flops investigated to get there.",
  journal  = "Security",
  number   =  990,
  pages    = "313--319",
  month    =  aug,
  year     =  2002,
  keywords = "*ACL(AUTONOMOUS CONTROL LEVELS); *AUTONOMY METRICS; *DETECTORS;
              *DRONES; *MACHINE INTELLIGENCE METRICS; *MEASUREMENT; *PLANNING
              PROGRAMMING BUDGETING; *WORKSHOPS; AERONAUTICAL LABORATORIES; AIR
              FORCE RESEARCH; Adjustable autonomy; CONTROL SYSTEMS; METRICS;
              Pilotless Aircraft; SELF OPERATION; SITUATIONAL AWARENESS;
              SURVIVAL SPACE;AFRL;CONTROL SYSTEMS;TALAF;Mendeley Import (Jan
              17)/AFRL\_STTR;Mendeley Import (Jan 17)/AFRL\_STTR/AFRL",
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Caserta2010-eo,
  title     = "Applications of Evolutionary Computation",
  author    = "Caserta, Marco and Ramirez, Adriana and Vo{\ss}, Stefan",
  editor    = "Di Chio, Cecilia and Brabazon, Anthony and Di Caro, Gianni A and
               Ebner, Marc and Farooq, Muddassar and Fink, Andreas and Grahl,
               J{\"o}rn and Greenfield, Gary and Machado, Penousal and O’Neill,
               Michael and Tarantino, Ernesto and Urquhart, Neil",
  abstract  = "We present a math-heuristic algorithm for the lot sizing problem
               with carryover. The proposed algorithm uses mathematical\$\$n
               programming techniques in a metaheuristic fashion to iteratively
               solve smaller portions of the original problem. More
               specifically,\$\$n we draw ideas from the corridor method to
               design and impose exogenous constraints on the original problem
               and, subsequently,\$\$n we solve to optimality the constrained
               problem using a MIP solver. The algorithm iteratively builds new
               corridors around the\$\$n best solution found within each
               corridor and, therefore, explores adjacent portions of the
               search space. In the attempt of\$\$n fostering diversification
               while exploring the original search space, we generate a pool of
               incumbent solutions for the corridor\$\$n method and, therefore,
               we reapply the corridor method using different starting points.
               The algorithm has been tested on instances\$\$n of a standard
               benchmark library and the reported results show the robustness
               and effectiveness of the proposed scheme.",
  publisher = "Springer Berlin Heidelberg",
  volume    =  6025,
  pages     = "462--471",
  series    = "Lecture Notes in Computer Science",
  year      =  2010,
  address   = "Berlin, Heidelberg",
  keywords  = "GameAI; TALAF; agent-based model; conformity; emergent; game
               theory;GameAI;TALAF;Mendeley Import (Jan 17);Mendeley Import
               (Jan 17)/AFRL\_STTR"
}

@INPROCEEDINGS{Smith2011-aa,
  title     = "An inclusive view of player modeling",
  booktitle = "Proceedings of the 6th International Conference on Foundations
               of Digital Games",
  author    = "Smith, Adam M and Lewis, Chris and Hullet, Kenneth and Sullivan,
               Anne",
  abstract  = "``Player modeling'' is a loose concept. It can equally apply to
               everything from a predictive model of player actions resulting
               from machine learning to a designer's description of a player's
               expected reactions in response to some piece of game content.
               This lack of a precise terminology prevents practitioners from
               quickly finding introductions to applicable modeling methods or
               determining viable alternatives to their own techniques. We
               introduce a vocabulary that distinguishes between the major
               existing player modeling applications and techniques. Four
               facets together define the kind for a model: the scope of
               application, the purpose of use, the domain of modeled details,
               and the source of a model's derivation or motivation. This
               vocabulary allows the identification of relevant player modeling
               methods for particular problems and clarifies the roles that a
               player model can take.",
  publisher = "ACM",
  pages     = "301--303",
  month     =  "29~" # jun,
  year      =  2011,
  keywords  = "Important; TALAF; game design; games; player modeling;
               taxonomy;Important;TALAF;GameAI;Mendeley Import (Jan
               17);Mendeley Import (Jan 17)/AFRL\_STTR"
}

@ARTICLE{Paranjape2006-qr,
  title    = "Combat aircraft agility metrics-a review",
  author   = "Paranjape, Aditya A and Ananthkrishnan, N",
  journal  = "J. Aerosp. Sci. Technol.",
  volume   =  58,
  number   =  2,
  pages    = "143--154",
  year     =  2006,
  keywords = "Combat Metrics;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@TECHREPORT{McManus1990-pq,
  title       = "Artificial Intelligence ({AI}) Based Tactical Guidance For
                 Fighter Aircraft",
  author      = "McManus, John W and Goodrich, Kenneth H",
  abstract    = "A research program investigating the use of Artificial
                 Intelligence (AI) techniques to aid in the development of a
                 Tactical Decision Generator (TDG) for Within Visual Range
                 (WVR) air combat engagements is discussed. The application of
                 AI programming and problem solving methods in the development
                 and implementation of the Computerized Logic For Air-to-Air
                 Warfare Simulations (CLAWS), a second generation TDG, is
                 presented. The Knowledge-Based Systems used by CLAWS to aid in
                 the tactical decision-making process are outlined in detail,
                 and the results of tests to evaluate the performance of CLAWS
                 versus a baseline TDG developed in FORTRAN to run in real-time
                 in the Langley Differential Maneuvering Simulator (DMS), are
                 presented. To date, these test results have shown significant
                 performance gains with respect to the TDG baseline in
                 one-versus-one air combat engagements, and the AI-based TDG
                 software has proven to be much easier to modify and maintain
                 than the baseline FORTRAN TDG programs. Alternate computing
                 environments and programming approaches, including the use of
                 parallel algorithms and heterogeneous computer networks are
                 discussed, and the design and performance of a prototype
                 concurrent TDG system are presented.",
  institution = "NASA Langley Technical Report Server",
  year        =  1990,
  keywords    = "Combat Metrics;Important;TALAF;Mendeley Import (Jan
                 17)/AFRL\_STTR"
}

@INPROCEEDINGS{Holmgard2014-zq,
  title     = "Evolving personas for player decision modeling",
  booktitle = "2014 {IEEE} Conference on Computational Intelligence and Games",
  author    = "Holmg{\aa}rd, C and Liapis, A and Togelius, J and Yannakakis, G
               N",
  abstract  = "This paper explores how evolved game playing agents can be used
               to represent a priori defined archetypical ways of playing a
               test-bed game, as procedural personas. The end goal of such
               procedural personas is substituting players when authoring game
               content manually, procedurally, or both (in a mixed-initiative
               setting). Building on previous work, we compare the performance
               of newly evolved agents to agents trained via Q-learning as well
               as a number of baseline agents. Comparisons are performed on the
               grounds of game playing ability, generalizability, and
               conformity among agents. Finally, all agents' decision making
               styles are matched to the decision making styles of human
               players in order to investigate whether the different methods
               can yield agents who mimic or differ from human decision making
               in similar ways. The experiments performed in this paper
               conclude that agents developed from a priori defined objectives
               can express human decision making styles and that they are more
               generalizable and versatile than Q-learning and hand-crafted
               agents.",
  pages     = "1--8",
  month     =  aug,
  year      =  2014,
  keywords  = "Decision making; Important; Navigation; Q-learning; agents
               conformity; agents decision making styles; archetypical ways;
               authoring; baseline agents; computer games; evolved game playing
               agents; evolving personas; game content; game playing ability;
               game playing generalizability; hand-crafted agents; human
               decision making; human players; learning (artificial
               intelligence); multi-agent systems; player decision modeling;
               procedural personas; test-bed game playing;Decision
               making;GameAI;Q-learning;TALAF;computer games;learning
               (artificial intelligence);Mendeley Import (Jan 17)/AFRL\_STTR"
}

@INPROCEEDINGS{Drachen2009-ao,
  title     = "Player modeling using self-organization in Tomb Raider:
               Underworld",
  booktitle = "2009 {IEEE} Symposium on Computational Intelligence and Games",
  author    = "Drachen, A and Canossa, A and Yannakakis, G N",
  abstract  = "We present a study focused on constructing models of players for
               the major commercial title Tomb Raider: Underworld (TRU).
               Emergent self-organizing maps are trained on high-level playing
               behavior data obtained from 1365 players that completed the TRU
               game. The unsupervised learning approach utilized reveals four
               types of players which are analyzed within the context of the
               game. The proposed approach automates, in part, the traditional
               user and play testing procedures followed in the game industry
               since it can inform game developers, in detail, if the players
               play the game as intended by the game design. Subsequently,
               player models can assist the tailoring of game mechanics in
               real-time for the needs of the player type identified.",
  pages     = "1--8",
  year      =  2009,
  keywords  = "Automatic testing; Computer industry; Computerized monitoring;
               Data mining; Emergent self-organizing maps; Gold; Instruments;
               Player modeling; Production; Self organizing feature maps; Tomb
               Raider Underworld; Tomb Raider: Underworld; Toy industry;
               Unsupervised learning; computer games; emergent self-organizing
               maps; game design; game industry; high-level playing behavior
               data obtained; learning (artificial intelligence); player
               modeling; self-organising feature maps; unsupervised learning;
               user modelling;GameAI;TALAF;computer games;learning (artificial
               intelligence);player modeling;Mendeley Import (Jan
               17)/AFRL\_STTR"
}

@INPROCEEDINGS{Weber2011-fq,
  title     = "A Particle Model for State Estimation in {Real-Time} Strategy
               Games",
  booktitle = "Seventh Artificial Intelligence and Interactive Digital
               Entertainment Conference",
  author    = "Weber, Ben George and Mateas, Michael and Jhala, Arnav",
  abstract  = "A big challenge for creating human-level game AI is building
               agents capable of operating in imperfect information
               environments. In real-time strategy games the technological
               progress of an opponent and locations of enemy units are
               partially observable. To overcome this limitation, we explore a
               particle-based approach for estimating the location of enemy
               units that have been encountered. We represent state estimation
               as an optimization problem, and automatically learn parameters
               for the particle model by mining a corpus of expert StarCraft
               replays. The particle model tracks opponent units and provides
               conditions for activating tactical behaviors in our StarCraft
               bot. Our results show that incorporating a learned particle
               model improves the performance of EISBot by 10\% over baseline
               approaches.",
  pages     = "103--108",
  month     =  "9~" # oct,
  year      =  2011,
  keywords  = "Real-Time Strategy Games;GameAI;TALAF;Mendeley Import (Jan
               17)/AFRL\_STTR",
  language  = "en"
}

@TECHREPORT{Kelly1979-in,
  title       = "Air Combat Maneuvering Performance Measurement",
  author      = "Kelly, Michael J and Wooldridge, Lee and Hennessy, Robert T
                 and Vreuls, Donald and Barnebey, Steve F and Cotton, John C
                 and Reed, John C",
  abstract    = "Due to the complex, dynamic and fast-moving nature of the air
                 combat task, performance assessment during air-to-air combat
                 provides many unique measurement problems. A combined
                 analytical and empirical technical approach was used to
                 develop a candidate measurement structure and algorithm for
                 the measurement of pilot performance during one-versus-one air
                 combat maneuvering. Nearly all of 28 candidate measures were
                 found to discriminate between high and low skilled pilots
                 during free engagements on the Simulator for Air-to-Air
                 Combat. Discriminant analyses provided a measurement algorithm
                 consisting of 13 measures which accounted for 51\% of the
                 variance in the performance data and which predicted
                 membership in high or low skill groups with 92\% accuracy.",
  volume      =  23,
  pages       = "324--328",
  institution = "CANYON RESEARCH GROUP INC WESTLAKE VILLAGE CA, CANYON RESEARCH
                 GROUP INC WESTLAKE VILLAGE CA",
  month       =  sep,
  year        =  1979,
  keywords    = "Combat Metrics; TALAF;TALAF;Mendeley Import (Jan
                 17)/AFRL\_STTR",
  language    = "en"
}

@ARTICLE{Yannakakis2009-xf,
  title    = "{Real-Time} Game Adaptation for Optimizing Player Satisfaction",
  author   = "Yannakakis, G N and Hallam, J",
  abstract = "A methodology for optimizing player satisfaction in games on the
              ``playware'' physical interactive platform is demonstrated in
              this paper. Previously constructed artificial neural network user
              models, reported in the literature, map individual playing
              characteristics to reported entertainment preferences for
              augmented-reality game players. An adaptive mechanism then
              adjusts controllable game parameters in real time in order to
              improve the entertainment value of the game for the player. The
              basic approach presented here applies gradient ascent to the user
              model to suggest the direction of parameter adjustment that leads
              toward games of higher entertainment value. A simple rule set
              exploits the derivative information to adjust specific game
              parameters to augment the entertainment value. Those adjustments
              take place frequently during the game with interadjustment
              intervals that maintain the user model's accuracy. Performance of
              the adaptation mechanism is evaluated using a game survey
              experiment. Results indicate the efficacy and robustness of the
              mechanism in adapting the game according to a user's individual
              playing features and enhancing the gameplay experience. The
              limitations and the use of the methodology as an effective
              adaptive mechanism for entertainment capture and augmentation are
              discussed.",
  journal  = "IEEE Trans. Comput. Intell. AI Games",
  volume   =  1,
  number   =  2,
  pages    = "121--133",
  month    =  jun,
  year     =  2009,
  keywords = "Augmented-reality games; artificial intelligence; artificial
              neural network user model; augmented reality; augmented-reality
              game player; computer games; controllable game parameter; game
              survey experiment; gradient ascent; neural nets; neuro-evolution;
              player satisfaction; player satisfaction optimisation; playware
              physical interactive platform; real-time adaptation; real-time
              game adaptation; user modeling;GameAI;TALAF;artificial
              intelligence;computer games;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@INPROCEEDINGS{Huynh1987-kl,
  title      = "Numerical optimization of air combat maneuvers",
  booktitle  = "Guidance, Navigation and Control Conference",
  author     = "Huynh, H and Costes, P H and Aumasson, C",
  publisher  = "American Institute of Aeronautics and Astronautics",
  pages      = "647--658",
  month      =  "17~" # aug,
  year       =  1987,
  address    = "Reston, Virigina",
  keywords   = "Combat Metrics;Important;TALAF;Mendeley Import (Jan
                17)/AFRL\_STTR",
  language   = "en",
  conference = "Guidance, Navigation and Control Conference"
}

@ARTICLE{Schreiber2007-ks,
  title    = "Distributed mission operations within-simulator training
              effectiveness baseline study. Volume 2. metric development and
              objectively quantifying the degree of learning",
  author   = "Schreiber, Brian T and Stock, William A and Bennett, Jr, Winston",
  abstract = "The current work reports only the objective data from
              AFRL-HE-AZ-TR-2006-0015, Volume I, Distributed Mission Operations
              Within-Simulator Training Effectiveness: Summary Report, but here
              we expand the reporting of objective data both in depth and
              breadth.We examined F-16 pilots participating in week-long
              Distributed Mission Operation (DMO) training exercises and
              compared extensive computer-collected data between
              beginning-of-week and end-of-week pilot performance on
              mirror-image scenarios. The DMO research environment in Mesa, AZ
              consisted of four high-fidelity F-16 simulators and one
              high-fidelity Airborne Warning and Control System simulator.
              Participating F-16 teams flew over 40 total scenarios according
              to a five-day syllabus, book-ended on Monday and Friday by
              mirror-image point defense air combat benchmark scenarios. Seven
              mission outcome measures were found to be significantly better on
              Friday than Monday: A 58.33\% decrease in enemy strikers reaching
              their target, 38.10\% greater distance from the base the F-16s
              disposed of the strikers, 54.77\% fewer F-16 mortalities, 75.26\%
              more enemy striker kills (before reaching base), 6.82\% higher
              proportion of Viper Advanced Medium Range Air-to-Air Missile
              (AMRAAM) shots resulting in a kill, 51.60\% lower proportion of
              enemy Alamo missile shots resulting in a kill, and a highly
              impressive 314.21\% increase in an overall summary scoring scheme
              developed by subject matter experts. Significant trends were also
              found for a number of other metrics assessing skills. Of all the
              measures investigated in the current work, not a single
              offensive/defensive trade-off was observed, which significantly
              strengthens our conclusion that significant within-simulator
              learning took place.",
  journal  = "Star",
  volume   =  45,
  number   =  5,
  year     =  2007,
  keywords = "10: Aerospace Engineering (General) (MT); 99: General (AH); Aero;
              Air combat; Benchmarking; Control systems; Folder -
              AFRL\{\_\}Wink; Important; Learning; Mechanical \& Transportation
              Engineering (MT); Mechanical \{\&\} Transportation Engineering
              (MT); Mesas; Military aircraft; Military planes; Missiles;
              Missions; Mortality; NotRead; Pilot performance; Pilots; Raw
              materials; Scoring; Shot; Simulators; Tradeoffs; Training;
              Warning;AFRL;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR;Mendeley
              Import (Jan 17)/AFRL\_STTR/AFRL"
}

@BOOK{Anders_Ericsson2009-cu,
  title     = "Development of Professional Expertise: Toward Measurement of
               Expert Performance and Design of Optimal Learning Environments",
  author    = "Anders Ericsson, K",
  abstract  = "Professionals such as medical doctors, airplane pilots, lawyers,
               and technical specialists find that some of their peers have
               reached high levels of achievement that are difficult to measure
               objectively. In order to understand to what extent it is
               possible to learn from these expert performers for the purpose
               of helping others improve their performance, we first need to
               reproduce and measure this performance. This book is designed to
               provide the first comprehensive overview of research on the
               acquisition and training of professional performance as measured
               by objective methods rather than by subjective ratings by
               supervisors. In this collection of articles, the world's
               foremost experts discuss methods for assessing the experts'
               knowledge and review our knowledge on how we can measure
               professional performance and design training environments that
               permit beginning and experienced professionals to develop and
               maintain their high levels of performance, using examples from a
               wide range of professional domains.",
  publisher = "Cambridge University Press",
  month     =  "22~" # jun,
  year      =  2009,
  keywords  = "AFRL;TALAF;Mendeley Import (Jan 17)/AFRL\_STTR;Mendeley Import
               (Jan 17)/AFRL\_STTR/AFRL",
  language  = "en"
}

@ARTICLE{Schreiber2007-fl,
  title    = "Distributed mission operations within-simulator training
              effectiveness baseline study. Volume 1. Summary report",
  author   = "Schreiber, Brian T and Bennett, Jr., Winston and Bennett, Jr,
              Winston and Bennett, Jr., Winston and Bennett, Jr, Winston",
  abstract = "Distributed Mission Operations (DMO) training consists of
              multiplayer networked environments enabling warfighting training
              on higher-order individual and team-oriented skills.
              Surprisingly, only sparse DMO training effectiveness literature
              can be found and very few studies contain objective data. The
              dataset used in this research represents the largest DMO
              effectiveness dataset known to exist today (76 teams/384 pilots
              on over 3,000 engagements), containing 33 months' worth of
              multi-faceted DMO data, including objective data from the
              simulators, multiple participant surveys, subject matter expert
              (SME) ratings of performance, and knowledge structure tests.
              Observed performance differences between the pre- and post-test
              mirror-image point-defense assessment sessions served as the
              primary basis for the evaluation. Results were dramatic: On the
              post-test, 58.33\{\{\}\{\%\}\{\}\} fewer enemy strikers reached
              their target and there were 54.77\{\{\}\{\%\}\{\}\} fewer F-16
              mortalities. Furthermore, there were corroborating significant
              improvements from the numerous measured skill metrics (e.g.,
              weapons employment), SME expert observer ratings, and participant
              self-report opinion ratings. These converging results provide
              substantial evidence that pilots become much more proficient on
              key aspects of combat mission objectives as a function of
              training within the simulator. Finding highly significant
              performance differences across multiple datasets between the pre-
              and post-tests with a combat-ready participant pool in a complex
              task/environment forms a formidable argument that DMO training
              yields considerable within-simulator warfighter competency
              improvement. In this report, we summarize the different dataset
              classes, overview the primary hypotheses and results associated
              with each, and discuss the convergence of the datasets to
              illustrate the 'big picture' DMO training effectiveness. As such,
              more detailed hypotheses, analyses, and discussions are contained
              in separate reports (Vols. II through V).",
  journal  = "Star",
  volume   =  45,
  number   =  5,
  year     =  2007,
  keywords = "10: Aerospace Engineering (General) (MT); 99: General (AH); AFRL;
              Aero; Convergence; DMO; Distributed Mission Operations;
              Employment; Folder - AFRL\{\{\}\{\_\}\{\}\}Wink; Hypotheses; MEC;
              Mechanical \{\&\} Transportation Engineering (MT); Mechanical
              \{\{\}\{\&\}\{\}\} Transportation Engineering (M; Military
              aircraft; Military planes; Mission Essential Competencies;
              Missions; Mortality; Networked environments; NotRead; Observers;
              Pilots; Pools; Ratings; Simulators; Skill acquisition; Surveys;
              TALAF; Training; Training effectiveness; Warfighter training;
              Weapons;AFRL;NotRead;TALAF;Mendeley Import (Jan 17);Mendeley
              Import (Jan 17)/AFRL\_STTR;Mendeley Import (Jan
              17)/AFRL\_STTR/AFRL"
}

@INCOLLECTION{Goodrich1990-ei,
  title     = "An integrated environment for tactical guidance research and
               evaluation",
  booktitle = "Orbital Debris Conference: Technical Issues andFuture Directions",
  author    = "Goodrich, Kenneth and Mcmanus, John",
  publisher = "American Institute of Aeronautics and Astronautics",
  month     =  apr,
  year      =  1990,
  keywords  = "TALAF;Mendeley Import (Jan 17)/AFRL\_STTR",
  language  = "en"
}

@ARTICLE{Wang2013-fy,
  title     = "Probabilistic movement modeling for intention inference in
               human-robot interaction",
  author    = "Wang, Z and Mulling, K and Deisenroth, M P and Ben Amor, H and
               Vogt, D and Scholkopf, B and Peters, J",
  abstract  = "Inference of human intention may be an essential step towards
               understanding human actions [21] and is
               hence\textbackslashnimportant for realizing efficient
               human-robot interaction. In this paper, we propose the
               Intention-Driven Dynamics Model (IDDM), a latent variable model
               for inferring unknown human intentions. We train the model based
               on observed human behaviors/actions and we introduce an
               approximate inference algorithm to efficiently infer the human's
               intention from an ongoing action.\textbackslashnWe verify the
               feasibility of the IDDM in two scenarios, i.e., target inference
               in robot table tennis and action recognition for interactive
               humanoid robots. In both tasks, the IDDM achieves substantial
               improvements over state-of-the-art regression and
               classification.",
  journal   = "Int. J. Rob. Res.",
  publisher = "MIT Press",
  volume    =  32,
  number    =  7,
  pages     = "841--858",
  series    = "Adaptive computation and machine learning",
  month     =  "1~" # jun,
  year      =  2013,
  address   = "Cambridge, Mass",
  keywords  = "Approximate inference; Gaussian process; Important; intention
               inference;Folder - Spring2016;TALAF;Mendeley Import (Jan
               17)/ReadingGroups;Mendeley Import (Jan 17)/AFRL\_STTR",
  language  = "en"
}

@INPROCEEDINGS{Cole2004-fh,
  title     = "Using a genetic algorithm to tune first-person shooter bots",
  booktitle = "Proceedings of the 2004 Congress on Evolutionary Computation
               ({IEEE} Cat. {No.04TH8753})",
  author    = "Cole, N and Louis, S J and Miles, C",
  abstract  = "First-person shooter robot controllers (bots) are generally
               rule-based expert systems written in C/C++. As such, many of the
               rules are parameterized with values, which are set by the
               software designer and finalized at compile time. The
               effectiveness of parameter values is dependent on the knowledge
               the programmer has about the game. Furthermore, parameters are
               non-linearly dependent on each other. This paper presents an
               efficient method for using a genetic algorithm to evolve sets of
               parameters for bots which lead to their playing as well as bots
               whose parameters have been tuned by a human with expert
               knowledge about the game's strategy. This indicates genetic
               algorithms as being a potentially useful method for tuning bots.",
  volume    =  1,
  pages     = "139--145 Vol.1",
  month     =  jun,
  year      =  2004,
  keywords  = "CONTROL SYSTEMS; Counter Strike game; Counting circuits; Expert
               systems; Game theory; Logic; NotRead; Programming profession;
               Robot control; Weapons; artificial intelligence; bots tuning;
               computer games; first-person shooter robot controllers; game
               artificial intelligence; genetic algorithm; genetic algorithms;
               software agents; software design;CONTROL SYSTEMS;Game
               theory;GameAI;NotRead;TALAF;artificial intelligence;computer
               games;genetic algorithm;genetic algorithms;software
               agents;Mendeley Import (Jan 17)/AFRL\_STTR"
}

@INPROCEEDINGS{Yannakakis2012-jp,
  title     = "Game {AI} revisited",
  booktitle = "Proceedings of the 9th conference on Computing Frontiers",
  author    = "Yannakakis, Geogios N",
  publisher = "ACM",
  pages     = "285--292",
  month     =  "15~" # may,
  year      =  2012,
  address   = "New York, New York, USA",
  keywords  = "game AI flagships; game artificial intelligence; game data
               mining; player experience modeling; procedural content
               generation;GameAI;TALAF;Mendeley Import (Jan
               17)/AFRL\_STTR;Mendeley Import (Jan 17)"
}

@ARTICLE{Bakkes2009-jh,
  title    = "Rapid and Reliable Adaptation of Video Game {AI}",
  author   = "Bakkes, S and Spronck, P and den Herik, J van",
  abstract = "Current approaches to adaptive game AI typically require numerous
              trials to learn effective behavior (i.e., game adaptation is not
              rapid). In addition, game developers are concerned that applying
              adaptive game AI may result in uncontrollable and unpredictable
              behavior (i.e., game adaptation is not reliable). These
              characteristics hamper the incorporation of adaptive game AI in
              commercially available video games. In this paper, we discuss an
              alternative to these current approaches. Our alternative approach
              to adaptive game AI has as its goal adapting rapidly and reliably
              to game circumstances. Our approach can be classified in the area
              of case-based adaptive game AI. In the approach, domain knowledge
              required to adapt to game circumstances is gathered automatically
              by the game AI, and is exploited immediately (i.e., without
              trials and without resource-intensive learning) to evoke
              effective behavior in a controlled manner in online play. We
              performed experiments that test case-based adaptive game AI on
              three different maps in a commercial real-time strategy (RTS)
              game. From our results, we may conclude that case-based adaptive
              game AI provides a strong basis for effectively adapting game AI
              in video games.",
  journal  = "IEEE Trans. Comput. Intell. AI Games",
  volume   =  1,
  number   =  2,
  pages    = "93--104",
  month    =  jun,
  year     =  2009,
  keywords = "artificial intelligence;computer games;AI;adaptive
              game;artificial intelligence;real-time strategy;video
              game;Adaptive behavior;game AI;rapid adaptation;real-time
              strategy (RTS) games;reliable adaptation;GameAI;TALAF;Mendeley
              Import (Jan 17)/AFRL\_STTR"
}

@ARTICLE{Marshall2013-ef,
  title    = "Games, Gameplay, and {BCI}: The State of the Art",
  author   = "Marshall, D and Coyle, D and Wilson, S and Callaghan, M",
  abstract = "Brain-computer interfaces (BCIs) and basic computer games have
              been interconnected since BCI development began, exploiting
              gameplay elements as a means of enhancing performance in BCI
              training protocols and entertaining and challenging participants
              while training to use a BCI. By providing the BCI user with an
              entertaining environment, researchers hope to assist users in
              becoming more proficient at controlling a BCI system. BCIs have
              been used to enrich the experience of abled-bodied and physically
              impaired users in various computer applications, in particular,
              computer games. BCI games have been reviewed previously, yet a
              critical evaluation of ``gameplay'' within BCI games has not been
              undertaken. Gameplay is a key aspect of any computer game and
              encompasses the challenges presented to the player, the actions
              made available to the player by the game designer to overcome the
              challenges and the interaction mechanism in the game. Here, the
              appropriateness of game genres (a category of games characterized
              by a particular set of gameplay challenges) and the associated
              gameplay challenges for different BCI paradigms is evaluated. The
              gameplay mechanics employed across a range of BCI games are
              reviewed and evaluated in terms of the BCI control strategy's
              suitability, considering the genre and gameplay mechanics
              employed. A number of recommendations for the field relating to
              genre-specific BCI-games development and assessing user
              performance are also provided for BCI game developers.",
  journal  = "IEEE Trans. Comput. Intell. AI Games",
  volume   =  5,
  number   =  2,
  pages    = "82--99",
  month    =  jun,
  year     =  2013,
  keywords = "brain-computer interfaces;computer games;BCI development;BCI
              system;BCI training protocols;brain computer interfaces;computer
              applications;computer games;game designer;gameplay;interaction
              mechanism;Computers;Control systems;Electric
              potential;Electroencephalography;Games;Training;Visualization;Brain--computer
              interfaces (BCIs);game
              design;gameplay;games;review;Games;Mendeley Import (Jan 17)/Other"
}

@INPROCEEDINGS{Chamberlain1990-ez,
  title     = "Analysis in {HUGIN} of Data Conflict",
  booktitle = "{UAI1990}",
  author    = "Chamberlain, Bo and Jensen, Finn Verner and Jensen, Frank and
               Nordahl, Torsten",
  abstract  = "After a brief introduction to causal probabilistic networks and
               the HUGIN approach, the problem of conflicting data is
               discussed. A measure of conflict is defined, and it is used in
               the medical diagnostic system MUNIN. Finally, it is discussed
               how to distinguish between conflicting data and a rare case.",
  month     =  "27~" # mar,
  year      =  1990,
  keywords  = "Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Sydney2013-ng,
  title     = "Optimizing algebraic connectivity by edge rewiring",
  author    = "Sydney, Ali and Scoglio, Caterina and Gruenbacher, Don",
  abstract  = "Robustness in complex networks is an ongoing research effort
               that seeks to improve the connectivity of networks against
               attacks and failures. Among other measures, algebraic
               connectivity has been used to characterize processes such as
               damped oscillation of liquids in connected pipes. Similar
               characterizations include the number of edges necessary to
               disconnect a network: the larger the algebraic connectivity, the
               larger the number of edges required to disconnect a network and
               hence, the more robust a network. In this paper, we answer the
               question, ``Which edge can we rewire to have the largest
               increase in algebraic connectivity?''. Furthermore, we extend
               the rewiring of a single edge to rewiring multiple edges to
               realize the maximal increase in algebraic connectivity. The
               answer to this question above can provide insights to decision
               makers within various domains such as communication and
               transportation networks, who seek an efficient solution to
               optimize the connectivity and thus increase the robustness of
               their networks. Most importantly, our analytical and numerical
               results not only provide insights to the number of edges to
               rewire, but also the location in the network where these edges
               would effectuate the maximal increase in algebraic connectivity
               and therefore, enable a maximal increase in robustness.",
  journal   = "Appl. Math. Comput.",
  publisher = "Elsevier Inc.",
  volume    =  219,
  number    =  10,
  pages     = "5465--5479",
  month     =  "15~" # jan,
  year      =  2013,
  keywords  = "Algebraic connectivity; Complex networks; Optimization;
               Rewiring; Robustness;Folder - CSCI5454-Algorithms;Mendeley
               Import (Jan 17)/Other"
}

@ARTICLE{Carrillo-arce2013-ja,
  title    = "Decentralized Multi-robot Cooperative Localization using
              Covariance Intersection",
  author   = "Carrillo-arce, Luis C and Nerurkar, Esha D",
  pages    = "1412--1417",
  year     =  2013,
  keywords = "Folder - Spring2016;Mendeley Import (Jan 17)/Assurances;Mendeley
              Import (Jan 17)/ReadingGroups"
}

@ARTICLE{Shi2014-br,
  title     = "An event-triggered approach to state estimation with multiple
               point- and set-valued measurements",
  author    = "Shi, Dawei and Chen, Tongwen and Shi, Ling",
  abstract  = "Abstract In this work, we consider state estimation based on the
               information from multiple sensors that provide their measurement
               updates according to separate event-triggering conditions. An
               optimal sensor fusion problem based on the hybrid measurement
               information (namely, point- and set-valued measurements) is
               formulated and explored. We show that under a commonly-accepted
               Gaussian assumption, the optimal estimator depends on the
               conditional mean and covariance of the measurement innovations,
               which applies to general event-triggering schemes. For the case
               that each channel of the sensors has its own event-triggering
               condition, closed-form representations are derived for the
               optimal estimate and the corresponding error covariance matrix,
               and it is proved that the exploration of the set-valued
               information provided by the event-triggering sets guarantees the
               improvement of estimation performance. The effectiveness of the
               proposed event-based estimator is demonstrated by extensive
               Monte Carlo simulation experiments for different categories of
               systems and comparative simulation with the classical Kalman
               filter.",
  journal   = "Automatica",
  publisher = "Elsevier Ltd",
  volume    =  50,
  number    =  6,
  pages     = "1641--1648",
  year      =  2014,
  keywords  = "Event-based estimation; Kalman filters; Sensor fusion; Wireless
               sensor networks;Folder - Spring2016;Mendeley Import (Jan
               17)/Assurances;Mendeley Import (Jan 17)/ReadingGroups"
}

@ARTICLE{De_Abreu2007-gv,
  title    = "Old and new results on algebraic connectivity of graphs",
  author   = "de Abreu, Nair Maria Maia",
  abstract = "This paper is a survey of the second smallest eigenvalue of the
              Laplacian of a graph G, best-known as the algebraic connectivity
              of G, denoted a(G). Emphasis is given on classifications of
              bounds to algebraic connectivity as a function of other graph
              invariants, as well as the applications of Fiedler vectors
              (eigenvectors related to a(G)) on trees, on hard problems in
              graphs and also on the combinatorial optimization problems.
              Besides, limit points to a(G) and characterizations of extremal
              graphs to a(G) are described, especially those for which the
              algebraic connectivity is equal to the vertex connectivity.",
  journal  = "Linear Algebra Appl.",
  volume   =  423,
  number   =  1,
  pages    = "53--73",
  month    =  "1~" # may,
  year     =  2007,
  keywords = "Folder - CSCI5454-Algorithms; Laplacian of graph; Algebraic
              connectivity; Vertex and edge connectivities; Bounds for the
              algebraic connectivity; Fiedler vectors; Limit points; Extremal
              graphs; Laplacian integral graphs;Folder -
              CSCI5454-Algorithms;Mendeley Import (Jan 17);Mendeley Import (Jan
              17)/Other"
}

@ARTICLE{Fiedler1973-vo,
  title    = "Algebraic connectivity of graphs",
  author   = "Fiedler, Miroslav",
  journal  = "Czechoslovak Math. J.",
  volume   =  23,
  number   =  2398,
  pages    = "298--305",
  year     =  1973,
  keywords = "Folder - CSCI5454-Algorithms;Folder -
              CSCI5454-Algorithms;Mendeley Import (Jan 17);Mendeley Import (Jan
              17)/Other"
}

@ARTICLE{Fiedler1975-gf,
  title     = "A property of eigenvectors of nonnegative symmetric matrices and
               its application to graph theory",
  author    = "Fiedler, Miroslav",
  journal   = "Czechoslovak Math. J.",
  publisher = "Institute of Mathematics, Academy of Sciences of the Czech
               Republic",
  volume    =  25,
  number    =  4,
  pages     = "619--633",
  year      =  1975,
  keywords  = "Folder - CSCI5454-Algorithms;Folder -
               CSCI5454-Algorithms;Mendeley Import (Jan 17);Mendeley Import
               (Jan 17)/Other"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Stone2015-te,
  title     = "Information Theory: A Tutorial Introduction",
  author    = "Stone, J V",
  abstract  = "Originally developed by Claude Shannon in the 1940s, information
               theory laid the foundations for the digital revolution, and is
               now an essential tool in telecommunications, genetics,
               linguistics, brain sciences, and deep space communication. In
               this richly illustrated book, accessible examples are used to
               introduce information theory in terms of everyday games like ‘20
               questions’ before more advanced topics are explored. Online
               MatLab and Python computer programs provide hands-on experience
               of information theory in action, and PowerPoint slides give
               support for teaching. Written in an informal style, with a
               comprehensive glossary and tutorial appendices, this text is an
               ideal primer for novices who wish to learn the essential
               principles and applications of information theory.",
  publisher = "Sebtel Press",
  pages     = "260",
  edition   =  1,
  year      =  2015,
  keywords  = "Information theory;Textbook;Mendeley Import (Jan
               17)/TextBooks;Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

@BOOK{Cappe2005-uw,
  title    = "Inference in Hidden Markov Models",
  author   = "Capp{\'e}, Olivier and Moulines, Eric and Ryd{\'e}n, Tobias",
  abstract = "This book is a comprehensive treatment of inference for hidden
              Markov models, including both algorithms and statistical theory.
              Topics range from filtering and smoothing of the hidden Markov
              chain to parameter estimation, Bayesian methods and estimation of
              the number of states. In a unified way the book covers both
              models with finite state spaces and models with continuous state
              spaces (also called state-space models) requiring approximate
              simulation-based algorithms that are also described in detail.
              Many examples illustrate the algorithms and theory. This book
              builds on recent developments to present a self-contained view.",
  volume   =  48,
  pages    = "574--575",
  year     =  2005,
  keywords = "Hidden Markov models;Textbook;Mendeley Import (Jan
              17)/TextBooks;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Ermachenko1975-lf,
  title    = "[Activation of the fermentative activity of yeasts by yeast cell
              fractions]",
  author   = "Ermachenko, V A and Braginskaia, F I and Krugliakova, K E",
  abstract = "H. Akaike, 'A new look at the statistical model identification',
              IEEE Transactions\textbackslashnon Automatic Control, Vol. 19,
              No. 6, pp. 716-723, 1974",
  journal  = "Izv. Akad. Nauk SSSR Biol.",
  volume   =  19,
  number   =  5,
  pages    = "769--771",
  month    =  sep,
  year     =  1975,
  keywords = "AIC;Information theory;Mendeley Import (Jan 17)/Assurances",
  language = "ru"
}

@ARTICLE{Burnham2004-ae,
  title    = "Multimodel Inference: Understanding {AIC} and {BIC} in Model
              Selection",
  author   = "Burnham, K P",
  abstract = "AICc justification",
  journal  = "Sociol. Methods Res.",
  volume   =  33,
  number   =  2,
  pages    = "261--304",
  month    =  "1~" # nov,
  year     =  2004,
  keywords = "aic; bic; model averaging; model selection; multimodel
              inference;AIC;Model selection;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Habbema1976-xd,
  title    = "Models for diagnosis and detection of combinations of diseases",
  author   = "Habbema, Jdf",
  journal  = "Decision making and medical care : can information science help?
              : Proceedings of the IFIP Working Conference on Decision Making
              and Medical Care",
  pages    = "399--411",
  year     =  1976,
  keywords = "Mendeley Import (Jan
              17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@BOOK{MacDonald1997-gz,
  title     = "Hidden Markov and Other Models for Discrete- valued Time Series",
  author    = "MacDonald, Iain L and Zucchini, Walter",
  abstract  = "Discrete-valued time series are common in practice, but methods
               for their analysis are not well-known. In recent years, methods
               have been developed which are specifically designed for the
               analysis of discrete-valued time series. Hidden Markov and Other
               Models for Discrete-Valued Time Series introduces a new,
               versatile, and computationally tractable class of models, the
               ``hidden Markov'' models. It presents a detailed account of
               these models, then applies them to data from a wide range of
               diverse subject areas, including medicine, climatology, and
               geophysics. This book will be invaluable to researchers and
               postgraduate and senior undergraduate students in statistics.
               Researchers and applied statisticians who analyze time series
               data in medicine, animal behavior, hydrology, and sociology will
               also find this information useful.",
  publisher = "CRC Press",
  month     =  "1~" # jan,
  year      =  1997,
  keywords  = "HMM;Mendeley Import (Jan 17)/Assurances;Mendeley Import (Jan
               17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language  = "en"
}

@ARTICLE{Leroux1992-di,
  title     = "Consistent Estimation of a Mixing Distribution",
  author    = "Leroux, Brian G",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  20,
  number    =  3,
  pages     = "1350--1360",
  year      =  1992,
  keywords  = "Mixture distribution; maximum likelihood; maximum penalized
               likelihood; model selection;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Leroux1992-du,
  title       = "Maximum-penalized-likelihood estimation for independent and
                 Markov-dependent mixture models",
  author      = "Leroux, B G and Puterman, M L",
  affiliation = "Health and Welfare Canada, Environmental Health Centre,
                 Ottawa, Ontario.",
  abstract    = "This paper concerns the use and implementation of
                 maximum-penalized-likelihood procedures for choosing the
                 number of mixing components and estimating the parameters in
                 independent and Markov-dependent mixture models. Computation
                 of the estimates is achieved via algorithms for the automatic
                 generation of starting values for the EM algorithm.
                 Computation of the information matrix is also discussed.
                 Poisson mixture models are applied to a sequence of counts of
                 movements by a fetal lamb in utero obtained by ultrasound. The
                 resulting estimates are seen to provide plausible mechanisms
                 for the physiological process.",
  journal     = "Biometrics",
  volume      =  48,
  number      =  2,
  pages       = "545--558",
  month       =  jun,
  year        =  1992,
  keywords    = "Mendeley Import (Jan
                 17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit",
  language    = "en"
}

@ARTICLE{Hughes1994-cn,
  title    = "A class of stochastic models for relating synoptic atmospheric
              patterns to regional hydrologic phenomena",
  author   = "Hughes, James P and Guttorp, Peter",
  abstract = "A model for multistation precipitation, conditional on synoptic
              atmospheric patterns, is presented. The model, which we call the
              nonhomogeneous hidden Markov model (NHMM), postulates the
              existence of an unobserved weather state, which serves as a link
              between the large-scale atmospheric measures and the small-scale
              spatially discontinuous precipitation field. The weather state
              effectively acts as an automatic classifier of atmospheric
              patterns. The weather state process is assumed to be
              conditionally Markov, given the atmospheric data. The rainfall
              process is then assumed to be conditionally independent given the
              weather state. Various parameterizations for the weather state
              process and the rainfall process are discussed, and a
              likelihood-based estimation procedure is described. Model-based
              estimates of the storm duration distribution and first and second
              moments of the rainfall process are derived. As an example the
              model is fit to a four-station network of rain gauge stations in
              Washington state. The observed first and second moments are
              reproduced very closely. The fitted duration distributions are
              somewhat lighter tailed than the observed distribution at two of
              the four stations but provide a good fit at the other two. We
              conclude that the NHMM has promise as a method of relating
              synoptic atmospheric data to rainfall and other regional or local
              hydrologic processes.",
  journal  = "Water Resour. Res.",
  volume   =  30,
  number   =  5,
  pages    = "1535--1546",
  month    =  "1~" # may,
  year     =  1994,
  keywords = "1854 Precipitation; 1869 Stochastic hydrology; 3319 General
              circulation; 3364 Synoptic-scale meteorology;HMM;Mendeley Import
              (Jan 17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@ARTICLE{Satten1996-kt,
  title     = "Markov Chains With Measurement Error: Estimating the `True'
               Course of a Marker of the Progression of Human Immunodeficiency
               Virus Disease",
  author    = "Satten, Glen A and Longini, Ira M",
  abstract  = "A Markov chain is a useful way of describing cohort data.
               Longitudinal observations of a marker of the progression of the
               human immunodeficiency virus (HIV), such as CD4 cell count,
               measured on members of a cohort study, can be analysed as a
               continuous time Markov chain by categorizing the CD4 cell counts
               into stages. Unfortunately, CD4 cell counts are subject to
               substantial measurement error and short timescale variability.
               Thus, fitting a Markov chain to raw CD4 cell count measurements
               does not determine the transition probabilities for the true or
               underlying CD4 cell counts; the measurements error results in a
               process that is too rough. Assuming independent measurement
               errors, we propose a likelihood-based method for estimating the
               'true' or underlying transition probabilities. The Markov
               structure allows efficient calculation of the likelihood by
               using hidden Markov model methodology. As example, we consider
               CD4 cell count data from 430 HIV-infected participants in the
               San Francisco Men's Health Study by categorizing the marker data
               into seven stages; up to 17 observations are available for each
               individual. We find that including measurement error both
               produces a significantly better fit and provides a model for CD4
               progression that is more biologically reasonable.",
  journal   = "J. R. Stat. Soc. Ser. C Appl. Stat.",
  publisher = "[Wiley, Royal Statistical Society]",
  volume    =  45,
  number    =  3,
  pages     = "275--309",
  year      =  1996,
  keywords  = "HMM;Mendeley Import (Jan
               17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@MISC{Lee2004-pv,
  title    = "Trust in Automation: Designing for Appropriate Reliance",
  author   = "Lee, J D and See, K A",
  abstract = "Automation is often problematic because people fail to rely upon
              it appropriately. Because people respond to technology socially,
              trust influences reliance on automation. In particular, trust
              guides reliance when complexity and unanticipated situations make
              a complete understanding of the automation impractical. This
              review considers trust from the organizational, sociological,
              interpersonal, psychological, and neurological perspectives. It
              considers how the context, automation characteristics, and
              cognitive processes affect the appropriateness of trust. The
              context in which the automation is used influences automation
              performance and provides a goal-oriented perspective to assess
              automation characteristics along a dimension of attributional
              abstraction. These characteristics can influence trust through
              analytic, analogical, and affective processes. The challenges of
              extrapolating the concept of trust in people to trust in
              automation are discussed. A conceptual model integrates research
              regarding trust in automation and describes the dynamics of
              trust, the role of context, and the influence of display
              characteristics. Actual or potential applications of this
              research include improved designs of systems that require people
              to manage imperfect automation.",
  journal  = "Human Factors: The Journal of the Human Factors and Ergonomics
              Society",
  volume   =  46,
  number   =  1,
  pages    = "50--80",
  year     =  2004,
  keywords = "
              trust\_definition;assurances;very\_similar\_to\_mine;automation;Mendeley
              Import (Jan 17)/ReadingGroups;Mendeley Import (Jan
              17)/Assurances/Trust Background;Mendeley Import (Jan
              17)/Assurances"
}

@PHDTHESIS{Lystig2001-sz,
  title    = "Evaluation of Hidden Markov Models",
  author   = "Lystig, Theodore Christian",
  abstract = "Hidden Markov models are a very rich class of models that have
              been used on problems as diverse as speech recognition, sodium
              ion channels, infectious disease processes, and rainfall
              occurrence. Since the late 1960's, however, it has been
              appreciated that even the seemingly mundane task of calculating
              the log-likelihood of hidden Markov models is not a trivial
              matter. This task, known as the evaluation problem, was addressed
              through the development of an early example of the
              Expectation-Maximization algorithm. In this thesis, the problem
              of evaluation is addressed along both quantitative and
              qualitative lines. Quantitatively, and efficient algorithm is
              developed for fast computation of the log-likelihood, the score,
              and the observed information matrix from a single pass through
              the data. This enables one to readily obtain standard errors of
              parameter estimates, something that is rarely achieved in most
              typical EM stettings. It also permits alternatie maximization
              techniques to the EM algorithm. Qualitatively, sound goodnes sof
              fit technniques based on an expansion of the score are developed
              that are consistent against a wide variety of model
              mis-specifications. These techniques are shown to have good power
              in a range of situations, and may be performed with relatively
              little ocmputational effort. A complementary goodness of fit
              method based on conditional residuals is also developed that
              enables one to effectively screen a wide variety of candidate
              predictor variables without requiring additional refitting of the
              model. The methods are evaluated through both simulations and
              real datasets.",
  year     =  2001,
  school   = "University of Washington",
  keywords = "HMM;Mendeley Import (Jan
              17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@INPROCEEDINGS{Udell2014-uc,
  title     = "Convex Optimization in Julia",
  booktitle = "2014 First Workshop for High Performance Technical Computing in
               Dynamic Languages",
  author    = "Udell, M and Mohan, K and Zeng, D and Hong, J and Diamond, S and
               Boyd, S",
  abstract  = "This paper describes Convex1, a convex optimization modeling
               framework in Julia. Convex translates problems from a
               user-friendly functional language into an abstract syntax tree
               describing the problem. This concise representation of the
               global structure of the problem allows Convex to infer whether
               the problem complies with the rules of disciplined convex
               programming (DCP), and to pass the problem to a suitable solver.
               These operations are carried out in Julia using multiple
               dispatch, which dramatically reduces the time required to verify
               DCP compliance and to parse a problem into conic form. Convex
               then automatically chooses an appropriate backend solver to
               solve the conic form problem.",
  publisher = "ACM",
  pages     = "18--28",
  month     =  nov,
  year      =  2014,
  address   = "New York",
  keywords  = "convex programming;functional languages;mathematics
               computing;DCP compliance;Julia;abstract syntax tree;conic form
               problem;convex optimization modeling framework;disciplined
               convex programming;user-friendly functional
               language;Abstracts;Convex functions;Frequency modulation;Object
               oriented modeling;Optimization;Programming;Symmetric
               matrices;Convex programming; automatic verification; symbolic
               computation; multiple dispatch;julia;Mendeley Import (Jan
               17)/Julia"
}

@INPROCEEDINGS{Olver2014-qe,
  title     = "A Practical Framework for {Infinite-Dimensional} Linear Algebra",
  booktitle = "2014 First Workshop for High Performance Technical Computing in
               Dynamic Languages",
  author    = "Olver, S and Townsend, A",
  abstract  = "We describe a framework for solving a broad class of
               infinite-dimensional linear equations, consisting of almost
               banded operators, which can be used to representing linear
               ordinary differential equations with general boundary
               conditions. The framework contains a data structure for on which
               row operations can be performed, allowing for the solution of
               infinite-dimensional linear equations by the adaptive QR
               approach. The algorithm achieves O(nopt) complexity, where nopt
               is the number of degrees of freedom required to achieve a
               desired accuracy, which is determined adaptively. In addition,
               special tensor product equations, such as partial differential
               equations on rectangles, can be solved by truncating the
               operator in the y-direction with ny degrees of freedom and using
               a generalized Schur decomposition to upper triangularize, before
               applying the adaptive QR approach to the x-direction, requiring
               O(n3y + n2ynoptx) operations. The framework is implemented in
               the ApproxFun package written in the Julia programming language,
               which achieves highly competitive computational costs by
               exploiting unique features of Julia. Using this framework,
               partial differential equations that require as many as 2.5
               million unknowns can be solved in less than 4 seconds.",
  publisher = "ACM",
  pages     = "57--62",
  month     =  nov,
  year      =  2014,
  address   = "New York",
  keywords  = "high level languages;linear algebra;linear differential
               equations;mathematics computing;ApproxFun package;Julia
               programming language;adaptive QR approach;general boundary
               conditions;infinite-dimensional linear
               algebra;infinite-dimensional linear equations;linear ordinary
               differential equations;special tensor product
               equations;Arrays;Complexity theory;Equations;Mathematical
               model;Vectors;julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Lubin2015-dh,
  title    = "Computing in Operations Research Using Julia",
  author   = "Lubin, Miles and Dunning, Iain",
  abstract = "The state of numerical computing is currently characterized by a
              divide between highly efficient yet typically cumbersome
              low-level languages such as C, C++, and Fortran and highly
              expressive yet typically slow high-level languages such as Python
              and MATLAB. This paper explores how Julia, a modern programming
              language for numerical computing which claims to bridge this
              divide by incorporating recent advances in language and compiler
              design (such as just-in-time compilation), can be used for
              implementing software and algorithms fundamental to the field of
              operations research, with a focus on mathematical optimization.
              In particular, we demonstrate algebraic modeling for linear and
              nonlinear optimization and a partial implementation of a
              practical simplex code. Extensive cross-language benchmarks
              suggest that Julia is capable of obtaining state-of-the-art
              performance.",
  journal  = "INFORMS J. Comput.",
  volume   =  27,
  number   =  2,
  pages    = "238--248",
  month    =  apr,
  year     =  2015,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@INPROCEEDINGS{Bezanson2014-na,
  title     = "Array Operators Using Multiple Dispatch: A design methodology
               for array implementations in dynamic languages",
  booktitle = "Proceedings of {ACM} {SIGPLAN} International Workshop on
               Libraries, Languages, and Compilers for Array Programming",
  author    = "Bezanson, Jeff and Chen, Jiahao and Karpinski, Stefan and Shah,
               Viral and Edelman, Alan",
  abstract  = "Arrays are such a rich and fundamental data type that they tend
               to be built into a language, either in the compiler or in a
               large low-level library. Defining this functionality at the user
               level instead provides greater flexibility for application
               domains not envisioned by the language designer. Only a few
               languages, such as C++ and Haskell, provide the necessary power
               to define n-dimensional arrays, but these systems rely on
               compile-time abstraction, sacrificing some flexibility. In
               contrast, dynamic languages make it straightforward for the user
               to define any behavior they might want, but at the possible
               expense of performance. As part of the Julia language project,
               we have developed an approach that yields a novel trade-off
               between flexibility and compile-time analysis. The core
               abstraction we use is multiple dispatch. We have come to believe
               that while multiple dispatch has not been especially popular in
               most kinds of programming, technical computing is its killer
               application. By expressing key functions such as array indexing
               using multi-method signatures, a surprising range of behaviors
               can be obtained, in a way that is both relatively easy to write
               and amenable to compiler analysis. The compact factoring of
               concerns provided by these methods makes it easier for
               user-defined types to behave consistently with types in the
               standard library.",
  publisher = "ACM",
  pages     = "56",
  month     =  "9~" # jun,
  year      =  2014,
  address   = "New York, \{NY\}, \{USA\}",
  keywords  = "Julia; array indexing; dynamic dispatch; multiple dispatch;
               static analysis; type inference;julia;Mendeley Import (Jan
               17)/Julia"
}

@ARTICLE{Stor2015-kf,
  title    = "Forward stable computation of roots of real polynomials with only
              real distinct roots",
  author   = "Stor, N Jakovcevic and Slapnicar, I",
  abstract = "As showed in (Fiedler, 1990), any polynomial can be expressed as
              a characteristic polynomial of a complex symmetric arrowhead
              matrix. This expression is not unique. If the polynomial is real
              with only real distinct roots, the matrix can be chosen real. By
              using accurate forward stable algorithm for computing eigenvalues
              of real symmetric arrowhead matrices from (Jakovcevic Stor,
              Slapnicar, Barlow, 2015), we derive a forward stable algorithm
              for computation of roots of such polynomials in O(n2) operations.
              The algorithm computes each root to almost full accuracy. In some
              cases, the algorithm invokes extended precision routines, but
              only in the non-iterative part. Our examples include numerically
              difficult problems, like the well-known Wilkinson's polynomials.
              Our algorithm compares favourably to other method for polynomial
              root-finding, like MPSolve or Newton's method.",
  pages    = "1--15",
  year     =  2015,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Udell2014-bk,
  title    = "Generalized Low Rank Models",
  author   = "Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd,
              Stephen",
  abstract = "Principal components analysis (PCA) is a well-known technique for
              approximating a data set represented by a matrix by a low rank
              matrix. Here, we extend the idea of PCA to handle arbitrary data
              sets consisting of numerical, Boolean, categorical, ordinal, and
              other data types. This framework encompasses many well known
              techniques in data analysis, such as nonnegative matrix
              factorization, matrix completion, sparse and robust PCA, k-means,
              k-SVD, and maximum margin matrix factorization. The method
              handles heterogeneous data sets, and leads to coherent schemes
              for compressing, denoising, and imputing missing entries across
              all data types simultaneously. It also admits a number of
              interesting interpretations of the low rank factors, which allow
              clustering of examples or of features. We propose several
              parallel algorithms for fitting generalized low rank models, and
              describe implementations and numerical results.",
  year     =  2014,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Olver2014-az,
  title    = "Sampling unitary invariant ensembles",
  author   = "Olver, Sheehan and Nadakuditi, Raj Rao and Trogdon, Thomas",
  abstract = "We develop an algorithm for sampling from the unitary invariant
              random matrix ensembles. The algorithm is based on the
              representation of their eigenvalues as a determinantal point
              process whose kernel is given in terms of orthogonal polynomials.
              Using this algorithm, statistics beyond those known through
              analysis are calculable through Monte Carlo simulation.
              Unexpected phenomena are observed in the simulations.",
  year     =  2014,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Townsend2014-vg,
  title    = "Fast computation of \{Gauss\} quadrature nodes and weights on the
              whole real line",
  author   = "Townsend, Alex and Trogdon, Thomas and Olver, Sheehan",
  abstract = "A fast and accurate algorithm for the computation of
              Gauss-Hermite and generalized Gauss-Hermite quadrature nodes and
              weights is presented. The algorithm is based on Newton's method
              with carefully selected initial guesses for the nodes and a fast
              evaluation scheme for the associated orthogonal polynomial. In
              the Gauss-Hermite case the initial guesses and evaluation scheme
              rely on explicit asymptotic formulas. For generalized
              Gauss-Hermite, the initial guesses are furnished by sampling a
              certain equilibrium measure and the associated polynomial
              evaluated via a Riemann-Hilbert reformulation. In both cases the
              n-point quadrature rule is computed in O(n) operations to an
              accuracy that is close to machine precision. For sufficiently
              large n, some of the quadrature weights have a value less than
              the smallest positive normalized floating-point number in double
              precision and we exploit this fact to achieve a complexity as low
              as O(sqrt(n))).",
  year     =  2014,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Jakovcevic_Stor2015-zz,
  title    = "Forward stable eigenvalue decomposition of rank-one modifications
              of diagonal matrices",
  author   = "Jakov{\v c}evi{\'c} Stor, N and Slapni{\v c}ar, I and Barlow, J L",
  abstract = "We present a new algorithm for solving an eigenvalue problem for
              a real symmetric matrix which is a rank-one modification of a
              diagonal matrix. The algorithm computes each eigenvalue and all
              components of the corresponding eigenvector with high relative
              accuracy in O(n) operations. The algorithm is based on a
              shift-and-invert approach. Only a single element of the inverse
              of the shifted matrix eventually needs to be computed with double
              the working precision. Each eigenvalue and the corresponding
              eigenvector can be computed separately, which makes the algorithm
              adaptable for parallel computing. Our results extend to the
              complex Hermitian case. The algorithm is similar to the algorithm
              for solving the eigenvalue problem for real symmetric arrowhead
              matrices from N. Jakov{\v c}evi{\'c} Stor et al. (2015) [16].",
  journal  = "Linear Algebra Appl.",
  volume   =  487,
  pages    = "301--315",
  month    =  dec,
  year     =  2015,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Townsend2014-gt,
  title    = "The automatic solution of partial differential equations using a
              global spectral method",
  author   = "Townsend, Alex and Olver, Sheehan",
  abstract = "A spectral method for solving linear partial differential
              equations (PDEs) with variable coefficients and general boundary
              conditions defined on rectangular domains is described, based on
              separable representations of partial differential operators and
              the one-dimensional ultraspherical spectral method. If a partial
              differential operator is of splitting rank 2, such as the
              operator associated with Poisson or Helmholtz, the corresponding
              PDE is solved via a generalized Sylvester matrix equation, and a
              bivariate polynomial approximation of the solution of degree
              (n\_x,n\_y) is computed in O(n\_x n\_y)^\{3/2\} operations.
              Partial differential operators of splitting rank >=3 are solved
              via a linear system involving a block-banded matrix in
              O(min(n\_x^\{3\} n\_y,n\_x n\_y^\{3\})) operations. Numerical
              examples demonstrate the applicability of our 2D spectral method
              to a broad class of PDEs, which includes elliptic and dispersive
              time-evolution equations. The resulting PDE solver is written in
              \{\textbackslashsc Matlab\} and is publicly available as part of
              \{\textbackslashsc Chebfun\}. It can resolve solutions requiring
              over a million degrees of freedom in under 60 seconds. An
              experimental implementation in the Julia language can currently
              perform the same solve in 10 seconds.",
  year     =  2014,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@TECHREPORT{Zhang2015-cl,
  title       = "Matrix Depot: An Extensible Test Matrix Collection for Julia",
  author      = "Zhang, Weijian and Higham, Nicholas J",
  abstract    = "Matrix Depot is a Julia software package that provides easy
                 access to a large and diverse collection of test matrices. Its
                 novelty is threefold. First, it is extensible by the user, and
                 so can be adapted to include the user’s own test problems. In
                 doing so it facilitates experimentation and makes it easier to
                 carry out reproducible research. Second, it amalgamates in a
                 single framework three different types of matrix collections,
                 comprising parametrized test matrices, regularization test
                 problems, and real-life sparse matrix data. Third, it fully
                 exploits the Julia language. It uses multiple dispatch to help
                 provide a simple interface and, in particular, to allow
                 matrices to be generated in any of the numeric data types
                 supported by the language.",
  pages       = "25",
  institution = "Manchester Institute for Mathematical Sciences, The University
                 of Manchester",
  month       =  dec,
  year        =  2015,
  keywords    = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Bezanson2012-tj,
  title    = "\{J\}ulia: A Fast Dynamic Language for Technical Computing",
  author   = "Bezanson, Jeff and Karpinski, Stefan and Shah, Viral B and
              Edelman, Alan",
  abstract = "Dynamic languages have become popular for scientific computing.
              They are generally considered highly productive, but lacking in
              performance. This paper presents Julia, a new dynamic language
              for technical computing, designed for performance from the
              beginning by adapting and extending modern programming language
              techniques. A design based on generic functions and a rich type
              system simultaneously enables an expressive programming model and
              successful type inference, leading to good performance for a wide
              range of programs. This makes it possible for much of the Julia
              library to be written in Julia itself, while also incorporating
              best-of-breed C and Fortran libraries.",
  month    =  sep,
  year     =  2012,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@INPROCEEDINGS{Knopp2014-vp,
  title     = "Experimental Multi-threading Support for the Julia Programming
               Language",
  booktitle = "2014 First Workshop for High Performance Technical Computing in
               Dynamic Languages",
  author    = "Knopp, T",
  abstract  = "Julia is a young programming language that is designed for
               technical computing. Although Julia is dynamically typed it is
               very fast and usually yields C speed by utilizing a just-in-time
               compiler. Still, Julia has a simple syntax that is similar to
               Matlab, which is widely known as an easy-to-use programming
               environment. While Julia is very versatile and provides
               asynchronous programming facilities in the form of tasks
               (coroutines) as well as distributed multi-process parallelism,
               one missing feature is shared memory multi-threading. In this
               paper we present our experiment on introducing multi-threading
               support in the Julia programming environment. While our
               implementation has some restrictions that have to be taken into
               account when using threads, the results are promising yielding
               almost full speedup for perfectly parallelizable tasks.",
  publisher = "ACM",
  pages     = "1--5",
  month     =  nov,
  year      =  2014,
  address   = "New York",
  keywords  = "computational linguistics;distributed shared memory
               systems;multi-threading;parallel languages;program
               compilers;Julia programming environment;Julia programming
               language;asynchronous programming facilities;distributed
               multiprocess parallelism;just-in-time compiler;shared memory
               multithreading;syntax;technical computing;Arrays;Computer
               languages;Dynamic programming;Instruction
               sets;Libraries;MATLAB;Resource management;julia;Mendeley Import
               (Jan 17)/Julia"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@TECHREPORT{Cyrus_Maher2015-hd,
  title       = "{CauseMap}: Fast inference of causality from complex time
                 series",
  author      = "Cyrus Maher, M and Hernandez, Ryan D",
  abstract    = "Background: Establishing health-related causal relationships
                 is a central pursuit in biomedical research. Yet, the
                 interdependent non-linearity of biological systems renders
                 causal dynamics laborious and at times impractical to
                 disentangle. This pursuit is further impeded by the dearth of
                 time series that are sufficiently long to observe and
                 understand recurrent patterns of flux. However, as data
                 generation costs plummet and technologies like wearable
                 devices democratize data collection, we anticipate a coming
                 surge in the availability of biomedically-relevant time series
                 data. Given the life-saving potential of these burgeoning
                 resources, it is critical to invest in the development of open
                 source software tools that are capable of drawing meaningful
                 insight from vast amounts of time series data.Results: Here we
                 present CauseMap, the first open source implementation of
                 convergent cross mapping (CCM), a method for establishing
                 causality from long time series data (> ~25 observations).
                 Compared to existing time series methods, CCM has the
                 advantage of being model-free and robust to unmeasured
                 confounding that could otherwise induce spurious associations.
                 CCM builds on Takens’ Theorem, a well-established result from
                 dynamical systems theory that requires only mild assumptions.
                 This theorem allows us to reconstruct high dimensional system
                 dynamics using a time series of only a single variable. These
                 reconstructions can be thought of as shadows of the true
                 causal system. If the reconstructed shadows can predict points
                 from the opposing time series, we can infer that the
                 corresponding variables are providing views of the same causal
                 system, and so are causally related. Unlike traditional
                 metrics, this test can establish the directionality of
                 causation, even in the presence of feedback loops.
                 Furthermore, since CCM can extract causal relationships from
                 times series of, e.g. a single individual, it may be a
                 valuable tool to personalized medicine. We implement CCM in
                 Julia, a high-performance programming language designed for
                 facile technical computing. Our software package, CauseMap, is
                 platform-independent and freely available as an official Julia
                 package.Conclusions: CauseMap is an efficient implementation
                 of a state-of-the-art algorithm for detecting causality from
                 time series data. We believe this tool will be a valuable
                 resource for biomedical research and personalized medicine.",
  publisher   = "PeerJ Inc.",
  number      = "e1053",
  institution = "PeerJ PrePrints",
  month       =  "25~" # feb,
  year        =  2015,
  keywords    = "Causality; Open source software; Time series methods;
                 Dynamical systems; Personalized medicine;julia;Mendeley Import
                 (Jan 17)/Julia",
  language    = "en"
}

@INPROCEEDINGS{Chen2014-rc,
  title           = "Parallel Prefix Polymorphism Permits Parallelization,
                     Presentation \& Proof",
  booktitle       = "2014 First Workshop for High Performance Technical
                     Computing in Dynamic Languages",
  author          = "Chen, Jiahao and Edelman, Alan",
  abstract        = "Polymorphism in programming languages enables code reuse.
                     Here, we show that polymorphism has broad applicability
                     far beyond computations for technical computing:
                     parallelism in distributed computing, presentation of
                     visualizations of runtime data flow, and proofs for formal
                     verification of correctness. The ability to reuse a single
                     codebase for all these purposes provides new ways to
                     understand and verify parallel programs.",
  publisher       = "IEEE",
  pages           = "47--56",
  year            =  2014,
  address         = "New York",
  keywords        = "julia;Mendeley Import (Jan 17)/Julia",
  conference      = "2014 First Workshop for High Performance Technical
                     Computing in Dynamic Languages (HPTCDL)"
}

@INPROCEEDINGS{Lin2013-ka,
  title     = "Online Learning of Nonparametric Mixture Models via Sequential
               Variational Approximation",
  booktitle = "Advances in Neural Information Processing Systems 26",
  author    = "Lin, Dahua",
  editor    = "Burges, C J C and Bottou, L and Welling, M and Ghahramani, Z and
               Weinberger, K Q",
  abstract  = "Reliance on computationally expensive algorithms for inference
               has been limiting the use of Bayesian nonparametric models in
               large scale applications. To tackle this problem, we propose a
               Bayesian learning algorithm for DP mixture models. Instead of
               following the conventional paradigm -- random initialization
               plus iterative update, we take an progressive approach. Starting
               with a given prior, our method recursively transforms it into an
               approximate posterior through sequential variational
               approximation. In this process, new components will be
               incorporated on the fly when needed. The algorithm can reliably
               estimate a DP mixture model in one pass, making it particularly
               suited for applications with massive data. Experiments on both
               synthetic data and real datasets demonstrate remarkable
               improvement on efficiency -- orders of magnitude speed-up
               compared to the state-of-the-art.",
  publisher = "Curran Associates, Inc.",
  pages     = "395--403",
  year      =  2013,
  keywords  = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Van_der_Meulen2014-th,
  title    = "Bayesian estimation of discretely observed multi-dimensional
              diffusion processes using guided proposals",
  author   = "van der Meulen, Frank and Schauer, Moritz",
  abstract = "Bayesian estimation of parameters of a diffusion based on
              discrete time observations poses a difficult problem due to the
              lack of a closed form expression for the likelihood.
              Data-augmentation has been proposed for obtaining draws from the
              posterior distribution of the parameters. Within this approach,
              the discrete time observations are augmented with diffusion
              bridges connecting these observations. This poses two challenges:
              (i) efficiently generating diffusion bridges; (ii) if unknown
              parameters appear in the diffusion coefficient, then direct
              implementation of data-augmentation results in an induced Markov
              chain which is reducible. In this paper we show how both
              challenges can be addressed in continuous time (before
              discretisation) by using guided proposals. These are Markov
              processes with dynamics described by the stochastic differential
              equation of the diffusion process with an additional term added
              to the drift coefficient to guide the process to hit the right
              end point of the bridge. The form of these proposals naturally
              provides a mapping that decouples the dependence between the
              diffusion coefficient and diffusion bridge using the driving
              Brownian motion of the proposals. As the guiding term has a
              singularity at the right end point, care is needed when
              discretisation is applied for implementation purposes. We show
              that this problem can be dealt with by appropriately time
              changing and scaling of the guided proposal process. In two
              examples we illustrate the performance of the algorithms we
              propose. The second of these concerns a diffusion approximation
              of a chemical reaction network with a four-dimensional diffusion
              driven by an eight-dimensional Brownian motion.",
  year     =  2014,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Hisamoto_Sorami2014-ff,
  title     = "技術計算のための新言語Julia (Julia: a new language for technical computing)",
  booktitle = "データサイエンティスト養成読本 R活用編 (Data scientist training reader: practical
               \{R\} edition)",
  author    = "{久本 空海 (Hisamoto, Sorami)} and {西薗 良太 (Nishizono,
               Ry\textbackslash=ota)}",
  publisher = "技術評論社 (Gijutsu-Hyohron)",
  chapter   =  7,
  series    = "Software Design plus",
  month     =  dec,
  year      =  2014,
  address   = "Tokyo",
  keywords  = "julia;Mendeley Import (Jan 17)/Julia"
}

@INPROCEEDINGS{Shah2013-en,
  title     = "Novel algebras for advanced analytics in Julia",
  booktitle = "2013 {IEEE} High Performance Extreme Computing Conference
               ({HPEC})",
  author    = "Shah, V B and Edelman, A and Karpinski, S and Bezanson, J and
               Kepner, J",
  abstract  = "A linear algebraic approach to graph algorithms that exploits
               the sparse adjacency matrix representation of graphs can provide
               a variety of benefits. These benefits include syntactic
               simplicity, easier implementation, and higher performance. One
               way to employ linear algebra techniques for graph algorithms is
               to use a broader definition of matrix and vector multiplication.
               We demonstrate through the use of the Julia language system how
               easy it is to explore semirings using linear algebraic
               methodologies.",
  pages     = "1--4",
  year      =  2013,
  keywords  = "high level languages;linear algebra;mathematics co; mathematics
               computing; Julia; Julia language system; advanced analytics;
               graph algorithms; linear algebra techniques; linear algebraic
               approach; linear algebraic methodologies; matrix multiplication;
               novel algebras; sparse adjacency matrix representation; vector
               multiplication; Electronic mail; Matrices; Sparse matrices;
               Standards; Syntactics;julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Slevinsky2014-pg,
  title    = "On the use of conformal maps for the acceleration of convergence
              of the trapezoidal rule and Sinc numerical methods",
  author   = "Slevinsky, Richard Mika{\"e}l and Olver, Sheehan",
  abstract = "We investigate the use of conformal maps for the acceleration of
              convergence of the trapezoidal rule and Sinc numerical methods.
              The conformal map is a polynomial adjustment to the sinh map, and
              allows the treatment of a finite number of singularities in the
              complex plane. In the case where locations are unknown, the
              so-called Sinc-Pad{\'e} approximants are used to provide
              approximate results. This adaptive method is shown to have almost
              the same convergence properties. We use the conformal maps to
              generate high accuracy solutions to several challenging
              integrals, nonlinear waves, and multidimensional integrals.",
  year     =  2014,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@INBOOK{Tate2014-tl,
  title     = "Seven More Languages in Seven Weeks: Languages That Are Shaping
               the Future",
  author    = "Tate, Bruce A and Dees, Ian and Daoud, Frederic and Moffitt,
               Jack",
  publisher = "Pragmatic Bookshelf",
  chapter   =  6,
  edition   =  1,
  year      =  2014,
  keywords  = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Tran2014-ne,
  title    = "Annealed Important Sampling for Models with Latent Variables",
  author   = "Tran, M-N and Strickland, C and Pitt, M K and Kohn, R",
  abstract = "This paper is concerned with Bayesian inference when the
              likelihood is analytically intractable but can be unbiasedly
              estimated. We propose an annealed importance sampling procedure
              for estimating expectations with respect to the posterior. The
              proposed algorithm is useful in cases where finding a good
              proposal density is challenging, and when estimates of the
              marginal likelihood are required. The effect of likelihood
              estimation is investigated, and the results provide guidelines on
              how to set up the precision of the likelihood estimation in order
              to optimally implement the procedure. The methodological results
              are empirically demonstrated in several simulated and real data
              examples.",
  year     =  2014,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Caulfield2014-te,
  title     = "Compositional Security Modelling",
  booktitle = "Human Aspects of Information Security, Privacy, and Trust",
  author    = "Caulfield, Tristan and Pym, David and Williams, Julian",
  editor    = "Tryfonas, Theo and Askoxylakis, Ioannis",
  abstract  = "Security managers face the challenge of formulating and
               implementing policies that deliver their desired system security
               postures --- for example, their preferred balance of
               confidentiality, integrity, and availability --- within budget
               (monetary and otherwise). In this paper, we describe a security
               modelling methodology, grounded in rigorous mathematical systems
               modelling and economics, that captures the managers’ policies
               and the behavioural choices of agents operating within the
               system. Models are executable, so allowing systematic
               experimental exploration of the system-policy co-design space,
               and compositional, so managing the complexity of large-scale
               systems.",
  publisher = "Springer International Publishing",
  volume    =  8533,
  pages     = "233--245",
  series    = "Lecture Notes in Computer Science",
  year      =  2014,
  address   = "Cham",
  keywords  = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Mariana_M_Odashima_Beatriz_G_Prado2016-im,
  title    = "Introduction to the equilibrium Green's functions: condensed
              matter examples with numerical implementations",
  author   = "{Mariana M. Odashima Beatriz G. Prado} and Vernek, E",
  abstract = "The Green's function method has applications in several fields in
              Physics, from classical differential equations to quantum
              many-body problems. In the quantum context, Green's functions are
              correlation functions, from which it is possible to extract
              information from the system under study, such as the density of
              states, relaxation times and response functions. Despite its
              power and versatility, it is known as a laborious and sometimes
              cumbersome method. Here we introduce the equilibrium Green's
              functions and the equation-of-motion technique, exemplifying the
              method in discrete lattices of non-interacting electrons. We
              start with simple models, such as the two-site molecule, the
              infinite and semi-infinite one-dimensional chains, and the
              two-dimensional ladder. Numerical implementations are developed
              via the recursive Green's function, implemented in Julia, an
              open-source, efficient and easy-to-learn scientific language. We
              also present a new variation of the surface recursive Green's
              function method, which can be of interest when simulating
              simultaneously the properties of surface and bulk.",
  pages    = "1--20",
  year     =  2016,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Jakovcevic_Stor2015-oy,
  title    = "Accurate eigenvalue decomposition of real symmetric arrowhead
              matrices and applications",
  author   = "Jakov{\v c}evi{\'c} Stor, Nevena and Slapni{\v c}ar, Ivan and
              Barlow, Jesse L",
  abstract = "We present a new algorithm for solving the eigenvalue problem for
              an n $\times$ n real symmetric arrowhead matrix. The algorithm
              computes all eigenvalues and all components of the corresponding
              eigenvectors with high relative accuracy in O(n^2) operations
              under certain circumstances. The algorithm is based on a
              shift-and-invert approach. Only a single element of the inverse
              of the shifted matrix eventually needs to be computed with double
              the working precision. Each eigenvalue and the corresponding
              eigenvector can be computed separately, which makes the algorithm
              adaptable for parallel computing. Our results extend to Hermitian
              arrowhead matrices, real symmetric diagonal-plus-rank-one
              matrices and singular value decomposition of real triangular
              arrowhead matrices.",
  journal  = "Linear Algebra Appl.",
  volume   =  464,
  pages    = "62--89",
  month    =  jan,
  year     =  2015,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Baldassi2014-ae,
  title       = "Fast and accurate multivariate Gaussian modeling of protein
                 families: predicting residue contacts and protein-interaction
                 partners",
  author      = "Baldassi, Carlo and Zamparo, Marco and Feinauer, Christoph and
                 Procaccini, Andrea and Zecchina, Riccardo and Weigt, Martin
                 and Pagnani, Andrea",
  affiliation = "Department of Applied Science and Technology and Center for
                 Computational Sciences, Politecnico di Torino, Torino, Italy;
                 Human Genetics Foundation-Torino, Torino, Italy. Department of
                 Applied Science and Technology and Center for Computational
                 Sciences, Politecnico di Torino, Torino, Italy; Human Genetics
                 Foundation-Torino, Torino, Italy. Department of Applied
                 Science and Technology and Center for Computational Sciences,
                 Politecnico di Torino, Torino, Italy. Human Genetics
                 Foundation-Torino, Torino, Italy. Department of Applied
                 Science and Technology and Center for Computational Sciences,
                 Politecnico di Torino, Torino, Italy; Human Genetics
                 Foundation-Torino, Torino, Italy. Sorbonne Universit{\'e}s,
                 Universit{\'e} Pierre et Marie Curie Paris 06, UMR 7238,
                 Computational and Quantitative Biology, Paris, France; Centre
                 National de la Recherche Scientifique, UMR 7238, Computational
                 and Quantitative Biology, Paris, France. Department of Applied
                 Science and Technology and Center for Computational Sciences,
                 Politecnico di Torino, Torino, Italy; Human Genetics
                 Foundation-Torino, Torino, Italy.",
  abstract    = "In the course of evolution, proteins show a remarkable
                 conservation of their three-dimensional structure and their
                 biological function, leading to strong evolutionary
                 constraints on the sequence variability between homologous
                 proteins. Our method aims at extracting such constraints from
                 rapidly accumulating sequence data, and thereby at inferring
                 protein structure and function from sequence information
                 alone. Recently, global statistical inference methods (e.g.
                 direct-coupling analysis, sparse inverse covariance
                 estimation) have achieved a breakthrough towards this aim, and
                 their predictions have been successfully implemented into
                 tertiary and quaternary protein structure prediction methods.
                 However, due to the discrete nature of the underlying variable
                 (amino-acids), exact inference requires exponential time in
                 the protein length, and efficient approximations are needed
                 for practical applicability. Here we propose a very efficient
                 multivariate Gaussian modeling approach as a variant of
                 direct-coupling analysis: the discrete amino-acid variables
                 are replaced by continuous Gaussian random variables. The
                 resulting statistical inference problem is efficiently and
                 exactly solvable. We show that the quality of inference is
                 comparable or superior to the one achieved by mean-field
                 approximations to inference with discrete variables, as done
                 by direct-coupling analysis. This is true for (i) the
                 prediction of residue-residue contacts in proteins, and (ii)
                 the identification of protein-protein interaction partner in
                 bacterial signal transduction. An implementation of our
                 multivariate Gaussian approach is available at the website
                 http://areeweb.polito.it/ricerca/cmp/code.",
  journal     = "PLoS One",
  volume      =  9,
  number      =  3,
  pages       = "e92721",
  month       =  "24~" # mar,
  year        =  2014,
  keywords    = "biocomp;julia;Mendeley Import (Jan 17)/Julia",
  language    = "en"
}

@INPROCEEDINGS{Huchette2014-or,
  title     = "Parallel Algebraic Modeling for Stochastic Optimization",
  booktitle = "2014 First Workshop for High Performance Technical Computing in
               Dynamic Languages",
  author    = "Huchette, J and Lubin, M and Petra, C",
  abstract  = "We present scalable algebraic modeling software, StochJuMP, for
               stochastic optimization as applied to power grid economic
               dispatch. It enables the user to express the problem in a
               high-level algebraic format with minimal boiler-plate. StochJuMP
               allows efficient parallel model instantiation across nodes and
               efficient data localization. Computational results are presented
               showing that the model construction is efficient, requiring
               roughly one percent of solve time. StochJuMP is configured with
               the parallel interior-point solver PIPS-IPM but is sufficiently
               generic to allow straight forward adaptation to other solvers.",
  publisher = "ACM",
  pages     = "29--35",
  month     =  nov,
  year      =  2014,
  address   = "New York",
  keywords  = "parallel programming;power generation dispatch;power
               grids;stochastic programming;PIPS-IPM;StochJuMP;data
               localization;high-level algebraic format;minimal
               boiler-plate;parallel algebraic modeling;parallel interior-point
               solver;parallel model instantiation;power grid economic
               dispatch;scalable algebraic modeling software;stochastic
               optimization;Computational modeling;Data models;Mathematical
               model;Optimization;Sparse matrices;Stochastic
               processes;Symmetric matrices;optimization; parallel programming;
               high performance computing; mathematical model; Power system
               modeling; scalability;julia;Mendeley Import (Jan 17)/Julia"
}

@INPROCEEDINGS{Heitzinger2014-ud,
  title     = "Julia and the Numerical Homogenization of {PDEs}",
  booktitle = "2014 First Workshop for High Performance Technical Computing in
               Dynamic Languages",
  author    = "Heitzinger, C and Tulzer, G",
  abstract  = "We discuss the advantages of using Julia for solving multiscale
               problems involving partial differential equations (PDEs).
               Multiscale problems are problems where the coefficients of a PDE
               oscillate rapidly on a microscopic length scale, but solutions
               are sought on a much larger, macroscopic domain. Solving
               multiscale problems requires both a theoretic result, i.e., a
               homogenization result yielding effective coefficients, as well
               as numerical solutions of the PDE at the microscopic and the
               macroscopic length scales. Numerical homogenization of PDEs with
               stochastic coefficients is especially computationally expensive.
               Under certain assumptions, effective coefficients can be found,
               but their calculation involves subtle numerical problems. The
               computational cost is huge due to the generally large number of
               stochastic dimensions. Multiscale problems arise in many
               applications, e.g., in uncertainty quantification, in the
               rational design of nanoscale sensors, and in the rational design
               of materials. Our code for the numerical stochastic
               homogenization of elliptic problems is implemented in Julia.
               Since multiscale problems pose new numerical problems, it is in
               any case necessary to develop new numerical codes. Julia is a
               dynamic language inspired by the Lisp family of languages, it is
               open-source, and it provides native-code compilation, access to
               highly optimized linear-algebra routines, support for parallel
               computing, and a powerful macro system. We describe our
               experience in using Julia and discuss the advantages of Julia's
               features in this problem domain.",
  publisher = "ACM",
  pages     = "36--40",
  month     =  nov,
  year      =  2014,
  address   = "New York",
  keywords  = "linear algebra;numerical analysis;parallel processing;partial
               differential equations;Julia;Lisp family;PDE;dynamic
               language;elliptic problems;homogenization result yielding
               effective coefficients;macroscopic domain;macroscopic length
               scales;microscopic length scales;multiscale problems;nanoscale
               sensors;native code compilation;numerical codes;numerical
               problems;numerical solutions;numerical stochastic
               homogenization;optimized linear algebra routines;parallel
               computing;partial differential equations;powerful macro
               system;stochastic coefficients;stochastic
               dimensions;Mathematical model;Microscopy;Nanoscale
               devices;Object oriented modeling;Sensors;Sparse
               matrices;Stochastic processes;Julia; high-performance computing;
               PDEs; numerical homogenization;julia;Mendeley Import (Jan
               17)/Julia"
}

@INPROCEEDINGS{Foulds2013-qa,
  title     = "Stochastic collapsed variational Bayesian inference for latent
               Dirichlet allocation",
  booktitle = "Proceedings of the 19th {ACM} {SIGKDD} international conference
               on Knowledge discovery and data mining",
  author    = "Foulds, James and Boyles, Levi and DuBois, Christopher and
               Smyth, Padhraic and Welling, Max",
  abstract  = "There has been an explosion in the amount of digital text
               information available in recent years, leading to challenges of
               scale for traditional inference algorithms for topic models.
               Recent advances in stochastic variational inference algorithms
               for latent Dirichlet allocation (LDA) have made it feasible to
               learn topic models on very large-scale corpora, but these
               methods do not currently take full advantage of the collapsed
               representation of the model. We propose a stochastic algorithm
               for collapsed variational Bayesian inference for LDA, which is
               simpler and more efficient than the state of the art method. In
               experiments on large-scale text corpora, the algorithm was found
               to converge faster and often to a better solution than previous
               methods. Human-subject experiments also demonstrated that the
               method can learn coherent topics in seconds on small corpora,
               facilitating the use of topic models in interactive document
               analysis software.",
  publisher = "ACM",
  pages     = "446--454",
  series    = "KDD '13",
  month     =  "11~" # aug,
  year      =  2013,
  address   = "New York, NY, USA",
  keywords  = "stochastic learning; topic models; variational
               inference;julia;Mendeley Import (Jan 17)/Julia"
}

@MASTERSTHESIS{Verstraete2014-ag,
  title    = "Parallelle abstracties voor het programmeren van {\{GPU's\}} in
              \{Julia\} (Parallel abstractions for programming {\{GPUs\}} in
              \{Julia\})",
  author   = "Verstraete, Pieter",
  editor   = "De Sutter, Bjorn and Besard, Tim",
  abstract = "This master's thesis explores the possibility to provide access
              to the computing power of a GPU from the high-level programming
              language Julia. An important requirement here is to keep the
              programmer's productivity at the same high level as if he would
              use Julia without a GPU. Indeed, very specialized and detailed
              technical knowledge is needed in order to program a GPU, making
              it complex and time-consuming. In many modern scientific domains
              quite a lot of brute computing power is required, but often these
              domains lack the technical expertise to use GPUs in an efficient
              manner. The purpose of this thesis is to provide access to a GPU
              from Julia in a way that shields the GPU details from the
              programmer. In a first step we define and implement in Julia
              abstractions that can be executed in parallel on the GPU. Next we
              adapt the Julia compiler such that it can translate these
              abstractions to GPU code. The resulting compiler infrastructure
              manages the GPU in a way that is transparent to the programmer.
              Finally we evaluate the abstractions and compiler infrastructure
              in the context of a concrete application, namely the trace
              transform.",
  year     =  2014,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Bezanson2014-xl,
  title    = "\{J\}ulia: A Fresh Approach to Numerical Computing",
  author   = "Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah,
              Viral B",
  abstract = "The Julia programming language is gaining enormous popularity.
              Julia was designed to be easy and fast. Most importantly, Julia
              shatters deeply established notions widely held in the applied
              community: 1. High-level, dynamic code has to be slow by some
              sort of law of nature. 2. It is sensible to prototype in one
              language and then recode in another language. 3. There are parts
              of a system for the programmer, and other parts best left
              untouched as they are built by the experts. Julia began with a
              deep understanding of the needs of the scientific programmer and
              the needs of the computer in mind. Bridging cultures that have
              often been distant, Julia combines expertise from computer
              science and computational science creating a new approach to
              scientific computing. This note introduces the programmer to the
              language and the underlying design theory. It invites the reader
              to rethink the fundamental foundations of numerical computing
              systems. In particular, there is the fascinating dance between
              specialization and abstraction. Specialization allows for custom
              treatment. We can pick just the right algorithm for the right
              circumstance and this can happen at runtime based on argument
              types (code selection via multiple dispatch). Abstraction
              recognizes what remains the same after differences are stripped
              away and ignored as irrelevant. The recognition of abstraction
              allows for code reuse (generic programming). A simple idea that
              yields incredible power. The Julia design facilitates this
              interplay in many explicit and subtle ways for machine
              performance and, most importantly, human convenience.",
  month    =  nov,
  year     =  2014,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Gaudreau2014-cu,
  title    = "Computing Energy Eigenvalues of Anharmonic Oscillators using the
              Double Exponential Sinc collocation Method",
  author   = "Gaudreau, Philippe and Slevinsky, Richard and Safouhi, Hassan",
  abstract = "A quantum anharmonic oscillator is defined by the Hamiltonian H,
              where the potential is given by V. Using the Sinc collocation
              method combined with the double exponential transformation, we
              develop a method to efficiently compute highly accurate
              approximations of energy eigenvalues for anharmonic oscillators.
              Convergence properties of the proposed method are presented.
              Using the principle of minimal sensitivity, we introduce an
              alternate expression for the mesh size for the Sinc collocation
              method which improves considerably the accuracy in computing
              eigenvalues for potentials with multiple wells. We apply our
              method to a number of potentials including potentials with
              multiple wells. The numerical results section clearly illustrates
              the high efficiency and accuracy of the proposed method. All our
              codes are written using the programming language Julia and are
              available upon request.",
  year     =  2014,
  keywords = "julia;Mendeley Import (Jan 17)/Julia"
}

@ARTICLE{Deisenroth2015-zq,
  title    = "Gaussian Processes for {Data-Efficient} Learning in Robotics and
              Control",
  author   = "Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward",
  abstract = "Autonomous learning has been a promising direction in control and
              robotics for more than a decade since data-driven learning allows
              to reduce the amount of engineering knowledge, which is otherwise
              required. However, autonomous reinforcement learning (RL)
              approaches typically require many interactions with the system to
              learn controllers, which is a practical limitation in real
              systems, such as robots, where many interactions can be
              impractical and time consuming. To address this problem, current
              learning approaches typically require task-specific knowledge in
              form of expert demonstrations, realistic simulators, pre-shaped
              policies, or specific knowledge about the underlying dynamics. In
              this paper, we follow a different approach and speed up learning
              by extracting more information from data. In particular, we learn
              a probabilistic, non-parametric Gaussian process transition model
              of the system. By explicitly incorporating model uncertainty into
              long-term planning and controller learning our approach reduces
              the effects of model errors, a key problem in model-based
              learning. Compared to state-of-the art RL our model-based policy
              search method achieves an unprecedented speed of learning. We
              demonstrate its applicability to autonomous learning in real
              robot and control tasks.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  37,
  number   =  2,
  pages    = "408--423",
  month    =  feb,
  year     =  2015,
  keywords = "Bayesian inference; Gaussian processes; Index Terms---Policy
              search; control; reinforcement learning !; robotics;Mendeley
              Import (Jan 17)/Robotics",
  language = "en"
}

@INPROCEEDINGS{Forte2010-sa,
  title     = "Robot learning by Gaussian process regression",
  booktitle = "19th International Workshop on Robotics in {Alpe-Adria-Danube}
               Region ({RAAD} 2010)",
  author    = "Forte, D and Ude, A and Kos, A",
  abstract  = "Intelligent robots cannot be programmed in advance for all
               possible situations, but they should be able to generalize based
               on the acquired knowledge. In robot learning based on imitation
               of human activity we often use statistical methods that
               generalize observed (learned) movements. The acquired data is
               used to generate useful robots responses in situations for which
               the robot has not been specifically instructed how to respond.
               The paper describes the robot learning with Gaussian process
               regression that creates the model and estimates the parameters
               for generalization of the acquired motor knowledge, which is
               accumulated as a database of example movements. New actions are
               synthesized by applying Gaussian process regression, where the
               goal and other characteristics of an action are utilized as
               queries to create an optimal control policy with respect to the
               previously acquired knowledge. The paper demonstrates that the
               proposed methodology can be integrated with an active vision
               system of a humanoid robot. 3D vision data is used to provide
               query points for statistical generalization.",
  publisher = "IEEE",
  pages     = "303--308",
  month     =  jun,
  year      =  2010,
  keywords  = "3D vision data; Control system synthesis; Databases; Gaussian
               process regression; Gaussian processes; Humanoid robots; Humans;
               Intelligent robots; Machine vision; Optimal control; Parameter
               estimation; Statistical analysis; active vision system; humanoid
               robot; humanoid robots; intelligent robots; learning systems;
               optimal control; optimal control policy; regression analysis;
               robot learning; robot vision; statistical methods;Mendeley
               Import (Jan 17)/Robotics"
}

@ARTICLE{Kaelbling1996-tw,
  title    = "Reinforcement Learning: A Survey",
  author   = "Kaelbling, L P and Littman, M L and Moore, A W",
  journal  = "J. Artif. Intell. Res.",
  volume   =  4,
  pages    = "237--285",
  year     =  1996,
  keywords = "reinforcement learning;Mendeley Import (Jan
              17)/ReinforcementLearning;Mendeley Import (Jan 17)/Robotics"
}

@INPROCEEDINGS{Jordan1997-nb,
  title     = "Hidden Markov decision trees",
  booktitle = "{NIPS}",
  author    = "Jordan, Michael I and Ghahramani, Zoubin and Saul, Lawrence K",
  abstract  = "We study a time series model that can be viewed as a decision
               tree with Markov temporal structure. The model is intractable
               for exact calculations, thus we utilize variational
               approximations. We consider three diierent distributions for the
               approximation: one in which the Markov calculations are
               performed exactly and the layers of the decision tree are
               decoupled, one in which the decision tree calculations are
               performed exactly and the time steps of the Markov chain are
               decoupled, and one in which a Viterbi-like assumption is made to
               pick out a single most likely state sequence. We present
               simulation results for artiicial data and the Bach chorales.",
  year      =  1997,
  keywords  = "HMM;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Tesch2012-mg,
  title    = "Expensive Multiobjective Optimization and Validation with a
              Robotics Application",
  author   = "Tesch, Matthew and Schneider, Jeff and Choset, Howie",
  abstract = "Many practical optimization problems, especially in robotics,
              involve multiple competing objectives, e.g. performance metrics
              such as speed and energy effi-ciency. Proper treatment of these
              objective functions is often overlooked. Addi-tionally,
              optimization of the performance of robotic systems can be
              restricted due to the expensive nature of testing control
              parameters on a physical system. This paper presents a
              multi-objective optimization (MOO) algorithm for
              expensive-to-evaluate functions which generates a Pareto set of
              solutions. This algorithm is compared against another leading MOO
              algorithm, and then used to optimize the speed and head stability
              of the sidewinding gait for a snake robot.",
  journal  = "NIPS",
  year     =  2012,
  keywords = "optimisation;robotics;Mendeley Import (Jan 17)/Robotics"
}

@INPROCEEDINGS{Tesch2013-kg,
  title     = "Expensive multiobjective optimization for robotics",
  booktitle = "2013 {IEEE} International Conference on Robotics and Automation",
  author    = "Tesch, M and Schneider, J and Choset, H",
  abstract  = "Many practical optimization problems in robotics involve
               multiple competing objectives - from design trade-offs to
               performance metrics of the physical system such as speed and
               energy efficiency. Proper treatment of these objective
               functions, while commonplace in fields such as economics, is
               often overlooked in robotics. Additionally, optimization of the
               performance of robotic systems can be restricted due to the
               expensive nature of testing control parameters on a physical
               system. This paper presents a multi-objective optimization (MOO)
               algorithm for expensive-to-evaluate functions that generates a
               Pareto set of solutions. This algorithm is compared against
               another leading MOO algorithm, and then used to optimize the
               speed and head stability of the sidewinding gait for a snake
               robot.",
  publisher = "IEEE",
  pages     = "973--980",
  month     =  may,
  year      =  2013,
  keywords  = "Estimation; MOO algorithm; Pareto optimisation; Pareto solution
               set; control parameters; design trade-offs; energy efficiency;
               expensive multiobjective optimization; expensive-to-evaluate
               functions; head stability; mobile robots; multiple competing
               objectives; physical system; robotic systems; sidewinding gait;
               snake robot; speed stability;
               stability;optimisation;robotics;Mendeley Import (Jan
               17)/Robotics"
}

@INPROCEEDINGS{Tesch2013-xq,
  title     = "Learning Stochastic Binary Tasks using Bayesian Optimization
               with Shared Task Knowledge",
  booktitle = "International Conference on Machine Learning: Workshop on Robot
               Learning (Atlanta, June 16-21 2013)",
  author    = "Tesch, Matthew and Schneider, Jeff and Choset, Howie",
  abstract  = "Robotic systems often have tunable parame-ters which can affect
               performance; Bayesian optimization methods provide for efficient
               pa-rameter optimization, reducing required tests on the robot.
               This paper addresses Bayesian optimization in the setting where
               perfor-mance is only observed through a stochastic binary
               outcome -- success or failure. We de-fine the stochastic binary
               optimization prob-lem, present a Bayesian framework using
               Gaussian processes for classification, adapt the existing
               expected improvement metric for the binary case, and benchmark
               its perfor-mance. We also exploit problem structure and task
               similarity to generate principled task priors allowing efficient
               search for diffi-cult tasks. This method is used to create an
               adaptive policy for climbing over obstacles of varying heights.",
  year      =  2013,
  keywords  = "optimisation;robotics;Mendeley Import (Jan 17)/Robotics"
}

@ARTICLE{Ayvali2015-bl,
  title         = "Using Bayesian Optimization to Guide Probing of a Flexible
                   Environment for Simultaneous Registration and Stiffness
                   Mapping",
  author        = "Ayvali, Elif and Srivatsan, Rangaprasad Arun and Wang, Long
                   and Roy, Rajarshi and Simaan, Nabil and Choset, Howie",
  abstract      = "One of the goals of computer-aided surgery is to match
                   intraoperative data to preoperative images of the anatomy
                   and add complementary information that can facilitate the
                   task of surgical navigation. In this context, mechanical
                   palpation can reveal critical anatomical features such as
                   arteries and cancerous lumps which are stiffer that the
                   surrounding tissue. This work uses position and force
                   measurements obtained during mechanical palpation for
                   registration and stiffness mapping. Prior approaches,
                   including our own, exhaustively palpated the entire organ to
                   achieve this goal. To overcome the costly palpation of the
                   entire organ, a Bayesian optimization framework is
                   introduced to guide the end effector to palpate stiff
                   regions while simultaneously updating the registration of
                   the end effector to an a priori geometric model of the
                   organ, hence enabling the fusion of ntraoperative data into
                   the a priori model obtained through imaging. This new
                   framework uses Gaussian processes to model the stiffness
                   distribution and Bayesian optimization to direct where to
                   sample next for maximum information gain. The proposed
                   method was evaluated with experimental data obtained using a
                   Cartesian robot interacting with a silicone organ model and
                   an ex vivo porcine liver.",
  month         =  "19~" # sep,
  year          =  2015,
  keywords      = "optimisation;robotics;Mendeley Import (Jan 17)/Robotics",
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1509.05830"
}

@ARTICLE{Kandasamy_undated-rf,
  title    = "High Dimensional Bayesian Optimisation and Bandits via Additive
              Models",
  author   = "Kandasamy, Kirthevasan and Schneider, Jeff and P{\'o}czos,
              Barnab{\'a}s",
  abstract = "Bayesian Optimisation (BO) is a technique used in optimising a
              D-dimensional function which is typically expensive to evaluate.
              While there have been many successes for BO in low dimen-sions,
              scaling it to high dimensions has been no-toriously difficult.
              Existing literature on the topic are under very restrictive
              settings. In this paper, we identify two key challenges in this
              endeavour. We tackle these challenges by assuming an addi-tive
              structure for the function. This setting is sub-stantially more
              expressive and contains a richer class of functions than previous
              work. We prove that, for additive functions the regret has only
              lin-ear dependence on D even though the function depends on all D
              dimensions. We also demon-strate several other statistical and
              computational benefits in our framework. Via synthetic exam-ples,
              a scientific simulation and a face detection problem we
              demonstrate that our method outper-forms naive BO on additive
              functions and on sev-eral examples where the function is not
              additive.",
  keywords = "optimisation;robotics;Mendeley Import (Jan 17)/Robotics"
}

@ARTICLE{Tesch_undated-vw,
  title    = "Adapting Control Policies for Expensive Systems to Changing
              Environments",
  author   = "Tesch, Matthew and Schneider, Jeff and Choset, Howie",
  abstract = "--- Many controlled systems must operate over a range of external
              conditions. In this paper, we focus on the problem of learning a
              policy to adapt a system's controller based on the value of these
              external conditions in order to always perform well (i.e.,
              maximize system output). In addition, we are concerned with
              systems for which it is expensive to run experiments, and
              therefore restrict the number that can be run during training. We
              formally define the problem setup and the notion of an optimal
              control policy. We propose two algorithms which aim to find such
              a policy while minimizing the number of system output
              evaluations. We present results comparing these algorithms and
              various other approaches and discuss the inherent tradeoffs in
              the proposed algorithms. Finally, we use these methods to train
              both simulated and physical snake robots to automatically adapt
              to changing terrain, and demonstrate improved performance on test
              courses with changing environ-ments.",
  keywords = "optimisation;robotics;Mendeley Import (Jan 17)/Robotics"
}

@INPROCEEDINGS{Tesch2011-ep,
  title     = "Using response surfaces and expected improvement to optimize
               snake robot gait parameters",
  booktitle = "2011 {IEEE/RSJ} International Conference on Intelligent Robots
               and Systems",
  author    = "Tesch, M and Schneider, J and Choset, H",
  abstract  = "Several categories of optimization problems suffer from
               expensive objective function evaluation, driving the need for
               smart selection of subsequent experiments. One such category of
               problems involves physical robotic systems, which often require
               significant time, effort, and monetary expenditure in order to
               run tests. To assist in the selection of the next experiment,
               there has been a focus on the idea of response surfaces in
               recent years. These surfaces interpolate the existing data and
               provide a measure of confidence in their error, serving as a
               low-fidelity surrogate function that can be used to more
               intelligently choose the next experiment. In this paper, we
               robustly implement a previous algorithm based on the response
               surface methodology with an expected improvement criteria. We
               apply this technique to optimize open-loop gait parameters for
               snake robots, and demonstrate improved locomotive capabilities.",
  publisher = "IEEE",
  pages     = "1069--1074",
  year      =  2011,
  keywords  = "Noise; Noise measurement; Optimization; Response surface
               methodology; Robot kinematics; Surface
               treatment;optimisation;robotics;Mendeley Import (Jan
               17)/Robotics"
}

@ARTICLE{Berenson2008-bb,
  title    = "An Optimization Approach to Planning for Mobile Manipulation",
  author   = "Berenson, Dmitry and Kuffner, James and Choset, Howie",
  abstract = "--- We present an optimization-based approach to grasping and
              path planning for mobile manipulators. We focus on pick-and-place
              operations, where a given object must be moved from its start
              configuration to its goal configuration by the robot. Given only
              the start and goal configurations of the object and a model of
              the robot and scene, our algorithm finds a grasp and a trajectory
              for the robot that will bring the object to its goal
              configuration. The algorithm consists of two phases: optimization
              and planning. In the optimization phase, the optimal robot
              configurations and grasp are found for the object in its start
              and goal configurations using a co-evolutionary algorithm. In the
              planning phase, a path is found connecting the two robot
              configurations found by the optimization phase using
              Rapidly-Exploring Random Trees (RRTs). We benchmark our algorithm
              and demonstrate it on a 10 DOF mobile manipulator performing
              complex pick-and-place tasks in simulation.",
  year     =  2008,
  keywords = "Evolutionary Robotics; Manipulation Planning; Motion and Path
              Planning;optimisation;robotics;Mendeley Import (Jan
              17)/Robotics;Mendeley Import (Jan 17)/Robotics/Planning"
}

@ARTICLE{Leger1999-pr,
  title    = "Automated Synthesis and Optimization of Robot Configurations : An
              Evolutionary Approach",
  author   = "Leger, Chris",
  abstract = "Robot configuration design is hampered by the lack of
              established, well-known design rules, and designers cannot easily
              grasp the space of possible designs and the im- pact of all
              design variables on a robots performance. Realistically, a human
              can only de- sign and evaluate several candidate configurations,
              though there may be thousands of competitive designs that should
              be investigated. In contrast, an automated approach to
              configuration synthesis can create tens of thousands of designs
              and measure the perfor- mance of each one without relying on
              previous experience or design rules. This thesis creates
              Darwin2K, an extensible, automated system for robot configu-
              ration synthesis. This research focuses on the development of
              synthesis capabilities re- quired for many robot design problems:
              a flexible and effective synthesis algorithm, useful simulation
              capabilities, appropriate representation of robots and their
              properties, and the ability to accomodate application-specific
              synthesis needs. Darwin2K can synthe- size and optimize
              kinematics, dynamics, structural geometry, actuator selection,
              and task and control parameters for a wide range of robots.
              Darwin2K uses an evolutionary algorithm to synthesize robots, and
              utilizes two new multi-objective selection procedures that are
              applicable to other evolutionary design domains. The evolutionary
              algorithm can effectively optimize multiple performance ob-
              jectives while satisfying multiple performance constraints, and
              can generate a range of so- lutions representing different
              trade-offs between objectives. Darwin2K uses a novel
              representation for robot configurations called the parameterized
              module configuration graph, enabling efficient and extensible
              synthesis of mobile robots, of single, multiple and bifurcating
              manipulators, and of robots with either modular or monolithic
              construction. Task-specific simulation is used to provide the
              synthesis algorithm with perfor- mance measurements for each
              robot. Darwin2K can automatically derive dynamic equa- tions for
              each robot it simulates, enabling dynamic simulation to be used
              during synthesis for the first time. Darwin2K also includes a
              variety of simulation components, including Jacobian and PID
              controllers, algorithms for estimating link deflection and for
              detecting collisions; modules for robot links, joints (including
              actuator models), tools, and bases (fixed and mobile); and
              metrics such as task coverage, task completion time, end effector
              error, actuator saturation, and link deflection. A significant
              component of the system is its extensible object-oriented
              software architecture, which allows new simulation capabili- ties
              and robot modules to be added without impacting the synthesis
              algorithm. The ar- chitecture also encourages re-use of existing
              toolkit components, allowing task-specific simulators to be
              quickly constructed. Darwin2Ks synthesis algorithm, simulation
              capabilities, and extensible architec- ture combine to allow
              synthesis of robots for a wide range of tasks. Results are
              presented for nearly 150 synthesis experiments for six different
              applications, including synthesis of a free-flying 22-DOF robot
              with multiple manipulators and a walking machine for zero-
              gravity truss walking. The synthesis system and results represent
              a significant advance in the state-of-the-art in automated
              synthesis for robotics.",
  journal  = "Des. Eng.",
  pages    = "1--234",
  year     =  1999,
  keywords = "robotics;Mendeley Import (Jan 17)/Robotics"
}

@BOOK{Shannon2015-cs,
  title    = "The mathematical theory of communication",
  author   = "Shannon, C E and Weaver, W",
  year     =  2015,
  keywords = "Information theory;Mendeley Import (Jan 17)/Other"
}

@MISC{DeepDive_undated-du,
  title    = "Factor Graph",
  author   = "{DeepDive}",
  abstract = "A simple tutorial about factor graphs",
  keywords = "Mendeley Import (Jan 17)/GraphicalModels"
}

@ARTICLE{Sutton_undated-hy,
  title    = "An Introduction to Conditional Random Fields for Relational
              Learning",
  author   = "Sutton, Charles and Mccallum, Andrew",
  keywords = "Mendeley Import (Jan 17)/GraphicalModels"
}

@BOOK{Lavalle2006-gt,
  title     = "Planning Algorithms",
  author    = "Lavalle, S M",
  abstract  = "This book presents a unified treatment of many different kinds
               of planning algorithms. The subject lies at the crossroads
               between robotics, control theory, artificial intelligence,
               algorithms, and computer graphics. The particular subjects
               covered include motion planning, discrete planning, planning
               under uncertainty, sensor-based planning, visibility,
               decision-theoretic planning, game theory, information spaces,
               reinforcement learning, nonlinear systems, trajectory planning,
               nonholonomic planning, and kinodynamic planning.",
  publisher = "Cambridge University Press, Cambridge, London and New York",
  pages     = "842",
  year      =  2006,
  keywords  = "Textbook;Mendeley Import (Jan 17)/TextBooks;Mendeley Import (Jan
               17)/Robotics/Planning"
}

@INCOLLECTION{Mateus2016-du,
  title     = "{Human-Aware} Navigation Using External Omnidirectional Cameras",
  booktitle = "Robot 2015: Second Iberian Robotics Conference",
  author    = "Mateus, Andr{\'e} and Miraldo, Pedro and Lima, Pedro U and
               Sequeira, Jo{\~a}o",
  editor    = "Reis, Lu{\'\i}s Paulo and Moreira, Ant{\'o}nio Paulo and Lima,
               Pedro U and Montano, Luis and Mu{\~n}oz-Martinez, Victor",
  abstract  = "If robots are to invade our homes and offices, they will have to
               interact more naturally with humans. Natural interaction will
               certainly include the ability of robots to plan their motion,
               accounting for the social norms enforced. In this paper we
               propose a novel solution for Human-Aware Navigation resorting to
               external omnidirectional static cameras, used to implement a
               vision-based person tracking system. The proposed solution was
               tested in a typical domestic indoor scenario in four different
               experiments. The results show that the robot is able to cope
               with human-aware constraints, defined after common proxemics
               rules.",
  publisher = "Springer International Publishing",
  pages     = "283--295",
  series    = "Advances in Intelligent Systems and Computing",
  year      =  2016,
  address   = "Lisbon",
  keywords  = "Mendeley Import (Jan 17)/Robotics/Planning",
  language  = "en"
}

@INCOLLECTION{Ventura2014-gn,
  title     = "Towards Optimal Robot Navigation in Domestic Spaces",
  booktitle = "{RoboCup} 2014: Robot World Cup {XVIII}",
  author    = "Ventura, Rodrigo and Ahmad, Aamir",
  abstract  = "The work presented in this paper is motivated by the goal of
               depend-able autonomous navigation of mobile robots. This goal is
               a fundamental require-ment for having autonomous robots in
               spaces such as domestic spaces and public establishments, left
               unattended by technical staff. In this paper we tackle this
               problem by taking an optimization approach: on one hand, we use
               a Fast March-ing Approach for path planning, resulting in
               optimal paths in the absence of un-mapped obstacles, and on the
               other hand we use a Dynamic Window Approach for guidance. To the
               best of our knowledge, the combination of these two meth-ods is
               novel. We evaluate the approach on a real mobile robot, capable
               of moving at high speed. The evaluation makes use of an external
               ground truth system. We report controlled experiments that we
               performed, including the presence of peo-ple moving randomly
               nearby the robot. In our long term experiments we report a total
               distance of 18 km traveled during 11 hours of movement time.",
  pages     = "318--331",
  year      =  2014,
  keywords  = "Mendeley Import (Jan 17)/Robotics/Planning"
}

@ARTICLE{Kruse2013-qw,
  title    = "Human-aware robot navigation: A survey",
  author   = "Kruse, Thibault and Pandey, Amit Kumar and Alami, Rachid and
              Kirsch, Alexandra",
  abstract = "Abstract Navigation is a basic skill for autonomous robots. In
              the last years human--robot interaction has become an important
              research field that spans all of the robot capabilities including
              perception, reasoning, learning, manipulation and navigation. For
              navigation, the presence of humans requires novel approaches that
              take into account the constraints of human comfort as well as
              social rules. Besides these constraints, putting robots among
              humans opens new interaction possibilities for robots, also for
              navigation tasks, such as robot guides. This paper provides a
              survey of existing approaches to human-aware navigation and
              offers a general classification scheme for the presented methods.",
  journal  = "Rob. Auton. Syst.",
  volume   =  61,
  number   =  12,
  pages    = "1726--1743",
  month    =  dec,
  year     =  2013,
  keywords = "Autonomous robot; Navigation; Human-aware; Human-centered
              environment; Survey;Mendeley Import (Jan 17)/Robotics/Planning"
}

@INPROCEEDINGS{Lillard2016-yg,
  title     = "Assurances for Enhanced Trust in Autonomous Systems",
  booktitle = "Submitted to {IROS}",
  author    = "Lillard, Austin and Frew, Eric W and Argrow, Brian and Lawrence,
               Dale and Ahmed, Nisar",
  year      =  2016,
  keywords  = "self-confidence;assurances;Mendeley Import (Jan 17)/Assurances"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Kuter2012-bv,
  title     = "Computational Mechanisms to Support Reporting of Self Confidence
               of Automated / Autonomous Systems",
  booktitle = "{AAAI} 2015 Fall Symposium",
  author    = "Kuter, Ugur and Miller, Chris",
  abstract  = "This paper describes a new candidate method of computing
               autonomous ``self confidence.'' We describe how to analyze a
               plan for possible but unexpected break down cases and how to
               adapt the plan to circumvent those conditions. We view the
               result plan as more stable than the original one. The ability of
               achieving such plan stability is the core of how we propose to
               compute a system’s self confidence in its decisions and plans.
               This paper summarizes this approach and presents a preliminary
               evaluation that shows our approach is promising",
  pages     = "18--21",
  year      =  2012,
  keywords  = "Technical Report FS-15-05;Mendeley Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Zucchini1999-yi,
  title     = "Illustrations of the use of pseudo-residuals in assessing the
               fit of a model",
  booktitle = "proceedings of the 14th international workshop on statistical
               modelling",
  author    = "Zucchini, Walter and Macdonald, Iain L",
  pages     = "37--40",
  year      =  1999,
  address   = "Graz, Austria",
  keywords  = "model checking; model selection; pseudo-residuals;Mendeley
               Import (Jan
               17)/Assurances/Quantifying/Consistency/HMMGoodnessOfFit"
}

@ARTICLE{Zhang2015-ql,
  title         = "Plan Explicability and Predictability for Robot Task
                   Planning",
  author        = "Zhang, Yu and Sreedharan, Sarath and Kulkarni, Anagha and
                   Chakraborti, Tathagata and Zhuo, Hankz Hankui and
                   Kambhampati, Subbarao",
  abstract      = "Intelligent robots and machines are becoming pervasive in
                   human populated environments. A desirable capability of
                   these agents is to respond to goal-oriented commands by
                   autonomously constructing task plans. However, such autonomy
                   can add significant cognitive load and potentially introduce
                   safety risks to humans when agents behave unexpectedly.
                   Hence, for such agents to be helpful, one important
                   requirement is for them to synthesize plans that can be
                   easily understood by humans. While there exists previous
                   work that studied socially acceptable robots that interact
                   with humans in ``natural ways'', and work that investigated
                   legible motion planning, there lacks a general solution for
                   high level task planning. To address this issue, we
                   introduce the notions of plan \{\textbackslashit
                   explicability\} and \{\textbackslashit predictability\}. To
                   compute these measures, first, we postulate that humans
                   understand agent plans by associating abstract tasks with
                   agent actions, which can be considered as a labeling
                   process. We learn the labeling scheme of humans for agent
                   plans from training examples using conditional random fields
                   (CRFs). Then, we use the learned model to label a new plan
                   to compute its explicability and predictability. These
                   measures can be used by agents to proactively choose or
                   directly synthesize plans that are more explicable and
                   predictable to humans. We provide evaluations on a synthetic
                   domain and with human subjects using physical robots to show
                   the effectiveness of our approach",
  month         =  "25~" # nov,
  year          =  2015,
  keywords      = "Mendeley Import (Jan 17)/Assurances/Quantifying/Consistency",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1511.08158"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Burrell2016-uy,
  title     = "How the machine ‘thinks’: Understanding opacity in machine
               learning algorithms",
  author    = "Burrell, Jenna",
  abstract  = "This article considers the issue of opacity as a problem for
               socially consequential mechanisms of classification and ranking,
               such as spam filters, credit card fraud detection, search
               engines, news trends, market segmentation and advertising,
               insurance or loan qualification, and credit scoring. These
               mechanisms of classification all frequently rely on
               computational algorithms, and in many cases on machine learning
               algorithms to do this work. In this article, I draw a
               distinction between three forms of opacity: (1) opacity as
               intentional corporate or state secrecy, (2) opacity as technical
               illiteracy, and (3) an opacity that arises from the
               characteristics of machine learning algorithms and the scale
               required to apply them usefully. The analysis in this article
               gets inside the algorithms themselves. I cite existing
               literatures in computer science, known industry practices (as
               they are publicly presented), and do some testing and
               manipulation of code as a form of lightweight code audit. I
               argue that recogni...",
  journal   = "Big Data \& Society",
  publisher = "SAGE PublicationsSage UK: London, England",
  volume    =  3,
  number    =  1,
  pages     = "78",
  month     =  "5~" # jan,
  year      =  2016,
  keywords  = "classification;transparency;Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

@INPROCEEDINGS{Kadous1999-rx,
  title     = "Learning Comprehensible Descriptions of Multivariate Time Series",
  booktitle = "In Proceedings of the 16 th International Conference of Machine
               Learning ({ICML-99}",
  author    = "Kadous, Mohammed Waleed",
  editor    = "Ivan Bratko, Saso Dzeroski",
  abstract  = "Supervised classification is one of the most active areas of
               machine learning research. Most work has focused on
               classification in static domains, where an instantaneous
               snapshot of attributes is meaningful. In many domains,
               attributes are not static; in fact, it is the way they vary
               temporally that can make classification possible. Examples of
               such domains include speech recognition, gesture recognition and
               electrocardiograph classification. While it is possible to use
               ad hoc, domain-specific techniques for ``flattening '' the time
               series to a learner-friendly representation, this fails to take
               into account both the special problems and special heuristics
               applicable to temporal data and often results in unreadable
               concept descriptions. Though traditional time series techniques
               can sometimes produce accurate classifiers, few can provide
               comprehensible descriptions. We propose a general architecture
               for classification and description of multivariate time series.
               It employs event primitive...",
  pages     = "454--463",
  year      =  1999,
  keywords  = "
               assurance\_explicit;trust\_informal\_treatment;interp\_models;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Dundas2011-mn,
  title         = "{IBSEAD}: - A {Self-Evolving} {Self-Obsessed} Learning
                   Algorithm for Machine Learning",
  author        = "Dundas, Jitesh and Chik, David",
  abstract      = "We present IBSEAD or distributed autonomous entity systems
                   based Interaction - a learning algorithm for the computer to
                   self-evolve in a self-obsessed manner. This learning
                   algorithm will present the computer to look at the internal
                   and external environment in series of independent entities,
                   which will interact with each other, with and/or without
                   knowledge of the computer's brain. When a learning algorithm
                   interacts, it does so by detecting and understanding the
                   entities in the human algorithm. However, the problem with
                   this approach is that the algorithm does not consider the
                   interaction of the third party or unknown entities, which
                   may be interacting with each other. These unknown entities
                   in their interaction with the non-computer entities make an
                   effect in the environment that influences the information
                   and the behaviour of the computer brain. Such details and
                   the ability to process the dynamic and unsettling nature of
                   these interactions are absent in the current learning
                   algorithm such as the decision tree learning algorithm.
                   IBSEAD is able to evaluate and consider such algorithms and
                   thus give us a better accuracy in simulation of the highly
                   evolved nature of the human brain. Processes such as dreams,
                   imagination and novelty, that exist in humans are not fully
                   simulated by the existing learning algorithms. Also, Hidden
                   Markov models (HMM) are useful in finding ``hidden''
                   entities, which may be known or unknown. However, this model
                   fails to consider the case of unknown entities which maybe
                   unclear or unknown. IBSEAD is better because it considers
                   three types of entities- known, unknown and invisible. We
                   present our case with a comparison of existing algorithms in
                   known environments and cases and present the results of the
                   experiments using dry run of the simulated runs of the
                   existing machine learning algorithms versus IBSEAD.",
  month         =  "30~" # jun,
  year          =  2011,
  keywords      = "Mendeley Import (Jan 17)/Assurances/Self-Aware",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1106.6186"
}

@ARTICLE{Khajah2016-xt,
  title         = "How deep is knowledge tracing?",
  author        = "Khajah, Mohammad and Lindsey, Robert V and Mozer, Michael C",
  abstract      = "In theoretical cognitive science, there is a tension between
                   highly structured models whose parameters have a direct
                   psychological interpretation and highly complex,
                   general-purpose models whose parameters and representations
                   are difficult to interpret. The former typically provide
                   more insight into cognition but the latter often perform
                   better. This tension has recently surfaced in the realm of
                   educational data mining, where a deep learning approach to
                   predicting students' performance as they work through a
                   series of exercises---termed deep knowledge tracing or
                   DKT---has demonstrated a stunning performance advantage over
                   the mainstay of the field, Bayesian knowledge tracing or
                   BKT. In this article, we attempt to understand the basis for
                   DKT's advantage by considering the sources of statistical
                   regularity in the data that DKT can leverage but which BKT
                   cannot. We hypothesize four forms of regularity that BKT
                   fails to exploit: recency effects, the contextualized trial
                   sequence, inter-skill similarity, and individual variation
                   in ability. We demonstrate that when BKT is extended to
                   allow it more flexibility in modeling statistical
                   regularities---using extensions previously proposed in the
                   literature---BKT achieves a level of performance
                   indistinguishable from that of DKT. We argue that while DKT
                   is a powerful, useful, general-purpose framework for
                   modeling student learning, its gains do not come from the
                   discovery of novel representations---the fundamental
                   advantage of deep learning. To answer the question posed in
                   our title, knowledge tracing may be a domain that does not
                   require `depth'; shallow models like BKT can perform just as
                   well and offer us greater interpretability and explanatory
                   power.",
  month         =  "14~" # mar,
  year          =  2016,
  keywords      = "Mendeley Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1604.02416"
}

@ARTICLE{Bottou_undated-lv,
  title    = "Two big challenges in machine learning",
  author   = "Bottou, L{\'e}on",
  keywords = "Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Lacave2002-cu,
  title     = "A review of explanation methods for Bayesian networks",
  author    = "Lacave, Carmen and D{\'\i}ez, Francisco J",
  journal   = "Knowl. Eng. Rev.",
  publisher = "Cambridge University Press",
  volume    =  17,
  number    =  02,
  pages     = "107--127",
  month     =  "1~" # jun,
  year      =  2002,
  keywords  = "trust\_informal\_treatment;assurance\_explicit;review;Mendeley
               Import (Jan 17)/Assurances/Quantifying/Consistency;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Gomez2015-je,
  title         = "{Real-Time} Stochastic Optimal Control for Multi-agent
                   Quadrotor Systems",
  author        = "G{\'o}mez, Vicen{\c c} and Thijssen, Sep and Symington,
                   Andrew and Hailes, Stephen and Kappen, Hilbert J",
  abstract      = "This paper presents a novel method for controlling teams of
                   unmanned aerial vehicles using Stochastic Optimal Control
                   (SOC) theory. The approach consists of a centralized
                   high-level planner that computes optimal state trajectories
                   as velocity sequences, and a platform-specific low-level
                   controller which ensures that these velocity sequences are
                   met. The planning task is expressed as a centralized
                   path-integral control problem, for which optimal control
                   computation corresponds to a probabilistic inference problem
                   that can be solved by efficient sampling methods. Through
                   simulation we show that our SOC approach (a) has significant
                   benefits compared to deterministic control and other SOC
                   methods in multimodal problems with noise-dependent optimal
                   solutions, (b) is capable of controlling a large number of
                   platforms in real-time, and (c) yields collective emergent
                   behaviour in the form of flight formations. Finally, we show
                   that our approach works for real platforms, by controlling a
                   team of three quadrotors in outdoor conditions.",
  pages         = "1--17",
  month         =  "16~" # feb,
  year          =  2015,
  keywords      = "Mendeley Import (Jan 17)/Robotics/r4sim",
  archivePrefix = "arXiv",
  primaryClass  = "cs.SY",
  eprint        = "1502.04548"
}

@ARTICLE{Allamraju_undated-tz,
  title    = "{Multi-Agent} Game Emulator ( {MAGE} ) for",
  author   = "Allamraju, Rakshit and Chowdhary, Girish",
  keywords = "Mendeley Import (Jan 17)/Robotics/r4sim"
}

@ARTICLE{Keivan_undated-mo,
  title    = "Generative scene models with analytical path-tracing",
  author   = "Keivan, Nima and Sibley, Gabe",
  pages    = "1--4",
  keywords = "Mendeley Import (Jan 17)/Robotics/r4sim"
}

@ARTICLE{Heckman_undated-zw,
  title    = "Simulation-in-the-loop for Planning and {Model-Predictive}
              Control",
  author   = "Heckman, Christoffer and Keivan, Nima and Sibley, Gabe",
  keywords = "Mendeley Import (Jan 17)/Robotics/r4sim"
}

@ARTICLE{Bennetts2015-gi,
  title    = "Integrated simulation of gas dispersion and mobile sensing
              systems",
  author   = "Bennetts, Victor Hernandez and Lilienthal, Achim J and
              Schaffernicht, Erik and Ferrari, Silvia and Albertson, John",
  pages    = "1--4",
  year     =  2015,
  keywords = "Mendeley Import (Jan 17)/Robotics/r4sim"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hauser_undated-xy,
  title    = "Rigid Body Simulation with Point Cloud Models in Klamp ’ t",
  author   = "Hauser, Kris",
  keywords = "Mendeley Import (Jan 17)/Robotics/r4sim"
}

@INCOLLECTION{Furrer2016-bz,
  title     = "{RotorS---A} Modular Gazebo {MAV} Simulator Framework",
  booktitle = "Robot Operating System ({ROS})",
  author    = "Furrer, Fadri and Burri, Michael and Achtelik, Markus and
               Siegwart, Roland",
  editor    = "Koubaa, Anis",
  abstract  = "In this chapter we present a modular Micro Aerial Vehicle (MAV)
               simulation framework, which enables a quick start to perform
               research on MAVs. After reading this chapter, the reader will
               have a ready to use MAV simulator, including control and state
               estimation. The simulator was designed in a modular way, such
               that different controllers and state estimators can be used
               interchangeably, while incorporating new MAVs is reduced to a
               few steps. The provided controllers can be adapted to a custom
               vehicle by only changing a parameter file. Different controllers
               and state estimators can be compared with the provided
               evaluation framework. The simulation framework is a good
               starting point to tackle higher level tasks, such as collision
               avoidance, path planning, and vision based problems, like
               Simultaneous Localization and Mapping (SLAM), on MAVs. All
               components were designed to be analogous to its real world
               counterparts. This allows the usage of the same controllers and
               state estimators, including their parameters, in the simulation
               as on the real MAV.",
  publisher = "Springer International Publishing",
  pages     = "595--625",
  series    = "Studies in Computational Intelligence",
  year      =  2016,
  keywords  = "Mendeley Import (Jan 17)/Robotics/r4sim",
  language  = "en"
}

@ARTICLE{Koenig2015-es,
  title    = "Gazebo in the {DRC} : development from the last three years",
  author   = "Koenig, Nathan and Foote, Tully",
  year     =  2015,
  keywords = "Mendeley Import (Jan 17)/Robotics/r4sim"
}

@ARTICLE{Degroote_undated-bw,
  title    = "Integrating Realistic Simulation Engines within the {MORSE}
              Framework",
  author   = "Degroote, Arnaud and Koch, Pierrick and Lacroix, Simon",
  keywords = "Mendeley Import (Jan 17)/Robotics/r4sim"
}

@ARTICLE{Wick1989-jx,
  title    = "The 1988 {AAAI} Workshop on Explanation",
  author   = "Wick, Michael R",
  abstract = "This article is a summary of the Workshop on Explanation held
              during the 1988 National Conference on Artificial Intelligence in
              St. Paul, Minnesota. The purpose of the workshop was to identify
              key research issues in the rapidly emerging area of expert system
              explanation.",
  journal  = "AI Magazine",
  volume   =  10,
  number   =  3,
  pages    = "22",
  month    =  "15~" # sep,
  year     =  1989,
  keywords = "Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Smolensky1988-sq,
  title     = "On the proper treatment of connectionism",
  author    = "Smolensky, Paul",
  journal   = "Behav. Brain Sci.",
  publisher = "Cambridge Univ Press",
  volume    =  11,
  number    =  01,
  pages     = "1--23",
  year      =  1988,
  keywords  = "Mendeley Import (Jan 17)/Assurances;Mendeley Import (Jan
               17)/CurrentStudy"
}

@TECHREPORT{Smolensky1987-tj,
  title       = "On the proper treatment of connectionism",
  author      = "Smolensky, Paul",
  number      = " CU-CS-359-87",
  institution = "University of Colorado, Boulder",
  year        =  1987,
  keywords    = "Mendeley Import (Jan 17)/Assurances"
}

@INPROCEEDINGS{Zhang2014-bn,
  title     = "Spectral Methods meet {EM}: A Provably Optimal Algorithm for
               Crowdsourcing",
  booktitle = "{NIPS}",
  author    = "Zhang, Yuchen and Chen, Xi and Zhou, Denny and Jordan, Michael I",
  abstract  = "The Dawid-Skene estimator has been widely used for inferring the
               true labels from the noisy labels provided by non-expert
               crowdsourcing workers. However, since the estimator maximizes a
               non-convex log-likelihood function, it is hard to theoretically
               justify its performance. In this paper, we propose a two-stage
               efficient algorithm for multi-class crowd labeling problems. The
               first stage uses the spectral method to obtain an initial
               estimate of parameters. Then the second stage refines the
               estimation by optimizing the objective function of the
               Dawid-Skene estimator via the EM algorithm. We show that our
               algorithm achieves the optimal convergence rate up to a
               logarithmic factor. We conduct extensive experiments on
               synthetic and real datasets. Experimental results demonstrate
               that the proposed algorithm is comparable to the most accurate
               empirical approach, while outperforming several other recently
               proposed methods.",
  pages     = "1260--1268",
  year      =  2014,
  keywords  = "Mendeley Import (Jan 17)/Other"
}

@INPROCEEDINGS{Santambrogio2010-ak,
  title     = "Enabling technologies for self-aware adaptive systems",
  booktitle = "2010 {NASA/ESA} Conference on Adaptive Hardware and Systems",
  author    = "Santambrogio, M D and Hoffmann, H and Eastep, J and Agarwal, A",
  abstract  = "Self-aware computer systems will be capable of adapting their
               behavior and resources thousands of times a second to
               automatically find the best way to accomplish a given goal
               despite changing environmental conditions and demands. Such a
               capability benefits a broad spectrum of computer systems from
               embedded systems to supercomputers and is particularly useful
               for meeting power, performance, and resource-metering challenges
               in mobile computing, cloud computing, multicore computing,
               adaptive and dynamic compilation environments, and parallel
               operating systems. Some of the challenges in implementing
               self-aware systems are a) knowing within the system what the
               goals of applications are and if they are meeting them, b)
               deciding what actions to take to help applications meet their
               goals, and c) developing standard techniques that generalize and
               can be applied to a broad range of self-aware systems. This work
               presents our vision for self-aware adaptive systems and proposes
               enabling technologies to address these three challenges. We
               describe a framework called Application Heartbeats that provides
               a general, standardized way for applications to monitor their
               performance and make that information available to external
               observers. Then, through a study of a self-optimizing
               synchronization library called Smartlocks, we demonstrate a
               powerful technique that systems can use to determine which
               optimization actions to take. We show that Heartbeats can be
               applied naturally in the context of reinforcement learning
               optimization strategies as a reward signal and that, using such
               a strategy, Smartlocks are able to significantly improve
               performance of applications on an important emerging class of
               multicore systems called asymmetric multicores.",
  publisher = "IEEE",
  pages     = "149--156",
  month     =  jun,
  year      =  2010,
  keywords  = "embedded systems;learning (artificial
               intelligence);multiprocessing systems;software
               libraries;software performance evaluation;system
               monitoring;Application Heartbeats;Smartlocks;adaptive
               compilation environment;asymmetric multicore;cloud
               computing;dynamic compilation environment;embedded
               system;environmental condition;mobile computing;multicore
               computing;multicore system;parallel operating
               system;reinforcement learning optimization strategy;self-aware
               adaptive system;self-aware computer system;self-optimizing
               synchronization library;Adaptive systems;Benchmark
               testing;Biomedical monitoring;Computers;Monitoring;Multicore
               processing;Mendeley Import (Jan 17)/Assurances/Self-Aware"
}

@ARTICLE{Gorbenko2012-pu,
  title    = "Robot {Self-Awareness}: Occam's Razor for Fluents",
  author   = "Gorbenko, Anna and Popov, Vladimir",
  abstract = "In this paper we consider robot self-awareness from the point of
              view of temporal relation based data mining. In particular, we
              use rational function approximations and consider Occam's razor
              for fluents. A robot with self-aware system has the possibility
              of dealing with complex novel situations more effectively than a
              robot without self-awareness (see e.g. [1, 2]). In [1], the
              authors considered robot self-awareness from the point of view of
              temporal relation based data mining. Temporal patterns can be
              expressed using fluents (see e.g. [1, 3]). A fluent is a
              proposition with temporal extent. For example, `` drinking-coffee
              '' can be defined as a fluent that is true whenever I am drinking
              coffee. This fluent can be represented as a binary time series x,
              where x[t] is 1 if and only if I am drinking coffee at time t.
              Respectively, temporal relations needed only to express relations
              of fluents. To build a system of self-awareness the robot needs a
              system of automatic selection of fluents and prediction of values
              of selected fluents. Using some kind of system of self-learning
              and accumulation of knowledge (see e.g. [1, 3]) the",
  journal  = "Int. J. Math. Anal.",
  volume   =  6,
  number   =  30,
  pages    = "1453--1455",
  year     =  2012,
  keywords = "41A20 Keywords; Mathematics Subject Classification; Occam's
              razor; fluents; robot self-awareness;Mendeley Import (Jan
              17)/MLTheory"
}

@ARTICLE{Cox2007-tk,
  title    = "Perpetual {Self-Aware} Cognitive Agents",
  author   = "Cox, Michael T",
  abstract = "To construct a perpetual self-aware cognitive agent that can
              continuously operate with independence, an introspective machine
              must be produced. To assemble such an agent, it is necessary to
              perform a full integration of cognition (planning, understanding,
              and learning) and metacognition (control and monitoring of
              cognition) with intelligent behaviors. The failure to do this
              completely is why similar, more limited efforts have not
              succeeded in the past. I outline some key computational
              requirements of metacognition by describing a multi- strategy
              learning system called Meta-AQUA and then discuss an integration
              of Meta-AQUA with a nonlinear state-space planning agent. I show
              how the resultant system, INTRO, can independently generate its
              own goals, and I relate this work to the general issue of
              self-awareness by machine.",
  journal  = "AI Magazine",
  volume   =  28,
  number   =  1,
  pages    = "32",
  month    =  "15~" # mar,
  year     =  2007,
  keywords = "introspection;Mendeley Import (Jan 17)/Assurances/Self-Aware"
}

@ARTICLE{Brachman2002-oz,
  title     = "Systems that know what they're doing",
  author    = "Brachman, R J",
  abstract  = "The author discusses cognitive computers with the ability to
               reason, learn and respond intelligently to things that they have
               never encountered before. A truly cognitive system would be able
               to learn from its experience, as well as by being instructed,
               and perform better on day two than it did on day one. It would
               be able to explain what it was doing and why it was doing it.
               The author considers application foundations.",
  journal   = "IEEE Intell. Syst.",
  publisher = "IEEE",
  volume    =  17,
  number    =  6,
  pages     = "67--71",
  month     =  nov,
  year      =  2002,
  keywords  = "Artificial intelligence; Cognition; Humans; Information
               processing; Machine learning; Machine learning algorithms;
               Robustness; Speech processing; Speech recognition; Symbiosis;
               application foundations; cognitive computers; cognitive system;
               cognitive systems; explanation; inference mechanisms; knowledge
               based systems; learning; learning (artificial intelligence);
               reasoning; systems architecture;Mendeley Import (Jan
               17)/Assurances"
}

@INPROCEEDINGS{Dwork2012-fq,
  title     = "Fairness through awareness",
  booktitle = "Proceedings of the 3rd Innovations in Theoretical Computer
               Science Conference",
  author    = "Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and
               Reingold, Omer and Zemel, Richard",
  abstract  = "We study fairness in classification, where individuals are
               classified, e.g., admitted to a university, and the goal is to
               prevent discrimination against individuals based on their
               membership in some group, while maintaining utility for the
               classifier (the university). The main conceptual contribution of
               this paper is a framework for fair classification comprising (1)
               a (hypothetical) task-specific metric for determining the degree
               to which individuals are similar with respect to the
               classification task at hand; (2) an algorithm for maximizing
               utility subject to the fairness constraint, that similar
               individuals are treated similarly. We also present an adaptation
               of our approach to achieve the complementary goal of ``fair
               affirmative action,'' which guarantees statistical parity (i.e.,
               the demographics of the set of individuals receiving any
               classification are the same as the demographics of the
               underlying population), while treating similar individuals as
               similarly as possible. Finally, we discuss the relationship of
               fairness to privacy: when fairness implies privacy, and how
               tools developed in the context of differential privacy may be
               applied to fairness.",
  publisher = "ACM",
  pages     = "214--226",
  month     =  "8~" # jan,
  year      =  2012,
  address   = "New York, New York, USA",
  keywords  = "
               classification;transparency;trust\_informal\_treatment;assurance\_explicit;des\_behavior;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Li2010-nx,
  title     = "Knows what it knows: a framework for self-aware learning",
  author    = "Li, Lihong and Littman, Michael L and Walsh, Thomas J and
               Strehl, Alexander L",
  abstract  = "We introduce a learning framework that combines elements of the
               well-known PAC and mistake-bound models. The KWIK (knows what it
               knows) framework was designed particularly for its utility in
               learning settings where active exploration can impact the
               training examples the learner is exposed to, as is true in
               reinforcement-learning and active-learning problems. We catalog
               several KWIK-learnable classes as well as open problems, and
               demonstrate their applications in experience-efficient
               reinforcement learning.",
  journal   = "Mach. Learn.",
  publisher = "Springer US",
  volume    =  82,
  number    =  3,
  pages     = "399--443",
  month     =  "25~" # nov,
  year      =  2010,
  keywords  = "Mendeley Import (Jan 17)/MLTheory/UnsupervisedLearning",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Zahalka2011-mq,
  title    = "An experimental test of Occam’s razor in classification",
  author   = "Zah{\'a}lka, Jan and {\v Z}elezn{\'y}, Filip",
  abstract = "A widely persisting interpretation of Occam's razor is that given
              two classifiers with the same training error, the simpler
              classifier is more likely to generalize better. Within a
              long-lasting debate in the machine learning community over
              Occam's razor, Domingos (Data Min. Knowl. Discov. 3:409--425,
              1999) rejects this interpretation and proposes that model
              complexity is only a confounding factor usually correlated with
              the number of mod-els from which the learner selects. It is thus
              hypothesized that the risk of overfitting (poor generalization)
              follows only from the number of model tests rather than the
              complexity of the selected model. We test this hypothesis on 30
              UCI data sets using polynomial classi-fication models. The
              results confirm Domingos' hypothesis on the 0.05 significance
              level and thus refutes the above interpretation of Occam's razor.
              Our experiments however also illustrate that decoupling the two
              factors (model complexity and number of model tests) is
              problematic.",
  journal  = "Mach. Learn.",
  volume   =  82,
  number   =  3,
  pages    = "475--481",
  month    =  "18~" # mar,
  year     =  2011,
  keywords = "Mendeley Import (Jan 17)/MLTheory"
}

@INPROCEEDINGS{Tse2015-tz,
  title     = "Human-robot information sharing with structured language
               generation from probabilistic beliefs",
  booktitle = "2015 {IEEE/RSJ} International Conference on Intelligent Robots
               and Systems ({IROS})",
  author    = "Tse, R and Campbell, M",
  abstract  = "This paper presents a framework for information sharing and
               fusion in cooperative tasks involving humans and robots. In this
               context, all information regarding the state of interest is
               recursively fused and maintained by each agent in a form of
               belief. For a robot agent, its belief is commonly and
               practically represented as a probability density function (pdf),
               formed by traditional sensor fusion and state estimation
               algorithms. In cooperative tasks with non-expert humans, a robot
               needs to effectively communicate its belief so that the gathered
               information can be easily processed and interpreted by the
               humans. The goal of this research is to provide two-way
               information exchange and fusion between robots and humans, the
               former operating on pdfs, while the latter on English sentences.
               This is achieved by considering two goodness measures: semantic
               correctness and information preservation. Based on the goodness
               measures studied, results show that the proposed framework is
               able to generate optimal statements describing the given belief
               pdfs and successfully recover the initial inputs used to
               generate them. Additionally, in order to describe complex belief
               pdfs, a Mixture of Statements (MoS) model is proposed such that
               the optimal expression can be generated through a composition of
               more than one statements. With a nonparametric Dirichlet Process
               MoS generation, it is found that the robot can determine
               correctly the number of statements as well as the corresponding
               reference parameters needed to describe all hypotheses
               underlying its belief.",
  volume    = "2015-Decem",
  pages     = "1242--1248",
  year      =  2015,
  keywords  = "Information management; Logistics; Probability density function;
               Robot sensing systems; Semantics; Time measurement; belief
               networks; human-robot interaction; language translation;
               probability; sensor fusion; state estimation; English sentences;
               human-robot information sharing; information preservation;
               mixture of statements model; nonparametric Dirichlet process MoS
               generation; probabilistic beliefs; robot agent; semantic
               correctness; state estimation algorithms; structured language
               generation; two-way information exchange;Mendeley Import (Jan
               17)/Human-RobotCollaboration"
}

@INPROCEEDINGS{Dragan2013-wd,
  title     = "Generating Legible Motion",
  booktitle = "Proceedings of Robotics: Science and Systems",
  author    = "Dragan, Anca and Srinivasa, Siddhartha",
  abstract  = "Legible motion --- motion that communicates its intent to a
               human observer --- is crucial for enabling seamless human-robot
               collaboration. In this paper, we propose a functional gradient
               optimization technique for autonomously generating legible
               motion. Our algorithm optimizes a legibility metric inspired by
               the psychology of action interpretation in humans, resulting in
               motion trajectories that purposefully deviate from what an
               observer would expect in order to better convey intent. A trust
               region constraint on the optimization ensures that the motion
               does not become too surprising or unpredictable to the observer.
               Our studies with novice users that evaluate the resulting
               trajectories support the applicability of our method and of such
               a trust region. They show that within the region, legibility as
               measured in practice does significantly increase. Outside of it,
               however, the trajectory becomes confusing and the users'
               confidence in knowing the robot's intent significantly
               decreases.",
  year      =  2013,
  keywords  = "human\_study;assurance\_predictability;reinforcement
               learning;ai\_planning;assurance\_explicit;trust\_formal\_treatment;in\_paper;Mendeley
               Import (Jan 17)/Assurances"
}

@ARTICLE{Kadous2005-of,
  title     = "Classification of Multivariate Time Series and Structured Data
               Using Constructive Induction",
  author    = "Kadous, Mohammed Waleed and Sammut, Claude",
  abstract  = "We present a method of constructive induction aimed at learning
               tasks involving multivariate time series data. Using
               metafeatures, the scope of attribute-value learning is expanded
               to domains with instances that have some kind of recurring
               substructure, such as strokes in handwriting recognition, or
               local maxima in time series data. The types of substructures are
               defined by the user, but are extracted automatically and are
               used to construct attributes.Metafeatures are applied to two
               real domains: sign language recognition and ECG classification.
               Using metafeatures we are able to generate classifiers that are
               either comprehensible or accurate, producing results that are
               comparable to hand-crafted preprocessing and comparable to human
               experts.",
  journal   = "Mach. Learn.",
  publisher = "Kluwer Academic Publishers",
  volume    =  58,
  number    = "2-3",
  pages     = "179--216",
  month     =  "1~" # feb,
  year      =  2005,
  keywords  = "time series;Mendeley Import (Jan
               17)/Assurances/Quantifying/Consistency",
  language  = "en"
}

@INPROCEEDINGS{Williams1996-xr,
  title     = "Computing With Infinite Networks",
  booktitle = "Advances in Neural Information Processing Systems 9",
  author    = "Williams, Christopher",
  abstract  = "For neural networks with a wide class of weight-priors, it can
               be shown that in the limit of an infinite number of hidden units
               the prior over functions tends to a Gaussian process. In this
               paper analytic forms are derived for the covariance function of
               the Gaussian processes corresponding to networks with sigmoidal
               and Gaussian hidden units. This allows predictions to be made
               efficiently using networks with an infinite number of hidden
               units, and shows that, somewhat paradoxically, it may be easier
               to compute with infinite networks than finite ones.",
  publisher = "MIT Press",
  pages     = "295--301",
  year      =  1996,
  keywords  = "Mendeley Import (Jan 17)/GPs"
}

@PHDTHESIS{Neal1995-xb,
  title    = "{BAYESIAN} {LEARNING} {FOR} {NEURAL} {NETWORKS}",
  author   = "Neal, Radford M",
  abstract = "Two features distinguish the Bayesian approach to learning models
              from data. First, beliefs derived from background knowledge are
              used to select a prior probability distribution for the model
              parameters. Second, predictions of future observations are made
              by integrating the model's predictions with respect to the
              posterior parameter distribution obtained by updating this prior
              to take account of the data. For neural network models, both
              these aspects present diiculties | the prior over network
              parameters has no obvious relation to our prior knowledge, and
              integration over the posterior is computationally very demanding.
              I address the problem by deening classes of prior distributions
              for network param-eters that reach sensible limits as the size of
              the network goes to innnity. In this limit, the properties of
              these priors can be elucidated. Some priors converge to Gaussian
              processes, in which functions computed by the network may be
              smooth, Brownian, or fractionally Brownian. Other priors converge
              to non-Gaussian stable processes. Interesting eeects are obtained
              by combining priors of both sorts in networks with more than one
              hidden layer.",
  year     =  1995,
  school   = "University of Toronto",
  keywords = "Mendeley Import (Jan 17)/MLTheory/DeepLearning"
}

@INPROCEEDINGS{Gal2015-wt,
  title     = "Dropout as a Bayesian Approximation: Insights and Applications",
  booktitle = "Deep Learning Workshop, {ICML}",
  author    = "Gal, Yarin and Ghahramani, Zoubin",
  abstract  = "Deep learning techniques are used more and more often, but they
               lack the ability to reason about uncertainty over the features.
               Features ex-tracted from a dataset are given as point
               esti-mates, and do not capture how much the model is confident
               in its estimation. This is in contrast to probabilistic Bayesian
               models, which allow rea-soning about model confidence, but often
               with the price of diminished performance. We show that a
               multilayer perceptron (MLP) with arbitrary depth and
               non-linearities, with dropout applied after every weight layer,
               is math-ematically equivalent to an approximation to a well
               known Bayesian model. This interpretation offers an explanation
               to some of dropout's key properties, such as its robustness to
               over-fitting. Our interpretation allows us to reason about
               un-certainty in deep learning, and allows the intro-duction of
               the Bayesian machinery into existing deep learning frameworks in
               a principled way. Our analysis suggests straightforward
               generalisa-tions of dropout for future research which should
               improve on current techniques.",
  year      =  2015,
  keywords  = "Mendeley Import (Jan 17)/BayesOpt;Mendeley Import (Jan
               17)/MLTheory/DeepLearning;Mendeley Import (Jan
               17)/Assurances/Quantifying/Consistency"
}

@INPROCEEDINGS{Snoek2015-lr,
  title     = "Scalable Bayesian Optimization Using Deep Neural Networks",
  booktitle = "Proceedings of The 32nd International Conference on Machine
               Learning",
  author    = "Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros,
               Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary,
               Md Mostofa Ali and {Prabhat} and Adams, Ryan P",
  abstract  = "Bayesian optimization is an effective methodology for the global
               optimization of functions with expensive evaluations. It relies
               on querying a distribution over functions defined by a
               relatively cheap surrogate model. An accurate model for this
               distribution over functions is critical to the effectiveness of
               the approach, and is typically fit using Gaussian processes
               (GPs). However, since GPs scale cubically with the number of
               observations, it has been challenging to handle objectives whose
               optimization requires many evaluations, and as such, massively
               parallelizing the optimization. In this work, we explore the use
               of neural networks as an alternative to GPs to model
               distributions over functions. We show that performing adaptive
               basis function regression with a neural network as the
               parametric form performs competitively with state-of-the-art
               GP-based approaches, but scales linearly with the number of data
               rather than cubically. This allows us to achieve a previously
               intractable degree of parallelism, which we apply to large scale
               hyperparameter optimization, rapidly finding competitive models
               on benchmark object recognition tasks using convolutional
               networks, and image caption generation using neural language
               models.",
  pages     = "2171--2180",
  month     =  "19~" # feb,
  year      =  2015,
  keywords  = "Mendeley Import (Jan 17)/BayesOpt"
}

@ARTICLE{Tan2013-ua,
  title    = "More than Accuracy: Interpretability",
  author   = "Tan, Chenhao",
  year     =  2013,
  keywords = "Mendeley Import (Jan 17)"
}

@INCOLLECTION{Otte2013-oo,
  title     = "Safe and Interpretable Machine Learning: A Methodological Review",
  booktitle = "Computational Intelligence in Intelligent Data Analysis",
  author    = "Otte, Clemens",
  editor    = "Moewes, Christian and N{\"u}rnberger, Andreas",
  abstract  = "When learning models from data, the interpretability of the
               resulting model is often mandatory. For example, safety-related
               applications for automation and control require that the
               correctness of the model must be ensured not only for the
               available data but for all possible input combinations. Thus,
               understanding what the model has learned and in particular how
               it will extrapolate to unseen data is a crucial concern. The
               paper discusses suitable learning methods for classification and
               regression. For classification problems, we review an approach
               based on an ensemble of nonlinear low-dimensional submodels,
               where each submodel is simple enough to be completely verified
               by domain experts. For regression problems, we review related
               approaches that try to achieve interpretability by using
               low-dimensional submodels (for instance, MARS and tree-growing
               methods). We compare them with symbolic regression, which is a
               different approach based on genetic algorithms. Finally, a novel
               approach is proposed for combining a symbolic regression model,
               which is shown to be easily interpretable, with a Gaussian
               Process. The combined model has an improved accuracy and
               provides error bounds in the sense that the deviation from the
               verified symbolic model is always kept below a defined limit.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "111--122",
  series    = "Studies in Computational Intelligence",
  year      =  2013,
  keywords  = "
               Safety\_AI;trust\_informal\_treatment;assurance\_explicit;classification;interp\_models;Mendeley
               Import (Jan 17);Mendeley Import (Jan 17)/Assurances",
  language  = "en"
}

@PHDTHESIS{Ruping2006-xj,
  title    = "Learning Interpretable Models",
  author   = "Ruping, Stephan",
  year     =  2006,
  school   = "University of Darmstadt",
  keywords = "
              trust\_informal\_treatment;assurance\_explicit;classification;interp\_models;Mendeley
              Import (Jan 17)/CurrentStudy;Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Rudin2013-no,
  title    = "Learning Theory Analysis for Association Rules and Sequential
              Event Prediction",
  author   = "Rudin, Cynthia and Letham, Benjamin and Madigan, David",
  abstract = "We present a theoretical analysis for prediction algorithms based
              on association rules. As part of this analysis, we introduce a
              problem for which rules are particularly natural, called ``
              sequential event prediction. '' In sequential event prediction,
              events in a sequence are revealed one by one, and the goal is to
              determine which event will next be revealed. The training set is
              a collection of past sequences of events. An example application
              is to predict which item will next be placed into a customer's
              online shopping cart, given his/her past purchases. In the
              context of this problem, algorithms based on association rules
              have distinct advantages over classical statistical and machine
              learning methods: they look at correlations based on subsets of
              co-occurring past events (items a and b imply item c), they can
              be applied to the sequential event prediction problem in a
              natural way, they can potentially handle the `` cold start ''
              problem where the training set is small, and they yield
              interpretable predictions. In this work, we present two
              algorithms that incorporate association rules. These algorithms
              can be used both for sequential event prediction and for
              supervised classification, and they are simple enough that they
              can possibly be understood by users, customers, patients,
              managers, etc. We provide generalization guarantees on these
              algorithms based on algorithmic stability analysis from
              statistical learning theory. We include a discussion of the
              strict minimum support threshold often used in association rule
              mining, and introduce an `` adjusted confidence '' measure that
              provides a weaker minimum support condition that has advantages
              over the strict minimum support. The paper brings together ideas
              from statistical learning theory, association rule mining and
              Bayesian analysis.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  14,
  pages    = "3385--3436",
  year     =  2013,
  keywords = "algorithmic stability; association rules; associative
              classification; sequence prediction; statistical learning
              theory;Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Kim2014-cr,
  title     = "The Bayesian Case Model: A Generative Approach for {Case-Based}
               Reasoning and Prototype Classification",
  booktitle = "Proceedings of Neural Information Processing Systems ({NIPS})",
  author    = "Kim, Been and Rudin, Cynthia and Shah, Julie",
  abstract  = "We present the Bayesian Case Model (BCM), a general framework
               for Bayesian case-based reasoning (CBR) and prototype
               classification and clustering. BCM brings the intuitive power of
               CBR to a Bayesian generative framework. The BCM learns
               prototypes, the `` quintessential '' observations that best
               represent clusters in a dataset, by performing joint inference
               on cluster labels, prototypes and impor-tant features.
               Simultaneously, BCM pursues sparsity by learning subspaces, the
               sets of features that play important roles in the
               characterization of the prototypes. The prototype and subspace
               representation provides quantitative benefits in
               inter-pretability while preserving classification accuracy.
               Human subject experiments verify statistically significant
               improvements to participants' understanding when using
               explanations produced by BCM, compared to those given by prior
               art.",
  year      =  2014,
  keywords  = "Mendeley Import (Jan 17)"
}

@ARTICLE{Letham2015-fx,
  title     = "Interpretable classifiers using rules and Bayesian analysis:
               Building a better stroke prediction model",
  author    = "Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H and
               Madigan, David",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Ann. Appl. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  9,
  number    =  3,
  pages     = "1350--1371",
  month     =  sep,
  year      =  2015,
  keywords  = "Bayesian analysis; classification; interpretability;Mendeley
               Import (Jan 17)",
  language  = "en"
}

@ARTICLE{Zeng_undated-us,
  title    = "Interpretable Classification Models for Recidivism Prediction",
  author   = "Zeng, Jiaming and Ustun, Berk and Rudin, Cynthia",
  abstract = "We investigate a long-debated question, which is how to create
              predictive models of re-cidivism that are sufficiently accurate,
              transparent, and interpretable to use for decision-making. This
              question is complicated as these models are used to support
              different decisions, from sentencing, to determining release on
              probation, to allocating preventative social services. Each case
              might have an objective other than classification accuracy, such
              as a desired true positive rate (TPR) or false positive rate
              (FPR). Each (TPR, FPR) pair is a point on the receiver operator
              characteristic (ROC) curve. We use popular machine learning
              methods to create models along the full ROC curve on a wide range
              of recidivism prediction problems. We show that many methods
              (SVM, SGB, Ridge Regression) produce equally accurate models
              along the full ROC curve. However, methods that designed for
              interpretability (CART, C5.0) cannot be tuned to produce models
              that are accurate and/or interpretable. To handle this
              shortcoming, we use a recent method called Supersparse Linear
              Integer Models (SLIM) to produce ac-curate, transparent, and
              interpretable scoring systems along the full ROC curve. These
              scoring systems can be used for decision-making for many
              different use cases, since they are just as accurate as the most
              powerful black-box machine learning models for many applications,
              but completely transparent, and highly interpretable.",
  keywords = "Mendeley Import (Jan 17)"
}

@INCOLLECTION{Gama2004-so,
  title     = "Learning with Drift Detection",
  booktitle = "Advances in Artificial Intelligence -- {SBIA} 2004",
  author    = "Gama, Jo{\~a}o and Medas, Pedro and Castillo, Gladys and
               Rodrigues, Pedro",
  editor    = "Bazzan, Ana L C and Labidi, Sofiane",
  abstract  = "Most of the work in machine learning assume that examples are
               generated at random according to some stationary probability
               distribution. In this work we study the problem of learning when
               the distribution that generate the examples changes over time.
               We present a method for detection of changes in the probability
               distribution of examples. The idea behind the drift detection
               method is to control the online error-rate of the algorithm. The
               training examples are presented in sequence. When a new training
               example is available, it is classified using the actual model.
               Statistical theory guarantees that while the distribution is
               stationary, the error will decrease. When the distribution
               changes, the error will increase. The method controls the trace
               of the online error of the algorithm. For the actual context we
               define a warning level, and a drift level. A new context is
               declared, if in a sequence of examples, the error increases
               reaching the warning level at example k w , and the drift level
               at example k d . This is an indication of a change in the
               distribution of the examples. The algorithm learns a new model
               using only the examples since k w . The method was tested with a
               set of eight artificial datasets and a real world dataset. We
               used three learning algorithms: a perceptron, a neural network
               and a decision tree. The experimental results show a good
               performance detecting drift and with learning the new concept.
               We also observe that the method is independent of the learning
               algorithm.",
  publisher = "Springer Berlin Heidelberg",
  volume    =  3171,
  pages     = "286--295",
  series    = "Lecture Notes in Computer Science",
  year      =  2004,
  address   = "Berlin, Heidelberg",
  keywords  = "Mendeley Import (Jan 17)/Assurances/Quantifying/Consistency"
}

@BOOK{Quinonero-Candela2009-fj,
  title     = "Dataset Shift in Machine Learning",
  author    = "Qui{\~n}onero-Candela, Joaquin",
  abstract  = "Dataset shift is a common problem in predictive modeling that
               occurs when the joint distribution of inputs and outputs differs
               between training and test stages. Covariate shift, a particular
               case of dataset shift, occurs when only the input distribution
               changes. Dataset shift is present in most practical
               applications, for reasons ranging from the bias introduced by
               experimental design to the irreproducibility of the testing
               conditions at training time. (An example is -email spam
               filtering, which may fail to recognize spam that differs in form
               from the spam the automatic filter has been built on.) Despite
               this, and despite the attention given to the apparently similar
               problems of semi-supervised learning and active learning,
               dataset shift has received relatively little attention in the
               machine learning community until recently. This volume offers an
               overview of current efforts to deal with dataset and covariate
               shift. The chapters offer a mathematical and philosophical
               introduction to the problem, place dataset shift in relationship
               to transfer learning, transduction, local learning, active
               learning, and semi-supervised learning, provide theoretical
               views of dataset and covariate shift (including decision
               theoretic and Bayesian perspectives), and present algorithms for
               covariate shift. Contributors [cut for catalog if necessary]Shai
               Ben-David, Steffen Bickel, Karsten Borgwardt, Michael
               Br{\"u}ckner, David Corfield, Amir Globerson, Arthur Gretton,
               Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Choon Hui Teo,
               Takafumi Kanamori, Klaus-Robert M{\"u}ller, Sam Roweis, Neil
               Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard
               Sch{\"o}lkopf Hidetoshi Shimodaira, Alex Smola, Amos Storkey,
               Masashi Sugiyama",
  publisher = "MIT Press",
  year      =  2009,
  keywords  = "
               Safety\_AI;Uncertainty;NotRead;trust\_informal\_treatment;assurance\_implicit;Mendeley
               Import (Jan 17)/Assurances",
  language  = "en"
}

@BOOK{Minsky1988-jh,
  title     = "Perceptrons: An Introduction to Computational Geometry",
  author    = "Minsky, Marvin Lee and Papert, Seymour",
  abstract  = "Perceptrons - the first systematic study of parallelism in
               computation - has remained a classical work on threshold
               automata networks for nearly two decades. It marked a historical
               turn in artificial intelligence, and it is required reading for
               anyone who wants to understand the connectionist
               counterrevolution that is going on today.Artificial-intelligence
               research, which for a time concentrated on the programming of
               ton Neumann computers, is swinging back to the idea that
               intelligence might emerge from the activity of networks of
               neuronlike entities. Minsky and Papert's book was the first
               example of a mathematical analysis carried far enough to show
               the exact limitations of a class of computing machines that
               could seriously be considered as models of the brain. Now the
               new developments in mathematical tools, the recent interest of
               physicists in the theory of disordered matter, the new insights
               into and psychological models of how the brain works, and the
               evolution of fast computers that can simulate networks of
               automata have given Perceptrons new importance.Witnessing the
               swing of the intellectual pendulum, Minsky and Papert have added
               a new chapter in which they discuss the current state of
               parallel computers, review developments since the appearance of
               the 1972 edition, and identify new research directions related
               to connectionism. They note a central theoretical challenge
               facing connectionism: the challenge to reach a deeper
               understanding of how ``objects'' or ``agents'' with
               individuality can emerge in a network. Progress in this area
               would link connectionism with what the authors have called
               ``society theories of mind.''Marvin L. Minsky is Donner
               Professor of Science in MIT's Electrical Engineering and
               Computer Science Department. Seymour A. Papert is Professor of
               Media Technology at MIT.",
  publisher = "M.I.T.Pr.",
  pages     = "295",
  edition   = "Expanded",
  year      =  1988,
  keywords  = "Machine learning;Textbook;Mendeley Import (Jan
               17)/TextBooks;Mendeley Import (Jan 17)/MLTheory",
  language  = "en"
}

@INCOLLECTION{Hand2002-xr,
  title     = "Pattern Detection and Discovery",
  booktitle = "Pattern Detection and Discovery",
  author    = "Hand, David J",
  editor    = "Hand, David J and Adams, Niall M and Bolton, Richard J",
  abstract  = "Data mining comprises two subdisciplines. One of these is based
               on statistical modelling, though the large data sets associated
               with data mining lead to new problems for traditional modelling
               methodol-ogy. The other, which we term pattern detection, is a
               new science. Pat-tern detection is concerned with defining and
               detecting local anomalies within large data sets, and tools and
               methods have been developed in parallel by several applications
               communities, typically with no aware-ness of developments
               elsewhere. Most of the work to date has focussed on the
               development of practical methodology, with little attention
               being paid to the development of an underlying theoretical base
               to parallel the theoretical base developed over the last century
               to underpin modelling approaches. We suggest that the time is
               now right for the development of a theoretical base, so that
               important common aspects of the work can be identified, so that
               key directions for future research can be characterised, and so
               that the various different application domains can benefit from
               the work in other areas. We attempt describe a unified approach
               to the subject, and also attempt to provide theoretical base on
               which future developments can stand.",
  publisher = "Springer",
  pages     = "1--12",
  chapter   =  1,
  year      =  2002,
  keywords  = "Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Doyle2014-gy,
  title     = "Rapid Adaptive Realistic Behavior Modeling is Viable for Use in
               Training",
  booktitle = "Proceedings of the 23rd Conference on Behavior Representation in
               Modeling and Simulation ({BRIMS})",
  author    = "Doyle, Margery J and Portrey, Antoinette M",
  abstract  = "For many years it has been recognized that the design,
               development, and execution of adaptive threat generation systems
               and the use of rapid modeling techniques within applied research
               and training environments poses many methodological and
               integration challenges. With support from the Air Force Research
               Laboratory, 711th Human Performance Wing (711/HPW) Warfighter
               Readiness Research Division (WRRD) at Wright-Patterson Air Force
               Base, Ohio, through collaboration with Tier1, Aptima, Charles
               River Analytics, CHI Systems, SoarTech, Alion, and
               Stottler-Henke, and AFRL's Performance and Learning Models
               (PALM) branch, Phase I of the `` Not-So-Grand Challenge ''
               (NSGC) was launched to assess the critical issues facing current
               and future threat generation systems, the models used in them,
               and the extent to which current behavior and systems modeling
               architectures could provide military training with flexible,
               adaptable accurate/credible models of human behavior and
               realistic threats.",
  year      =  2014,
  keywords  = "Mendeley Import (Jan 17)/AFRL\_STTR/AFRL"
}

@ARTICLE{Bottou2016-az,
  title    = "Optimization Methods for {Large-Scale} Machine Learning",
  author   = "Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge",
  abstract = "This paper provides a review and commentary on the past, present,
              and future of numerical optimization algorithms in the context of
              machine learning applications. Through case studies on text
              classification and the training of deep neural networks, we
              discuss how optimization problems arise in machine learning and
              what makes them challenging. A major theme of our study is that
              large-scale machine learning represents a distinctive setting in
              which the stochastic gradient (SG) method has traditionally
              played a central role while conventional gradient-based nonlinear
              optimization techniques typically falter. Based on this
              viewpoint, we present a com-prehensive theory of a
              straightforward, yet versatile SG algorithm, discuss its
              practical behavior, and highlight opportunities for designing
              algorithms with improved performance. This leads to a discussion
              about the next generation of optimization methods for large-scale
              machine learning, including an investigation of two main streams
              of research on techniques that diminish noise in the stochastic
              directions and methods that make use of second-order derivative
              approximations.",
  year     =  2016,
  keywords = "Mendeley Import (Jan 17)"
}

@ARTICLE{Mnih_undated-zh,
  title    = "Asynchronous Methods for Deep Reinforcement Learning",
  author   = "Mnih, Volodymyr and Puigdom{\`e}nech Badia, Adri{\`a} and Mirza,
              Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P
              and Silver, David and Kavukcuoglu, Koray and Com, Korayk@google
              and Deepmind, Google",
  abstract = "We propose a conceptually simple and lightweight framework for
              deep reinforce-ment learning that uses asynchronous gradient
              descent for optimization of deep neural network controllers. We
              present asynchronous variants of four standard reinforcement
              learning algorithms and show that parallel actor-learners have a
              stabilizing effect on training allowing all four methods to
              successfully train neural network controllers. The best
              performing method, an asynchronous variant of actor-critic,
              surpasses the current state-of-the-art on the Atari domain while
              training for half the time on a single multi-core CPU instead of
              a GPU. Furthermore, we show that asynchronous actor-critic
              succeeds on a wide variety of continuous motor control problems
              as well as on a new task of navigating random 3D mazes using a
              visual input.",
  keywords = "Mendeley Import (Jan 17)"
}

@ARTICLE{Amodei2016-xi,
  title         = "Concrete Problems in {AI} Safety",
  author        = "Amodei, Dario and Olah, Chris and Steinhardt, Jacob and
                   Christiano, Paul and Schulman, John and Man{\'e}, Dan",
  abstract      = "Rapid progress in machine learning and artificial
                   intelligence (AI) has brought increasing attention to the
                   potential impacts of AI technologies on society. In this
                   paper we discuss one such potential impact: the problem of
                   accidents in machine learning systems, defined as unintended
                   and harmful behavior that may emerge from poor design of
                   real-world AI systems. We present a list of five practical
                   research problems related to accident risk, categorized
                   according to whether the problem originates from having the
                   wrong objective function (``avoiding side effects'' and
                   ``avoiding reward hacking''), an objective function that is
                   too expensive to evaluate frequently (``scalable
                   supervision''), or undesirable behavior during the learning
                   process (``safe exploration'' and ``distributional shift'').
                   We review previous work in these areas as well as suggesting
                   research directions with a focus on relevance to
                   cutting-edge AI systems. Finally, we consider the high-level
                   question of how to think most productively about the safety
                   of forward-looking applications of AI.",
  month         =  "21~" # jun,
  year          =  2016,
  keywords      = "
                   Safety\_AI;trust\_informal\_treatment;assurance\_explicit;perf\_prediction;des\_behavior;Mendeley
                   Import (Jan 17)/Assurances/Self-Aware;Mendeley Import (Jan
                   17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1606.06565"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Liang2015-jv,
  title    = "Bayesian Sensitivity Analysis and Uncertainty Integration for
              Robust Optimization",
  author   = "Liang, Chen and Mahadevan, Sankaran",
  abstract = "This paper presents a comprehensive methodology that combines
              uncertainty quantification, uncertainty propagation, and design
              optimization using a Bayesian framework. The epistemic
              uncertainty due to input data uncertainty is considered. Two
              types of uncertainty models for input variables and/or their
              distribution parameters are addressed: 1) uncertainty modeled as
              family of distributions, and 2) uncertainty modeled as interval
              data. A Bayesian approach is adopted to update the uncertainty
              models, where the likelihood functions are constructed using
              limited experimental data. Global sensitivity analysis, which
              previously only considered aleatory inputs in the context of
              probabilistic representation, is extended in this paper to
              quantify the contributions of both aleatory and epistemic
              uncertainty sources for multioutput problems using an auxiliary
              variable approach. Gaussian process surrogate modeling is
              employed to replace the expensive physics models and improve the
              computational efficiency. A previously developed
              bias-minimization technique, which only dealt with single-output
              functions, is extended to reduce the surrogate model error for a
              multioutput function. A decoupled robustness-based design
              optimization framework is developed to include both aleatory and
              epistemic uncertainties. The proposed methodology is illustrated
              using the NASA Langley Research Center’s multidisciplinary
              uncertainty quantification challenge problem.",
  journal  = "Journal of Aerospace Information Systems",
  volume   =  12,
  number   =  1,
  pages    = "189--203",
  year     =  2015,
  keywords = "Mendeley Import (Jan 17)/AFRL\_STTR/Related\_JAIS"
}

@ARTICLE{Grande2014-pz,
  title    = "Experimental Validation of Bayesian Nonparametric Adaptive
              Control Using Gaussian Processes",
  author   = "Grande, Robert C and Chowdhary, Girish and How, Jonathan P",
  abstract = "Many current model reference adaptive control methods employ
              parametric adaptive elements in which the number of parameters
              are fixed a priori and the hyperparameters, such as the
              bandwidth, are predefined, often through expert judgment. As an
              alternative to these methods, a nonparametric model using
              Gaussian processes was recently proposed. Using Gaussian
              processes, it is possible to maintain constant coverage over the
              operating domain by adaptively selecting new kernel locations as
              well as adapt hyperparameters in an online setting to improve
              model prediction. In this work, the first extensive experimental
              flight results are presented using Gaussian process/model
              reference adaptive control. Experimental results show that
              Gaussian process/model reference adaptive control outperforms
              traditional model reference adaptive control methods that use
              radial basis function neural networks in terms of tracking error
              as well as transient behavior on trajectory following using a
              quadrotor. Results show an improvement of a factor of two to
              three over preexisting state-of-the-art methods. Additionally,
              many model reference adaptive control frameworks treat the
              adaptive element as being known exactly, and they do not
              incorporate certainty of the prior model into the control policy.
              In this paper, the notion of a Bayesian scaling factor is
              introduced that scales the adaptive element in order to
              incorporate the uncertainty of the prior model and current model
              confidence. The stability and convergence of using the Bayesian
              scaling factor in a closed loop is proven.",
  journal  = "Journal of Aerospace Information Systems",
  volume   =  11,
  number   =  9,
  pages    = "565--578",
  year     =  2014,
  keywords = "Mendeley Import (Jan 17)/AFRL\_STTR/Related\_JAIS"
}

@INPROCEEDINGS{Adebayo2015-fa,
  title     = "The Hidden Cost of Efficiency: Fairness and Discrimination in
               Predictive Modeling",
  booktitle = "Bloomberg Data for Good Exchaange Conference",
  author    = "Adebayo, Julius and Kagal, Lalana",
  abstract  = "We present a data transformation procedure that completely
               eliminates all linear information regarding a sensitive
               at-tribute in a large scale individual level data with several
               correlated attributes. The algorithm presented here forms a
               component of a larger fairness rating system being devel-oped.
               The goal of the rating system is to elucidate black-box models
               and bring interpretability to any predictive model no matter how
               complex. As part of the system, we learn lower dimensional
               interpretable versions of potentially com-plex models, attempt
               to learn a causal structure of the un-derlying data, and propose
               an augumentation to the un-derlying black-box algorithm as a way
               of reducing bias in predictive modeling. This work is still
               ongoing, but here we highlight one component of the system: the
               orthogonal projection algorithm. The orthogonal projection
               algorithm combines principal components analysis of the data set
               with orthogonalization with respect to sensitive attribute(s).
               The orthogonalization algorithm presented is motivated by
               appli-cations where there is a need to drastically 'sanitize' a
               data set of all information relating to sensitive(s)
               attribute(s) in the data, or that perhaps could be inferred from
               the data, before analysis of the data using a data mining
               algorithm. Our proposed methodology outperforms other privacy
               pre-serving methodologies by more than 20 percent in lowering
               the ability to reconstruct sensitive attribute from a sample
               large scale individual level data. In high stakes contexts such
               as determination of access to credit, employment, and insur-ance
               where discrimination based on sensitive attributes such as race,
               gender, and sexual orientation is prohibited by law, our
               proposed algorithm provides a way to help reduce the information
               content of such sensitive attributes in available data, hence
               limiting bias.",
  year      =  2015,
  address   = "NYC, NY USA",
  keywords  = "Mendeley Import (Jan 17)"
}

@ARTICLE{Seshia2016-ck,
  title         = "Towards Verified Artificial Intelligence",
  author        = "Seshia, Sanjit A and Sadigh, Dorsa and Shankar Sastry, S",
  abstract      = "Verified artificial intelligence (AI) is the goal of
                   designing AI-based systems that are provably correct with
                   respect to mathematically-specified requirements. This paper
                   considers Verified AI from a formal methods perspective. We
                   describe five challenges for achieving Verified AI, and five
                   corresponding principles for addressing these challenges.",
  month         =  "27~" # jun,
  year          =  2016,
  keywords      = "Mendeley Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1606.08514"
}

@ARTICLE{Goodfellow2014-cz,
  title         = "Explaining and Harnessing Adversarial Examples",
  author        = "Goodfellow, Ian J and Shlens, Jonathon and Szegedy,
                   Christian",
  abstract      = "Several machine learning models, including neural networks,
                   consistently misclassify adversarial examples---inputs
                   formed by applying small but intentionally worst-case
                   perturbations to examples from the dataset, such that the
                   perturbed input results in the model outputting an incorrect
                   answer with high confidence. Early attempts at explaining
                   this phenomenon focused on nonlinearity and overfitting. We
                   argue instead that the primary cause of neural networks'
                   vulnerability to adversarial perturbation is their linear
                   nature. This explanation is supported by new quantitative
                   results while giving the first explanation of the most
                   intriguing fact about them: their generalization across
                   architectures and training sets. Moreover, this view yields
                   a simple and fast method of generating adversarial examples.
                   Using this approach to provide examples for adversarial
                   training, we reduce the test set error of a maxout network
                   on the MNIST dataset.",
  month         =  "20~" # dec,
  year          =  2014,
  keywords      = "Mendeley Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1412.6572"
}

@ARTICLE{Azimi2012-gk,
  title         = "Hybrid Batch Bayesian Optimization",
  author        = "Azimi, Javad and Jalali, Ali and Fern, Xiaoli",
  abstract      = "Bayesian Optimization aims at optimizing an unknown
                   non-convex/concave function that is costly to evaluate. We
                   are interested in application scenarios where concurrent
                   function evaluations are possible. Under such a setting, BO
                   could choose to either sequentially evaluate the function,
                   one input at a time and wait for the output of the function
                   before making the next selection, or evaluate the function
                   at a batch of multiple inputs at once. These two different
                   settings are commonly referred to as the sequential and
                   batch settings of Bayesian Optimization. In general, the
                   sequential setting leads to better optimization performance
                   as each function evaluation is selected with more
                   information, whereas the batch setting has an advantage in
                   terms of the total experimental time (the number of
                   iterations). In this work, our goal is to combine the
                   strength of both settings. Specifically, we systematically
                   analyze Bayesian optimization using Gaussian process as the
                   posterior estimator and provide a hybrid algorithm that,
                   based on the current state, dynamically switches between a
                   sequential policy and a batch policy with variable batch
                   sizes. We provide theoretical justification for our
                   algorithm and present experimental results on eight
                   benchmark BO problems. The results show that our method
                   achieves substantial speedup (up to \%78) compared to a pure
                   sequential policy, without suffering any significant
                   performance loss.",
  month         =  "25~" # feb,
  year          =  2012,
  keywords      = "Mendeley Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1202.5597"
}

@ARTICLE{Osband_undated-mz,
  title    = "Deep Exploration via Bootstrapped {DQN}",
  author   = "Osband, Ian and Blundell, Charles and Pritzel, Alexander and Roy,
              Benjamin Van",
  abstract = "Efficient exploration remains a major challenge for reinforcement
              learning (RL). Common dithering strategies for exploration, such
              as do not carry out temporally-extended (or deep) exploration;
              this can lead to exponentially larger data requirements. However,
              most algorithms for statistically efficient RL are not
              computationally tractable in complex en-vironments. Randomized
              value functions offer a promising approach to efficient
              exploration with generalization, but existing algorithms are not
              compatible with nonlinearly parameterized value functions. As a
              first step towards addressing such contexts we develop
              bootstrapped DQN. We demon-strate that bootstrapped DQN can
              combine deep exploration with deep neural networks for
              exponentially faster learning than any dithering strat-egy. In
              the Arcade Learning Environment bootstrapped DQN substantially
              improves learning speed and cumulative performance across most
              games.",
  keywords = "Mendeley Import (Jan 17)"
}

@ARTICLE{Werling2015-wg,
  title         = "{On-the-Job} Learning with Bayesian Decision Theory",
  author        = "Werling, Keenon and Chaganty, Arun and Liang, Percy and
                   Manning, Chris",
  abstract      = "Our goal is to deploy a high-accuracy system starting with
                   zero training examples. We consider an ``on-the-job''
                   setting, where as inputs arrive, we use real-time
                   crowdsourcing to resolve uncertainty where needed and output
                   our prediction when confident. As the model improves over
                   time, the reliance on crowdsourcing queries decreases. We
                   cast our setting as a stochastic game based on Bayesian
                   decision theory, which allows us to balance latency, cost,
                   and accuracy objectives in a principled way. Computing the
                   optimal policy is intractable, so we develop an
                   approximation based on Monte Carlo Tree Search. We tested
                   our approach on three datasets---named-entity recognition,
                   sentiment classification, and image classification. On the
                   NER task we obtained more than an order of magnitude
                   reduction in cost compared to full human annotation, while
                   boosting performance relative to the expert provided labels.
                   We also achieve a 8\% F1 improvement over having a single
                   human label the whole set, and a 28\% F1 improvement over
                   online learning.",
  month         =  "10~" # jun,
  year          =  2015,
  keywords      = "Mendeley Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1506.03140"
}

@ARTICLE{Khani2016-ja,
  title         = "Unanimous Prediction for 100\% Precision with Application to
                   Learning Semantic Mappings",
  author        = "Khani, Fereshte and Rinard, Martin and Liang, Percy",
  abstract      = "Can we train a system that, on any new input, either says
                   ``don't know'' or makes a prediction that is guaranteed to
                   be correct? We answer the question in the affirmative
                   provided our model family is well-specified. Specifically,
                   we introduce the unanimity principle: only predict when all
                   models consistent with the training data predict the same
                   output. We operationalize this principle for semantic
                   parsing, the task of mapping utterances to logical forms. We
                   develop a simple, efficient method that reasons over the
                   infinite set of all consistent models by only checking two
                   of the models. We prove that our method obtains 100\%
                   precision even with a modest amount of training data from a
                   possibly adversarial distribution. Empirically, we
                   demonstrate the effectiveness of our approach on the
                   standard GeoQuery dataset.",
  month         =  "20~" # jun,
  year          =  2016,
  keywords      = "Mendeley Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1606.06368"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wiesemann2013-ct,
  title    = "Robust Markov Decision Processes",
  author   = "Wiesemann, Wolfram and Kuhn, Daniel and Rustem, Ber{\c c}",
  abstract = "Markov decision processes (MDPs) are powerful tools for decision
              making in uncertain dynamic environments. However, the solutions
              of MDPs are of limited practical use because of their sensitivity
              to distributional model parameters, which are typically unknown
              and have to be estimated by the decision maker. To counter the
              detrimental effects of estimation errors, we consider robust MDPs
              that offer probabilistic guarantees in view of the unknown
              parameters. To this end, we assume that an observation history of
              the MDP is available. Based on this history, we derive a
              confidence region that contains the unknown parameters with a
              prespecified probability 1 − Afterward, we determine a policy
              that attains the highest worst-case performance over this
              confidence region. By construction, this policy achieves or
              exceeds its worst-case performance with a confidence of at least
              1 − Our method involves the solution of tractable conic programs
              of moderate size.",
  journal  = "Mathematics of OR",
  volume   =  38,
  number   =  1,
  pages    = "153--183",
  month    =  feb,
  year     =  2013,
  keywords = "Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Ghosh2016-qu,
  title     = "Trusted Machine Learning for Probabilistic Models",
  booktitle = "Reliable Machine Learning in the Wild at {ICML} 2016",
  author    = "Ghosh, Shalini and Lincoln, Patrick and Tiwari, Ashish and Zhu,
               Xiaojin",
  abstract  = "In several mission-critical domains (e.g., self-driving cars,
               cybersecurity, robotics) where ma-chine learning algorithms are
               being used heav-ily, it is becoming increasingly important to
               en-sure that the learned models satisfy some domain properties
               (e.g., temporal constraints). Towards this goal, we propose
               Trusted Machine Learning (TML), wherein we combine the strengths
               of ma-chine learning and model checking. If the desired logical
               properties are not satisfied by a trained model, we modify
               either the model ('model re-pair') or the data from which the
               model is learned ('data repair'). We outline a concrete case
               study based on the Markov Chain model of a car con-troller for
               'lane changing' --- we demonstrate how we can ensure that such a
               model, learned from data, satisfies properties specified in
               Probabilistic Computation Tree Logic (PCTL).",
  year      =  2016,
  keywords  = "Mendeley Import (Jan 17)/Assurances/Self-Aware"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Cadamuro_undated-uw,
  title    = "Debugging Machine Learning Extended Abstract",
  author   = "Cadamuro, Gabriel and Gilad-Bachrach, Ran and Zhu, Xiaojin",
  abstract = "Paste the appropriate copyright statement here. ACM now supports
              three different copyright statements: • ACM copyright: ACM holds
              the copyright on the work. This is the historical approach. •
              License: The author(s) retain copyright, but ACM receives an
              exclusive publication license. • Open Access: The author(s) wish
              to pay for the work to be open access. The additional fee must be
              paid to ACM. This text field is large enough to hold the
              appropriate release statement assuming it is single spaced in a
              sans-serif 7 point font. Every submission will be assigned their
              own unique DOI string to be included here. Abstract Creating a
              machine learning solution for a real world prob-lem often
              requires multiple iterations of investigation and improvement
              until it reaches satisfactory performance. Even after deployment,
              it is common to discover limitations of the model or changes in
              the target concept that neces-sitate modifications to the
              training data and parameters. However, as of today, there is no
              common wisdom about what these iterations consist of, nor what
              debugging tools are needed to aid the investigative process. In
              this work we present a novel technique to help model developers
              find the root causes of prediction error on test items
              (henceforth 'bugs') and so help the developer to fix them. Given
              an ob-served bug our method aims to identify the training items
              most responsible for biasing the model towards giving the wrong
              prediction on the specific test item. This set of train-ing items
              can aid in discovery of common errors like faulty training labels
              or poor training data coverage. Our method is applicable over
              many different learners, including deep neural nets with large
              and complex model representations, as well as many different data
              types.",
  keywords = "Author Keywords Machine Learning; D25 [Software]; PATTERN
              RECOGNI-TION---Models; SOFTWARE ENGINEER-ING---Testing and
              Debugging;Mendeley Import (Jan 17)"
}

@ARTICLE{Bottou2013-sx,
  title    = "Counterfactual Reasoning and Learning Systems: The Example of
              Computational Advertising",
  author   = "Bottou, L{\'e}on and Peters, Jonas and Ch, Peters@stat and
              Qui{\~n}onero-Candela, Joaquin and Charles, Denis X and
              Chickering, D Max and Portugaly, Elon and Ray, Dipankar and
              Simard, Patrice and Snelson, Ed",
  abstract = "This work shows how to leverage causal inference to understand
              the behavior of complex learning systems interacting with their
              environment and predict the consequences of changes to the
              sys-tem. Such predictions allow both humans and algorithms to
              select the changes that would have improved the system
              performance. This work is illustrated by experiments on the ad
              placement system associated with the Bing search engine.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  14,
  pages    = "3207--3260",
  year     =  2013,
  keywords = "Mendeley Import (Jan 17)"
}

@ARTICLE{Bottou2011-gp,
  title         = "From Machine Learning to Machine Reasoning",
  author        = "Bottou, Leon",
  abstract      = "A plausible definition of ``reasoning'' could be
                   ``algebraically manipulating previously acquired knowledge
                   in order to answer a new question''. This definition covers
                   first-order logical inference or probabilistic inference. It
                   also includes much simpler manipulations commonly used to
                   build large learning systems. For instance, we can build an
                   optical character recognition system by first training a
                   character segmenter, an isolated character recognizer, and a
                   language model, using appropriate labeled training sets.
                   Adequately concatenating these modules and fine tuning the
                   resulting system can be viewed as an algebraic operation in
                   a space of models. The resulting model answers a new
                   question, that is, converting the image of a text page into
                   a computer readable text. This observation suggests a
                   conceptual continuity between algebraically rich inference
                   systems, such as logical or probabilistic inference, and
                   simple manipulations, such as the mere concatenation of
                   trainable learning systems. Therefore, instead of trying to
                   bridge the gap between machine learning systems and
                   sophisticated ``all-purpose'' inference mechanisms, we can
                   instead algebraically enrich the set of manipulations
                   applicable to training systems, and build reasoning
                   capabilities from the ground up.",
  month         =  "9~" # feb,
  year          =  2011,
  keywords      = "Mendeley Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1102.1808"
}

@ARTICLE{Srinivas2009-vw,
  title         = "Gaussian Process Optimization in the Bandit Setting: No
                   Regret and Experimental Design",
  author        = "Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M
                   and Seeger, Matthias",
  abstract      = "Many applications require optimizing an unknown, noisy
                   function that is expensive to evaluate. We formalize this
                   task as a multi-armed bandit problem, where the payoff
                   function is either sampled from a Gaussian process (GP) or
                   has low RKHS norm. We resolve the important open problem of
                   deriving regret bounds for this setting, which imply novel
                   convergence rates for GP optimization. We analyze GP-UCB, an
                   intuitive upper-confidence based algorithm, and bound its
                   cumulative regret in terms of maximal information gain,
                   establishing a novel connection between GP optimization and
                   experimental design. Moreover, by bounding the latter in
                   terms of operator spectra, we obtain explicit sublinear
                   regret bounds for many commonly used covariance functions.
                   In some important cases, our bounds have surprisingly weak
                   dependence on the dimensionality. In our experiments on real
                   sensor data, GP-UCB compares favorably with other
                   heuristical GP optimization approaches.",
  month         =  "21~" # dec,
  year          =  2009,
  keywords      = "Mendeley Import (Jan 17)/BayesOpt;Mendeley Import (Jan
                   17)/BayesOpt/Acquisition/InfillFxns",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "0912.3995"
}

@PHDTHESIS{Schonlau1997-wq,
  title    = "Computer experiments and global optimization",
  author   = "Schonlau, M",
  year     =  1997,
  school   = "University of Waterloo",
  keywords = "Mendeley Import (Jan 17)/BayesOpt;Mendeley Import (Jan
              17)/BayesOpt/Acquisition/InfillFxns"
}

@MISC{Gretton2015-ei,
  title       = "Introduction to {RKHS}, and some simple kernel algorithms",
  author      = "Gretton, Arthur",
  pages       = "30",
  institution = "University College London",
  year        =  2015,
  keywords    = "Mendeley Import (Jan 17)/CurrentStudy"
}

@BOOK{Hand2002-bm,
  title     = "Pattern Detection and Discovery: {ESF} Exploratory Workshop
               London, {UK}, September 16--19, 2002 Proceedings",
  author    = "Hand, David J and Adams, Niall M and Bolton, Richard J",
  editor    = "Hand, David J and Adams, Niall M and Bolton, Richard J",
  publisher = "Springer Berlin Heidelberg",
  pages     = "227",
  series    = "Lecture Notes in Computer Science",
  year      =  2002,
  keywords  = "Mendeley Import (Jan 17)"
}

@BOOK{Paiva2000-tj,
  title     = "Affective Interactions: Towards a New Generation of Computer
               Interfaces",
  author    = "Paiva, Ana",
  abstract  = "Affective computing is a fascinating new area of research
               emerging in computer science. It dwells on problems where
               ``computing is related to, arises from or deliberately
               influences emotions'' (Picard 1997). Following this new research
               direction and considering the human element as crucial in
               designing and implementing interactive intelligent interfaces,
               affective computing is now influencing the way we shape, design,
               construct, and evaluate human-computer interaction and
               computer-mediated communcation. This book originates from a
               workshop devoted to affective interactions. It presents revised
               full versions of several papers accepted in preliminary version
               for the workshop and various selectively solicited papers by key
               people as well as an introductory survey by the volume editor
               and interview with Rosaling Picard, a pioneer researcher in the
               field. The book competently assesses the state of the art in
               this fascinating new field.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "270",
  month     =  "13~" # dec,
  year      =  2000,
  keywords  = "Mendeley Import (Jan 17)",
  language  = "en"
}

@ARTICLE{Ribeiro2016-uc,
  title         = "``Why Should {I} Trust You?'': Explaining the Predictions of
                   Any Classifier",
  author        = "Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos",
  abstract      = "Despite widespread adoption, machine learning models remain
                   mostly black boxes. Understanding the reasons behind
                   predictions is, however, quite important in assessing trust,
                   which is fundamental if one plans to take action based on a
                   prediction, or when choosing whether to deploy a new model.
                   Such understanding also provides insights into the model,
                   which can be used to transform an untrustworthy model or
                   prediction into a trustworthy one. In this work, we propose
                   LIME, a novel explanation technique that explains the
                   predictions of any classifier in an interpretable and
                   faithful manner, by learning an interpretable model locally
                   around the prediction. We also propose a method to explain
                   models by presenting representative individual predictions
                   and their explanations in a non-redundant way, framing the
                   task as a submodular optimization problem. We demonstrate
                   the flexibility of these methods by explaining different
                   models for text (e.g. random forests) and image
                   classification (e.g. neural networks). We show the utility
                   of explanations via novel experiments, both simulated and
                   with human subjects, on various scenarios that require
                   trust: deciding if one should trust a prediction, choosing
                   between models, improving an untrustworthy classifier, and
                   identifying why a classifier should not be trusted.",
  month         =  "16~" # feb,
  year          =  2016,
  keywords      = "
                   trust\_informal\_treatment;assurance\_explicit;classification;interp\_models;Mendeley
                   Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1602.04938"
}

@ARTICLE{Ribeiro2016-hv,
  title         = "{Model-Agnostic} Interpretability of Machine Learning",
  author        = "Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos",
  abstract      = "Understanding why machine learning models behave the way
                   they do empowers both system designers and end-users in many
                   ways: in model selection, feature engineering, in order to
                   trust and act upon the predictions, and in more intuitive
                   user interfaces. Thus, interpretability has become a vital
                   concern in machine learning, and work in the area of
                   interpretable models has found renewed interest. In some
                   applications, such models are as accurate as
                   non-interpretable ones, and thus are preferred for their
                   transparency. Even when they are not accurate, they may
                   still be preferred when interpretability is of paramount
                   importance. However, restricting machine learning to
                   interpretable models is often a severe limitation. In this
                   paper we argue for explaining machine learning predictions
                   using model-agnostic approaches. By treating the machine
                   learning models as black-box functions, these approaches
                   provide crucial flexibility in the choice of models,
                   explanations, and representations, improving debugging,
                   comparison, and interfaces for a variety of users and
                   models. We also outline the main challenges for such
                   methods, and review a recently-introduced model-agnostic
                   explanation approach (LIME) that addresses these challenges.",
  month         =  "16~" # jun,
  year          =  2016,
  keywords      = "Mendeley Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1606.05386"
}

@ARTICLE{Lipton2016-ug,
  title         = "The Mythos of Model Interpretability",
  author        = "Lipton, Zachary C",
  abstract      = "Supervised machine learning models boast remarkable
                   predictive capabilities. But can you trust your model? Will
                   it work in deployment? What else can it tell you about the
                   world? We want models to be not only good, but
                   interpretable. And yet the task of interpretation appears
                   underspecified. Papers provide diverse and sometimes
                   non-overlapping motivations for interpretability, and offer
                   myriad notions of what attributes render models
                   interpretable. Despite this ambiguity, many papers proclaim
                   interpretability axiomatically, absent further explanation.
                   In this paper, we seek to refine the discourse on
                   interpretability. First, we examine the motivations
                   underlying interest in interpretability, finding them to be
                   diverse and occasionally discordant. Then, we address model
                   properties and techniques thought to confer
                   interpretability, identifying transparency to humans and
                   post-hoc explanations as competing notions. Throughout, we
                   discuss the feasibility and desirability of different
                   notions, and question the oft-made assertions that linear
                   models are interpretable and that deep neural networks are
                   not.",
  month         =  "10~" # jun,
  year          =  2016,
  keywords      = "
                   assurance\_predictability;supervised\_learning;assurance\_competence;assurance\_normality;trust\_informal\_treatment;assurance\_explicit;Mendeley
                   Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1606.03490"
}

@ARTICLE{Montavon2015-mv,
  title         = "Explaining {NonLinear} Classification Decisions with Deep
                   Taylor Decomposition",
  author        = "Montavon, Gr{\'e}goire and Bach, Sebastian and Binder,
                   Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert",
  abstract      = "Nonlinear methods such as Deep Neural Networks (DNNs) are
                   the gold standard for various challenging machine learning
                   problems, e.g., image classification, natural language
                   processing or human action recognition. Although these
                   methods perform impressively well, they have a significant
                   disadvantage, the lack of transparency, limiting the
                   interpretability of the solution and thus the scope of
                   application in practice. Especially DNNs act as black boxes
                   due to their multilayer nonlinear structure. In this paper
                   we introduce a novel methodology for interpreting generic
                   multilayer neural networks by decomposing the network
                   classification decision into contributions of its input
                   elements. Although our focus is on image classification, the
                   method is applicable to a broad set of input data, learning
                   tasks and network architectures. Our method is based on deep
                   Taylor decomposition and efficiently utilizes the structure
                   of the network by backpropagating the explanations from the
                   output to the input layer. We evaluate the proposed method
                   empirically on the MNIST and ILSVRC data sets.",
  month         =  "8~" # dec,
  year          =  2015,
  keywords      = "
                   trust\_informal\_treatment;assurance\_explicit;classification;explain;Mendeley
                   Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1512.02479"
}

@ARTICLE{Abdollahi2016-vn,
  title         = "Explainable Restricted Boltzmann Machines for Collaborative
                   Filtering",
  author        = "Abdollahi, Behnoush and Nasraoui, Olfa",
  abstract      = "Most accurate recommender systems are black-box models,
                   hiding the reasoning behind their recommendations. Yet
                   explanations have been shown to increase the user's trust in
                   the system in addition to providing other benefits such as
                   scrutability, meaning the ability to verify the validity of
                   recommendations. This gap between accuracy and transparency
                   or explainability has generated an interest in automated
                   explanation generation methods. Restricted Boltzmann
                   Machines (RBM) are accurate models for CF that also lack
                   interpretability. In this paper, we focus on RBM based
                   collaborative filtering recommendations, and further assume
                   the absence of any additional data source, such as item
                   content or user attributes. We thus propose a new
                   Explainable RBM technique that computes the top-n
                   recommendation list from items that are explainable.
                   Experimental results show that our method is effective in
                   generating accurate and explainable recommendations.",
  month         =  "22~" # jun,
  year          =  2016,
  keywords      = "
                   trust\_informal\_treatment;assurance\_explicit;interp\_models;Mendeley
                   Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1606.07129"
}

@ARTICLE{Kamruzzaman2010-vz,
  title         = "Extraction of Symbolic Rules from Artificial Neural Networks",
  author        = "Kamruzzaman, S M and Islam, Md Monirul",
  abstract      = "Although backpropagation ANNs generally predict better than
                   decision trees do for pattern classification problems, they
                   are often regarded as black boxes, i.e., their predictions
                   cannot be explained as those of decision trees. In many
                   applications, it is desirable to extract knowledge from
                   trained ANNs for the users to gain a better understanding of
                   how the networks solve the problems. A new rule extraction
                   algorithm, called rule extraction from artificial neural
                   networks (REANN) is proposed and implemented to extract
                   symbolic rules from ANNs. A standard three-layer feedforward
                   ANN is the basis of the algorithm. A four-phase training
                   algorithm is proposed for backpropagation learning.
                   Explicitness of the extracted rules is supported by
                   comparing them to the symbolic rules generated by other
                   methods. Extracted rules are comparable with other methods
                   in terms of number of rules, average number of conditions
                   for a rule, and predictive accuracy. Extensive experimental
                   studies on several benchmarks classification problems, such
                   as breast cancer, iris, diabetes, and season classification
                   problems, demonstrate the effectiveness of the proposed
                   approach with good generalization ability.",
  month         =  "23~" # sep,
  year          =  2010,
  keywords      = "Mendeley Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1009.4570"
}

@ARTICLE{Adler2016-yt,
  title         = "Auditing Black-box Models for Indirect Influence",
  author        = "Adler, Philip and Falk, Casey and Friedler, Sorelle A and
                   Rybeck, Gabriel and Scheidegger, Carlos and Smith, Brandon
                   and Venkatasubramanian, Suresh",
  abstract      = "Data-trained predictive models see widespread use, but for
                   the most part they are used as black boxes which output a
                   prediction or score. It is therefore hard to acquire a
                   deeper understanding of model behavior, and in particular
                   how different features influence the model prediction. This
                   is important when interpreting the behavior of complex
                   models, or asserting that certain problematic attributes
                   (like race or gender) are not unduly influencing decisions.
                   In this paper, we present a technique for auditing black-box
                   models, which lets us study the extent to which existing
                   models take advantage of particular features in the dataset,
                   without knowing how the models work. Our work focuses on the
                   problem of indirect influence: how some features might
                   indirectly influence outcomes via other, related features.
                   As a result, we can find attribute influences even in cases
                   where, upon further direct examination of the model, the
                   attribute is not referred to by the model at all. Our
                   approach does not require the black-box model to be
                   retrained. This is important if (for example) the model is
                   only accessible via an API, and contrasts our work with
                   other methods that investigate feature influence like
                   feature selection. We present experimental evidence for the
                   effectiveness of our procedure using a variety of publicly
                   available datasets and models. We also validate our
                   procedure using techniques from interpretable learning and
                   feature selection, as well as against other black-box
                   auditing procedures.",
  month         =  "23~" # feb,
  year          =  2016,
  keywords      = "Mendeley Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1602.07043"
}

@ARTICLE{Dundas2011-us,
  title         = "Implementing Human-like Intuition Mechanism in Artificial
                   Intelligence",
  author        = "Dundas, Jitesh and Chik, David",
  abstract      = "Human intuition has been simulated by several research
                   projects using artificial intelligence techniques. Most of
                   these algorithms or models lack the ability to handle
                   complications or diversions. Moreover, they also do not
                   explain the factors influencing intuition and the accuracy
                   of the results from this process. In this paper, we present
                   a simple series based model for implementation of human-like
                   intuition using the principles of connectivity and unknown
                   entities. By using Poker hand datasets and Car evaluation
                   datasets, we compare the performance of some well-known
                   models with our intuition model. The aim of the experiment
                   was to predict the maximum accurate answers using intuition
                   based models. We found that the presence of unknown
                   entities, diversion from the current problem scenario, and
                   identifying weakness without the normal logic based
                   execution, greatly affects the reliability of the answers.
                   Generally, the intuition based models cannot be a substitute
                   for the logic based mechanisms in handling such problems.
                   The intuition can only act as a support for an ongoing logic
                   based model that processes all the steps in a sequential
                   manner. However, when time and computational cost are very
                   strict constraints, this intuition based model becomes
                   extremely important and useful, because it can give a
                   reasonably good performance. Factors affecting intuition are
                   analyzed and interpreted through our model.",
  month         =  "29~" # jun,
  year          =  2011,
  keywords      = "Mendeley Import (Jan 17)/Assurances",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1106.5917"
}

@ARTICLE{Lake2016-wb,
  title       = "Building Machines That Learn and Think Like People",
  author      = "Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B
                 and Gershman, Samuel J",
  affiliation = "Center for Data Science,New York University. Department of
                 Brain and Cognitive Sciences,MIT. Department of Brain and
                 Cognitive Sciences,MIT. Department of Psychology and Center
                 for Brain Science,Harvard University.",
  abstract    = "Recent progress in artificial intelligence (AI) has renewed
                 interest in building systems that learn and think like people.
                 Many advances have come from using deep neural networks
                 trained end-to-end in tasks such as object recognition, video
                 games, and board games, achieving performance that equals or
                 even beats humans in some respects. Despite their biological
                 inspiration and performance achievements, these systems differ
                 from human intelligence in crucial ways. We review progress in
                 cognitive science suggesting that truly human-like learning
                 and thinking machines will have to reach beyond current
                 engineering trends in both what they learn, and how they learn
                 it. Specifically, we argue that these machines should (a)
                 build causal models of the world that support explanation and
                 understanding, rather than merely solving pattern recognition
                 problems; (b) ground learning in intuitive theories of physics
                 and psychology, to support and enrich the knowledge that is
                 learned; and (c) harness compositionality and
                 learning-to-learn to rapidly acquire and generalize knowledge
                 to new tasks and situations. We suggest concrete challenges
                 and promising routes towards these goals that can combine the
                 strengths of recent neural network advances with more
                 structured cognitive models.",
  journal     = "Behav. Brain Sci.",
  pages       = "1--101",
  month       =  "24~" # nov,
  year        =  2016,
  keywords    = "Mendeley Import (Jan 17)",
  language    = "en"
}

@ARTICLE{Tang2016-yp,
  title         = "Visualizing Large-scale and High-dimensional Data",
  author        = "Tang, Jian and Liu, Jingzhou and Zhang, Ming and Mei,
                   Qiaozhu",
  abstract      = "We study the problem of visualizing large-scale and
                   high-dimensional data in a low-dimensional (typically 2D or
                   3D) space. Much success has been reported recently by
                   techniques that first compute a similarity structure of the
                   data points and then project them into a low-dimensional
                   space with the structure preserved. These two steps suffer
                   from considerable computational costs, preventing the
                   state-of-the-art methods such as the t-SNE from scaling to
                   large-scale and high-dimensional data (e.g., millions of
                   data points and hundreds of dimensions). We propose the
                   LargeVis, a technique that first constructs an accurately
                   approximated K-nearest neighbor graph from the data and then
                   layouts the graph in the low-dimensional space. Comparing to
                   t-SNE, LargeVis significantly reduces the computational cost
                   of the graph construction step and employs a principled
                   probabilistic model for the visualization step, the
                   objective of which can be effectively optimized through
                   asynchronous stochastic gradient descent with a linear time
                   complexity. The whole procedure thus easily scales to
                   millions of high-dimensional data points. Experimental
                   results on real-world data sets demonstrate that the
                   LargeVis outperforms the state-of-the-art methods in both
                   efficiency and effectiveness. The hyper-parameters of
                   LargeVis are also much more stable over different data sets.",
  month         =  "1~" # feb,
  year          =  2016,
  keywords      = "Mendeley Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1602.00370"
}

@ARTICLE{Dzindolet2003-ts,
  title    = "The role of trust in automation reliance",
  author   = "Dzindolet, Mary T and Peterson, Scott A and Pomranky, Regina A
              and Pierce, Linda G and Beck, Hall P",
  abstract = "A recent and dramatic increase in the use of automation has not
              yielded comparable improvements in performance. Researchers have
              found human operators often underutilize (disuse) and overly rely
              on (misuse) automated aids (Parasuraman and Riley, 1997). Three
              studies were performed with Cameron University students to
              explore the relationship among automation reliability, trust, and
              reliance. With the assistance of an automated decision aid,
              participants viewed slides of Fort Sill terrain and indicated the
              presence or absence of a camouflaged soldier. Results from the
              three studies indicate that trust is an important factor in
              understanding automation reliance decisions. Participants
              initially considered the automated decision aid trustworthy and
              reliable. After observing the automated aid make errors,
              participants distrusted even reliable aids, unless an explanation
              was provided regarding why the aid might err. Knowing why the aid
              might err increased trust in the decision aid and increased
              automation reliance, even when the trust was unwarranted. Our
              studies suggest a need for future research focused on
              understanding automation use, examining individual differences in
              automation reliance, and developing valid and reliable
              self-report measures of trust in automation.",
  journal  = "Int. J. Hum. Comput. Stud.",
  volume   =  58,
  number   =  6,
  pages    = "697--718",
  year     =  2003,
  keywords = "Automation trust; Automation reliance; Misuse;
              Disuse;automation;human\_study;trust\_formal\_treatment;Mendeley
              Import (Jan 17);Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Baehrens2010-bs,
  title    = "How to Explain Individual Classification Decisions",
  author   = "Baehrens, David and Schroeter, Timon and Harmeling, Stefan and
              Kawanabe, Motoaki and Hansen, Katja and M{\"u}ller, Klaus-Robert",
  journal  = "J. Mach. Learn. Res.",
  volume   =  11,
  number   = "Jun",
  pages    = "1803--1831",
  year     =  2010,
  keywords = "Mendeley Import (Jan 17)"
}

@ARTICLE{Trumbelj2010-sr,
  title    = "An Efficient Explanation of Individual Classifications using Game
              Theory",
  author   = "Trumbelj, Erik {\AA} and Kononenko, Igor",
  journal  = "J. Mach. Learn. Res.",
  volume   =  11,
  number   = "Jan",
  pages    = "1--18",
  year     =  2010,
  keywords = "Mendeley Import (Jan 17)"
}

@ARTICLE{Freitas2014-xf,
  title     = "Comprehensible classification models: a position paper",
  author    = "Freitas, Alex A",
  journal   = "ACM SIGKDD Explorations Newsletter",
  publisher = "ACM",
  volume    =  15,
  number    =  1,
  pages     = "1--10",
  month     =  "17~" # mar,
  year      =  2014,
  keywords  = "Bayesian network classifiers; decision table; decision tree;
               monotonicity constraint; nearest neighbors; rule
               induction;Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Matsuda2005-qv,
  title     = "Applying Programming by Demonstration in an Intelligent
               Authoring Tool for Cognitive Tutors",
  booktitle = "{AAAI} Workshop on Human Comprehensible Machine Learning",
  author    = "Matsuda, Noboru and Cohen, William W and Koedinger, Kenneth R",
  abstract  = "We are building an intelligent authoring tool for Cognitive
               Tutors, a highly successful form of computer-based tutoring. The
               primary target users (the authors) are educators who are not
               familiar with cognitive task analysis and AI program-ming, which
               are essential tasks in building Cognitive Tutors. Instead of
               asking authors to write a cognitive model by hand, a Simulated
               Student embedded in the authoring tool lets an author
               demonstrate how to perform the tasks in the subject domain, for
               instance, solving an algebra equation. The Simulated Student
               observes an author's demonstration and induces a set of
               production rules that replicate the demon-strated performances.
               Correct production rules, as well as production rules that are
               incorrect but similar to those a human student might produce,
               can be directly embedded in the Cognitive Tutor. We give a
               preliminary evaluation of an implemented Simulated Students
               based on inductive logic programming and path-finding.",
  year      =  2005,
  address   = "Pittsburgh",
  keywords  = "Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Thomaz2005-qj,
  title     = "{Real-Time} Interactive Reinforcement Learning for Robots",
  booktitle = "{AAAI} Workshop on Human Comprehensible Machine Learning",
  author    = "Thomaz, Andrea Lockerd and Hoffman, Guy and Breazeal, Cynthia",
  abstract  = "It is our goal to understand the role real-time human
               in-teraction can play in machine learning algorithms for robots.
               In this paper we present Interactive Reinforce-ment Learning
               (IRL) as a plausible approach for train-ing human-centric
               assistive robots by natural interac-tion. We describe an
               experimental platform to study IRL, pose questions arising from
               IRL, and discuss ini-tial observations obtained during the
               development of our system.",
  year      =  2005,
  address   = "Pittsburgh",
  keywords  = "Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Stumpf2005-tx,
  title     = "Predicting User Tasks: {I} Know What You're Doing!",
  booktitle = "{AAAI} Workshop on Human Comprehensible Machine Learning",
  author    = "Stumpf, Simone and Bao, Xinlong and Dragunov, Anton and
               Dietterich, Thomas G and Herlocker, Jon and Johnsrude, Kevin and
               Li, Lida and Shen, Jianqiang",
  abstract  = "Knowledge workers spend the majority of their working hours
               processing and manipulating information. These users face
               continual costs as they switch between tasks to retrieve and
               create information. The TaskTracer project at Oregon State
               University is investigating the possibilities of a desktop
               software system that will record in detail how knowledge workers
               complete tasks, and intelligently leverage that information to
               increase efficiency and productivity. Our approach combines
               human-computer interaction and machine learning to assign each
               observed action (opening a file, saving a file, sending an
               email, cutting and pasting information, etc.) to a task for
               which it is likely being performed. In this paper we report on
               ways we have applied machine learning in this environment and
               lessons learned so far.",
  year      =  2005,
  address   = "Pittsburgh",
  keywords  = "Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Vasile2005-ie,
  title     = "{TRIPPER}: Rule learning using taxonomies",
  booktitle = "{AAAI} Workshop on Human Comprehensible Machine Learning",
  author    = "Vasile, Flavian and Silvescu, Adrian and Kang, Dae-Ki and
               Honavar, Vasant",
  abstract  = "In many application domains, there is a need for learning
               algorithms that generate accurate as well as comprehensible
               classifiers. In this paper, we present TRIPPER -a rule induction
               algorithm that extends RIPPER, a widely used rule-learning
               algorithm. TRIPPER exploits background knowledge in the form of
               taxonomies over values of features used to describe data. We
               compare the performance of TRIPPER with that of RIPPER on a text
               classification problem (using the Reuters 21578 dataset).
               Experiments were performed using WordNet (a human-generated
               taxonomy), as well as a taxonomy generated by WTL (Word Taxonomy
               Learning) algorithm. Our experiments show that the rules
               generated by TRIPPER are generally more accurate and more
               concise (and hence more comprehensible) than those generated by
               RIPPER.",
  year      =  2005,
  address   = "Pittsburgh",
  keywords  = "Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Burge2005-uz,
  title     = "Comprehensibility of Generative vs. Class Discriminative Dynamic
               Bayesian Multinets",
  booktitle = "{AAAI} Workshop on Human Comprehensible Machine Learning",
  author    = "Burge, John and Lane, Terran",
  abstract  = "We investigate the comprehensibility of dynamic Bayesian
               multinets (DBMs) and the dynamic Bayesian networks (DBNs) that
               compose them. Specifically, we compare the DBM structures
               resulting from searches employing generative and class
               discriminative scoring functions. The DBMs are used to model the
               temporal relationships among RVs and show how the relationships
               change between different classes of data. We apply our technique
               to the identification of dynamic relationships among
               neuro-anatomical regions of interest in both healthy and
               demented elderly patients based on functional magnetic resonance
               imaging (fMRI) data. The structures resulting from both
               generative and class discriminative scores were found to be
               useful by our collaborating neuroscientist, but for differing
               reasons. For example, generative scores result in structures
               that illuminate highly likely relationships and are more easily
               interpreted. Conversely, structures resulting from class
               discriminating scores are capable of representing more subtle
               changes and can illuminate important behavioral differences not
               apparent from structures learned from generative scores.",
  year      =  2005,
  address   = "Pittsburgh",
  keywords  = "Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Datta2016-we,
  title     = "Algorithmic Transparency via Quantitative Input Influence",
  booktitle = "Proceedings of the 37th {IEEE} Symposium on Security and Privacy",
  author    = "Datta, A and Sen, S and Zick, Y",
  year      =  2016,
  keywords  = "Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{El-Arini2012-by,
  title     = "Transparent user models for personalization",
  booktitle = "Proceedings of the 18th {ACM} {SIGKDD} international conference
               on Knowledge discovery and data mining",
  author    = "El-Arini, Khalid and Paquet, Ulrich and Herbrich, Ralf and Van
               Gael, Jurgen and Ag{\"u}era y Arcas, Blaise",
  publisher = "ACM",
  pages     = "678--686",
  month     =  "12~" # aug,
  year      =  2012,
  address   = "New York, New York, USA",
  keywords  = "Twitter; graphical models; personalization;
               transparency;Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Sinha2002-px,
  title     = "The Role of Transparency in Recommender Systems",
  booktitle = "{CHI'02} extended abstracts on Human factors in computing
               systems. {ACM}",
  author    = "Sinha, R and Swearingen, K",
  year      =  2002,
  keywords  = "Mendeley Import (Jan 17)"
}

@UNPUBLISHED{Dewey_undated-kg,
  title    = "Learning What to Value",
  author   = "Dewey, Daniel",
  abstract = "We examine ultraintelligent reinforcement learning agents.
              Reinforcement learning can only be used in the real world to
              define agents whose goal is to maximize expected rewards, and
              since this goal does not match with human goals, AGIs based on
              reinforcement learning will often work at crosspurposes to us. We
              define value learners, agents that can be designed to learn and
              maximize any initially unknown utility function so long as we
              provide them with an idea of what constitutes evidence about that
              utility function. 1 Agents and Implementations Traditional agents
              [2, 3] interact with their environments cyclically: in cycle k,
              an agent acts with action y k , then perceives observation x k .
              The interaction history of an agent with lifespan m is a string y
              1 x 1 y 2 x 2 ...y m x m , also written yx 1:m or yx $\leq$m .
              Beyond these interactions, a traditional agent is isolated from
              its environment, so an agent can be formalized as an agent
              function from an interaction history yx <k to an action y k .
              Since we are concerned not with agents in the abstract, but with
              very powerful agents in the real world, we introduce the concept
              of an agent implementation. An agent implementation is a physical
              structure that, in the absence of interference from its
              environment, implements an agent function. In cycle k, an
              unaltered agent implementation executes its agent function on its
              recalled interaction history yx <k , sends the resulting y k into
              the environment as output, then receives and records an
              observation x k . An agent implementation's behavior is only
              guaranteed to match its implemented function so long as effects
              from the environment do not destroy the agent or alter its
              functionality. In keeping with this realism, an agent
              implementation's environment is considered to be the real world
              in which we live. We may engineer some parts of the world to meet
              our specifications, but (breaking with some traditional agent
              formulations) we do not consider the environment to be completely
              under our control, to be defined as we wish. Why would one want
              to study agent implementations? For narrowlyintelligent agents,
              the distinction between traditional agents and agent
              implementations may not be worth making. For ultraintelligent
              agents, the distinction is quite important: agent implementations
              offer us better predictions about how powerful agents will affect
              their environments and their own machinery, and are the basis for
              understanding realworld agents that model, defend, maintain, and
              improve themselves.",
  journal  = "MIRI",
  keywords = "MIRI;Mendeley Import (Jan 17)/Assurances/Self-Aware"
}

@ARTICLE{Bostrom2012-uf,
  title    = "{THE} {SUPERINTELLIGENT} {WILL}: {MOTIVATION} {AND}
              {INSTRUMENTAL} {RATIONALITY} {IN} {ADVANCED} {ARTIFICIAL}
              {AGENTS}",
  author   = "Bostrom, Nick",
  abstract = "www.nickbostrom.com [Forthcoming in Minds and Machines, 2012]
              ABSTRACT This paper discusses the relation between intelligence
              and motivation in artificial agents, developing and briefly
              arguing for two theses. The first, the orthogonality thesis,
              holds (with some caveats) that intelligence and final goals
              (purposes) are orthogonal axes along which possible artificial
              intellects can freely vary---more or less any level of
              intelligence could be combined with more or less any final goal.
              The second, the instrumental convergence thesis, holds that as
              long as they possess a sufficient level of intelligence, agents
              having any of a wide range of final goals will pursue similar
              intermediary goals because they have instrumental reasons to do
              so. In combination, the two theses help us understand the
              possible range of behavior of superintelligent agents, and they
              point to some potential dangers in building such an agent.",
  journal  = "Minds Mach.",
  volume   =  22,
  number   =  2,
  pages    = "7185",
  year     =  2012,
  keywords = "MIRI;Mendeley Import (Jan 17)/Assurances/Self-Aware"
}

@BOOK{Yudkowsky_undated-ni,
  title    = "Rationality: From {AI} to Zombies",
  author   = "Yudkowsky, Eliezer",
  keywords = "Mendeley Import (Jan 17)/Assurances/Self-Aware"
}

@TECHREPORT{Hutter2010-of,
  title       = "Sequential model-based optimization for general algorithm
                 configuration (extended version)",
  author      = "Hutter, F and Hoos, H H and Leyton-Brown, K",
  institution = "Technical Report TR-2010-10, University of British Columbia,
                 Computer Science",
  year        =  2010,
  keywords    = "Mendeley Import (Jan 17)"
}

@INPROCEEDINGS{Thornton2013-ll,
  title     = "{Auto-WEKA}: combined selection and hyperparameter optimization
               of classification algorithms",
  booktitle = "Proceedings of the 19th {ACM} {SIGKDD} international conference
               on Knowledge discovery and data mining",
  author    = "Thornton, Chris and Hutter, Frank and Hoos, Holger H and
               Leyton-Brown, Kevin",
  abstract  = "Many different machine learning algorithms exist; taking into
               account each algorithm's hyperparameters, there is a
               staggeringly large number of possible alternatives overall. We
               consider the problem of simultaneously selecting a learning
               algorithm and setting its hyperparameters, going beyond previous
               work that attacks these issues separately. We show that this
               problem can be addressed by a fully automated approach,
               leveraging recent innovations in Bayesian optimization.
               Specifically, we consider a wide range of feature selection
               techniques (combining 3 search and 8 evaluator methods) and all
               classification approaches implemented in WEKA's standard
               distribution, spanning 2 ensemble methods, 10 meta-methods, 27
               base classifiers, and hyperparameter settings for each
               classifier. On each of 21 popular datasets from the UCI
               repository, the KDD Cup 09, variants of the MNIST dataset and
               CIFAR-10, we show classification performance often much better
               than using standard selection and hyperparameter optimization
               methods. We hope that our approach will help non-expert users to
               more effectively identify machine learning algorithms and
               hyperparameter settings appropriate to their applications, and
               hence to achieve improved performance.",
  publisher = "ACM",
  pages     = "847--855",
  month     =  "11~" # aug,
  year      =  2013,
  address   = "New York, New York, USA",
  keywords  = "hyperparameter optimization; model selection; weka;Mendeley
               Import (Jan 17)"
}

@ARTICLE{Wang2016-ln,
  title         = "Parallel Bayesian Global Optimization of Expensive Functions",
  author        = "Wang, Jialei and Clark, Scott C and Liu, Eric and Frazier,
                   Peter I",
  abstract      = "We consider parallel global optimization of derivative-free
                   expensive-to-evaluate functions, and propose an efficient
                   method based on stochastic approximation for implementing a
                   conceptual Bayesian optimization algorithm proposed by
                   Ginsbourger et al. (2007). To accomplish this, we use
                   infinitessimal perturbation analysis (IPA) to construct a
                   stochastic gradient estimator and show that this estimator
                   is unbiased. We also show that the stochastic gradient
                   ascent algorithm using the constructed gradient estimator
                   converges to a stationary point of the q-EI surface, and
                   therefore, as the number of multiple starts of the gradient
                   ascent algorithm and the number of steps for each start grow
                   large, the one-step Bayes optimal set of points is
                   recovered. We show in numerical experiments that our method
                   for maximizing the q-EI is faster than methods based on
                   closed-form evaluation using high-dimensional integration,
                   when considering many parallel function evaluations, and is
                   comparable in speed when considering few. We also show that
                   the resulting one-step Bayes optimal algorithm for parallel
                   global optimization finds high quality solutions with fewer
                   evaluations that a heuristic based on approximately
                   maximizing the q-EI. A high quality open source
                   implementation of this algorithm is available in the open
                   source Metrics Optimization Engine (MOE).",
  month         =  "16~" # feb,
  year          =  2016,
  keywords      = "Acquisition/InfillFxns;BayesOpt;TALAF;batch
                   selection;qEI;Mendeley Import (Jan
                   17)/BayesOpt/Acquisition/InfillFxns;Mendeley Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1602.05149"
}

@MISC{Johnson2014-xr,
  title    = "The {NLopt} nonlinear-optimization package",
  author   = "Johnson, Steven G",
  year     =  2014,
  keywords = "Mendeley Import (Jan 17)/AFRL\_STTR"
}

@ARTICLE{Lahijanian_undated-xt,
  title    = "Social Trust : a Major Challenge for the Future of Autonomous
              Systems",
  author   = "Lahijanian, Morteza and Kwiatkowska, Marta",
  keywords = "Mendeley Import (Jan 17)/PaperReviews"
}

@ARTICLE{Wang2016-ph,
  title    = "{Trust-Based} Symbolic Robot Motion Planning with
              {Human-in-the-Loop}",
  author   = "Wang, Yue",
  pages    = "2015--2016",
  year     =  2016,
  keywords = "Mendeley Import (Jan 17)/PaperReviews"
}

@ARTICLE{Junges_undated-lv,
  title    = "Probabilistic Verification for Cognitive Models",
  author   = "Junges, Sebastian and Jansen, Nils and Katoen, Joost-Pieter and
              Topcu, Ufuk",
  pages    = "1--5",
  keywords = "Mendeley Import (Jan 17)/PaperReviews"
}

@MISC{Swersky2014-sq,
  title    = "{FreezeThaw} Bayesian Optimization",
  author   = "Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott",
  abstract = "In this paper we develop a dynamic form of Bayesian optimization
              for machine learning models with the goal of rapidly finding good
              hyperparameter settings. Our method uses the partial information
              gained during the training of a machine learning model in order
              to decide whether to pause training and start a new model, or
              resume the training of a previouslyconsidered model. We
              specifically tailor our method to machine learning problems by
              developing a novel positivedefinite covariance kernel to capture
              a variety of training curves. Furthermore, we develop a Gaussian
              process prior that scales gracefully with additional temporal
              observations. Finally, we provide an informationtheoretic
              framework to automate the decision process. Experiments on
              several common machine learning models show that our approach is
              extremely effective in practice.",
  year     =  2014,
  keywords = "BayesOpt;Mendeley Import (Jan 17)"
}

@ARTICLE{Jaderberg_undated-fr,
  title    = "Decoupled Neural Interfaces using Synthetic Gradients",
  author   = "Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon
              and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray",
  abstract = "Training directed neural networks typically requires
              forwardpropagating data through a computation graph, followed by
              backpropagating error signal, to produce weight updates. All
              layers, or more generally, modules, of the network are therefore
              locked, in the sense that they must wait for the remainder of the
              network to execute forwards and propagate error backwards before
              they can be updated. In this work we break this constraint by
              decoupling modules by introducing a model of the future
              computation of the network graph. These models predict what the
              result of the modelled subgraph will produce using only local
              information. In particular we focus on modelling error gradients:
              by using the modelled synthetic gradient in place of true
              backpropagated error gradients we decouple subgraphs, and can
              update them independently and asynchronously i.e. we realise
              decoupled neural interfaces. We show results for feedforward
              models, where every layer is trained asynchronously, recurrent
              neural networks (RNNs) where predicting one's future gradient
              extends the time over which the RNN can effectively model, and
              also a hierarchical RNN system with ticking at different
              timescales. Finally, we demonstrate that in addition to
              predicting gradients, the same framework can be used to predict
              inputs, resulting in models which are decoupled in both the
              forward and backwards pass -- amounting to independent networks
              which colearn such that they can be composed into a single
              functioning corporation.",
  keywords = "neural\_networks;Mendeley Import (Jan 17)/MLTheory/DeepLearning"
}

@ARTICLE{Picheny2013-mp,
  title     = "A Nonstationary {Space-Time} Gaussian Process Model for
               Partially Converged Simulations",
  author    = "Picheny, Victor and Ginsbourger, David",
  abstract  = "In the context of expensive numerical experiments, a promising
               solution for alleviating the computational costs consists of
               using partially converged simulations instead of exact
               solutions. The gain in computational time is at the price of
               precision in the response. This work addresses the issue of
               fitting a Gaussian process model to partially converged
               simulation data for further use in prediction. The main
               challenge consists of the adequate approximation of the error
               due to partial convergence, which is correlated in both design
               variables and time directions. Here, we propose fitting a
               Gaussian process in the joint space of design parameters and
               computational time. The model is constructed by building a
               nonstationary covariance kernel that reflects accurately the
               actual structure of the error. Practical solutions are proposed
               for solving parameter estimation issues associated with the
               proposed model. The method is applied to a computational fluid
               dynamics test case and shows significant improvement in
               prediction compared to a classical kriging model.",
  journal   = "SIAM/ASA Journal on Uncertainty Quantification",
  publisher = "American Statistical Association",
  volume    =  1,
  number    =  1,
  pages     = "57--78",
  year      =  2013,
  keywords  = "Mendeley Import (Jan 17)"
}

@ARTICLE{Dietterich2015-qz,
  title     = "Rise of concerns about {AI}: reflections and directions",
  author    = "Dietterich, Thomas G and Horvitz, Eric J",
  abstract  = "Research, leadership, and communication about AI futures. lives,
               including those lost to accidents on our roadways and to errors
               made in medicine. Over the longer-term, advances in machine
               intelligence will have deeply beneficial influences on
               healthcare, education, transportation, commerce, and the overall
               march of science. Beyond the creation of new applications and
               services, the pursuit of insights about the computational D
               ISCUSSIONS ABOUT ARTIFI-CIAL intelligence (AI) have jumped into
               the public eye over the past year, with sev-eral luminaries
               speaking about the threat of AI to the future of humanity. Over
               the last several de-cades, AI---automated perception, learning,
               reasoning, and decision making---has become commonplace in our
               lives. We plan trips using GPS systems that rely on the A*
               algorithm to optimize the route. Our smartphones understand our
               speech, and Siri, Cor-tana, and Google Now are getting bet-ter
               at understanding our intentions. Machine vision detects faces as
               we take pictures with our phones and recogniz-es the faces of
               individual people when we post those pictures to Facebook.
               Internet search engines rely on a fabric of AI subsystems. On
               any day, AI pro-vides hundreds of millions of people with search
               results, traffic predictions, and recommendations about books
               and movies. AI translates among lan-guages in real time and
               speeds up the operation of our laptops by guessing what we will
               do next. Several compa-nies are working on cars that can drive
               themselves---either with partial hu-man oversight or entirely
               autonomous-ly. Beyond the influences in our daily lives, AI
               techniques are playing roles in science and medicine. AI is
               already at work in some hospitals helping physi-cians understand
               which patients are at highest risk for complications, and AI
               algorithms are finding important nee-dles in massive data
               haystacks, such as identifying rare but devastating side
               ef-fects of medications. The AI in our lives today provides a
               small glimpse of more profound con-tributions to come. For
               example, the fielding of currently available technol-ogies could
               save many thousands of",
  journal   = "Commun. ACM",
  publisher = "ACM",
  volume    =  58,
  number    =  10,
  pages     = "38--40",
  month     =  "28~" # sep,
  year      =  2015,
  keywords  = "Mendeley Import (Jan 17)/Assurances"
}

@ARTICLE{Daftry2016-hi,
  title         = "Introspective Perception: Learning to Predict Failures in
                   Vision Systems",
  author        = "Daftry, Shreyansh and Zeng, Sam and Andrew Bagnell, J and
                   Hebert, Martial",
  abstract      = "As robots aspire for long-term autonomous operations in
                   complex dynamic environments, the ability to reliably take
                   mission-critical decisions in ambiguous situations becomes
                   critical. This motivates the need to build systems that have
                   situational awareness to assess how qualified they are at
                   that moment to make a decision. We call this self-evaluating
                   capability as introspection. In this paper, we take a small
                   step in this direction and propose a generic framework for
                   introspective behavior in perception systems. Our goal is to
                   learn a model to reliably predict failures in a given
                   system, with respect to a task, directly from input sensor
                   data. We present this in the context of vision-based
                   autonomous MAV flight in outdoor natural environments, and
                   show that it effectively handles uncertain situations.",
  month         =  "28~" # jul,
  year          =  2016,
  keywords      = "introspection;Mendeley Import (Jan 17)/Assurances/Self-Aware",
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1607.08665"
}

@ARTICLE{Ghavamzadeh2016-xh,
  title         = "Bayesian Reinforcement Learning: A Survey",
  author        = "Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle
                   and Tamar, Aviv",
  abstract      = "Bayesian methods for machine learning have been widely
                   investigated, yielding principled methods for incorporating
                   prior information into inference algorithms. In this survey,
                   we provide an in-depth review of the role of Bayesian
                   methods for the reinforcement learning (RL) paradigm. The
                   major incentives for incorporating Bayesian reasoning in RL
                   are: 1) it provides an elegant approach to action-selection
                   (exploration/exploitation) as a function of the uncertainty
                   in learning; and 2) it provides a machinery to incorporate
                   prior knowledge into the algorithms. We first discuss models
                   and methods for Bayesian inference in the simple single-step
                   Bandit model. We then review the extensive recent literature
                   on Bayesian methods for model-based RL, where prior
                   information can be expressed on the parameters of the Markov
                   model. We also present Bayesian methods for model-free RL,
                   where priors are expressed over the value function or policy
                   class. The objective of the paper is to provide a
                   comprehensive survey on Bayesian RL algorithms and their
                   theoretical and empirical properties.",
  month         =  "14~" # sep,
  year          =  2016,
  keywords      = "Mendeley Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1609.04436"
}

@INPROCEEDINGS{Bergstra2011-ap,
  title     = "Algorithms for {Hyper-Parameter} Optimization",
  booktitle = "{NIPS} Proceedings",
  author    = "Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and
               K{\'e}gl, Bal{\'a}zs",
  abstract  = "Several recent advances to the state of the art in image
               classification benchmarks have come from better configurations
               of existing techniques rather than novel approaches to feature
               learning. Traditionally, hyper-parameter optimization has been
               the job of humans because they can be very efficient in regimes
               where only a few trials are possible. Presently, computer
               clusters and GPU processors make it possible to run more trials
               and we show that algorithmic approaches can find better results.
               We present hyper-parameter optimization results on tasks of
               training neural networks and deep belief networks (DBNs). We
               optimize hyper-parameters using random search and two new greedy
               sequential methods based on the expected improvement criterion.
               Random search has been shown to be sufficiently efficient for
               learning neural networks for several datasets, but we show it is
               unreliable for training DBNs. The sequential algorithms are
               applied to the most difficult DBN learning problems from
               [Larochelle et al., 2007] and find significantly better results
               than the best previously reported. This work contributes novel
               techniques for making response surface models P (y|x) in which
               many elements of hyper-parameter assignment (x) are known to be
               irrelevant given particular values of other elements.",
  pages     = "2546--2554",
  year      =  2011,
  keywords  = "Mendeley Import (Jan 17)"
}

@ARTICLE{Marmin2016-xy,
  title         = "Efficient batch-sequential Bayesian optimization with
                   moments of truncated Gaussian vectors",
  author        = "Marmin, S{\'e}bastien and Chevalier, Cl{\'e}ment and
                   Ginsbourger, David",
  abstract      = "We deal with the efficient parallelization of Bayesian
                   global optimization algorithms, and more specifically of
                   those based on the expected improvement criterion and its
                   variants. A closed form formula relying on multivariate
                   Gaussian cumulative distribution functions is established
                   for a generalized version of the multipoint expected
                   improvement criterion. In turn, the latter relies on
                   intermediate results that could be of independent interest
                   concerning moments of truncated Gaussian vectors. The
                   obtained expansion of the criterion enables studying its
                   differentiability with respect to point batches and
                   calculating the corresponding gradient in closed form.
                   Furthermore , we derive fast numerical approximations of
                   this gradient and propose efficient batch optimization
                   strategies. Numerical experiments illustrate that the
                   proposed approaches enable computational savings of between
                   one and two order of magnitudes, hence enabling
                   derivative-based batch-sequential acquisition function
                   maximization to become a practically implementable and
                   efficient standard.",
  month         =  "9~" # sep,
  year          =  2016,
  keywords      = "Mendeley Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1609.02700"
}

@INPROCEEDINGS{Gonzalez2016-tt,
  title     = "{GLASSES}: Relieving The Myopia Of Bayesian Optimisation",
  booktitle = "Proceedings of the 19th International Conference on Artificial
               Intelligence and Statistics",
  author    = "Gonzalez, Javier and Osborne, Michael and Lawrence, Neil",
  abstract  = "We present GLASSES: Global optimisation with Look-Ahead through
               Stochastic Simulation and Expected-loss Search. The majority of
               global optimisation approaches in use are myopic, in only
               considering the impact of the next function value; the
               non-myopic approaches that do exist are able to consider only a
               handful of future evaluations. Our novel algorithm, GLASSES,
               permits the consideration of dozens of evaluations into the
               future. This is done by approximating the ideal look-ahead loss
               function, which is expensive to evaluate, by a cheaper
               alternative in which the future steps of the algorithm are
               simulated beforehand. An Expectation Propagation algorithm is
               used to compute the expected value of the loss. We show that the
               far-horizon planning thus enabled leads to substantive
               performance gains in empirical tests.",
  pages     = "790--799",
  year      =  2016,
  keywords  = "Mendeley Import (Jan 17)"
}

@ARTICLE{GomezBombarelli2016-ls,
  title    = "Automatic chemical design using a datadriven continuous
              representation of molecules",
  author   = "G{\'o}mezBombarelli, Rafael and Duvenaud, David and
              Hern{\'a}ndezLobato, Jos{\'e} Miguel and AguileraIparraguirre,
              Jorge and Hirzel, Timothy D and Adams, Ryan P and AspuruGuzik,
              Al{\'a}n",
  abstract = "We report a method to convert discrete representations of
              molecules to and from a multidimensional continuous
              representation. This generative model allows efficient search and
              optimization through openended spaces of chemical compounds. We
              train deep neural networks on hundreds of thousands of existing
              chemical structures to construct two coupled functions: an
              encoder and a decoder. The encoder converts the discrete
              representation of a molecule into a realvalued continuous vector,
              and the decoder converts these continuous vectors back to the
              discrete representation from this latent space. Continuous
              representations allow us to automatically generate novel chemical
              structures by performing simple operations in the latent space,
              such as decoding random vectors, perturbing known chemical
              structures, or interpolating between molecules. Continuous
              representations also allow the use of powerful gradientbased
              optimization to efficiently guide the search for optimized
              functional compounds. We demonstrate our method in the design of
              druglike molecules as well as organic lightemitting diodes.",
  year     =  2016,
  keywords = "Mendeley Import (Jan 17)"
}

@ARTICLE{Thompson1933-rd,
  title     = "On the Likelihood that One Unknown Probability Exceeds Another
               in View of the Evidence of Two Samples",
  author    = "Thompson, William R",
  journal   = "Biometrika",
  publisher = "[Oxford University Press, Biometrika Trust]",
  volume    =  25,
  number    = "3/4",
  pages     = "285--294",
  year      =  1933,
  keywords  = "Mendeley Import (Jan 17)/BayesOpt/Acquisition/InfillFxns"
}

@ARTICLE{Genz1992-qt,
  title    = "Numerical Computation of Multivariate Normal Probabilities",
  author   = "Genz, Alan",
  abstract = "Abstract The numerical computation of a multivariate normal
              probability is often a difficult problem. This article describes
              a transformation that simplifies the problem and places it into a
              form that allows efficient calculation using standard numerical
              multiple integration algorithms. Test results are presented that
              compare implementations of two algorithms that use the
              transformation with currently available software.",
  journal  = "J. Comput. Graph. Stat.",
  volume   =  1,
  number   =  2,
  pages    = "141--149",
  year     =  1992,
  keywords = "Mendeley Import (Jan 17)/BayesOpt"
}

@INPROCEEDINGS{Library2016-wv,
  title     = "Fall 2016 {AAAI} Symposia",
  booktitle = "Proceedings of Fall 2016 {AAAI} Symposia",
  author    = "Library, Digital and Intelligence, Artificial",
  publisher = "AAAI Press",
  pages     = "364",
  year      =  2016,
  keywords  = "Mendeley Import (Jan 17)"
}

@ARTICLE{Charikar2016-za,
  title         = "Learning from Untrusted Data",
  author        = "Charikar, Moses and Steinhardt, Jacob and Valiant, Gregory",
  abstract      = "The vast majority of theoretical results in machine learning
                   and statistics assume that the available training data is a
                   reasonably reliable reflection of the phenomena to be
                   learned or estimated. Similarly, the majority of machine
                   learning and statistical techniques used in practice are
                   brittle to the presence of large amounts of biased or
                   malicious data. In this work we propose two novel frameworks
                   in which to study estimation, learning, and optimization in
                   the presence of significant fractions of arbitrary data. The
                   first framework, which we term list-decodable learning, asks
                   whether it is possible to return a list of answers, with the
                   guarantee that at least one of them is accurate. For
                   example, given a dataset of $n$ points for which an unknown
                   subset of $\alpha n$ points are drawn from a distribution of
                   interest, and no assumptions are made about the remaining
                   $(1-\alpha)n$ points, is it possible to return a list of
                   $poly(1/\alpha)$ answers, one of which is correct? The
                   second framework, which we term the semi-verified learning
                   model considers the extent to which a small dataset of
                   trusted data (drawn from the distribution in question) can
                   be leveraged to enable the accurate extraction of
                   information from a much larger but untrusted dataset (of
                   which only an $\alpha$-fraction is drawn from the
                   distribution). We show strong positive results in both
                   settings, and provide an algorithm for robust learning in a
                   very general stochastic optimization setting. This general
                   result has immediate implications for robust estimation in a
                   number of settings, including for robustly estimating the
                   mean of distributions with bounded second moments, robustly
                   learning mixtures of such distributions, and robustly
                   finding planted partitions in random graphs in which
                   significant portions of the graph have been perturbed by an
                   adversary.",
  month         =  "7~" # nov,
  year          =  2016,
  keywords      = "Mendeley Import (Jan 17)",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1611.02315"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Israelsen2017-zb,
  title     = "Towards Adaptive Training of Agent-based Sparring Partners for
               Fighter Pilots",
  booktitle = "{InfoTech@Aerospace} Conference",
  author    = "Israelsen, Brett W and Ahmed, Nisar and Center, Kenneth and
               Green, Roderick and Jr, Winston Bennett",
  abstract  = "A key requirement for the current generation of artificial
               decision-makers is that they should adapt well to changes in
               unexpected situations. This paper addresses the situation in
               which an AI for aerial dog fighting, with tunable parameters
               that govern its behavior, must optimize behavior with respect to
               an objective function that is evaluated and learned through
               simulations. Bayesian optimization with a Gaussian Process
               surrogate is used as the method for investigating the objective
               function. One key benefit is that during optimization, the
               Gaussian Process learns a global estimate of the true objective
               function, with predicted outcomes and a statistical measure of
               confidence in areas that haven’t been investigated yet. Having a
               model of the objective function is important for being able to
               understand possible outcomes in the decision space, for example
               this is crucial for training and providing feedback to human
               pilots. However, standard Bayesian optimization does not perform
               consistently or provide an accurate Gaussian Process surrogate
               function for highly volatile objective functions. We treat these
               problems by introducing a novel sampling technique called Hybrid
               Repeat/Multi-point Sampling. This technique gives the AI ability
               to learn optimum behaviors in a highly uncertain environment.
               More importantly, it not only improves the reliability of the
               optimization, but also creates a better model of the entire
               objective surface. With this improved model the agent is
               equipped to more accurately/efficiently predict performance in
               unexplored scenarios.",
  pages     = "1--15",
  year      =  2017,
  address   = "Grapevine, TX",
  keywords  = "BayesOpt;Mendeley Import (Jan 17)"
}

@ARTICLE{Vickers2003-co,
  title       = "How many repeated measures in repeated measures designs?
                 Statistical issues for comparative trials",
  author      = "Vickers, Andrew J",
  affiliation = "Integrative Medicine Service, Biostatistics Service, Memorial
                 Sloan Kettering Cancer Center, Howard 13, 1275 York Avenue NY,
                 NY 10021, USA. vickersa@mskcc.org",
  abstract    = "BACKGROUND: In many randomized and non-randomized comparative
                 trials, researchers measure a continuous endpoint repeatedly
                 in order to decrease intra-patient variability and thus
                 increase statistical power. There has been little guidance in
                 the literature as to selecting the optimal number of repeated
                 measures. METHODS: The degree to which adding a further
                 measure increases statistical power can be derived from simple
                 formulae. This ``marginal benefit'' can be used to inform the
                 optimal number of repeat assessments. RESULTS: Although
                 repeating assessments can have dramatic effects on power,
                 marginal benefit of an additional measure rapidly decreases as
                 the number of measures rises. There is little value in
                 increasing the number of either baseline or post-treatment
                 assessments beyond four, or seven where baseline assessments
                 are taken. An exception is when correlations between measures
                 are low, for instance, episodic conditions such as headache.
                 CONCLUSIONS: The proposed method offers a rational basis for
                 determining the number of repeat measures in repeat measures
                 designs.",
  journal     = "BMC Med. Res. Methodol.",
  publisher   = "BioMed Central",
  volume      =  3,
  pages       = "22",
  month       =  "27~" # oct,
  year        =  2003,
  keywords    = "Mendeley Import (Jan 17)/BayesOpt/Acquisition/InfillFxns",
  language    = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ernest2016-vi,
  title    = "Genetic Fuzzy based Artificial Intelligence for Unmanned Combat
              Aerial Vehicle Control in Simulated Air Combat Missions",
  author   = "Ernest, Nicholas and Carroll, David",
  abstract = "Volume 6 • Issue 1 • 1000144 J Def Manag ISSN: 2167-0374 JDFM, an
              open access journal While this automates the process, the
              computational cost of the genetic algorithm for searching over a
              practically infinite solution space causes the traditional
              genetic fuzzy system to be infeasible in more complex problems.
              Methodologies have been developed to help mitigate this, but one
              method in particular is capable of applying fuzzy control to
              problems of this scope. The Genetic Fuzzy Tree (GFT) has shown an
              incredible ability to obtain unparalleled levels of performance
              in very large and complex problems that contain all of the
              difficulties that alternative intelligent systems have issues
              coping with. This new subtype of genetic fuzzy system was
              recently developed during Dr. Ernest's graduate studies, under
              the guidance of Dr.'s Cohen and Schumacher and supported by the
              Dayton Area Graduate Studies Institute. The aim of this initial
              work was to control a flight of ground strike UCAVs in a
              low-fidelity simulation environment [5,6]. The success of this
              study led to Psibernetix Inc. partnering with the US air force
              research laboratory (AFRL) to apply the GFT methodology to a much
              more complex problem. Just as UAVs represented a revolutionary
              capability for the USAF in the mid-1990s, Manned-Unmanned
              Autonomous Teaming in an air combat environment will certainly
              represent a revolutionary leap in capability of airpower in the
              near future. Air combat, as it is performed by human pilots today
              is a highly dynamic application of aerospace physics, skill, art,
              and intuition to maneuver a fighter aircraft and missile against
              an adversary moving at high speeds in three dimensions. Today's
              fighters close on each other at speeds in excess of 1,500 MPH
              while flying at altitudes above 40,000 feet. The selection and
              application of air-to-air tactics requires assessing a tactical
              advantage or disadvantage and reacting appropriately in
              microseconds. The cost of mistakes is high.",
  journal  = "J Def Manag",
  volume   =  06,
  number   =  01,
  year     =  2016,
  keywords = "Mendeley Import (Jan 17)"
}

@BOOK{Forrester2008-zo,
  title     = "Engineering design via surrogate modelling: a practical guide",
  author    = "Forrester, Alexander and Sobester, Andras and Keane, Andy",
  publisher = "John Wiley \& Sons",
  year      =  2008,
  keywords  = "Mendeley Import (Jan 17)"
}

@ARTICLE{Croarkin2016-kv,
  title    = "e-Handbook of Statistical Methods",
  author   = "Croarkin, Carroll and Tobian, Paul",
  journal  = "NIST/SEMATECH, Available online: http://www. itl. nist.
              gov/div898/handbook",
  year     =  2016,
  keywords = "Mendeley Import (Jan 17)"
}

@BOOK{Pyzdek2003-lk,
  title     = "Quality engineering handbook",
  author    = "Pyzdek, Thomas and Keller, Paul A",
  publisher = "CRC Press",
  year      =  2003,
  keywords  = "Mendeley Import (Jan 17)"
}

@PHDTHESIS{Gal2016-to,
  title    = "Uncertainty in Deep Learning",
  author   = "Gal, Yarin",
  year     =  2016,
  keywords = "Mendeley Import (Jan 17)/Assurances/Quantifying/Consistency"
}

@MISC{Wikipedia2016-bq,
  title    = "Algebraic Connectivity --- \{W\}ikipedia\{,\} The Free
              Encyclopedia",
  author   = "{Wikipedia}",
  year     =  2016,
  keywords = "Mendeley Import (Jan 17)"
}

@BOOK{Anderson1999-ay,
  title     = "{LAPACK} Users' guide -- Symmetric Eigenvalue Reduction",
  author    = "Anderson, Edward and Bai, Zhaojun and Bischof, Christian and
               Blackford, Susan and Dongarra, Jack and Du Croz, Jeremy and
               Greenbaum, Anne and Hammarling, Sven and McKenney, A and
               Sorensen, D",
  publisher = "Siam",
  volume    =  9,
  year      =  1999,
  keywords  = "Mendeley Import (Jan 17)"
}

@MISC{Wikipedia2016-od,
  title    = "Simulated Annealing --- \{W\}ikipedia\{,\} The Free Encyclopedia",
  author   = "{Wikipedia}",
  year     =  2016,
  keywords = "Mendeley Import (Jan 17)"
}

@MISC{Wikipedia2016-ri,
  title    = "Jacobi eigenvalue algorithm --- \{W\}ikipedia\{,\} The Free
              Encyclopedia",
  author   = "{Wikipedia}",
  year     =  2016,
  keywords = "Mendeley Import (Jan 17)"
}
