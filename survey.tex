\input{methodology.tex}

\section{A Survey of Assurances} \label{sec:survey}
\edit{\textbf{RECOUNT CITED PAPERS IN EACH SECTION, AND UPDATE THE FIGURE}}
Now that AIA, trust, TRBs, and assurances have been defined the survey of assurances is now presented. There are many different ways in which this survey could be organized; we choose to present it based on the different goals of the main groups of researchers who have been working in efforts related to assurances.

Early in reading the related literature it became clear that there were two main groups: 1) those researchers who have formally addressed the topic of trust between humans and AIAs of some form, and 2) a much larger body of those who have informally considered trust in their work (or concepts related to trust). Here we consider formal treatment of trust to include those who acknowledge a human trust model and who gather data from human users in order to measure the effect of assurances on trust. Informal treatment of trust includes those who reference the concept and/or components of trust, but who do not gather user data to verify the effects of proposed assurances. 

Another way that the landscape of researchers might be divided is by the kinds of assurances they investigate. The first group consider what we call `implicit' assurances. Implicit assurances embody any assurances that are not deliberately designed into the AIA to influence trust or TRBs. The second group consider `explicit' assurances, which are assurances that were explicitly created by a designer with the intent of affecting a user's trust. Implicit assurances can be thought of as side-effects of the design process; for example HAL 9000 could have been designed with a circular `red-eye' looking sensor because it was cost-effective, however it is possible that users who interact with HAL might find the `red-eye' sensor to suspicious, and thus lose trust in HAL. Conversely, the same `red-eye' may have been explicitly designed and selected based on several studies that indicated that users find it easier to trust advanced AIAs with `red-eye' sensors instead of similarly shaped green sensors.

Much of the research that formally considers trust has focused on implicit assurances. This is likely due to the focus on investigating what properties of an autonomous systems can affect a user's trust. It is possible to argue that someone who finds that reliability affects a user's trust is investigating an explicit assurance, but for the purposes of this paper we try to stay true to the intent of the researcher when performing the work. More recently, as seen by a large spike in interest in `interpretable', and `explainable' AIAs in government, academic, and public circles, we have seen the emergence of groups who acknowledge that the concept of trust in human-AIA relationships, and who want to design systems accordingly.

In view of these four main groups of researchers, we organize the survey by creating four quadrants shown in Figure~\ref{fig:trust_assurance_intention}. In the remainder of this section we survey each of these quadrants separately in order to gain some understanding of the lessons that each has to offer when we consider the design of assurances.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Trust_vs_Assurance_Intention.pdf}
    \caption{Figure depicting how many of the papers surveyed here consider trust both formally and informally, as well as those who investigate explicit and implicit assurances}
    \label{fig:trust_assurance_intention}
\end{figure}

\begin{itemize}
    \item Quadrant I. (implicit assurances, formal trust treatment) -- Gather user data, consider a trust model, consider assurances that are implicit (i.e. those who care about human-AIA trust, but aren't designing assurance algorithms)
    \item Quadrant II. (explicit assurances, formal trust treatment) -- Gather user data, consider a trust model, consider assurances that are explicit (i.e. those who formally acknowledge human-AIA trust, and design assurances to affect it)
    \item Quadrant III. (explicit assurances, informal trust treatment) -- Do not gather data from users, reference trust (or its components interpretability, etc..), consider assurances that are explicit (i.e. those who know that the concept of `trust' is important, but that only use an informal notion of it when designing assurances)
    \item Quadrant IV. (implicit assurances, informal trust treatment) -- Not interested in affects on user trust, but reference (possibly only allude to) concepts that are related to trust as defined in this paper. Investigate approaches for creating AIAs with improved properties or characteristics. This work is subtly different from that in Quadrant III in the degree/intent to which trust concepts were considered. In Quadrant III trust components were clearly the main focus of the research, whereas in this quadrant the relationship to trust is only visible to someone who knows what they are looking for (i.e. those whose work is relevant for designing assurances, but don't know it)
\end{itemize}

\input{q1.tex}
\input{q2.tex}
\input{q3.tex}
\input{q4.tex}
