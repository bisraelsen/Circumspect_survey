\input{methodology.tex}

\section{Survey Sections} \label{sec:survey}
Now that AIA, trust, TRBs, and assurances have been defined we are ready to begin the survey of assurances. Early in reading the related literature it became clear that there were two main groups: 1) those researchers who have formally addressed the topic of trust between humans and AIAs of some form, and 2) a much larger body of those who have informally considered trust in their work. Here we consider formal treatment of trust to include those who acknowledge a human trust model and who perform experiments that attempt to measure the effect of assurances on trust. Informal treatment of trust includes those who only reference a nebulous idea of trust, and who do not actually perform experiments to verify that proposed assurances actually do affect trust. 

There are two other  groups: 1) those who focus on explicitly designed assurances, or those assurances intentionally created by designers with the intent of affecting a user's trust, and 2) those who focus on implicit assurances, or those assurances that exist without the expressed intention of affecting the trust of user's.

Much of the research that formally considers trust has focused on implicit assurances. This is likely due to the focus on analyzing the effect of autonomous systems on trust and not on designing systems for trust. However, as seen by a large spike in interest in `interpretable', and `explainable' AIAs in government, academic, and public circles, there is a clearly an important need to better understand the roles of and methods for implementing designed assurances. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Trust_vs_Assurance_Intention.pdf}
    \caption{Figure depicting how many papers that consider trust formally and informally, and consider explicit and implicit assurances}
    \label{fig:trust_assurance_intention}
\end{figure}

The remainder of this document will focus on surveying the assurance research in the formal/informal trust consideration and explicit/implicit assurance plane. Figure \ref{fig:trust_assurance_intention} shows the number of papers considered in this survey that lay in each quadrant of that plane. The quadrants are defined as:

\begin{itemize}
    \item \hyperref[sec:q1]{Quadrant I.} (implicit assurances, formal trust treatment) -- Use human experiments, consider a trust model, assurances are implicit (i.e. those who care about human-AIA trust, but aren't designing assurance algorithms)
    \item \hyperref[sec:q2]{Quadrant II.} (explicit assurances, formal trust treatment) -- Use human experiments, consider a trust model, assurances are explicit (i.e. those who formally acknowledge human-AIA trust, and design assurances to affect it)
    \item \hyperref[sec:q3]{Quadrant III.} (explicit assurances, informal trust treatment) -- No human experiments, reference trust (or interpretability, etc..), proposed assurances are explicit (i.e. those who know that `trust' is important, but that only use a vaguely defined idea of trust to make an algorithm that \emph{might} be an assurance)
    \item \hyperref[sec:q4]{Quadrant IV.} (implicit assurances, informal trust treatment) -- don't reference trust, designed properties of AIA for better performance, typically reference some of the trust components such as predictability, and competence, but only in the context of the designer being happy. (i.e. those whose work is relevant for assurances, but don't know it)
\end{itemize}

It is clear that much of the research that is relevant has occurred in the `informal trust treatment' half of the plane. To satisfy the need for better understanding the relationship between trust, assurances, and TRBs in AIAs and improve their general usability, it is clear that some effort must be made to identify opportunities for additional research in Quadrant II, i.e. to improve the availability of assurances that have been formally and explicitly designed to affect user trust. One key observation is that there is plenty of opportunity to `move' research from Quadrants I., III., and IV. to Quadrant II. In essence, this would mean taking proposed methods from these other quadrants and putting them to the test using a formal understanding of trust and appropriately designed user experiments.

\input{q1.tex}
\input{q2.tex}
\input{q3.tex}
\input{q4.tex}
