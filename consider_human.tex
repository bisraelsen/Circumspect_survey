\subsection{Considering the User in Designing Assurances} \label{sec:consider_human}
    In a relationship between a human and an AIA the human user will have TRBs. In general the human user will not have knowledge regarding what assurances are implicit or explicit. It is critical to note that in the absence of explicit assurances the human will instead use implicit assurances to inform their TRBs.

    As a simple example, in \cite{Muir1994-ow} a simple flow-rate is provided to the user, they then created a mental model of the reliability of the AIA to inform their TRBs. Creating this mental model takes time, and the model is prone to cognitive biases (discussed a bit more below).

    To the user all assurances are the same, that is to say that any property or behavior of an AIA that affects trust is an assurance, and that it doesn't matter whether the assurance was designed or not (is explicit or implicit). As illustrated in Figure \ref{fig:assurance_path}, perception of the assurance is the first step in which the human user is involved (the two previous steps belong to the AIA). This highlights the idea that human perception (both sensory and cognitive) needs to be taken into account when designing assurances.

\subsubsection{Perception of Assurance}
    An important consideration when designing assurances is whether a human can perceive the assurances being given. If so, to what extent is the information from the assurance transfered (i.e. how much information was lost in the communication)? A few examples include: an AIA giving an auditory assurance in a noisy room and the user not hearing it, or an AIA displaying an assurance to a user that has obstructed vision.

    What perceptions are beneficial to a certain task? Looking at \cite{Dragan2013-wd} and \cite{Wu2016-ei} shows that sometimes the same technique can have different effects. In \cite{Dragan2013-wd} the AIA is made more trustworthy by making the motions more human-like, whereas in \cite{Wu2016-ei} making the AIA more human-like resulted in a decrease of trustworthiness. This highlights the need to understand the environment in which the AIA and human will interact, and to modify assurances appropriately based on desirable perceptions.
    
    A second more nuanced point is whether the user can process the information received. This is perhaps most easily illustrated by considering a non-expert user who cannot understand highly technical assurances regarding the AIA. However, less trivial manifestations may be troubling. This point was not directly addressed in the survey papers, but evidence of its existence were seen. For example in quadrant I \cite{Riley1996-qm}, and \cite{Freedy2007-sg} both observed evidence of framing effects (or the tendency of humans to have biased trust based on the order in which they encountered AIA with different levels of competency). This suggests the existence of other kinds of typical cognitive behaviors such as recency effects (having biased trust based on recent experiences), that might affect the ability of a human user to. With these cognitive affects we see that the human brain inherently has limitations to being able to process assurances. These limitations must be considered when designing assurances.

\subsubsection{The Effects of Assurances on User Trust}
    When it comes time to measure the effect of assurances on a human's trust the surveyed literature uses two main approaches. The first is self-reported changes based on questionairres, the second involves measuring changes of user's TRBs. Evidence is presented in \cite{Dzindolet2003-ts} that illustrates that sometimes changes is self-reported trust do not result in changes in TRBs. As previously discussed, the goal of assurances are to elicit appropriate TRBs from the human user towards the AIA. From this perspective measuring changes in TRBs is the more objective approach to measure the effect of assurances. Attempting to measure the self-reported trust of a user has a couple of key weaknesses: First, measuring self-reported trust does not measure TRBs and thus misses the goal of assurances; second, gathering self-reported data is also subject to typical biases of different users, and may fluctuate from task to task for the same user. In view of these arguments we recommend that the effects of assurances be primarily measured through changes in TRBs, with self-reports being used as a secondary source of information.
