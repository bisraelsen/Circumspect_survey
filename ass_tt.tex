\emph{Tutoring vs Telling:}
Assurances investigated to date are largely designed for one-way `telling' of information, i.e. that they do not consider and adapt to the experience or other traits of different users. The ability to adapt to different users, and tutor them to appropriately trust AIAs will become more critical as time passes, due to the diversity of user bases for advanced AIAs and time that users will spend interacting with them on complex tasks. A tutoring assurance might, for example, be a planned dynamic sequence of assurances that would change in time to adapt to the user's needs via two way user-AIA communication. This might include modification of assurances to help a user avoid boredom or fatigue in long-duration applications requiring user supervision, or to use the system differently in varying circumstances. It is not surprising that, to our knowledge, no research has been done with respect to tutoring a user in a trust relationship. This is a complex problem that requires understanding how different users learn and identifying potential strategies for eliciting appropriate TRBs from them. However, many interesting avenues for pursuing these ideas may come from the work on educational tutoring systems \cite{Wenger2014-ld} and algorithmic teaching  \cite{Balbach2009-jw}.
