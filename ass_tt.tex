\subsubsection{Tutoring vs Telling} \label{sec:teach_tell}
This is another question rarely investigated in the literature surveyed, but critical in the context of different users and long-term human-AIA interaction. We suggest that assurances can also be classified by whether they consider tutoring (or leading) the user to help them understand, or whether they are static and unchanging regardless of the user, their experience, or their expertise.

A tutoring assurance would be a planned, dynamic, sequence of assurances that would change in time to adapt to the user's needs. This might include modification of assurances to help a user avoid boredom, or to use the system differently in varying circumstances. For example the first time an autonomous vehicle encounters snow with a certain user, it might take time to give special assurances. Or a user that is so used to an AIA that its TRBs begin to drift to disuse, and the AIA gives a special assurance to correct that.

It isn't surprising that, to our knowledge, no research has been done with respect to tutoring a user in a trust relationship. This is a complex problem to address that would involve understanding how different users learn, and what an appropriate strategy would be to teach them to have appropriate TRBs. However, a rich resource (not investigated in this paper) would be the work on tutoring systems \citet{Wenger2014-ld} and algorithmic teaching \citet{Balbach2009-jw}.
