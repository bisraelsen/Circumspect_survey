\subsubsection{Tutoring vs Telling} \label{sec:teach_tell}
    This is also a point rarely seen in the literature surveyed, but critical in the context of different users and long-term human-AIA interaction. We suggest that assurances can also be classified by whether they consider tutoring (or leading) the user to help them understand, or whether they just produce a static and unchanging value regardless of the user, their experience, or their expertise. This is a point mentioned (in terms of explainability) by \citet{Lacave2002-cu}, and \citet{Lacher2014-yc}.

    A tutoring assurance would be a planned, dynamic, sequence of assurances that would change in time to adapt to the user's needs (as discussed in section \ref{sec:consider_human}. This might include modification of assurances to help a user avoid boredom, or to use the system differently in varying circumstances. For example the first time an autonomous vehicle encounters snow with a certain user, it might take time to give special assurances. Or a user that is so used to an AIA that its TRBs begin to drift to disuse, and the AIA gives a special assurance to correct that.

    It isn't surprising that, to our knowledge, no research has been done with respect to tutoring a user in a trust relationship. This is a complex problem to address that would involve understanding how different users learn, and what an appropriate strategy would be to teach them to have appropriate TRBs. However, a rich resource (not investigated in this paper) would be the work on tutoring systems. There is definitely an open area for research that investigates the advantages of tutoring assurances versus those of telling assurances, and how easy they are to implement in contrast to the added time and effort needed to design tutoring assurances.
