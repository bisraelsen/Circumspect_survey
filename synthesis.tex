\section{A Refined View of Assurances} \label{sec:synthesis}
    From the review of Quadrants I. through IV. of the formal/informal, explicit/implicit plane, we are able to find some insights with respect to assurances and can discuss them in a more comprehensive way. Using insights from the survey a refined version of Figure~\ref{fig:SimpleTrust_one_way} can be constructed. Figure~\ref{fig:refined_assurances} incorporates all details from Section~\ref{sec:background} as well as adding some insights from the survey that give direction about the design of assurances in human-AIA trust relationships. Below we discuss the design of explicit assurances in this more detailed framework -- some of these insights might also apply to implicit assurances, but implicit assurances will not be directly discussed in this paper.

    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.9\textwidth]{Figures/RefinedTrust_one_way}
        \caption{Detailed extension of Figure~\ref{fig:SimpleTrust_one_way}. The AIA, User , and User TRBs blocks are defined as discussed in Section~\ref{sec:background} (with the exception of the `Perception' blocks added to the AIA and User boxes). The AIA Assurances box has been filled using insights from the surveyed material. The boxes that are greyed out will not be discussed in this section.}
        \label{fig:refined_assurances}
    \end{figure}

\subsection{Calculated and Designed Explicit Assurances}
    Recall that an assurance is defined as \emph{any} behavior or property of an AIA that affects a user's trust, therefore an explicit assurance is any assurance that was consciously implemented/applied by the designer before deployment of the AIA. As such, it is possible to design assuring properties into the system a priori. It is likewise possible to design assuring behaviors into an AIA.

    From the literature, there are a couple high-level ideas that surround the calculation of assurances, these are: quantifying uncertainty, and reducing complexity. A third category that must be considered is planning strategies of assurance.

    \paragraph{Quantifying Uncertainty} Being able to quantify the different kinds of uncertainty in the AIA is necessary before attempting to express that uncertainty to a human user. There are several different kinds of uncertainty that might be considered such as uncertainty in sensing, uncertainty in planning, and uncertainty in locomotion. The general idea is that a model or method needs to be incorporated in the AIA that will represent the different kinds of uncertainty to the human user in some way. A human user could use such information to inform their trust in the `situational normality', `competence', and/or `predictability' of the AIA. 

    How have researchers approached the need to quantify uncertainty? In the surveyed literature we have seen the following main approaches. In some cases uncertainty is already represented intrinsically by the algorithms and/or models being used in the AIA. \cite{Wang2016-id} address using the built-in statistical representations of transitions and observations as the basis of quantifying uncertainty in a POMDP robot. This is an approach that has straightforward analogs in systems that use algorithms and/or models that inherently consider uncertainty.

    Models and methods that intrinsically contain or represent uncertainty are frequently available. However, even when that it is the case -- such as with POMDPs -- there are types of uncertainties that may still not be considered. \cite{Aitken2016-fb}, suggest further metrics of uncertainty beyond those intrinsically captured by a POMDP model (POMDPs contain representation of transition probabilities, and observation probabilities). Using the UGV road-network problem as an example, if the UGV calculates a distribution over possible outcomes, how favorable is that distribution? Or, given a certain road-network what kind of accuracy can be expected from the POMDP solver that the UGV is equipped with? In a similar vein \cite{Kuter2012-bv} simulated ways of breaking a plan (`counter examples'), and evaluated the system's capability of repairing the broken plan in those instances. Generally these considerations might be described as addressing `uncertainties in application', which are those uncertainties that arise when trying to apply certain algorithms and models in various environments.

    Perhaps the most obvious (but not necessarily simple) approach is to quantify the uncertainty of a classifier (regression methods have analogous approaches). \cite{Gurau2016-hs} (along with many others see Section \ref{sec:performance_prediction}) approached this problem by creating a GP model of an image classification model. In this way they, in essence, constructed a truth model from empirical training data and used that model to quantify uncertainty in different test scenarios. In contrast , \cite{Zhang2014-he} proposed quantifying the uncertainty of a classifier based solely on the input itself. They admit that this approach may have shortcomings based on the type of classifier used, but show that for many methods (i.e. algorithms that are not learning based) the method is very reasonable. Of course the methods share common drawbacks of being solely supervised learning approaches, however even in the unsupervised domain such as reinforcement learning, methods for avoiding highly uncertain (un-safe) rewards use some form of external expert knowledge (see \cite{Garcia2015-rs, Lipton2016-dq}).

    Uncertainty is easier to assess if some kind of oracle, or reference is available for comparison. Quantifying the similarity between the empirical experience and the available reference can be a measure of uncertainty. This was illustrated by \cite{Kaipa2015-hy} who used a 3d part model to compute the confidence of a robot in identifying a part to be retrieved from a bin, by quantifying the difference between the measurements of an identified part, and those of the true part. Of course this approach loses its appeal when a truth model isn't available. This shouldn't detract from the intent of finding some kind of reference (truth or otherwise) in which the reasoning, sensing, and other processes of an AIA can be compared to evaluate uncertainty.

    The evaluation of statistical models involves a very similar concept, given a theoretical statistical model as a reference does empirical experience support or detract from the hypothesis that the model is still valid? These ideas have been investigated by \cite{Laskey1991-mf, Laskey1995-jp, Laskey2015-gz, Zagorecki2015-qy, Habbema1976-xd, Ghosh2016-dl} who try to assess whether the reference model is still valid. These approaches can quantify the degree to which the statistical models are still true, and this measurement can be used as an indication of uncertainty.

    Generally, the capability of quantifying uncertainty enables an AIA to be able to express assurances related to the `situational normality', `competence', and `predictability' of the system in a given situation. One might imagine that in the UGV road-network problem the UGV expressing high uncertainty in its plan would influence the competence component of the user's trust. Conversely, if an uncertainty measure is not available the user might take this as an implicit assurance that the AIA is perfectly confident, or based on the user's experience they might conclude that since all AIA plans have been flawed in their experience, the plan of this AIA must be flawed as well.

    \paragraph{Reducing Complexity} Many researchers have attempted to remove complexity from the models and logic of the AIA to make the methods more interpretable (or comprehensible, or explainable, \ldots) to a human user. As with quantifying uncertainty, making an AIA more interpretable can also inform a user's trust regarding the `situational normality', `competence', and `predictability' of it. Perhaps \cite{Sheridan1984-kx} was the first to suggest this type of approach with respect to human-AIA relationships. Of course this presupposes that many of the methods used by AIAs are `complex' by some measure, we claim that the fact that experts are required to understand some of the methods (and even then it may not be totally possible) proves this supposition. Complexity only exists in the presence of some reference frame, which is the designer's. Generally complexity is said to increase with the number of variables, steps of reasoning, the size of data, etcetera.

    This reduction in complexity was addressed in several different ways. In practice this has been manifest in approaches as simple as making summary statistics, or calculating averages (e.g. \cite{Muir1994-ow,Muir1996-gt}. Reducing complexity was an important goal to \cite{Aitken2016-fb} whose aim is to reduce uncertainties in models, outcomes, computational approaches, and others into a single `self-confidence' measure between $-1$ and $1$; this involves another layer of algorithms to operate on top of the various other algorithms being used in the AIA. In short, all of the approaches from Section \ref{sec:model_interp} investigate this need.

    A useful example is explaining one's research to others. When explaining to a researcher from the same field jargon, and shared knowledge can make the explanation easier. However, when trying to explain one's research to a non-expert one must adapt the explanation to be more general and simple; this will inevitably result in the loss of detailed information, but will hopefully convey the main principles to the user. There is always a tension between a highly accurate model -- that must necessarily be complex -- and a simple yet less faithful model. 

    To this end, we find work by \cite{Ruping2006-xj}, \cite{Van_Belle2012-dt}, \cite{Ribeiro2016-uc}, and \cite{Choi2016-by} especially interesting. These methods seem promising in their attempts to make/discover/learn models with scalable interpretability based on given criteria like required depth of understanding, level of expertise, and time to gain understanding. 
    
    Research efforts such as \cite{Caruana2015-za,Huysmans2011-th,Faghmous2014-og,Venna2007-yj,Vellido2012-nm,Kadous1999-rx,Lomas2012-ie} tend to focus on creating and using models that are inherently more interpretable to humans. These efforts include constraining the feature space to be more simple, reducing dimensionality, learning more understandable features, and theoretically founding the models (i.e. interpretable science).

    One shortcoming of all of the work in Quadrants III and IV, is that they theoretically reduce the complexity of the models being used, but they have not verified this with human studies. It is quite likely that they have succeeded to some extent, but it is not clear what scale of improvements have been made. However, given the lack of a formal definition for `reducing complexity' (i.e. are there units of complexity, is it a relative or absolute scale?) it is difficult to be too critical.
    
    While it is possible that there are inherently interpretable models that can be designed that can compete with other non-interpretable models, it seems like this may not be the best long-term approach to reducing complexity; this is primarily due to the lack of scalability for engineers and scientists to frequently design new algorithms. We believe that investigating methods that generate explanations from non-interpretable models is a more promising direction. The main reason is that the idea of interpretable models is not well-defined and, in reality, doesn't really exist as a single tangible goal. Instead there is a continuum of interpretability that is based on the complexity of the problem, the time required for a user to interpret (i.e. a few seconds or months of study), the expertise of the user, and others. Investigating the generation of interpretations and explanations that are user specific and model agnostic would be the best of both worlds. These ideas are much more aligned with the efforts of \cite{Ruping2006-xj} and others who seek to use models with scalable resolution and accuracy.

    \paragraph{Assurance by Design} No matter how much engineers like to think about automating everything, realistically a human will need to be involved at some level for the foreseeable future, if only because the main pursuit of human-AIA trust directly involves a human. The above two approaches alone can largely use existing methods, however some researchers directly modified their methods and models in the AIA to be more meaningful to humans.

    This can be seen in \citet{Freitas2006-qo} where he considered putting a human in the learning process, he essentially modified the objective function of the learning algorithm. In essence the objective function was now based on a large set of human preferences (and biases). This kind of approach is promising, in that it can be used to encode many human qualities that cannot be easily quantified, or even explained. However, there are trade-offs that can be undesirable in many situations as well. We often use designed, objective, learning algorithms to avoid human biases. It is interesting to note that using a human in the loop can offer more interpretability to a learning process, while at the same time making the learning process itself less procedural. So, using a human in the loop can make the \emph{result} more understandable by a human, but the \emph{process} will be rendered less understandable.

    \citet{Amodei2016-xi} discuss several different considerations related to AI safety (where safety can be viewed as something a user trusts to be competent and predictable in its operation, see other papers in Section~\ref{sec:safety} as well). They spend quite a bit of time discussing the situation when AI objectives don't correspond, or align, with human objectives (this is something addressed in a specific situation by \cite{Hadfield-Menell2016-ws}, also \cite{Bostrom2012-uf}). One of the main insights is that the source of discord between what humans expect and what AIAs actually do is because of poorly designed objective functions that you might refer to as myopic, or focusing on a specific objective to the extent that a human can no longer relate to the objective of the AIA. This suggests to designers that significant time may be required to design objectives that align with human objectives, this alignment will automatically make the AIA more predictable, and competent in the user's eyes. Validation and verification (from Section~\ref{sec:VV}) can have this same affect; a system that is validated and verified is a system that is assuring by design.

    Several efforts have modified standard learning approaches (like the ones discussed in the previous section) or designed their own, in such a way as to make the models/plans inherently more assuring to a human. For example \cite{Choi2016-by} intestinally restructured a neural network to pay attention to variables that users care about, in this way the outputs were rendered more interpretable. In their work \cite{Abdollahi2016-vn} added a new layer to a recommender system that only existed to contain explanations for a human user. Finally, \cite{Jovanovic2016-gw,Swartout1983-ko} 

    \paragraph{Planning Explicit Assurances} Planning assurances is critical when trying to attain desired TRBs from a human user. In this context when we say `planning explicit assurances' we mean formulating a plan for the expression of assurances over time, with the goal of more effectively and appropriately expressing assurances to a human user. Having said this we recognize that planning is not a capability available to all AIAs. In cases where AIAs don't have the ability to plan, they may be designed beforehand with some kind of static plan (or policy) of assurance. Otherwise, in more advanced AIAs, the AIA might take into account TRBs to plan an assurance strategy to assist the human to use it appropriately.

    When planning assurances the AIA must be able to account for limitations of users, and its own limitations in expressing assurances. For example a user may not be able to understand the necessary information needed to use the AIA more appropriately. Also, the AIA may need to take a long-term strategy to teach the user, as opposed to only presenting the user with canned, static, assurances. Some of the important user considerations will be discussed further in Section~\ref{sec:express_assurances}.
    
    One important consideration for planning an assurance strategy is whether the human user can correctly process the information received. This is perhaps most easily illustrated by considering a non-expert user who cannot understand highly technical assurances regarding the AIA. However, less trivial manifestations may be troubling. This point was not directly addressed in the survey papers, but evidence of its existence were seen. For example \cite{Riley1996-qm}, and \cite{Freedy2007-sg} both observed evidence of framing effects during experiments (the tendency of a user to become too trusting once they are in the mode of trusting). This suggests the existence of other kinds of typical cognitive behaviors such as recency effects (having biased trust based on recent experiences), and the existence of bias in the perception of assurances. This will be addressed further in Section~\ref{sec:express_assurances}.

    Few papers are referenced in this subsection because it is nearly unexplored (to our knowledge) in the context of human-AIA trust relationships. However, there are several fairly new programs that are interested in this question (i.e. explainable artificial intelligence (XAI) \cite{Gunning2017-ih}, and assured autonomy)\edit{do you have references for these??? that would be good} . Assuming an AIA can give assurances there are important questions like: what is the best way to present them? How can they be adapted for different kinds of users? How can the AIA teach or tutor the human over time? These questions are virtually unexplored to date, but their answers are critical to designing more robust and effective assurances.

\subsection{Expression and Perception of Assurances} \label{sec:express_assurances}
    Expression and Perception of assurances have been combined in this section because they share several critical aspects. In designing assurances the medium, and method of expression must be selected taking into consideration the limitations of the AIA. Here medium denotes the means by which an assurances is expressed, this could be through any of senses by which humans perceive, such as sight, sound, touch, smell, and taste. The method of assurance is the way by which the assurance is expressed. An example may help: a plot may be conveyed through sight in the typical way, or through audible words (for example when communicating to a blind person); in this case the plot is the method, and sight or sound are the different mediums through which it can be communicated. An AIA might be limited in methods of expression because it does not have a display, or a speaker. In that situation how is the user supposed to receive an assurance?

    A designer must also consider whether a human can perceive the assurances being given. If so, to what extent is the information from the assurance transfered (i.e. how much information was lost in the communication)? A few examples include: an AIA giving an auditory assurance in a noisy room and the user not hearing it (such as an alert bell in a factory where the workers use ear-plugs), or an AIA attempting to display an assurance to a user that has obstructed vision. If an assurance is not expressed, or not perceived by the user, it is useless and has no effect. For example, an AIA may have the ability to store data about its performance, and compute a statistic regarding its reliability, but if it cannot express (or communicate) that information in some way, the information is useless.

    A user will \emph{always} have some kind of TRB towards an AIA (if only to choose to ignore the AIA). In the absence of explicit assurances the human will instead use implicit assurances to inform their TRBs. However, the general human user will not have knowledge regarding which assurances are implicit or explicit -- humans participating in research from Quadrants I and II were not aware which assurances were designed by the researchers and which weren't. Recall from Section~\ref{sec:assurances} that to a user all assurances are the same; that is to say that any property or behavior of an AIA that affects trust is an assurance, and it doesn't matter whether the assurance was designed or not (is explicit or implicit).

    The literature surveyed gives some insights into what methods and mediums of expression have been used to date. We summarize that research below.

    \paragraph{Methods:} One of the main methods by which to express an assurance is by displaying a value, such as a flow-rate, as in \cite{Muir1996-gt}. While this sounds banal it actually contains some nuanced points. The interesting part is that a value such as a flow-rate actually conveys no assurance to a human user without the human user then creating a mental model of the AIA value and inferring something like the reliability. The user's trust dimensions (`competence', 'predictability', etcetera) are then affected by this perception. This approach was also used by \cite{Wickens1999-la,Sheridan1984-kx,Hutchins2015-if} among others. It is effective, but relies heavily on the assumption that the user will create a model that is `good enough' out of the sequential display of those values.

    In a similar vein, \cite{Freedy2007-sg,Desai2012-rc,Salem2015-md} trained operators to recognize signs of failure or mannersims in different interactions with an AIA. Again, this approach relies on human users to make models that are `good enough' in order to correctly decide how to appropriately use the AIA. The main drawback of this work, and of that above is the blind reliance on users being able to make correct statistical models (of things like reliability) from noisy observations.

    Another approach is to rely on the physical presence of the AIA in proximity to the human user such as investigated by \cite{Bainbridge2011-pl, Dragan2013-wd}. Generally this work looks at the communication of assurances through physical movements. In the case of \cite{Bainbridge2011-pl} this was done through the gestures of a robot that was both physically present and virtually present (through a television screen). In the case of \cite{Dragan2013-wd} this was done by users directly observing motion patterns of a robotic arm. Again, this approach utilizes the `theory of mind' where the user tries to interpret what the AIAs goals and intentions are only through correlating observations (i.e. I see the hand moving towards the cup, so the robot probably wants the cup).

    More direct methods of expressing assurances include displaying the intended movements through visual projection of a planned path \cite{Chadalavada2015-wx} -- this is subtly, but significantly different from making the user infer the intended intention. Analogously, natural language expressions (written or otherwise) attempt a more active method of expressing an assurance (such as \cite{Wang2016-id}). Finally 
    \cite{Van_Belle2013-ph, Huysmans2011-th, Hutchins2015-if}, and others investigate displaying plans and logic in different formats such as tables or trees, bar charts.

    Any of these methods can be more or less effective based on the task, or situation in which they are used. In \cite{Chen2014-dk,Wallace2001-fm,Kuhn1997-qc,Lacave2002-cu} they consider the ways in which uncertainty should be displayed (i.e. as a distribution, summary statistics, fractions or decimals), unsurprisingly we find that the answer is `it depends'. Things such as the experience of the user, or the nature of the information being displayed affect the user's ability to interpret the assurance.

    One final point is that there are several potential sources of explicit assurance that lack appropriate expressions, and thus cannot be effectively utilized as assurances. For example, it is unclear the best way for an AIA to express that it has been validated and verified on situations similar to the current one. Similarly, what other methods exist for communicating statistical distributions besides showing a plot (only useful for 1 or 2 dimensional distributions) or showing sufficient statistics? Investigating how assurances can be expressed in effective, and efficient ways is critical to human-AIA trust relationships.

    \paragraph{Mediums:} In this survey we saw several different mediums used to convey an assurance. Some used sight such as in \cite{Chadalavada2015-wx} where the robot gave visual feedback to a user, or in \cite{Muir1996-gt} where performance data were visible on a computer screen. Others used sight as well when communicating via natural language (i.e. \cite{Wang2016-id}) -- it is also a simple matter to convert written natural language output to spoken natural language now.
    
    The other senses (touch, smell, and taste) are not well explored in literature related to human-AIA. Generally, any human sense could be used as a medium, besides sight, and sound. Tactile feedback has been used extensively in robotics where it is called `haptic feedback' (where the user receives mechanical feedback through the robot controls). This meduim is use to create a more immersive user interface in robotics, to help users feel more connected to the robot. One can imagine smell and taste having an obvious application in the assurances of a cooking robot, other applications certainly exist as well and are open to further research.

    \paragraph{Human-like assurance:} It is generally presumed that making something more human-like will make an AIA more trustworthy. An algorithm may be human-like when it represents knowledge in a way that a human would understand, or executes logic in a way that a human can follow. A robot that is humanoid becomes more human-like in appearance (as investigated in \cite{Bainbridge2011-pl}), a system that uses natural language becomes more human-like in communication (for example in \cite{Lacave2002-cu}). Generally, since human-AIA trust relies on an interface between humans and AIAs, there must be some kind of method by which the AIA and human draw closer together in some way. As humans are the designers of AIAs this is typically done by making an interface where the AIA become more human-like, by implicit or explicit means, if only when it comes time to express assurances.

    Contrasting \cite{Dragan2013-wd} and \cite{Wu2016-ei} shows that sometimes the same technique can have different effects when used in different situations. In \cite{Dragan2013-wd} the AIA is made more trustworthy by making the robot motions more human-like, whereas in \cite{Wu2016-ei} making the AIA more human-like resulted in a decrease of trustworthiness. In this case the difference came from the type of task, in the first case the robot was physically working in proximity to a human, in the other case the user was playing a competitive game against the AIA.

    We therefore see that making a system more `human-like' can both positively and negatively affect trust between human and AIA. 
    \citet{Tripp2011-rx} noted that humans trust more `human-like' AIAs in more human-like ways. Perhaps in this case `human-like' applies to how difficult it is to fully understand and predict the AIA. This is definitely true with humans, one can never be sure how a human will act in given situations. Following on this idea the benefits or drawbacks of human-like characteristics are likely amplified by the risk of the situation in which being unpredictable is not conducive to increased trust.

    \paragraph{Efficiency:} Some kinds of expression are very `one-dimensional' in that they only use one medium, or method. This, again, is seen in practice by utilization of plotting a certain value over time. Because of this much of the research to date involves assurances that are not robust to loss in transfer. Because of this, exploring ways in which assurances can be robustly communicated is a clear opportunity for those trying to design assurances. This is akin to a human speaking with their voice, making facial expressions, and gestures with their hands as well; simultaneously utilizing several mediums/methods helps to ensure an assurance will be perceived correctly. Of course, repeating the same message over a thousand times is wasteful, and so enters the idea of efficiency in expression.

    Perhaps less obvious is a situation in which the user has to supplement an incomplete assurance. As a simple example, in \cite{Muir1994-ow} a simple flow-rate is provided to the user, they then supposedly create a mental model of the reliability -- based on repeated observations over time of the AIA to inform their TRBs. Creating this mental model takes time, and the model is prone to cognitive biases (discussed a bit more below). In this case the assurance is communicated slowly and indirectly.
    
    Generally, a highly efficient assurance would have precise information communicated in a way that is easy for the user to perceive, with little loss. Whereas, an inefficient assurance may be more vague, wasteful (i.e. repeating the same thing a thousand times), and susceptible to loss in communication. The likely solutions to efficiency lay in selecting appropriate methods, and mediums for expression of the assurance.

    \paragraph{Implicit and Explicit Assurances:} It is worth considering, in more detail, what implications this has on the designer. The foremost consideration is that an analysis of the interaction between the human and user need to be made in order to identify the critical assurances for a given scenario. For example, in the road network problem, an analysis might find that the most critical assurances are about the competence of the UGV's planner. In this case the designer must take time to design a planning-competence assurance.

    One difficulty arises form this approach, which is that there doesn't seem to be a way to determine what passive assurances might drown out active assurances. Following from the example above, the designer may have built an excellent planning-competence assurance, but failed to consider the effect of how the UGV appears -- it may be old, have loose panels, and rust holes. Generally, designers overlook implicit assurances (i.e. do not consider them explicitly in design) because they assume that they will have no effect (i.e. why does it matter if there are rust-holes if the UGV works?). This can stem from either: 1) ignorance of human-AIA trust dynamics, or 2) lack of identifying which assurances are most important to a human user.

    While it might be nice, it seems unreasonable, inefficient, and unwise to perform a study of \emph{every possible} assurance from an AIA to a human and then select the most important. Perhaps one way a designer might try to identify which assurances are important is to perform human studies where feedback about which characteristics of the AIA most affected the trust of the user. An approach like this would help to point out if explicit assurances are being noticed, and if there are implicit assurances that are overly influential, or that overwhelm the explicitly designed assurances. With such feedback designers would have a realistic idea about whether their explicit assurances are having the desired effect.

\subsection{Observing Effects of Assurances} \label{sec:measuring_effects}

    There are two different situations when it would be desirable to measure the effects of assurances. The first is when the designer wants to understand the effectiveness, the second would be when the AIA itself needs to measure whether assurances are effective or not. To our knowledge there has not been any work regarding the second situation where an AIA would measure response to assurances and then adapt behaviors appropriately (at least not in the trust cycle setting), however this is arguably the ultimate goal so that AIAs can themselves modify assurances to meet different needs. What does this mean practically? It seems that any method that is made for the designer to measure the effects of assurances could also be deployed into an AIA. The surveyed literature gives some insights into how that has been done to date.
   
    When it comes time to measure the effect of assurances on a human's trust there are two main approaches. The first is self-reported changes based on questionnaires, the second involves measuring changes of user's TRBs. The questionnaire approach was used extensively in many of the papers surveyed (as investigated by \cite{Mcknight2011-gv,Muir1996-gt,Wickens1999-la,Salem2015-md,Kaniarasu2013-ho} an others), questions like `how trustworthy do you feel the system is?', or `to what extent do you find the system to be interpretable?'. These kinds of questions can be useful in verifying whether the assurances are having the expected effect. It is not unreasonable to imagine that an AIA might be equipped with a method by which it can ask the user questions about their trust, process those responses, and modify assurances appropriately.
    
    However, evidence is presented in \cite{Dzindolet2003-ts} that illustrates that sometimes changes is self-reported trust do not result in changes in TRBs. From the AIAs perspective this means that --- unless the object of the assurances is to make the person's level of self-reported trust change --- the assurances are not providing any benefit. As previously discussed, the goal of assurances is to elicit appropriate TRBs from the human user. From this perspective, measuring changes in TRBs is the more objective approach to measure the effect of assurances.

    Generally researchers in the field have measured, in some way, how frequently the AIA was able to run in autonomous mode before being turned off \cite{Freedy2007-sg,Desai2012-rc}. This metric seems very reasonable, and seems to be a promising approach with some extensions. Other researchers calculated whether the user was willing to cooperate with the AIA or not \cite{Salem2015-md,Wu2016-ei,Bainbridge2011-pl}. Perhaps a better defined metric would be the likelihood of appropriate use of a certain capability by the user, albeit more difficult to formally define/calculate in different situations. As a concrete example, in the UGV road-network example there isn't really an option to `turn off' the UGV. Instead the remote operator can make decisions such as accepting a plan designed by the UGV. In this situation the effect of assurances might be measured by how likely the operator is to accept a generated plan instead of overriding it (recall that the goal may not be to have the generated plan accepted 100\% of the time, rather that it be accepted with respect to how appropriate it is in a given situation).

    Self-reports are likely the most useful when trying to understand the true effects of an assurance. Does a certain assurance, assumed to affect `situational normality', actually do that? There is space for quite a lot of research in this realm. Does displaying a specific plot actually convey  information about `predictability'? This information can be used to inform the selection of the methods of assurance.

    In practical application (such as in the UGV road-network problem), the user, and the human-AIA team, care more about whether TRBs are appropriate or not. It doesn't help if an assurance helps the user feel that the AIA is more competent, if the user doesn't treat the AIA any differently than before the assurance. This assumes that it is possible for appropriate TRBs be measured in the first place. For example, if appropriate behavior is for the user to verify a sensor reading, can the AIA perceive that? In that situation perhaps the easiest approach would be to ask the user, but what if the user lied? Is there a way to verify the TRB is actually appropriate? This is something that has gained notoriety with autonomous cars, where the car can drive itself, but the user still needs to sit in the driver's seat and be attentive just in case the vehicle cannot perform correctly. We claim that it is as important to design methods of perceiving appropriate (and inappropriate) TRBs as it is to design assurances. 
%
% \subsection{The Imprecise Nature of Assurances} \label{sec:imprecise_nature}
    % Due to the nature of trust (and humans in general), a single assurance might be targeted at influencing the competence dimension of trust, but it may also have effects on other dimensions. As an example an assurance that targets predictability may also have an affect on the probability of depending.
%
    % Besides being difficult to separate effects on a single user, individual users are different as well. Thus no assurance will have an identical effect when given to two separate users. This makes it difficult to have precise effects on user trust behaviors.
%
    % One might attempt to mitigate this uncertainty by using expressions that are more precise than others, such as displaying a probability distribution rather than on a maximum likelihood. This gets into some considerations about how the presentation of information affects the ability of a human to understand.
