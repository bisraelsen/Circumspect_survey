\subsection{User Assessment} \label{sec:user_assessment}
In this section we address assurances that are based solely on user assessment; in other words the AIA expends little or no computational effort to `digest data' to turn into assurance information for the user, and instead relies the user's own cognitive abilities to draw assurances from their observations. Such assurances are clearly not integral to the function of the AIA, as they might, for example, be realized by incorporating simple displays, print statements, or other `raw data' indicators into a basic user interface. 
This category is in contrast to Section~\ref{sec:vis_dr}, where the AIA is designed to process data to assist the user in understanding how to trust the AIA appropriately. This approach is enticing for many system designers given how easy it is to implement at any stage of AIA design (even as an after-thought). However the effectiveness of this approach rests on several, strong, assumptions:

\begin{itemize}
    \item The user can form a `good enough' mental model of the AIA on their own to inform appropriate TRBs;
    \item Different users have `similar enough' capabilities and experiences to draw appropriate inferences on their own;
    \item There are no other compelling sources of information that will confound the assurance;
    \item Common cognitive biases won't interfere with the long-term operation/supervision of the system (e.g. recency, framing or anchoring effects that skew user's perception of non-linear changes in performance variables like power/fuel consumption). 
\end{itemize}

The weight of each of these assumptions relies heavily on the task to be performed, and the characteristics of the typical users. For example, in situations with highly trained personnel (i.e. military, or manufacturing facility) all users will have similar level of capability; thus `user assessment' is a viable and effective solution. 
In other scenarios with more diverse users and operating environments these assumptions begin to break down (i.e. mass market consumer products).

\subsubsection{Common Approaches:}
As suggested in the section's name users can form assurances by any method of perception. The most commonly investigated approaches are: simple, visual, `display of raw data'; and `by inspection' performance-based assessment. 

\paragraph{Display of Raw Data:}
Assurances associated with displaying AIA performance variables sound banal (e.g. flow rate for an automated pump \cite{Muir1996-gt}), but they actually make use of a nuanced point: the displayed performance value actually serves to inform the user's own mental model of the trustworthiness of an AIA capability. That is, the user's trust in the AIA's capability does not change only in response to the instantaneous `goodness/badness' of the AIA's performance, but accounts for the past history of the AIA's performance as well as any observed discrepancies between the AIA's expected behavior and its actual behavior. The user's trust dimensions (`competence', 'predictability', etc) are then affected by their perception of trustworthiness according to the combined model and data delivered by the display. This approach (also noted and discussed by \cite{Wickens1999-la,Sheridan1984-kx,Hutchins2015-if}) is effective, but relies heavily on the implicit assumption that the user will create a `good enough statistical model' of the AIA's behavior from data presented by the AIA. With this in mind, one might train a user to recognize signs of failure/success in different interactions with an AIA as assurances \cite{Freedy2007-sg,Desai2012-rc,Salem2015-md}. The main drawback of this idea is that it still relies on users' ability to construct `good enough' mental models of AIA behavior and characteristics from noisy observations to avoid misinterpreting AIA behaviors. It can also require intensive and costly special effort for non-expert or non-specialist users. A more ideal approach in such cases would be to design explicit assurances that help users construct correct/consistent mental process models of AIA behavior and thus reduce the risk of misinterpretation.

\paragraph{Performance-based:}
Users can also be assured by directly assessing the performance of an AIA on their own without any additional aiding or prompting.  Put simply: \emph{making stuff that (obviously) doesn't break improves trust}. \citet{Riley1996-qm} investigated how reliability and workload affected the participant's likelihood of trusting in automation. Two simulated environments were created to this end. First was for participants to use/not use an automated aid (with variable reliability) to classify characters while also performing a distraction task. Interestingly, they found that pilots (those with extensive experience working with automated systems) had a bias to use more automation, but reacted similarly to students in the face of dynamic reliability changes.

In a similar vein \citet{Desai2012-rc} investigated the effects of robot reliability on the trust of human operators. In this case, a human participant needed to work with an autonomous robot to search for victims in a building, while avoiding obstacles. The operator had the ability to switch the robot from manual (teleoperated) mode, to semi-autonomous, or autonomous mode depending on how they thought they could trust the system to perform. During this experiment the reliability of the robot was changed in order to observe the effects on the operator's reliance to the robot. Trust was measured by the amount of time the robot spent in different levels of autonomy (i.e. manual vs. autonomous), and it was found that trust changed based on the levels of reliability of the robot. \citet{Yu2018-qw} also had similar findings in their study of operators utilizing an `automatic quality monitor'.

\subsubsection{Grounding Example:}
In the case of the `VIP Escort' problem (described in Section~\ref{sec:mot_example}), user assessment might be used as an assurance in the following way, starting with the assumptions that:

\begin{itemize}
    \item The UGV has just begun an attempt to escape the road-network
    \item The user can observe the location of the UGV on the road network
    \item The user has access to the speedometer of the UGV
    \item The user has been trained and understands how the UGV functions
\end{itemize}

As the user monitors the UGVs progress they notice that, on a particular stretch of road, the speedometer reading seems very high, and the UGV stops moving. They recall from training that in situations where the speedometer shows a high speed and the UGV isn't moving it is likely that the UGV is spinning out or high-centered. They are able to diagnose the failure and dispatch the appropriate assistance.

\paragraph{\textbf{Discussion of Example:}} In this case the user was able to diagnose a problem based on the UGV not moving and the speedometer being high. They were able to do so because they were familiar with the system and were trained to be able to recognize this kind of situation. In future interaction the user \emph{might} associate the failure to certain characteristics of the road, or other properties of the task\ldots \emph{or} just feel like the UGV isn't very competent.
