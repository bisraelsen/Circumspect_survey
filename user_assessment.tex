%%nra TODO: not much, just some very minor polishing, can pretty much leave this section as is, reads pretty well.

\subsection{User Assessment} \label{sec:user_assessment}
In this section we address assurances that are based solely on user assessment; in other words the AIA expends little or no `computational effort', and instead relies the human's cognitive abilities to draw assurances from their observations. These  assurances are not integral to the function of the AIA. This category is in contrast to Section~\ref{sec:vis_dr} in which the AIA is designed to process data to assist the user in understanding how to trust it appropriately. This approach is pretty enticing given how easy it is to implement in AIAs at any stage of design (even as an after-thought). However the effectiveness rests on several, strong, assumptions:

\begin{itemize}
    \item The user is capable of forming a `good enough' model to inform appropriate TRBs
    \item Different users have `similar enough' capabilities
    \item There are no other compelling sources of information that will confound the assurance
    \item Common cognitive biases won't interfere with the long-term operation of the system
\end{itemize}

The weight of each of these assumptions relies heavily on the task to be performed, and the characteristics of the typical users. For example, in situations with highly trained personnel (i.e. military, or manufacturing facility) all users will have similar level of capability; thus `user assessment' is a viable and effective solution. In other scenarios with more diverse users and operating environments these assumptions being to break down.

\subsubsection{Common Approaches:}
As suggested in the section's name users can form assurances by any method of perception. The most commonly investigated approach is by simple, visual, `display of raw data', and `performance based' assessment. Other forms of human perception are not well investigated in this context, but work in perception of businesses and products is probably very applicable.

\paragraph{Display of Raw Data:}
Assurances associated with displaying AIA performance variables sound banal (e.g. flow rate for an automated pump \cite{Muir1996-gt}), but actually involves a nuanced point: the displayed performance value actually serves to inform the user's own mental model of the trustworthiness of an AIA capability. That is, the user's trust in the AIA's capability does not change only in response to the instantaneous `goodness/badness' of the AIA's performance, but accounts for the past history of the AIA's performance as well as any observed discrepancies between the AIA's expected behavior and its actual behavior. The user's trust dimensions (`competence', 'predictability', etc) are then affected by their perception of trustworthiness according to the combined model and data delivered by the display. This approach (also noted and discussed by \cite{Wickens1999-la,Sheridan1984-kx,Hutchins2015-if}) is effective, but relies heavily on the implicit assumption that the user will create a `good enough statistical model' of the AIA's behavior from data presented by the AIA. With this in mind, one might train a user to recognize signs of failure/success in different interactions with an AIA as assurances \cite{Freedy2007-sg,Desai2012-rc,Salem2015-md}. The main drawback of this idea is that it still relies on users' ability to construct `good enough' mental models of AIA behavior and characteristics from noisy observations to avoid misinterpreting AIA behaviors. However, this training can require intensive and costly special effort for non-expert or non-specialist users. A more ideal approach in such cases would be to design explicit assurances that help users construct correct/consistent mental process models of AIA behavior and thus reduce the risk of misinterpretation.

\paragraph{Performance based:}
Users can also be assured by assessing the performance of an AIA.  Put simply: \emph{making stuff that doesn't break improves trust}. \citet{Riley1996-qm} investigated how reliability and workload affected the participant's likelihood of trusting in automation. Two simulated environments were created to this end. First was for participants to use/not use an automated aid (with variable reliability) to classify characters while also performing a distraction task. Interestingly, they found that pilots (those with extensive experience working with automated systems) had a bias to use more automation, but reacted similarly to students in the face of dynamic reliability changes.

In a similar vein \citet{Desai2012-rc} investigated the effects of robot reliability on the trust of human operators. In this case a human participant needed to work with an autonomous robot to search for victims in a building, while avoiding obstacles. The operator had the ability to switch the robot from manual (teleoperated) mode, to semi-autonomous, or autonomous mode depending on how they thought they could trust the system to perform. During this experiment the reliability of the robot was changed in order to observe the effects on the operator's reliance to the robot. Trust was measured by the amount of time the robot spent in different levels of autonomy (i.e. manual vs. autonomous), and it was found that trust changed based on the levels of reliability of the robot. \citet{Yu2018-qw} also had similar findings in their study of operators utilizing an `automatic quality monitor'.

\subsubsection{Grounding Example:}
In the case of the `VIP Escort' problem (described in Section~\ref{sec:mot_example}), user assessment might be used as an assurance in the following way:

We make the following assumptions

\begin{itemize}
    \item The UGV has just begun an attempt to escape the road-network
    \item The user can observe the location of the UGV on the road network
    \item The user has access to the speedometer of the UGV
    \item The user has been trained and understands how the UGV functions
\end{itemize}

As the user monitors the UGVs progress they notice that, on a particular stretch of road, the speedometer reading seems very high, and the UGV stops moving. They recall from training that in situations where the speedometer shows a high speed and the UGV isn't moving it is likely that the UGV is spinning out or high-centered. They are able to diagnose the failure and dispatch the appropriate assistance.

\paragraph{\textbf{Discussion of Example:}} In this case the user was able to diagnose a problem based on the UGV not moving and the speedometer being high. They were able to do so because they were familiar with the system and were trained to be able to recognize this kind of situation.
