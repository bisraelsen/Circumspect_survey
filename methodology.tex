\section{Methodology} \label{sec:methodology}
    This survey examines research of those who are formally and informally addressing the idea of human-AIA trust. In particular, attention is devoted to ideas that are applicable to the trust relationship between a single human user and a single AIA. While theoretically a two-way trust model could be considered (i.e. in which the AIA also has trust in the user), attention is restricted here to a one-way trust relationship that considers only how user trust evolves in response to assurances from the AIA. 

    It should be noted that it is almost impossible to perform a fully comprehensive survey of all AIA assurances, due to the broad spectrum of possible assurances, and AIAs in general. One could rightly argue that control engineers treat metrics like gain and phase margins as assurances for automatic feedback control systems, in much the same way that  machine learning practitioners treat training and test accuracy as assurances for learning algorithms -- and hence concepts related to robustness, stability, etc. for feedback control systems ought also be included in this survey. While assurances can be used in both the most simple `automatic' systems (like a thermostat), this survey will focus on assurances in more advanced AIAs that make decisions under uncertainty; however, the admittedly narrow scope of this survey does not impede the development of fundamental insights and principles in designing assurances.

    Initially, in order to find applicable research, papers that formally addressed trust, and tried to create models of it were investigated; this with the aim of trying to understand how trust might be influenced. Secondly, we researched some historical literature regarding trust between humans and some form of non-human entity (typically technological in nature). This mainly led to fields like e-commerce, automation, and human-robot interaction. Third, we looked at work regarding `interpretable', `comprehensible', `transparent', `explainable', and other similar types of learning and modeling methods. Finally, with that literature as a background, we searched for research disciplines investigating computational methods that would be useful as assurances, but in which trust itself is not the main focus.

    This information is then used to construct an informed definition and classification of assurances based off of empirical information of methods that are currently in use, or being investigated. In doing so several ideas for future research are identified.
