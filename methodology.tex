\section{Methodology} \label{sec:methodology}
    This survey examines research of those who are formally and informally addressing the idea of human-AIA trust. In particular, attention is devoted to ideas that are applicable to the trust relationship between a single human user and a single AIA. While theoretically a two-way trust model could be considered (i.e. in which the AIA also has trust in the user), attention is restricted here to a one-way trust relationship that considers only how user trust evolves in response to assurances from the AIA. 
    %that is that the autonomy has perfect trust towards a user -- NRA: not necessary to actually bring this up or make this assumption; basically it's not relevant to the scope of what you are surveying.

    It should be noted that it is almost impossible to perform a fully comprehensive survey of all AIA assurances, due to the broad spectrum of possible assurances, and AIAs in general. One could rightly argue that control engineers treat metrics like gain and phase margins as assurances for automatic feedback control systems, in much the same way that  machine learning practitioners treat training and test accuracy as assurances for learning algorithms -- and hence concepts related to robustness, stability, etc. for feedback control systems ought also be included in this survey. While assurances can be used in both the most simple `automatic' systems (like a thermostat), this survey will focus on assurances in more advanced AIAs that make decisions under uncertainty; however, the admittedly narrow scope of this survey does not impede the development of fundamental insights and principles in designing assurances.
%However, it is my opinion that the somewhat narrow view of the surveyed literature here does not significantly hinder definition or classification of assurances. 

    Initially, in order to find applicable research, papers that formally addressed trust, and tried to create models of it were investigated; this with the aim of trying to understand how trust might be influenced. Secondly, we researched some historical literature regarding trust between humans and some form of non-human entity (typically technological in nature). This mainly lead to fields like e-commerce, automation, and human-robot interaction. Third, we looked at work regarding `interpretable', `comprehensible', `transparent', `explainable', and other similar types of learning and modeling methods. Finally, with that reading as a background, we searched for research disciplines that are investigating methods that would be useful as assurances, but in which trust is not the main focus.

    This information is then used to construct an informed definition and classification of assurances based off of empirical information of methods that are currently in use, or being investigated. In doing so several ideas for future research are identified.
