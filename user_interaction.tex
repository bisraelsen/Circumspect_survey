\subsection{User Interaction} \label{sec:user_interaction}
Despite the oft-repeated sentiment that advanced AIAs will `soon' be able to operate with little or no human involvement, those who have more practical experience with AIAs are much more skeptical of this claim, and point out that it is highly unrealistic to expect AIAs to ever function `perfectly out of the box' with true total autonomy \cite{Bradshaw2013-ck}. 
A popular and promising avenue for surmounting the inevitable shortcomings of AIAs, and thus engendering trust in users, has therefore been to put the users `in-the-loop' (or `on-the-loop') as collaborative partners who can augment (or supervise) AIA capabilities. 
In formulating algorithms for AIA capabilities that leverage user inputs, the user becomes analogous to a supervisor working alongside those they supervise; in doing so they are able to provide useful feedback in real-time, lend their expertise, and better appreciate the decisions and outcomes of the team's work. Such collaborative problem solving not only gives users a chance to directly assess AIA competence and predictability through experience (assurances), but also provides a way for users to continuously engage AIAs in accordance with their actual capabilities (appropriate TRBs). 
Note that user interaction techniques are \emph{not} the same as user assessment techniques discussed later, since user assessment techniques do not involve fundamentally changing AIA algorithms or capabilities to exploit user interaction.


\subsubsection{Common Approaches:} 
Users can be exploited to provide or augment any of the AIA capabilities in Fig. \ref{fig:AIcapabilities} on many different levels. 
At one extreme, a user might fully replace or augment a subset of core AIA capabilities, e.g. to act as a high-level `sensor' and planner for an autonomous robot in a navigation task \cite{Kaupp2008-yr}.  
On the other extreme, the human might have a very weak involvement in the core perception functionality of an AIA, e.g. to validate the labeling of image data. 
Since the literature in this area is quite vast and ongoing research quite active, we focus here, for the sake of brevity, only on a few typical methods from the human-robot interaction literature where the AIA (an autonomous robot) engages the user as an additional `sensor'/perception agent or `controller'/planning agent. The references cited in these works also point to a host of other related and relevant techniques, which in turn can (and have) been adapted to other AIA capabilities such as learning, reasoning, knowledge representation, etc.

\citet{Sweet2016-dw} investigate the use of  humans as `soft' sensors for target localization tasks, whereby semantic natural language observations (`Target is by the bridge', `Nothing in the street') can be directly combined with conventional `hard' robot sensor data (from cameras, lidar, sonar, etc.) in order to improve and augment the robot's Bayesian state estimation algorithms. 
They apply their approach in a scenario called `Cops and Robots' where a single `cop' robot tries to locate mobile `robber' robots in a semantically rich indoor environment. 
In this case the human acts as a `deputy' that remotely interacts with the system. The human can see security camera footage of the building in which the cop is searching, and can offer natural language feedback to the cop robot when appropriate. If the human offers information, it can be fused into the cop robot's estimation model, but in the meantime the cop robot operates autonomously to plan its motion without human assistance. 
Along similar lines, \citet{Kaupp2008-yr} empirically identify the appropriate level of autonomy for a robotic navigation system while taking into account the amount of sensory interaction required by a human supervisor. In this case the robot has sensors of its own, but can also ask for user input when the value of information (VOI) is high enough (i.e. is it worth asking a human sensor for information given that there is a cost?); they define the threshold VOI by performing human trials before deployment of the system in order to optimize the involvement of the human user.

\citet{Tellex2014-uc} consider planning algorithms that are augmented by human natural language commands for an autonomous assembly robot that can detect when it has failures (conditions that don't match expectations based on internal models). When this occurs the robot requests help from the human user to resolve the problem. In this way the human and robot are dependent on each other to accomplish a task. Since the user knows that, if needed, the robot will ask for help, they can more appropriately trust that unknown problems won't occur without them being informed.
\citet{Freedy2007-sg} studied performance measures for  mixed-initiative human-autonomous robot teams (where users and robots share planning and decision authority), and examined the extent to which such teams can only be successful if ``humans know how to appropriately trust and hence appropriately rely on the automation''. They explore this idea by using a tactical reconnaissance scenario where human participants supervised an unmanned ground vehicle (UGV)  platoon with three levels of autonomous targeting/firing capability (low, medium, high); these levels were dependent on the experimental conditions. The operator needed to monitor the UGV in case it couldn't perform as desired; in such cases the operator could intervene to resolve the problem. Operators were trained to recognize signs of task failure, and to only intervene if they thought the mission completion time would suffer. 

\subsubsection{Grounding Example:}
In the case of the `VIP Escort' problem (described in Section~\ref{sec:mot_example}), user interaction might be used as an assurance in the following way, starting with the assumptions that:

\begin{itemize}
    \item The UGV has just begun an attempt to escape the road-network
    \item An interface system exists by which the operator can receive and provide information to the UGV
\end{itemize}

The UGV is capable of operating autonomously, but also can benefit by asking for assistance or information when necessary, e.g. using a natural language interface for augmented planning and sensor fusion. In this way the functionality of the UGV can be greatly improved via interaction with the user. As the user interfaces with the UGV and is able to provide feedback and information about the best known location of the pursuer based on information unavailable to the UGV they have more trust in the competence, predictability, and situational normality of the UGV.

\paragraph{\textbf{Discussion of Example:}} In this scenario the user is more immersed in the functioning of the UGV. Not only are they able to respond to queries from the UGV, but they can also provide direct observations as well. Subsequently, the user feels more immersed in the functioning of the UGV and is more cognizant of appropriate TRBs.
