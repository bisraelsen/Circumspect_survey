%%(i) assurance argument: what specifically is the assurance signal/rationale? --  Recall 7 deadly myths of autonomous systems (applicable more generally to AIAs):  Autonomous systems are not actually all that autonomous, and are rarely, if ever, perfectly functional `out of the box', and so most AIAs at some point will have to have some affordances for required/optional human inputs to assist with problem solving or resolve other difficulties; these affordances for human inputs also naturally provide opportunities for generating assurances -- as mentioned in value alignment section, it is difficult to capture all aspects of decision making and reasoning under uncertainty for complex tasks, and inevitably situations may arise which require some more direct forms of human input pertaining to actual task, whether as intervention or assistance or validation/clarification, etc. 
%%(ii) what is mechanism for generating appropriate TRBs? -- by having humans in the loop/on the loop, humans have better sense of what AIA knows, understands, and is/is not capable of, i.e. working alongside AIA allows user to construct a more detailed theory of mind for competency, predictability, and situation normality. This also provides the user with opportunity to gather additional pieces of evidence that are not explicitly signaled/communicated and build their own interpretation of how the AIA operates -- this is a double-edged sword, since user can either correctly deduce principles that govern AIA behavior, or incorrectly ascribe explanations of AIA behavior on the basis of limited/incomplete information and experience...also, user inputs run the risk of throwing AIA off track, either because of malinent or mistakes...depending on type of interface/interaction available, this can also undercut autonomy of AIA if user hacks/games the interface to obtain desired responses, e.g. GNC 2011 paper -- the act of doing this constitutes an abuse of autonomous system and could also open the door to misuse, or even disuse if interaction does not seem natural or sensible or helpful to user ... addition of suitable UIs can also increase the cost of developing and validating AIA, though this arguably points to need for considering user interaction during AIA system design, rather than after the fact...
%%(iii) how can designers build/exploit for AIA assurances, i.e. what techniques available for user interaction along these lines?: humans can fulfill a few different roles to augment the capabilities outlined in Fig 2. AIA capabilities; most of these come from standpoint of collaborative reasoning, learning, control/planning, and sensing/perception -- within these, user can be involved at very high level in terms of providing supervisory or abstracted inputs, e.g. HTNs/Playbook delegation, or can get into finer details...includes online IRL/imitation learning, as well as interfaces for semantic natural language, gesturing, sketching, speech recog, etc.  --  most often geared towards providing natural or user-friendly interfaces...
%%(a) ...  
%%(b) ...
%%(c) ...
\subsection{User Interaction} \label{sec:user_interaction}
Despite the oft-repeated sentiment that advanced AIAs will `soon' be able to operate with little or no human involvement, those with more practical experience with AIAs are much more skeptical and point out that it is highly unrealistic to expect AIAs to ever function `perfectly out of the box' with true total autonomy \cite{Bradshaw2013-ck}. 
A popular and promising avenue for surmounting the inevitable shortcomings of AIAs, and thus engendering trust in users, has therefore been to put the users `in-the-loop' (or `on-the-loop') as collaborative partners who can augment (or supervise) AIA capabilities. 
%%This has been, for instance, the modus operandi for industrial automation. 
In formulating algorithms for AIA capabilities that leverage user inputs, the user becomes analogous to a supervisor working `in the trenches' with those they supervise; in doing so they are able to provide useful feedback in real-time, lend their expertise, and better appreciate the decisions and outcomes of the team's work. Such collaborative problem solving not only gives users a chance to directly assess AIA competence and predictability (assurances), but also provides a way for users to continuously engage AIAs in accordance with their actual capabilities (appropriate TRBs). 

%%Humans playing a significant role in the functionality of an AIA is analogous to a supervisor working `in the trenches' with those they supervise; in doing so they are able to provide feedback in real-time, lend their expertise, and better appreciate the decisions and outcomes of the team's work.

\subsubsection{Common Approaches:} 
%This includes work from disciplines such as human-robot collaboration, human-computer interaction, cooperative control, cooperative sensing, and many others.  
%%\nisarcomm{what are the main ideas to cover beyond the individual papers? first talk about what *could* be done in terms of human roles (humans as planners/controllers, sensors/perception augments, tutors/trainers, etc.) -- then discuss how each approach contributes to acting as an assurance for an AIA, using refs as specific examples -- otherwise a laundry list of papers doesn't really help organize or convey any part of your argument here }\brettcomm{In what ways can users interact? make a list. Humans as sensors, humans as controllers,\ldots} \brettcomm{In what ways can users interact? make a list. Humans as sensors, humans as controllers,\ldots}
%In general, users and AIAs can collaborate on many different levels. 
Users can be exploited to provide or augment any of the AIA capabilities in Fig. \ref{fig:AIcapabilities} on many different levels. 
At one extreme, a user might fully replace or augment a subset of core AIA capabilities, e.g. to act as a high-level `sensor' and planner for an autonomous robot in a navigation task \cite{Kaupp2008-yr}.  
On the other extreme, the human might have a very weak involvement in the core perception functionality of an AIA, e.g. to validate the labeling of image data. %There are bodies of literature that address humans as sensors, and humans as controllers of AIAs. 
%
%There is also research in which humans and robots \emph{share} responsibility for different functions.
Since the literature in this area is quite vast, we focus here on describing only on a few typical methods from human-robot interaction literature where the AIA (robot) engages the user as an additional `sensor'/perception agent or `controller'/planning agent, for the sake of brevity. 

Enabling mobile robots and human to share or `fuse' information can have an effect on trust. \citet{Sweet2016-dw} investigate how to enable using humans as `soft' sensors, and then fuse that information into that of the `hard' robot sensors in order to improve and augment the robot's Bayesian state estimation capabilities. They apply their approach in a scenario called `Cops and Robots' where a single `cop' robot tries to locate mobile `robber' robots. In this case the human acts as a deputy that remotely interacts with the system. The human can see security camera footage of the building in which the cop is searching, and can offer natural language feedback to the cop robot when appropriate. If the human offers information, it can be fused into the cop robot's estimation model, but in the meantime the cop robot operates autonomously to plan its motion without human assistance. 
%%Similarly \citet{Tse2015-tz} consider a framework for robots and humans to share and fuse information in a cooperative context. \nisarcomm{so...? there's a lot more to be said here...also belongs in info visualization??}

In order to be able to design a system that can be useful to a human operator, \citet{Kaupp2008-yr,Kaupp2005-pk} empirically identify the appropriate level of automation for a system while taking into account the amount of interaction required by a human operator. In this case the robot has sensors of its own, but can also ask for user input when the value of information (VOI) is high enough (i.e. is it worth asking a human for information given that there is a cost?); they define the threshold VOI by human trials before deployment of the system in order to optimize the involvement of the human user.

\citet{Tellex2014-uc} consider an autonomous assembly robot that can detect when it has failures (conditions that don't match expectations based on internal models). When this occurs the robot requests help from the human user to resolve the problem. In this way the human and robot are dependent on each other to accomplish a task. Since the user knows that, if needed, the robot will ask for help they can more appropriately trust that unknown problems won't occur without them being informed.

\citet{Freedy2007-sg} studied how mixed-initiative human-AIA teams might have their performance measured, and examined the extent to which such teams can only be successful if ``humans know how to appropriately trust and hence appropriately rely on the automation''. They explore this idea by using a tactical reconnaissance scenario where human participants supervised an unmanned ground vehicle (UGV)  platoon with three levels of autonomous targeting/firing capability (low, medium, high); these levels were dependent on the experimental conditions. The operator needed to monitor the UGV in case it couldn't perform as desired; in such cases the operator could intervene to resolve the problem. Operators were trained to recognize signs of task failure, and to only intervene if they thought the mission completion time would suffer.

\subsubsection{Grounding Example:}
In the case of the `VIP Escort' problem (described in Section~\ref{sec:mot_example}), operator interaction might be used as an assurance in the following way:

We make the following assumptions

\begin{itemize}
    \item The UGV has just begun an attempt to escape the road-network
    \item An interface system exists by which the operator can receive and provide information to the UGV
\end{itemize}

The UGV is capable of operating autonomously, but also has the ability to ask for assistance or information when necessary. In this way the functionality of the UGV can be greatly improved via interaction with the user. As the user interfaces with the UGV and is able to provide feedback and information about the best known location of the pursuer based on information unavailable to the UGV they have more trust in the competence, predictability, and situational normality of the UGV.

\paragraph{\textbf{Discussion of Example:}} In this scenario the user is more immersed in the functioning of the UGV. Not only are they able to respond to queries from the UGV, but they can also provide direct observations as well. Subsequently, the user feels more immersed in the functioning of the UGV and is more cognizant of appropriate TRBs.
