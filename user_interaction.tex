\subsection{User Interaction} \label{sec:user_interaction}
To date one of the more common approaches to engender trust in users has been to put the users `in-the-loop'. This has been, and still is, modus operandi in the automation industry and others. While some think that more advanced AIAs will `soon' be able to operate with little human involvement, in general those who have more practical experience with AIAs are more reserved.

In this section we review some methods by which engineers have made AIA assurances by making the performance of the system highly dependent on the user's participation. This includes work from disciplines such as human-robot collaboration, cooperative control, cooperative sensing, and others. Humans playing a significant role in the functionality of an AIA is analogous to a supervisor working `in the trenches' with those they supervise; in doing so they are able to provide feedback in real-time, lend their expertise, and better appreciate the decisions and outcomes of the team's work.

\subsubsection{Common Approaches:}
\nisarcomm{what are the main ideas to cover beyond the individual papers? first talk about what *could* be done in terms of human roles (humans as planners/controllers, sensors/perception augments, tutors/trainers, etc.) -- then discuss how each approach contributes to acting as an assurance for an AIA, using refs as specific examples -- otherwise a laundry list of papers doesn't really help organize or convey any part of your argument here }\brettcomm{In what ways can users interact? make a list. Humans as sensors, humans as controllers,\ldots} \brettcomm{In what ways can users interact? make a list. Humans as sensors, humans as controllers,\ldots}

In general, a human and AIA can interact on many different levels. At the extreme of the most fundamental extremes, the human might fully replace a system or sub-system of the AIA. For example, the human might act as the sensors for an AIA. On the other extreme, the human might have a very weak involvement in the core functionality of the AIA. There are bodies of literature that address humans as sensors, and humans as controllers of AIAs. There is also research in which humans and robots \emph{share} responsibility for different functions.

Enabling the robot and human to share or `fuse' information can have an effect on trust. \citet{Sweet2016-dw} investigate how to enable using humans as `soft' sensors, and then fuse that information into that of the `hard' robot sensors in order to improve and augment the robot's Bayesian state estimation capabilities. They apply their approach in a scenario called `cops and robots' where a single `cop' robot tries to locate `robber' robots. In this case the human acts as a deputy that remotely interacts with the system. The human can see security camera footage of the building in which the cop is searching, and can offer natural language feedback to the cop robot when appropriate. If the human offers information it can be fused into the cop robot's estimation model, but in the meantime the cop robot operates autonomously without assistance. Similarly, \citet{Tse2015-tz} consider a framework for robots and humans to share and fuse information in a cooperative context.
Similarly \citet{Tse2015-tz} consider a framework for robots and humans to share and fuse information in a cooperative context. \nisarcomm{so...? there's a lot more to be said here...}

In order to be able to design a system that can be useful to a human operator, \citet{Kaupp2008-yr,Kaupp2005-pk} empirically identify the appropriate level of automation for a system while taking into account the amount of interaction required by a human operator. In this case the robot has sensors of its own, but can also ask for user input when the value of information (VOI) is high enough (i.e. is it worth asking a human for information given that there is a cost?); they define the threshold VOI by human trials before deployment of the system in order to optimize the involvement of the human user.

\citet{Tellex2014-uc} consider an autonomous assembly robot that can detect when it has failures (conditions that don't match expectations based on internal models). When this occurs the robot requests help from the human user to resolve the problem. In this way the human and robot are dependent on each other to accomplish a task. Since the user knows that, if needed, the robot will ask for help they can more appropriately trust that unknown problems won't occur without them being informed.

\citet{Freedy2007-sg} studied how mixed-initiative human-AIA teams might have their performance measured, and examined the extent to which such teams can only be successful if ``humans know how to appropriately trust and hence appropriately rely on the automation''. They explore this idea by using a tactical reconnaissance scenario where human participants supervised an unmanned ground vehicle (UGV)  platoon with three levels of autonomous targeting/firing capability (low, medium, high); these levels were dependent on the experimental conditions. The operator needed to monitor the UGV in case it couldn't perform as desired; in such cases the operator could intervene to resolve the problem. Operators were trained to recognize signs of task failure, and to only intervene if they thought the mission completion time would suffer.

\subsubsection{Grounding Example:}
In the case of the `VIP Escort' problem (described in Section~\ref{sec:mot_example}), operator interaction might be used as an assurance in the following way:

We make the following assumptions

\begin{itemize}
    \item The UGV has just begun an attempt to escape the road-network
    \item An interface system exists by which the operator can receive and provide information to the UGV
\end{itemize}

The UGV is capable of operating autonomously, but also has the ability to ask for assistance or information when necessary. In this way the functionality of the UGV can be greatly improved via interaction with the user. As the user interfaces with the UGV and is able to provide feedback and information about the best known location of the pursuer based on information unavailable to the UGV they have more trust in the competence, predictability, and situational normality of the UGV.

\paragraph{\textbf{Discussion of Example:}} In this scenario the user is more immersed in the functioning of the UGV. Not only are they able to respond to queries from the UGV, but they can also provide direct observations as well. Subsequently, the user feels more immersed in the functioning of the UGV and is more cognizant of appropriate TRBs.
