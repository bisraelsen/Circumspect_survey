%trbs.tex
%\nisarcomm{trim, merge, move earlier?....}

%Researchers of all disciplines widely accept that 
Trust ultimately leads to some kind of meaningful behavior or action which reflects the level of trust \cite{Lewis1985-pr}. 
%; this idea was highlighted by \citet{Lewis1985-pr}.  
These are called `trust-related behaviors' (TRBs) \cite{McKnight2001-fa}. %, which is the term that will be used in this survey. 
In the case of a human-AIA relationship per Fig. \ref{fig:SimpleTrust_one_way}, %and \ref{fig:RoadNet}, 
Some example TRBs could include the kinds of tasks the human user assigns to the AIA, accepting and following through on a plan produced by the AIA, or directing that a new plan be made, or switching off autonomous capabilities altogether to teleoperate and perform tasks manually through a physical mechanism that the AIA otherwise controls.  %the vehicle. 

%\subsubsection{Calibration of Trust-Related Behaviors}
    
    Trust is not a univariate quantity that can be objectively measured. Rather, it is a multidimensional phenomenon whose `relative magnitudes and directions' must be observed through changes in TRBs, or qualitative self-reports reported in surveys \cite{Muir1996-gt}. It thus comes as no surprise that TRBs are the more objective measure due to the fact that people are not always consistent in their ratings, and may sincerely feel different levels of trust while performing similar TRBs. \citet{Parasuraman1997-co} were interested in understanding the use of automation by humans, and defined terms to describe that use. Here it is proposed that, by extension, those terms also apply to the behaviors of humans towards more advanced AIAs. Within this scope the definitions are as follows: \textit{Misuse:} over-reliance on an AIA (which could manifest itself in a user's unrealistically optimistic expectations of performance); \textit{Disuse:} under-utilization of an AIA (e.g. a user turning off the AIA, or failing to use all of its capabilities); \textit{Abuse:} Inappropriate application of an AIA (where \emph{application} in this case means the choice to deploy an AIA in a certain context).

    \nisarcomm{can trim down this parag a bit...}
    Following Fig.~\ref{fig:SimpleTrust_one_way}, an AIA's assurances are ideally designed to steer the user away from misuse, disuse, or abuse of the AIA, i.e. towards otherwise appropriate TRBs. This can only be done by properly `calibrating' assurances to suitably influence user trust. This is a point that, to some extent, has been informally mentioned in \cite{Muir1994-ow,Lillard2016-yg,Lee2004-pv,Hutchins2015-if}. Note that other researchers who propose `calibration' (or other similar concepts) often suggest calibrating \emph{trust} as opposed to TRBs. \citet{Dzindolet2003-ts} found that providing system performance feedback tended to increase user's \textit{self-reported trust}, even though user's resulting TRBs did not reflect self-reported trust levels. This shows the danger of calibrating `trust', as opposed to calibrating the TRBs. 
    TRB calibration focuses on concrete and measurable behaviors that are universally applicable. 
    In contrast, trust calibration involves influencing a quantity that is directly immeasurable, and that, when measured indirectly, is subject to individual human differences and biases. 