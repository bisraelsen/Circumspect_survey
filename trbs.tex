\subsection{Trust-Related Behaviors} \label{sec:trbs}
Something that is well accepted among researchers of all disciplines is that trust ultimately leads to some kind of behavior or action; this idea was highlighted by \citet{Lewis1985-pr}.  \citet{McKnight2001-fa} call these `trust-related behaviors` (TRBs), which is the term that will be used in this survey. In the case of a human-AIA relationship that the author is concerned with, TRBs could include the kinds of tasks the human user assigns to the UGV such as accepting and following through on its plan, or directing that a new plan be made.

\subsubsection{Calibration of Trust-Related Behaviors}
    Trust is not a quantity that can be objectively measured. Rather, its relative magnitude must be observed through changes in TRBs, or qualitative self-reports reported in surveys \cite{Muir1996-gt}. It comes as no surprise that TRBs are the more objective measure due to the fact that people are not always consistent in their ratings, and may sincerely feel different levels of trust while performing similar TRBs. \citet{Parasuraman1997-co} were interested in understanding the use of automation by humans, and defined terms to describe that use. Here it is proposed that, by extension, those terms also apply to the behaviors of humans towards more advanced AIAs. Within this scope the definitions are as follows:
    
    \begin{description}
        \item [Misuse:] The over-reliance on an AIA (which could manifest itself in expecting too much accuracy from and AIA)
        \item [Disuse:] The under-utilization of and AIA (which could be manifest in a user turning off the AIA, or failing to use all of its capabilities)
        \item [Abuse:] Inappropriate application of automation (where \emph{application} in this case means the choice to deploy an AIA in a certain context, such as the choice to use a quad-copter underwater).
    \end{description}

    Recall the diagram in Figure \ref{fig:SimpleTrust_one_way}; the AIA has influence on the user's TRBs by way of assurances. We propose that the AIA's goal should be that the user should not misuse, disuse, or abuse it. Consider a space of all TRBs towards an AIA, this space would include misuse, disuse, abuse, and appropriate use (all TRBs not in misuse, disuse, or abuse). In order to ensure that humans use AIAs appropriately, it is critical that the user TRBs be calibrated to elicit behaviors that are within the set of appropriate behaviors. This can only be done by influencing the user trust. This is a point that, to some extent, has been informally mentioned in \citet{Muir1994-ow,Muir1987-mk,Lillard2016-yg,Lee2004-pv,Hutchins2015-if}.

    A critical oversight of other researchers who mention `calibration' (or other synonymous concepts) is that they suggest calibrating \emph{trust} as opposed to TRBs. \citet{Dzindolet2003-ts} studied the effect of performance feedback on user's self-reported trust, and found that it increased; however the appropriate TRBs toward the system did not reflect the level of self-reported trust. This shows the danger of calibrating `trust', as opposed to calibrating the TRBs.

    Calibrating TRBs focuses on concrete and measurable behaviors that are universally applicable. In contrast, calibrating trust involves influencing a quantity that is directly immeasurable, and that, when measured indirectly, is subject to the biases and uncertainties of humans, along with inherent differences between different users. Viewing the task from this point of view, the findings of \citeauthor{Dzindolet2003-ts} are not surprising.

    It is desirable for AIAs to be designed in order to encourage appropriate TRBs, as opposed to the alternative of purposefully misleading users misuse or abuse. There is a valid argument that many of today's AIAs that ignore (or whose designers ignored) TRBs and assurances can be `unwittingly malicious' in that they do not actively attempt to guide user's TRBs to lay within the space of appropriate TRBs.
