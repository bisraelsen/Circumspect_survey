Trust ultimately leads to some kind of meaningful behavior or action which reflects the level of an individual's trust \cite{Lewis1985-pr}. 
These actions are called `trust-related behaviors' (TRBs) \cite{McKnight2001-fa}. 
In the case of a human-AIA relationship per Fig. \ref{fig:SimpleTrust_one_way}, 
some example TRBs could include the kinds of tasks the human user assigns to the AIA, accepting and following through on a plan produced by the AIA, directing that a new plan be made, or switching off autonomous capabilities altogether to teleoperate and perform tasks manually through a physical mechanism that the AIA otherwise controls.  

Trust is not a univariate quantity that can be objectively measured. Rather, it is a multidimensional phenomenon whose `relative magnitudes and directions' must be observed through changes in TRBs, or qualitative self-reports gathered via surveys \cite{Muir1996-gt}. It thus comes as no surprise that TRBs are the more objective method of observation due to the fact that people are not always consistent in their ratings, and may sincerely feel different levels of trust while performing similar TRBs~\cite{Dzindolet2003-ts}. \citet{Parasuraman1997-co} were interested in understanding the use of automation by humans, and defined terms to describe that use. Here it is proposed that, by extension, those terms also apply to the behaviors of humans towards more advanced AIAs. Within this scope the definitions are as follows: \textit{Misuse:} over-reliance on an AIA (which could manifest itself in a user's unrealistically optimistic expectations of performance); \textit{Disuse:} under-utilization of an AIA (e.g. a user turning off the AIA prematurely, or failing to use all of its capabilities); \textit{Abuse:} Inappropriate application of an AIA (where \emph{application} in this case means the choice to deploy an AIA in a certain context).

Following Fig.~\ref{fig:SimpleTrust_one_way}, AIA assurances should ideally be designed to steer the user away from misuse, disuse, or abuse of the AIA, i.e. towards otherwise appropriate TRBs, by properly `calibrating' assurances to suitably influence user trust. This point, to some extent, has been alluded to in \cite{Muir1994-ow,Lillard2015-yg,Lee2004-pv,Hutchins2015-if}. Other researchers who propose `calibration' (or related concepts) suggest calibrating \emph{trust} as opposed to TRBs. \citet{Dzindolet2003-ts} found that providing system performance feedback tended to increase users' \textit{self-reported trust}, even though resulting TRBs did not reflect self-reported trust levels. This highlights the danger of calibrating `trust', as opposed to calibrating the TRBs. Whereas TRB calibration focuses on concrete and measurable behaviors, trust calibration involves influencing something that is directly immeasurable and subject to individual biases when indirect measurements are attempted.
