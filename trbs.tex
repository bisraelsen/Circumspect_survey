\subsection{Trust-Related Behaviors} \label{sec:trbs}
Researchers of all disciplines widely accept that trust ultimately leads to some kind of behavior or action; this idea was highlighted by \citet{Lewis1985-pr}.  \citet{McKnight2001-fa} call these `trust-related behaviors` (TRBs), which is the term that will be used in this survey. In the case of a human-AIA relationship per Figs. \ref{fig:SimpleTrust_one_way} and \ref{fig:RoadNet}, TRBs could include the kinds of tasks the human user assigns to the UGV, such as accepting and following through on its plan, or directing that a new plan be made, or switching off autonomous capabilities altogether to teleoperate the vehicle.

\subsubsection{Calibration of Trust-Related Behaviors}
    Trust is not a univariate quantity that can be objectively measured. Rather, it is a multidimensional phenomenon whose `relative magnitudes and directions' must be observed through changes in TRBs, or qualitative self-reports reported in surveys \cite{Muir1996-gt}. It comes as no surprise that TRBs are the more objective measure due to the fact that people are not always consistent in their ratings, and may sincerely feel different levels of trust while performing similar TRBs. \citet{Parasuraman1997-co} were interested in understanding the use of automation by humans, and defined terms to describe that use. Here it is proposed that, by extension, those terms also apply to the behaviors of humans towards more advanced AIAs. Within this scope the definitions are as follows:
    
    \begin{description}
        \item [Misuse:] The over-reliance on an AIA (which could manifest itself in a user's unrealistically optimistic expectations of AIA performance in a given situation)
        \item [Disuse:] The under-utilization of and AIA (which could be manifest in a user turning off the AIA, or failing to use all of its capabilities)
        \item [Abuse:] Inappropriate application of automation (where \emph{application} in this case means the choice to deploy an AIA in a certain context, e.g. using a quad-copter underwater).
    \end{description}

Recall the diagram in Fig.~\ref{fig:SimpleTrust_one_way}; the AIA has influence on the user's TRBs by way of assurances. We propose that the AIA's assurances should be designed to steer the user away from misuse, disuse, or abuse of the AIA. Considering the space of all TRBs towards an AIA, this space would include misuse, disuse, abuse, and appropriate use (all TRBs not in misuse, disuse, or abuse). To ensure that humans use AIAs appropriately, it is critical that the user TRBs be calibrated to elicit behaviors that are within the set of appropriate behaviors. This can only be done by influencing the user trust. This is a point that, to some extent, has been informally mentioned in \cite{Muir1994-ow,Muir1987-mk,Lillard2016-yg,Lee2004-pv,Hutchins2015-if}.

    A critical oversight of other researchers who mention `calibration' (or other synonymous concepts) is that they suggest calibrating \emph{trust} as opposed to TRBs. \citet{Dzindolet2003-ts} studied the effect of performance feedback on user's \textit{self-reported trust}, and found that it increased; however, the resulting TRBs toward the system did not reflect the level of self-reported trust. This shows the danger of calibrating `trust', as opposed to calibrating the TRBs. TRB calibration focuses on concrete and measurable behaviors that are universally applicable. In contrast, trust calibration involves influencing a quantity that is directly immeasurable, and that, when measured indirectly, is subject to the biases and uncertainties of humans, along with inherent differences between different users. From this view, the findings of \citeauthor{Dzindolet2003-ts} are not surprising.

It is desirable for AIAs to be designed in order to encourage appropriate TRBs, as opposed to the alternative of purposefully misleading users to misuse, disuse, or abuse. One may even go so far as to argue that many of today's AIAs that ignore (or whose designers ignored) TRBs and assurances can be `unwittingly malicious', since they do not actively attempt to guide user's TRBs to lay within the space of appropriate TRBs.
